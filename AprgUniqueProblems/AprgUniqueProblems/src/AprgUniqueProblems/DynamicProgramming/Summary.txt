
Dynamic programming is a technique that combines the correctness of complete search and the efficiency of greedy algorithms. 
Dynamic programming can be applied if the problem can be divided into overlapping subproblems that can be solved independently.
There are two uses for dynamic programming:
-> Finding an optimal solution: We want to find a solution that is as large as possible or as small as possible.
-> Counting the number of solutions: We want to calculate the total number of possible solutions.

We will first see how dynamic programming can be used to find an optimal solution, and then we will use the same idea for counting the solutions.
Understanding dynamic programming is a milestone in every competitive programmer’s career. 
While the basic idea is simple, the challenge is how to apply dynamic programming to different problems. 

The dynamic programming algorithm is based on a recursive function that goes through all possibilities how to form the sum, like a brute force algorithm. 
However, the dynamic programming algorithm is efficient because it uses memoization and calculates the answer to each subproblem only once.

The idea of dynamic programming is to use memoization to efficiently calculate values of a recursive function. 
This means that the values of the function are stored in an array after calculating them. 
For each parameter, the value of the function is calculated recursively only once, and after this, the value can be directly retrieved from the array.



Dynamic Programming is an algorithmic paradigm that solves a given complex problem by breaking it into subproblems and stores the results of subproblems to avoid computing the same results again. 
Following are the two main properties of a problem that suggests that the given problem can be solved using Dynamic programming.
1) Overlapping Subproblems 
2) Optimal Substructure

1) Overlapping Subproblems: 
Like Divide and Conquer, Dynamic Programming combines solutions to sub-problems. 
Dynamic Programming is mainly used when solutions of same subproblems are needed again and again. 
In dynamic programming, computed solutions to subproblems are stored in a table so that these don’t have to be recomputed. 
So Dynamic Programming is not useful when there are no common (overlapping) subproblems because there is no point storing the solutions if they are not needed again. 
For example, Binary Search doesn’t have common subproblems. 
If we take an example of following recursive program for Fibonacci Numbers, there are many subproblems which are solved again and again.

2) Optimal Substructure: A given problems has Optimal Substructure Property if optimal solution of the given problem can be obtained by using optimal solutions of its subproblems. 
For example, the Shortest Path problem has following optimal substructure property: 
If a node x lies in the shortest path from a source node u to destination node v then the shortest path from u to v is combination of shortest path from u to x and shortest path from x to v. 
The standard All Pair Shortest Path algorithm like Floyd–Warshall and Single Source Shortest path algorithm for negative weight edges like Bellman–Ford are typical examples of Dynamic Programming. 

On the other hand, the Longest Path problem doesn’t have the Optimal Substructure property.
Here by Longest Path we mean longest simple path (path without cycle) between two nodes. 
Consider the following unweighted graph given in the CLRS book. 
There are two longest paths from q to t: q→r→t and q→s→t. 
Unlike shortest paths, these longest paths do not have the optimal substructure property. 
For example, the longest path q→r→t is not a combination of longest path from q to r and longest path from r to t, 
because the longest path from q to r is q→s→t→r and the longest path from r to t is r→q→s→t. 
 


Tabulation vs Memoization

There are following two different ways to store the values so that the values of a sub-problem can be reused. 
Here, will discuss two patterns of solving DP problem: 
1) Tabulation: Bottom Up
2) Memoization: Top Down
 

Tabulation Method – Bottom Up Dynamic Programming 

As the name itself suggests starting from the bottom and cumulating answers to the top. Let’s discuss in terms of state transition. 
Let’s describe a state for our DP problem to be dp[x] with dp[0] as base state and dp[n] as our destination state. 
So, we need to find the value of destination state i.e dp[n]. 
If we start our transition from our base state i.e dp[0] and follow our state transition relation to reach our destination state dp[n], 
we call it Bottom Up approach as it is quite clear that we started our transition from the bottom base state and reached the top most desired state. 

Now, Why do we call it tabulation method? 
To know this let’s first write some code to calculate the factorial of a number using bottom up approach. 
Once, again as our general procedure to solve a DP we first define a state. 
In this case, we define a state as dp[x], where dp[x] is to find the factorial of x. 

Now, it is quite obvious that dp[x+1] = dp[x] * (x+1) 
// Tabulated version to find factorial x.
int dp[MAXN];
// base case
int dp[0] = 1;
for (int i = 1; i< =n; i++)
{
    dp[i] = dp[i-1] * i;
}

The above code clearly follows the bottom-up approach as it starts its transition from the bottom-most base case dp[0] and reaches its destination state dp[n]. 
Here, we may notice that the dp table is being populated sequentially and we are directly accessing the calculated states from the table itself and hence, we call it tabulation method. 

 
Memoization Method – Top Down Dynamic Programming 

Once, again let’s describe it in terms of state transition. 
If we need to find the value for some state say dp[n] and instead of starting from the base state that i.e dp[0] 
we ask our answer from the states that can reach the destination state dp[n] following the state transition relation, then it is the top-down fashion of DP. 

Here, we start our journey from the top most destination state and compute its answer by taking in count the values of states that can reach the destination state, till we reach the bottom most base state. 
Once again, let’s write the code for the factorial problem in the top-down fashion 
 
// Memoized version to find factorial x.
// To speed up we store the values of calculated states initialized to -1
int dp[MAXN]
// return fact x!
int solve(int x)
{
    if(x==0)
        return 1;
    if(dp[x]!=-1)
        return dp[x];
    return (dp[x] = x * solve(x-1));
}

As we can see we are storing the most recent cache up to a limit so that if next time we got a call from the same state we simply return it from the memory. 
So, this is why we call it memoization as we are storing the most recent state values. 

In this case the memory layout is linear that’s why it may seem that the memory is being filled in a sequential manner like the tabulation method, 
but you may consider any other top down DP having 2D memory layout like Min Cost Path, here the memory is not filled in a sequential manner. 

Summary:
                   | Tabulation                                                  | Memoization
------------------------------------------------------------------------------------------------------------------------------------------------------------
State              | State transition relation is difficult to think             | State transition relation is easy to think
------------------------------------------------------------------------------------------------------------------------------------------------------------
Code               | Code gets complicated when a lot of conditions are required | Code is easy and less complicated
------------------------------------------------------------------------------------------------------------------------------------------------------------
Speed              | Fast, as we directly access previous states from the table  | Slow due to a lot of recursive calls and return statements
------------------------------------------------------------------------------------------------------------------------------------------------------------
Subproblem solving | If all subproblems must be solved at least once,            | If some sub problems in the subproblem space need not be solved at all, 
approach           | a bottom-up dynamic programming algorithm usually out       | the memoized solution has the advantage of solving only 
                   | performs a top-down memoized algorithm by a constant factor | those subproblems that are definitely required
------------------------------------------------------------------------------------------------------------------------------------------------------------
Table entries      | In tabulated version, starting from the first entry,        | Unlike the tabulated version, all entries of the lookup table are not 
Table entries      | all entries are filled one by one                           | necessarily filled in Memoized version. The table is filled on demand.
------------------------------------------------------------------------------------------------------------------------------------------------------------



