-> How to solve problems or design algorithms when you're stuck?

---> Start with a simple/small case.
-----> Try to formulate a simple solution. 
-------> If it doesn't work. Why? Check if you can modify the algorithm.
-----> It also corrects your assumptions(formulas, graphs, process of the program) if its works or not.

---> Ask (yourself) what is target time complexity and space complexity?
-----> Sometimes this can clarify your thinking.

---> Try brute force or a naive solution
-----> Check on how you can make it better?

---> Identity the problem if this is something you can formulate on your own. Or just try using an existing algorithm.

---> What can you do currently? Are there options on you can do limited?
-----> Focus on what can you do and try to fit it in the current problem.

---> Can you reuse something you know?
-----> There is a good chance that you know the solution, you just havent used it

---> If sorting is needed, check if a hashtable should suffice instead.

---> Think of an approach:
-----> Check for patterns
-------> Check for the answer for one case, continuously check the answers for other cases incrementally and look for patterns.
-------> Track running sum, or running delta, think if this can be used in the problem.
-------> Check the output of simple cases, do you see some pattern?
-----> Can you draw it graphically to see how the problem should behave?
-------> This would give you another insight on the problem?
-----> Can it me modeled in graph?
-----> Can you write an equation for it?
-----> Can logic statements help? (can quine mckluskey help?)
-----> Can reducing it to bits or bit manipulation help? (this includes representing items to bits)
-----> Does a loop help?
-------> Does a loop with a stack/queue/priority_queue help?
---------> How about multiple stack/queue/priority_queue (Largest multiple of 3 given its digits)?
-------> In subset problems, a good approach might be to have the elements as the first level of the loop so it wouldnt be processed again (0-1 knapsack)
-----> Does tracking multiple indices work? (two pointers approach, multiple tracking of specific prime factors)
-----> Does a hashmap or array/indexed based map help?
-----> Can an index/cell represent something to be maintained (like in maximum subarray sum or range sum query)?
-------> Some hard problems are a combinations of this(subarray with elements(closure), largest bitonic sequence, maximum submatrix sum(kadane per partial sums in a row/column))
-----> Does doing reverse (std::reverse) or rotate (std::rotate) helps?
-----> Can you check if you can solve the problem incrementally (dynamic programming)?
-----> Does recursion help?
-----> Can you check if there some symmetry and you can formulate that into your solution?
-----> Can you do it on approximate solution instead of finding the perfect solution?
-----> Does introducing randomness help?
-----> Does sweepline approach work?
-------> Sweepline occurs more often than you think!
-----> Does Top Down Approach work? Or Bottom Up Approach?
-----> Does Eager Approach work? Or Lazy Approach?
-----> Does a table-based approach work? (in some cases, table values can help reduce the computational complexity)
-----> Does "hole"-ing up a fill work? (trace points in a circle problem)? 
-------> Instead of building the problem bit by bit, build the whole thing and remove the unneeded parts.
-----> Does using states help (similar to KMP)?
-------> Use DFA or NFA similar to KMP, to cover state-like behavior.
-----> Does backtracking approach work?
-------> Can you solve it incrementally?
-------> How would you do the backtrack action? How would you reset to the previous state (via recursion or some data structure)?
-------> Is it similar to "Generating Subsets/Permutations", "Knight's Tour", "Rat in a Maze", "N Queen Problem", "Sudoku"?
-----> Is it reducible to another problem?
-------> Reduction makes the problem easier because you can compartmentalize on how to solve it.
-----> Lastly, step back and think about the overall input and output of the problem (maybe you missed some simple fact about the problem).

---> Think about improvements:
-----> Parallelism
-----> Caching
-----> Are there any "free" or "unused" stuffs (memory or iteration)? Can you use if or something (optimized your code for example)?

---> Change your perspective:
-----> Can you change the variable your iterating on? (like an event driven approach)
-----> Work backwards
-------> Try having the solution and think on how you can come up with the problem.
-----> Think of special cases
-------> Think of initial cases on how they should be processed
-------> Think of end cases on how they should be processed (like on binary search two indices)

---> Think of any exisiting data structures that can help
-----> vectors, trees, linked lists, sets, maps (especially hash map), priority queues, queues(Sliding window minimum), stack(smaller element that precedes the element), 
-----> disjoint set (union-find)
-----> k-d tree
-----> range queries (accumulator or selector, BinaryIndexedTree/FenwickTree, Segment Trees)
-------> try index compression and range updates
-----> tries (also ternary trie)
-----> suffix array
-------> z-array
-----> Van Emde Boas tree, fibonacci heap (to impress someone)
-----> Ultimately, think about combining this data structures to solve a problem.

---> Think of any algorithms that can help
-----> Don't reinvent the wheel. Today, majority of the known problems have an efficient algorithm for it.

---> Analyze what kind of problem it is
-----> Try Divide And Conquer
-------> Can you split the problem into multiple pieces?
---------> "The collected water problem" also falls into this category.
-----> Try to "Meet in the middle" 
-------> This is a technique where the search space is divided into two parts of about equal size. 
---------> A separate search is performed for both of the parts, and finally the results of the searches are combined.
-------> The technique can be used if there is an efficient way to combine the results of the searches. 
---------> In such a situation, the two searches may require less time than one large search. 
---------> Typically, we can turn a factor of 2n into a factor of 2n/2 using the meet in the middle technique.
-----> Try Dynamic Programming
-------> Can you solve it incrementally?
-------> Does it have overlapping subproblems and substructure?
-----> Try Greedy Algorithm
-------> Can you reduce the problem so that you can choose the best sub-solution just based on the sub-problem alone?
-----> Try Search and enumeration
-------> Can you do a search and reduce it to graph-like problem?
-----> Try Graphing Algorithm
-------> Is it a graphing problem? Can you use(DFS, BFS, Vertex Ordering(topological sort))
-----> Try Backtracking
-------> Can you find solutions by searching step by step (or level by level) and backtracking if the solution does not work?
-------> Can you solve it incrementally?
-------> How would you do the backtrack action? How would you reset to the previous state (via recursion or some data structure)?
-----> Try branch and bound
-------> Can you stop search when the found solution is worse than the current best solution or approximate solution?
-----> Try Minimax
-------> Is setting the maximum and minimum for each branch search help on identifying which branches to pursue?
-----> Try a Randomized algorithms
-------> Monte Carlo algorithms return a correct answer with high-probability.
-------> Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.
-----> Try Linear Programming
-------> Is it optimization problem involving linear equations? Can you use simplex?
-----> Try Heuristics?
-------> Is finding the solution impractical? Can you do an approach that will work majority of the time but its not proven as the best solution?




-> What is an Algorithm?
---> The word Algorithm means "A set of rules to be followed in calculations or other problem-solving operations" 
-----> Or "A procedure for solving a mathematical problem in a finite number of steps that frequently by recursive operations". 
-----> Therefore, Algorithm refers to a sequence of finite steps to solve a particular problem.
---> An algorithm is a finite sequence of rigorous instructions, typically used to solve a class of specific problems or to perform a computation.
-----> Algorithms are used as specifications for performing calculations and data processing. 
-----> By making use of artificial intelligence, algorithms can perform automated deductions (referred to as automated reasoning) 
-----> and use mathematical and logical tests to divert the code execution through various routes (referred to as automated decision-making). 
-----> Using human characteristics as descriptors of machines in metaphorical ways was already practiced by Alan Turing with terms such as "memory", "search" and "stimulus".
---> In contrast, a heuristic is an approach to problem solving that may not be fully specified or may not guarantee correct or optimal results, 
-----> especially in problem domains where there is no well-defined correct or optimal result.
---> As an effective method, an algorithm can be expressed within a finite amount of space and time,
-----> and in a well-defined formal language for calculating a function.
-----> Starting from an initial state and initial input (perhaps empty), 
-----> the instructions describe a computation that, when executed, proceeds through a finite[8] number of well-defined successive states, 
-----> eventually producing "output" and terminating at a final ending state. 
-----> The transition from one state to the next is not necessarily deterministic; some algorithms, known as randomized algorithms, incorporate random input.

-> Algorithmic analysis

---> It is frequently important to know how much of a particular resource (such as time or storage) is theoretically required for a given algorithm. 
-----> Methods have been developed for the analysis of algorithms to obtain such quantitative answers (estimates); 
-----> for example, an algorithm which adds up the elements of a list of n numbers would have a time requirement of O(n), using big O notation. 
-----> At all times the algorithm only needs to remember two values: the sum of all the elements so far, and its current position in the input list. 
-----> Therefore, it is said to have a space requirement of O(1), if the space required to store the input numbers is not counted, or O(n) if it is counted.

---> Different algorithms may complete the same task with a different set of instructions in less or more time, space, or 'effort' than others. 
-----> For example, a binary search algorithm (with cost O(log n)) outperforms a sequential search (cost O(n)) when used for table lookups on sorted lists or arrays.

---> Formal versus empirical
-----> Empirical algorithmics, Profiling (computer programming), and Program optimization
-----> The analysis, and study of algorithms is a discipline of computer science, and is often practiced abstractly without the use of a specific programming language or implementation. 
-------> In this sense, algorithm analysis resembles other mathematical disciplines in that it focuses on the underlying properties of the algorithm and not on the specifics of any particular implementation. 
-------> Usually pseudocode is used for analysis as it is the simplest and most general representation. 
-----> However, ultimately, most algorithms are usually implemented on particular hardware/software platforms and their algorithmic efficiency is eventually put to the test using real code. 
-------> For the solution of a "one off" problem, the efficiency of a particular algorithm may not have significant consequences (unless n is extremely large) 
-------> but for algorithms designed for fast interactive, commercial or long life scientific usage it may be critical. 
-------> Scaling from small n to large n frequently exposes inefficient algorithms that are otherwise benign.
-----> Empirical testing is useful because it may uncover unexpected interactions that affect performance. 
-------> Benchmarks may be used to compare before/after potential improvements to an algorithm after program optimization. 
-------> Empirical tests cannot replace formal analysis, though, and are not trivial to perform in a fair manner.

---> Execution efficiency
-----> To illustrate the potential improvements possible even in well-established algorithms, a recent significant innovation, 
-------> relating to FFT algorithms (used heavily in the field of image processing), can decrease processing time up to 1,000 times for applications like medical imaging. 
-------> In general, speed improvements depend on special properties of the problem, which are very common in practical applications.
-------> Speedups of this magnitude enable computing devices that make extensive use of image processing (like digital cameras and medical equipment) to consume less power.

---> Cost models
-----> Time efficiency estimates depend on what we define to be a step. 
-------> For the analysis to correspond usefully to the actual run-time, the time required to perform a step must be guaranteed to be bounded above by a constant. 
-------> One must be careful here; for instance, some analyses count an addition of two numbers as one step. 
-------> This assumption may not be warranted in certain contexts.
------->  For example, if the numbers involved in a computation may be arbitrarily large, the time required by a single addition can no longer be assumed to be constant.
-----> Two cost models are generally used:
-------> the uniform cost model, also called uniform-cost measurement (and similar variations), assigns a constant cost to every machine operation, regardless of the size of the numbers involved
-------> the logarithmic cost model, also called logarithmic-cost measurement (and similar variations), assigns a cost to every machine operation proportional to the number of bits involved
-----> The latter is more cumbersome to use, so it's only employed when necessary, for example in the analysis of arbitrary-precision arithmetic algorithms, like those used in cryptography.
-----> A key point which is often overlooked is that published lower bounds for problems are often given for a model of computation that is more restricted than the set of operations that you could use in practice 
-------> and therefore there are algorithms that are faster than what would naively be thought possible.

---> Run-time analysis
-----> Run-time analysis is a theoretical classification that estimates and anticipates the increase in running time 
-------> (or run-time or execution time) of an algorithm as its input size (usually denoted as n) increases.
-------> Run-time efficiency is a topic of great interest in computer science: 
-------> A program can take seconds, hours, or even years to finish executing, depending on which algorithm it implements. 
-------> While software profiling techniques can be used to measure an algorithm's run-time in practice, 
-------> they cannot provide timing data for all infinitely many possible inputs; 
-------> the latter can only be achieved by the theoretical methods of run-time analysis.
-----> Shortcomings of empirical metrics
-------> Since algorithms are platform-independent (i.e. a given algorithm can be implemented in an arbitrary programming language on an arbitrary computer running an arbitrary operating system), 
---------> there are additional significant drawbacks to using an empirical approach to gauge the comparative performance of a given set of algorithms.
-------> Take as an example a program that looks up a specific entry in a sorted list of size n. Suppose this program were implemented on Computer A, 
---------> a state-of-the-art machine, using a linear search algorithm, and on Computer B, a much slower machine, using a binary search algorithm. 
---------> Benchmark testing on the two computers running their respective programs might look something like the following:
-----------------------------------------------------------------------------------------------
| n (list size)  | Computer A run-time (in nanoseconds) | Computer B run-time (in nanoseconds)
-----------------------------------------------------------------------------------------------
| 16             | 8                                    | 100,000
| 63             | 32                                   | 150,000
| 250            | 125                                  | 200,000
| 1,000          | 500                                  | 250,000
-----------------------------------------------------------------------------------------------
-------> Based on these metrics, it would be easy to jump to the conclusion that Computer A is running an algorithm that is far superior in efficiency to that of Computer B. 
---------> However, if the size of the input-list is increased to a sufficient number, that conclusion is dramatically demonstrated to be in error:
-----------------------------------------------------------------------------------------------
| n (list size)  | Computer A run-time (in nanoseconds) | Computer B run-time (in nanoseconds)
-----------------------------------------------------------------------------------------------
| 16             | 8                                    | 100,000
| 63             | 32                                   | 150,000
| 250            | 125                                  | 200,000
| 1,000          | 500                                  | 250,000
| ...            | ...                                  | ...
| 1,000,000      | 500,000                              | 500,000
| 4,000,000      | 2,000,000                            | 550,000
| 16,000,000     | 8,000,000                            | 600,000
| ...            | ...                                  | ...
| 63,072 × 10^12 | 31,536 × 1012 ns, or 1 year          | 1,375,000 ns, or 1.375 milliseconds
-----------------------------------------------------------------------------------------------
-------> Computer A, running the linear search program, exhibits a linear growth rate. 
---------> The program's run-time is directly proportional to its input size. 
---------> Doubling the input size doubles the run-time, quadrupling the input size quadruples the run-time, and so forth. 
---------> On the other hand, Computer B, running the binary search program, exhibits a logarithmic growth rate. 
---------> Quadrupling the input size only increases the run-time by a constant amount (in this example, 50,000 ns). 
---------> Even though Computer A is ostensibly a faster machine, 
-----------> Computer B will inevitably surpass Computer A in run-time because it's running an algorithm with a much slower growth rate.

---> Orders of growth
-----> Informally, an algorithm can be said to exhibit a growth rate on the order of a mathematical function if beyond a certain input size n, 
-------> the function f(n) times a positive constant provides an upper bound or limit for the run-time of that algorithm. 
-------> In other words, for a given input size n greater than some n0 and a constant c, the run-time of that algorithm will never be larger than c × f(n).
-------> This concept is frequently expressed using Big O notation. 
-------> For example, since the run-time of insertion sort grows quadratically as its input size increases, insertion sort can be said to be of order O(n^2).
-----> Big O notation is a convenient way to express the worst-case scenario for a given algorithm, 
-------> although it can also be used to express the average-case — 
-------> for example, the worst-case scenario for quicksort is O(n^2), but the average-case run-time is O(n log n).
-----> Empirical orders of growth
-------> Assuming the run-time follows power rule, t ≈ kna, the coefficient a can be found [8] by taking empirical measurements of run-time {t1, t2} at some problem-size points {n1, n2}, 
---------> and calculating t2/t1 = (n2/n1)a so that a = log(t2/t1)/log(n2/n1). 
---------> In other words, this measures the slope of the empirical line on the log–log plot of run-time vs. input size, at some size point. 
---------> If the order of growth indeed follows the power rule (and so the line on log–log plot is indeed a straight line), 
---------> the empirical value of will stay constant at different ranges, and if not, it will change (and the line is a curved line)—
---------> but still could serve for comparison of any two given algorithms as to their empirical local orders of growth behaviour. 
---------> Applied to the above table:
---------------------------------------------------------------------------------------------------------------------------------------------------------------
| n (list size)  | Computer A run-time (in nanoseconds) | Local order of growth (n^(?)) | Computer B run-time (in nanoseconds) | Local order of growth (n^(?))
---------------------------------------------------------------------------------------------------------------------------------------------------------------
| 15             | 7                                    |                               | 100,000                              |                               
| 65             | 32                                   | 1.04                          | 150,000                              | 0.28
| 250            | 125                                  | 1.01                          | 200,000                              | 0.21
| 1,000          | 500                                  | 1.00                          | 250,000                              | 0.16
| ...            | ...                                  | ...                           | ...                                  | ...
| 1,000,000      | 500,000                              | 1.00                          | 500,000                              | 0.10
| 4,000,000      | 2,000,000                            | 1.00                          | 550,000                              | 0.07
| 16,000,000     | 8,000,000                            | 1.00                          | 600,000                              | 0.06
| ...            | ...                                  | ...                           | ...                                  | ...
---------------------------------------------------------------------------------------------------------------------------------------------------------------
-------> It is clearly seen that the first algorithm exhibits a linear order of growth indeed following the power rule. 
---------> The empirical values for the second one are diminishing rapidly, suggesting it follows another rule of growth 
---------> and in any case has much lower local orders of growth (and improving further still), empirically, than the first one. 


-> Asymptotic Analysis

---> NOTE: The CLS book restricts the definitions of functions to be asymptotically non negative, other books just use limits on their definitions.

---> (1) Θ Notation (BIG THETA Notation): 
-----> This is for an ASYMPTOTIC TIGHT BOUND.
-----> Theta notation encloses the function from above and below ("sandwiched"). 
-----> This is used for analyzing the GENERAL-CASE (not neccessarily average) complexity of an algorithm. 
-----> Formal definition:
-------> Let g and f be the function from the set of natural numbers to itself. 
-------> The function f is said to be Θ(g), if THERE ARE constants c1,c2>0 and n0>0 such that c1*g(n) ≤ f(n) ≤ c2*g(n) for all n ≥ n0
-----> Mathematical Representation:
-------> Θ(g(n)) = {f(n): THERE EXIST positive constants c1, c2 and n0 such that 0 ≤ c1*g(n) ≤ f(n) ≤ c2*g(n) for all n ≥ n0}
-------> Note: Θ(g) is a set of functions

---> (2) O Notation (Big Oh Notation):
-----> This is for an ASYMPTOTIC UPPER BOUND.
-----> This represents the UPPER BOUND of the running time of an algorithm. 
-----> This is used for the WORST-CASE complexity of an algorithm.
-----> Formal definition:
-------> Let g and f be the function from the set of natural numbers to itself. 
-------> The function f is said to be O(g), if THERE ARE constants c>0 and n0>0 such that f(n) ≤ c*g(n) for all n ≥ n0
-----> Mathematical Representation:
-------> O(g(n)) = { f(n): THERE EXIST positive constants c and n0 such that 0 ≤ f(n) ≤ cg(n) for all n ≥ n0 }

---> (3) Ω Notation (Big Omega Notation):
-----> This is for an ASYMPTOTIC LOWER BOUND.
-----> This represents the LOWER BOUND of the running time of an algorithm. 
-----> This is used for the BEST-CASE complexity of an algorithm.
-----> Formal definition:
-------> Let g and f be the function from the set of natural numbers to itself. 
-------> The function f is said to be Ω(g), if THERE ARE constants c>0 and n0>0 such that c*g(n) ≤ f(n) for all n ≥ n0
-----> Mathematical Representation:
-------> Ω(g(n)) = { f(n): THERE EXIST positive constants c and n0 such that 0 ≤ cg(n) ≤ f(n) for all n ≥ n0 }
-------> Note: Θ(g) is a set of functions

---> (4) o Notation (Small Oh Notation):
-----> This is for an ASYMPTOTIC UPPER BOUND.
-----> It also means that its NOT ASYMTOTICALLY TIGHT.
-----> It also means that its "DOMINATED". (for example f(n) = o(g(n)): f is dominated by g asymptotically).
-----> This represents the UPPER BOUND of the running time of an algorithm. 
-----> This is used for the WORST-CASE complexity of an algorithm.
-----> Formal definition:
-------> Let g and f be the function from the set of natural numbers to itself. 
-------> The function f is said to be o(g), if ANY constant c>0, THERE EXISTS n0>0 such that f(n) < c*g(n) for all n ≥ n0
-----> Mathematical Representation:
-------> o(g(n)) = { f(n): FOR ANY positive constant c>0, THERE EXISTS a constant n0>0 such that 0 ≤ f(n) < cg(n) for all n ≥ n0 }

---> (3) ω Notation (Small Omega Notation):
-----> This is for an ASYMPTOTIC LOWER BOUND.
-----> It also means that its NOT ASYMTOTICALLY TIGHT.
-----> It also means that it "DOMINATES". (for example f(n) = ω(g(n)): f dominates g asymptotically).
-----> This represents the LOWER BOUND of the running time of an algorithm. 
-----> This is used for the BEST-CASE complexity of an algorithm.
-----> Formal definition:
-------> Let g and f be the function from the set of natural numbers to itself. 
-------> The function f is said to be ω(g), if ANY constant c>0, THERE EXISTS n0>0 such that c*g(n) < f(n) for all n ≥ n0
-----> Mathematical Representation:
-------> ω(g(n)) = { f(n): FOR ANY positive constant c>0, THERE EXISTS a constant n0>0 such that 0 ≤ c*g(n) < f(n) for all n ≥ n0 }

---> Family of Bachmann–Landau notations
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|  Notation        |  Name                        |  Description                                                       |  Formal definition                                      |  Limit definition                                   |
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
|  f(n) = o(g(n))  |  Small O; Small Oh           |  f is dominated by g asymptotically                                |  V k>0 E n0 V n>n0 : |f(n)| < k*g(n)                    |  lim n→∞ |f(n)| / g(n) = 0                          |
|  f(n) = O(g(n))  |  Big O; Big Oh; Big Omicron  |  |f| is bounded above by g (up to constant factor) asymptotically  |  E k>0 E n0 V n>n0 : |f(n)| ≤ k*g(n)                    |  lim sup n→∞ |f(n)| / g(n) < ∞                      |
|  f(n) = Θ(g(n))  |  Big Theta                   |  f is bounded both above and below by g asymptotically             |  E k1>0 E k2>0 E n0 V n>n0: k1*g(n) ≤ |f(n)| ≤ k2*g(n)  |  f(n) = O(g(n)) and f(n) = Ω(g(n)) (Knuth version)  |
|  f(n) ~ g(n)     |  On the order of             |  f is equal to g asymptotically                                    |  V ε>0 E n0 V n>n0 : |(f(n)/g(n))−1| < ε                |  lim n→∞ f(n) / g(n) = 1                            |
|  f(n) = Ω(g(n))  |  Big Omega (Knuth)           |  f is bounded below by g asymptotically                            |  E k>0 E n0 V n>n0 : |f(n)| ≥ k*g(n)                    |  lim inf n→∞ |f(n)| / g(n) > 0                      |
|  f(n) = ω(g(n))  |  Small Omega                 |  f dominates g asymptotically                                      |  V k>0 E n0 V n>n0 : |f(n)| > k*g(n)                    |  lim n→∞ |f(n)| / g(n) = ∞                          |
|  f(n) = Ω(g(n))  |  Big Omega (number theory)   |  |f| is not dominated by g asymptotically                          |  E k>0 V n0 E n>n0 : |f(n)| ≥ k*g(n)                    |  lim sup n→∞ |f(n)| / g(n) > 0                      |
----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
-----> Note: 'V' is acutally '∀' which means "any" or "for all"
-----> Note: 'E' is acutally '∃' which means "there exists" or "there is at least one" or "for some"
-----> The limit definitions assume g(n)>0 for sufficiently large n. 
-------> The table is (partly) sorted from smallest to largest, in the sense that o , O , Θ , ~ , (Knuth's version of) Ω , ω on functions 
-------> correspond to < , ≤ , ≈ , = ,  ≥ , >  on the real line (the Hardy-Littlewood version of Ω , however, doesn't correspond to any such description).
-----> Computer science uses the big O , big Theta Θ , little o , little omega ω and Knuth's big Omega Ω notations.
-------> Analytic number theory often uses the big O, small o, Hardy–Littlewood's big Omega Ω (with or without the +, − or ± subscripts) and ~ notations.
-------> The small omega ω notation is not used as often in analysis. 

---> Properties of Asymptotic Notations: 

-----> (1) General Properties:
-------> If f(n) is O(g(n)) then a*f(n) is also O(g(n)), where a is a constant.
-------> If f(n) is Θ(g(n)) then a*f(n) is also Θ(g(n)), where a is a constant. 
-------> If f(n) is Ω(g(n)) then a*f(n) is also Ω(g(n)), where a is a constant.
-------> If f(n) is o(g(n)) then a*f(n) is also o(g(n)), where a is a constant. 
-------> If f(n) is ω(g(n)) then a*f(n) is also ω(g(n)), where a is a constant.
-------> Example:
---------> f(n) = 2n²+5 is O(n²) 
---------> then, 7*f(n) = 7(2n²+5) = 14n²+35 is also O(n²).

-----> (2) Transitive Properties:
-------> If f(n) is O(g(n)) and g(n) is O(h(n)) then f(n) = O(h(n)).
-------> If f(n) is Θ(g(n)) and g(n) is Θ(h(n)) then f(n) = Θ(h(n)).
-------> If f(n) is Ω(g(n)) and g(n) is Ω(h(n)) then f(n) = Ω(h(n)).
-------> If f(n) is o(g(n)) and g(n) is o(h(n)) then f(n) = o(h(n)).
-------> If f(n) is ω(g(n)) and g(n) is ω(h(n)) then f(n) = ω(h(n)).
-------> Example:
---------> If f(n) = n, g(n) = n² and h(n)=n³
---------> n is O(n²) and n² is O(n³) then, n is O(n³)

-----> (3) Reflexive Properties: 
-------> If f(n) is given then f(n) is O(f(n)). 
-------> If f(n) is given then f(n) is Θ(f(n)).
-------> If f(n) is given then f(n) is Ω(f(n)).
-------> Example:
---------> f(n) = n² ; O(n²) i.e O(f(n))

-----> (4) Symmetric Properties: 
-------> f(n) is Θ(g(n)) if and only if g(n) is Θ(f(n)).
-------> This property is only satisfied on the Θ notation.
-------> Example:
---------> If f(n) = n² and g(n) = n²
---------> then, f(n) = Θ(n²) and g(n) = Θ(n²)

-----> (5) Transpose Symmetric Properties:
-------> If f(n) is O(g(n)) then g(n) is Ω(f(n)).
-------> If f(n) is o(g(n)) then g(n) is ω(f(n)).
-------> Example:
---------> If f(n) = n , g(n) = n²
---------> then n is O(n²) and n² is Ω(n) 

-------> (6) Some More Properties: 
---------> 1) If f(n) = O(g(n)) and f(n) = Ω(g(n)) then f(n) = Θ(g(n))
---------> 2) If f(n) = O(g(n)) and d(n) = O(e(n))
---------> then f(n) + d(n) = O(max(g(n), e(n))) 
---------> Example:
-----------> f(n) = n i.e O(n) 
-----------> d(n) = n² i.e O(n²) 
-----------> then f(n) + d(n) = n + n² i.e O(n²)
---------> 3) If f(n)=O(g(n)) and d(n)=O(e(n)) 
---------> then f(n) * d(n) = O( g(n) * e(n) )
---------> Example: 
-----------> f(n) = n i.e O(n) 
-----------> d(n) = n² i.e O(n²)
-----------> then f(n) * d(n) = n * n² = n³ i.e O(n³)
-----------> Note: If  f(n) = O(g(n)) then g(n) = Ω(f(n))

---> Matters of notation

-----> Equals sign
-------> The statement "f(x) is O(g(x))" as defined above is usually written as f(x) = O(g(x)). Some consider this to be an abuse of notation, 
---------> since the use of the equals sign could be misleading as it suggests a symmetry that this statement does not have. 
---------> As de Bruijn says, O(x) = O(x2) is true but O(x2) = O(x) is not.[9] Knuth describes such statements as "one-way equalities", since if the sides could be reversed, 
---------> "we could deduce ridiculous things like n = n2 from the identities n = O(n2) and n2 = O(n2)."
---------> In another letter, Knuth also pointed out that "the equality sign is not symmetric with respect to such notations", as, in this notation, 
---------> "mathematicians customarily use the = sign as they use the word "is" in English: Aristotle is a man, but a man isn't necessarily Aristotle".
-------> For these reasons, it would be more precise to use set notation and write f(x) ∈ O(g(x)) (read as: "f(x) is an element of O(g(x))", or "f(x) is in the set O(g(x))"), 
---------> thinking of O(g(x)) as the class of all functions h(x) such that |h(x)| ≤ C|g(x)| for some constant C.
---------> However, the use of the equals sign is customary.
-----> Other arithmetic operators
-------> Big O notation can also be used in conjunction with other arithmetic operators in more complicated equations. 
---------> For example, h(x) + O(f(x)) denotes the collection of functions having the growth of h(x) plus a part whose growth is limited to that of f(x). Thus,
-----------> g(x)=h(x)+O(f(x))
---------> expresses the same as
-----------> g(x)-h(x)=O(f(x))
-------> Example
---------> Suppose an algorithm is being developed to operate on a set of n elements. 
-----------> Its developers are interested in finding a function T(n) that will express how long the algorithm will take to run (in some arbitrary measurement of time) in terms of the number of elements in the input set. 
-----------> The algorithm works by first calling a subroutine to sort the elements in the set and then perform its own operations. 
-----------> The sort has a known time complexity of O(n2), and after the subroutine runs the algorithm must take an additional 55n3 + 2n + 10 steps before it terminates. 
-----------> Thus the overall time complexity of the algorithm can be expressed as T(n) = 55n3 + O(n2). Here the terms 2n + 10 are subsumed within the faster-growing O(n2). 
-----------> Again, this usage disregards some of the formal meaning of the "=" symbol, but it does allow one to use the big O notation as a kind of convenient placeholder.
-----> Multiple uses
-------> In more complicated usage, O(...) can appear in different places in an equation, even several times on each side. 
---------> For example, the following are true for n → ∞ :
-----------> (n+1)^2 = n^2 + O(n)
-----------> (n + O(n^(1/2))) * (n + O(log ⁡ n))^2 = n^3 + O(n^(5/2))
-----------> n^O(1) = O(e^n)
-------> The meaning of such statements is as follows: for any functions which satisfy each O(·) on the left side, 
---------> there are some functions satisfying each O(·) on the right side, 
---------> such that substituting all these functions into the equation makes the two sides equal. 
---------> For example, the third equation above means: "For any function f(n) = O(1), there is some function g(n) = O(en) such that nf(n) = g(n)." 
---------> In terms of the "set notation" above, the meaning is that the class of functions represented by the left side is a subset of the class of functions represented by the right side. 
---------> In this use the "=" is a formal symbol that unlike the usual use of "=" is not a symmetric relation. 
---------> Thus for example nO(1) = O(en) does not imply the false statement O(en) = nO(1).

---> Orders of common functions

-----> Here is a list of classes of functions that are commonly encountered when analyzing the running time of an algorithm. 
-------> In each case, c is a positive constant and n increases without bound. The slower-growing functions are generally listed first.
-----------------------------------------------------------------------------------------------
|  Notation                |  Name                                  |  Example
-----------------------------------------------------------------------------------------------
|  O(1)                    |  constant                              |  Determining if a binary number is even or odd; Calculating (-1)^n; Using a constant-size lookup table
|  O(log log n)            |  double logarithmic                    |  Average number of comparisons spent finding an item using interpolation search in a sorted array of uniformly distributed values
|  O(log ⁡ n)                |  logarithmic                           |  Finding an item in a sorted array with a binary search or a balanced search tree as well as all operations in a binomial heap
|  O((log n)^c) and c>1    |  polylogarithmic                       |  Matrix chain ordering can be solved in polylogarithmic time on a parallel random-access machine.
|  O(n^c) and 0 < c < 1    |  fractional power                      |  Searching in a kd tree
|  O(n)                    |  linear                                |  Finding an item in an unsorted list or in an unsorted array; adding two n-bit integers by ripple carry
|  O(n * log ^{*}n)}       |  n log-star n                          |  Performing triangulation of a simple polygon using Seidel's algorithm, or the union–find algorithm. 
|  O(n log n) = O(log ⁡ n!)  |  linearithmic, loglinear, quasilinear  |  Performing a fast Fourier transform; fastest possible comparison sort; heapsort and merge sort
|  O(n^2)                  |  quadratic                             |  Multiplying two n-digit numbers by schoolbook multiplication; simple sorting algorithms, such as bubble sort, selection sort and insertion sort;
|  O(n^c) and c>1          |  polynomial or algebraic               |  Tree-adjoining grammar parsing; maximum matching for bipartite graphs; finding the determinant with LU decomposition
|  Ln[α,c] and 0<α<1       |  L-notation or sub-exponential         |  Factoring a number using the quadratic sieve or number field sieve
|  O(c^n) and c > 1        |  exponential                           |  Finding the (exact) solution to the travelling salesman problem using dynamic programming; determining if two logical statements are equivalent using brute-force search
|  O(n!)                   |  factorial                             |  Solving the travelling salesman problem via brute-force search; finding the determinant with Laplace expansion; enumerating all partitions of a set
-----------------------------------------------------------------------------------------------
-----> Note that log*⁡ (n) = {0, if n ≤ 1, and 1+log*⁡ (log n) if n>1}
-----> The statement f(n)=O(n!) is sometimes weakened to f(n)=O(n^n) to derive simpler formulas for asymptotic complexity. 
-------> For any k>0 and c>0, O(n^c*(log ⁡ n)^k) is a subset of O(n^(c + ε)) for any ε > 0, so may be considered as a polynomial with some bigger order. 



-> Classification

---> There are various ways to classify algorithms, each with its own merits.


---> By implementation

-----> (1) Recursion
-------> A recursive algorithm is one that invokes (makes reference to) itself repeatedly
---------> until a certain condition (also known as termination condition) matches, which is a method common to functional programming. 
-------> Iterative algorithms use repetitive constructs like loops and sometimes additional data structures like stacks to solve the given problems. 
-------> Some problems are naturally suited for one implementation or the other. 
-------> For example, towers of Hanoi is well understood using recursive implementation. 
-------> Every recursive version has an equivalent (but possibly more or less complex) iterative version, and vice versa.

-----> (2) Logical
-------> An algorithm may be viewed as controlled logical deduction. 
-------> This notion may be expressed as: Algorithm = logic + control.[74] 
-------> The logic component expresses the axioms that may be used in the computation and the control component determines the way in which deduction is applied to the axioms. 
-------> This is the basis for the logic programming paradigm. 
-------> In pure logic programming languages, the control component is fixed and algorithms are specified by supplying only the logic component. 
-------> The appeal of this approach is the elegant semantics: a change in the axioms produces a well-defined change in the algorithm.
 
-----> (3) Serial, parallel or distributed
-------> Algorithms are usually discussed with the assumption that computers execute one instruction of an algorithm at a time. 
-------> Those computers are sometimes called serial computers. 
-------> An algorithm designed for such an environment is called a serial algorithm, as opposed to parallel algorithms or distributed algorithms. 
-------> Parallel algorithms take advantage of computer architectures where several processors canwork on a problem at the same time, 
-------> whereas distributed algorithms utilize multiple machines connected with a computer network. 
-------> Parallel or distributed algorithms divide the problem into more symmetrical or asymmetrical subproblems and collect the results back together. 
-------> The resource consumption in such algorithms is not only processor cycles on each processor but also the communication overhead between the processors. 
-------> Some sorting algorithms can be parallelized efficiently, but their communication overhead is expensive. Iterative algorithms are generally parallelizable. 
-------> Some problems have no parallel algorithms and are called inherently serial problems.

-----> (4) Deterministic or non-deterministic
-------> Deterministic algorithms solve the problem with exact decision at every step of the algorithm 
-------> whereas non-deterministic algorithms solve problems via guessing although typical guesses are made more accurate through the use of heuristics.

-----> (5) Exact or approximate
-------> While many algorithms reach an exact solution, approximation algorithms seek an approximation that is closer to the true solution. 
-------> The approximation can be reached by either using a deterministic or a random strategy. 
-------> Such algorithms have practical value for many hard problems. 
-------> One of the examples of an approximate algorithm is the Knapsack problem, where there is a set of given items. 
---------> Its goal is to pack the knapsack to get the maximum total value. 
---------> Each item has some weight and some value. 
---------> Total weight that can be carried is no more than some fixed number X. 
---------> So, the solution must consider weights of items as well as their value.

-----> (6) Quantum algorithm
-------> They run on a realistic model of quantum computation. 
-------> The term is usually used for those algorithms which seem inherently quantum, or use some essential feature of Quantum computing such as quantum superposition or quantum entanglement.



---> By design paradigm

-----> Another way of classifying algorithms is by their design methodology or paradigm. 
-------> There is a certain number of paradigms, each different from the other. 
-------> Furthermore, each of these categories includes many different types of algorithms. 
-------> Some common paradigms are:

-----> (1) Brute-force or exhaustive search
-------> This is the naive method of trying every possible solution to see which is best.

-----> (2) Divide and conquer
-------> A divide-and-conquer algorithm repeatedly reduces an instance of a problem to one or more smaller instances of the same problem 
-------> (usually recursively) until the instances are small enough to solve easily. 
-------> One such example of divide and conquer is merge sorting. Sorting can be done on each segment of data 
-------> after dividing data into segments and sorting of entire data can be obtained in the conquer phase by merging the segments. 
-------> A simpler variant of divide and conquer is called a decrease-and-conquer algorithm, 
-------> which solves an identical subproblem and uses the solution of this subproblem to solve the bigger problem. 
-------> Divide and conquer divides the problem into multiple subproblems and so the conquer stage is more complex than decrease and conquer algorithms. 
-------> An example of a decrease and conquer algorithm is the binary search algorithm.
-------> The approach usually is to break the problem into sub-problems, then solve a single sub-problem and merge all the parital solutions together for the final solution. 
---------> It often consists of these three steps:
-----------> Divide
-----------> Solve
-----------> Combine

-----> (3) Search and enumeration
-------> Many problems (such as playing chess) can be modeled as problems on graphs. 
-------> A graph exploration algorithm specifies rules for moving around a graph and is useful for such problems. 
-------> This category also includes search algorithms, branch and bound enumeration and backtracking.

-----> (4) Randomized algorithm
-------> Such algorithms make some choices randomly (or pseudo-randomly). 
-------> They can be very useful in finding approximate solutions for problems where finding exact solutions can be impractical (see heuristic method below). 
-------> For some of these problems, it is known that the fastest approximations must involve some randomness. 
-------> Whether randomized algorithms with polynomial time complexity can be the fastest algorithms for some problems is an open question known as the P versus NP problem. 
-------> There are two large classes of such algorithms:
---------> Monte Carlo algorithms return a correct answer with high-probability. E.g. RP is the subclass of these that run in polynomial time.
---------> Las Vegas algorithms always return the correct answer, but their running time is only probabilistically bound, e.g. ZPP.

-----> (5) Reduction of complexity
-------> This technique involves solving a difficult problem by transforming it into a better-known problem for which we have (hopefully) asymptotically optimal algorithms. 
-------> The goal is to find a reducing algorithm whose complexity is not dominated by the resulting reduced algorithm's. 
-------> For example, one selection algorithm for finding the median in an unsorted list involves first sorting the list (the expensive portion)
-------> and then pulling out the middle element in the sorted list (the cheap portion). 
-------> This technique is also known as transform and conquer.

-----> (6) Back tracking
-------> In this approach, multiple solutions are built incrementally and abandoned when it is determined that they cannot lead to a valid full solution.
-------> The backtracking algorithm basically continuously builds a solution by searching among all possible solutions. 
---------> Using this algorithm, we keep on building a solution following a certain criteria/process. 
---------> Whenever a solution fails, its disregarded, and we trace back to the failure point to build on the next solution 
---------> and continue this process till we find the correct solution or all possible solutions have been processed.
-------> This technique is very useful in solving combinatorial problems that have a single unique solution. 
---------> Where we have to find the correct combination of steps that lead to fulfillment of the task.  
---------> Such problems have multiple stages and there are multiple options at each stage. 
---------> This approach is based on exploring each available option at every stage one-by-one. 
---------> While exploring an option if a point is reached that doesn’t seem to lead to the solution, 
---------> the program control backtracks one step, and starts exploring the next option. 
---------> In this way, the program explores all possible course of actions and finds the route that leads to the solution.  
---------> Example: N-queen problem, maize problem.


---> Optimization problems

-----> For optimization problems there is a more specific classification of algorithms; 
-----> an algorithm for such problems may fall into one or more of the general categories described above as well as into one of the following:

-----> (1) Linear programming
-------> When searching for optimal solutions to a linear function bound to linear equality and inequality constraints, 
-------> the constraints of the problem can be used directly in producing the optimal solutions. 
-------> There are algorithms that can solve any problem in this category, such as the popular simplex algorithm. 
-------> Problems that can be solved with linear programming include the maximum flow problem for directed graphs. 
-------> If a problem additionally requires that one or more of the unknowns must be an integer then it is classified in integer programming. 
-------> A linear programming algorithm can solve such a problem if it can be proved that all restrictions for integer values are superficial, i.e., the solutions satisfy these restrictions anyway. 
-------> In the general case, a specialized algorithm or an algorithm that finds approximate solutions is used, depending on the difficulty of the problem.
 
-----> (2) Dynamic programming
-------> When a problem shows optimal substructures—meaning the optimal solution to a problem can be constructed from optimal solutions to subproblems—and overlapping subproblems, 
---------> meaning the same subproblems are used to solve many different problem instances, a quicker approach called dynamic programming avoids recomputing solutions that have already been computed. 
-------> For example, Floyd–Warshall algorithm, the shortest path to a goal from a vertex in a weighted graph can be found by using the shortest path to the goal from all adjacent vertices. 
-------> Dynamic programming and memoization go together. 
-------> The main difference between dynamic programming and divide and conquer is that subproblems are more or less independent in divide and conquer, whereas subproblems overlap in dynamic programming. 
-------> The difference between dynamic programming and straightforward recursion is in caching or memoization of recursive calls. 
-------> When subproblems are independent and there is no repetition, memoization does not help; hence dynamic programming is not a solution for all complex problems. 
-------> By using memoization or maintaining a table of subproblems already solved, dynamic programming reduces the exponential nature of many problems to polynomial complexity.

-----> (3) The greedy method
-------> A greedy algorithm is similar to a dynamic programming algorithm in that it works by examining substructures, in this case not of the problem but of a given solution. 
-------> Such algorithms start with some solution, which may be given or have been constructed in some way, and improve it by making small modifications. 
-------> For some problems they can find the optimal solution while for others they stop at local optima, that is, at solutions that cannot be improved by the algorithm but are not optimum. 
-------> The most popular use of greedy algorithms is for finding the minimal spanning tree where finding the optimal solution is possible with this method. 
-------> Huffman Tree, Kruskal, Prim, Sollin are greedy algorithms that can solve this optimization problem.
 
-----> (4) The heuristic method
-------> In optimization problems, heuristic algorithms can be used to find a solution close to the optimal solution in cases where finding the optimal solution is impractical. 
-------> These algorithms work by getting closer and closer to the optimal solution as they progress. 
-------> In principle, if run for an infinite amount of time, they will find the optimal solution. 
-------> Their merit is that they can find a solution very close to the optimal solution in a relatively short time. 
-------> Such algorithms include local search, tabu search, simulated annealing, and genetic algorithms. 
-------> Some of them, like simulated annealing, are non-deterministic algorithms while others, like tabu search, are deterministic. 
-------> When a bound on the error of the non-optimal solution is known, the algorithm is further categorized as an approximation algorithm.

-----> (5) Branch and Bound: 
-------> This technique is very useful in solving combinatorial optimization problem that have multiple solutions and we are interested in find the most optimum solution. 
---------> In this approach, the entire solution space is represented in the form of a state space tree. 
---------> As the program progresses each state combination is explored, and the previous solution is replaced by new one if it is not the optimal than the current solution. 
---------> Example: Job sequencing, Travelling salesman problem.


---> By field of study

-----> Every field of science has its own problems and needs efficient algorithms. 
-------> Related problems in one field are often studied together. 
-------> Some example classes are search algorithms, sorting algorithms, merge algorithms, numerical algorithms, graph algorithms, string algorithms, 
-------> computational geometric algorithms, combinatorial algorithms, medical algorithms, machine learning, cryptography, data compression algorithms and parsing techniques.

-----> Fields tend to overlap with each other, and algorithm advances in one field may improve those of other, sometimes completely unrelated, fields. 
-------> For example, dynamic programming was invented for optimization of resource consumption in industry but is now used in solving a broad range of problems in many fields.

-----> Searching Algorithm
-------> A search algorithm is an algorithm (if more than one, algorithms) designed to solve a search problem. 
---------> Search algorithms work to retrieve information stored within particular data structure, 
---------> or calculated in the search space of a problem domain, with either discrete or continuous values. 
-------> Searching algorithms are used for searching elements or groups of elements from a certain data structure. 
---------> It can be classified on different types based on the approach or the data structure used.

-----> Sorting Algorithm: 
-------> A sorting algorithm is an algorithm that puts elements of a list into an order. 
-------> The most frequently used orders are numerical order and lexicographical order, and either ascending or descending. 
-------> Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists. 
-------> Sorting is also often useful for canonicalizing data and for producing human-readable output.
-------> Formally, the output of any sorting algorithm must satisfy two conditions:
---------> (1) The output is in monotonic order (each element is no smaller/larger than the previous element, according to the required order).
---------> (2) The output is a permutation (a reordering, yet retaining all of the original elements) of the input.
-------> For optimum efficiency, the input data should be stored in a data structure which allows random access rather than one that allows only sequential access. 

-----> Hashing Algorithm: 
-------> A hash function is any function that can be used to map data of arbitrary size to fixed-size values. 
-------> The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. 
-------> The values are usually used to index a fixed-size table called a hash table. 
-------> Use of a hash function to index a hash table is called hashing or scatter storage addressing. 



---> By design approaches

-----> (1) Top-Down Approach: 
-------> In the top-down approach, a large problem is divided into small sub-problem 
-------> and keep repeating the process of decomposing problems until the complex problem is solved.

-----> (2) Bottom-up approach: 
-------> The bottom-up approach is also known as the reverse of top-down approaches.
-------> In approach different, part of a complex program is solved and then this is combined into a complete solution.



---> By complexity

-----> Algorithms can be classified by the amount of time they need to complete compared to their input size:
-------> Constant time: if the time needed by the algorithm is the same, regardless of the input size. E.g. an access to an array element.
-------> Logarithmic time: if the time is a logarithmic function of the input size. E.g. binary search algorithm.
-------> Linear time: if the time is proportional to the input size. E.g. the traverse of a list.
-------> Polynomial time: if the time is a power of the input size. E.g. the bubble sort algorithm has quadratic time complexity.
-------> Exponential time: if the time is an exponential function of the input size. E.g. Brute-force search.

-----> Some problems may have multiple algorithms of differing complexity, while other problems might have no algorithms or no known efficient algorithms. 
-------> There are also mappings from some problems to other problems. 
-------> Owing to this, it was found to be more suitable to classify the problems themselves instead of the algorithms into equivalence classes based on the complexity of the best possible algorithms for them.



---> Continuous algorithms

-----> The adjective "continuous" when applied to the word "algorithm" can mean:
-------> An algorithm operating on data that represents continuous quantities, even though this data is represented by discrete approximations—such algorithms are studied in numerical analysis; or
-------> An algorithm in the form of a differential equation that operates continuously on the data, running on an analog computer.[79]



-> What makes a good algorithm?

---> Characteristics of a good algorithm:
-----> Clear and Unambiguous: 
-------> The algorithm should be clear and unambiguous. 
-------> Each of its steps should be clear in all aspects and must lead to only one meaning.
-----> Well-Defined Inputs: 
-------> If an algorithm says to take inputs, it should be well-defined inputs. 
-----> Well-Defined Outputs: 
-------> The algorithm must clearly define what output will be yielded and it should be well-defined as well. 
-----> Finite-ness: 
-------> The algorithm must be finite, i.e. it should terminate after a finite time.
-----> Feasible: 
-------> The algorithm must be simple, generic, and practical, such that it can be executed with the available resources. 
-------> It must not contain some future or fictional hardware or technology.
-----> Language Independent: 
-------> The algorithm designed must be language-independent.
-------> This means that the steps can be implemented on any language and the output is still the same.

---> Properties of good algorithm:
-------> It should terminate after a finite time.
-------> It should produce at least one output.
-------> It should take zero or more input.
-------> It should be deterministic means giving the same output for the same input case.
-------> Every step in the algorithm must be effective i.e. every step should have some purpose.

-> How to Design an Algorithm?

---> In order to write an algorithm, the following things are needed as a pre-requisite: 
-----> The problem that is to be solved by this algorithm i.e. clear problem definition.
-----> The constraints of the problem must be considered while solving the problem.
-----> The input to be taken to solve the problem.
-----> The output to be expected when the problem is solved.
-----> The solution to this problem, is within the given constraints.

-> How to analyze an Algorithm? 

---> For a standard algorithm to be good, it must be efficient. 
---> Hence the efficiency of an algorithm must be checked and maintained. 
---> It can be in two stages:
-----> (1) Priori Analysis: “Priori” means “before”. 
-------> Hence Priori analysis means checking the algorithm before its implementation. 
-------> In this, the algorithm is checked when it is written in the form of theoretical steps. 
-------> This Efficiency of an algorithm is measured by assuming that all other factors, for example, processor speed, are constant and have no effect on the implementation. 
-------> This is done usually by the algorithm designer. This analysis is independent of the type of hardware and language of the compiler. It gives the approximate answers for the complexity of the program.
-----> (2) Posterior Analysis: “Posterior” means “after”. 
-------> Hence Posterior analysis means checking the algorithm after its implementation. 
-------> In this, the algorithm is checked by implementing it in any programming language and executing it. 
-------> This analysis helps to get the actual and real analysis report about correctness, space required, time consumed etc. 
-------> That is, it is dependent on the language of the compiler and the type of hardware used.

-> What is Algorithm complexity and how to find it?

---> An algorithm is defined as complex based on the amount of Space and Time it consumes.
---> Hence the Complexity of an algorithm refers to the measure of the Time that it will need to execute and get the expected output, 
---> and the Space it will need to store all the data (input, temporary data and output). 
---> Hence these two factors define the efficiency of an algorithm. 
---> The two factors of Algorithm Complexity are:
-----> Time Factor: Time is measured by counting the number of key operations such as comparisons in the sorting algorithm.
-----> Space Factor: Space is measured by counting the maximum memory space required by the algorithm.
---> Therefore the complexity of an algorithm can be divided into two types:
---> (1) Space Complexity: 
-----> The space complexity of an algorithm refers to the amount of memory used by the algorithm to store the variables and get the result. 
-----> This can be for inputs, temporary operations, or outputs. 
-----> How to calculate Space Complexity?
-----> The space complexity of an algorithm is calculated by determining the following 2 components: 
-------> (1) Fixed Part: This refers to the space that is definitely required by the algorithm. For example, input variables, output variables, program size, etc.
-------> (2) Variable Part: This refers to the space that can be different based on the implementation of the algorithm. For example, temporary variables, dynamic memory allocation, recursion stack space, etc.
-----> Therefore Space complexity S(P) of any algorithm P is S(P) = C + SP(I), where C is the fixed part and S(I) is the variable part of the algorithm, which depends on instance characteristic I.
---> (2) Time Complexity: 
-----> The time complexity of an algorithm refers to the amount of time that is required by the algorithm to execute and get the result. 
-----> This can be for normal operations, conditional if-else statements, loop statements, etc.
-----> How to calculate Time Complexity?
-----> The time complexity of an algorithm is also calculated by determining the following 2 components: 
-------> (1) Constant time part: Any instruction that is executed just once comes in this part. For example, input, output, if-else, switch, etc.
-------> (2) Variable Time Part: Any instruction that is executed more than once, say n times, comes in this part. For example, loops, recursion, etc.
-----> Therefore Time complexity T(P) of any algorithm P is T(P) = C + TP(I), where C is the constant time part and TP(I) is the variable part of the algorithm, which depends on the instance characteristic I.

-> Why algorithm and its analysis important? 

---> In the analysis of the algorithm, it generally focused on CPU (time) usage, Memory usage, Disk usage, and Network usage. 
---> All are important, but the most concern is about the CPU time. Be careful to differentiate between:
-----> (1) Performance: How much time/memory/disk/etc. is used when a program is run. This depends on the machine, compiler, etc. as well as the code we write.
-----> (2) Complexity: How do the resource requirements of a program or algorithm scale, i.e. what happens as the size of the problem being solved by the code gets larger.
---> Note: Complexity affects performance but not vice-versa.
---> Algorithm Analysis:
-----> Algorithm analysis is an important part of computational complexity theory,
-------> which provides theoretical estimation for the required resources of an algorithm to solve a specific computational problem. 
-----> Analysis of algorithms is the determination of the amount of time and space resources required to execute it.
---- Why Analysis of Algorithms is important?
-----> To predict the behavior of an algorithm without implementing it on a specific computer.
-----> It is much more convenient to have simple measures for the efficiency of an algorithm than to implement the algorithm 
-----> and test the efficiency every time a certain parameter in the underlying computer system changes.
-----> It is impossible to predict the exact behavior of an algorithm. There are too many influencing factors.
-----> The analysis is thus only an approximation; it is not perfect.
-----> More importantly, by analyzing different algorithms, we can compare them to determine the best one for our purpose.
-----> Types of Algorithm Analysis:
------> (1) Best case: 
--------> Define the input for which algorithm takes less time or minimum time. 
--------> In the best case calculate the lower bound of an algorithm. 
--------> Example: 
----------> In the linear search when search data is present at the first location of large data then the best case occurs.
------> (2) Worst Case: 
--------> Define the input for which algorithm takes a long time or maximum time. 
--------> In the worst calculate the upper bound of an algorithm. 
--------> Example: 
----------> In the linear search when search data is not present at all then the worst case occurs.
------> (3) Average case: 
--------> In the average case take all random inputs and calculate the computation time for all inputs.
--------> And then we divide it by the total number of inputs.
--------> Average case = all random case time / total no of cases

-> Analysis of Loops

---> (1) Constant or O(1): 
-----> Time complexity of a function (or set of statements) is considered as O(1) if it doesn’t contain loop, recursion, and call to any other non-constant time function. 
-----> For example, swap() function has O(1) time complexity. 
-----> A loop or recursion that runs a constant number of times is also considered as O(1). For example, the following loop is O(1). 
-------> // Here c is a constant   
-------> for (int i = 1; i <= c; i++) {  
------->      // some O(1) expressions
-------> }

---> (2) Linear or O(n): 
-----> Time Complexity of a loop is considered as O(n) if the loop variables are incremented/decremented by a constant amount.
-----> For example following functions have O(n) time complexity.  
-------> // Here c is a positive integer constant   
-------> for (int i = 1; i <= n; i += c) {  
------->      // some O(1) expressions
-------> }
-------> 
-------> for (int i = n; i > 0; i -= c) {
------->      // some O(1) expressions
-------> }

---> (3) Polynomial or O(n^c): 
-----> Time complexity of nested loops is equal to the number of times the innermost statement is executed. 
-----> For example, the following sample loops have O(n2) time complexity 
-------> for (int i = 1; i <=n; i += c) {
------->     for (int j = 1; j <=n; j += c) {
------->        // some O(1) expressions
------->     }
-------> }
-------> 
-------> for (int i = n; i > 0; i -= c) {
------->     for (int j = i+1; j <=n; j += c) {
------->        // some O(1) expressions
-------> }
-----> For example, Selection sort and Insertion Sort have O(n2) time complexity. 

---> (4) Logarithmic or O(Logn) 
-----> Time Complexity of a loop is considered as O(Logn) if the loop variables are divided/multiplied by a constant amount. 
-----> And also for recursive call in recursive function the Time Complexity is considered as O(Logn).
-------> for (int i = 1; i <=n; i *= c) {
------->     // some O(1) expressions
-------> }
-------> for (int i = n; i > 0; i /= c) {
------->     // some O(1) expressions
-------> }
-------> //Recursive function
-------> void recurse(n)
-------> {
------->     if(n==0)
------->         return;
------->     else{
------->         // some O(1) expressions
------->     }
------->     recurse(n-1);
-------> }
-----> For example, Binary Search(refer iterative implementation) has O(Logn) time complexity. 
-----> Let us see mathematically how it is O(Log n). The series that we get in the first loop is 1, c, c2, c3, … ck. 
-----> If we put k equals to Logcn, we get cLogcn which is n. 

---> 5) Double logarithmic or O(LogLogn) 
-----> Time Complexity of a loop is considered as O(LogLogn) if the loop variables are reduced/increased exponentially by a constant amount. 
-------> // Here c is a constant greater than 1   
-------> for (int i = 2; i <=n; i = pow(i, c)) { 
------->     // some O(1) expressions
-------> }
-------> //Here fun is sqrt or cuberoot or any other constant root
-------> for (int i = n; i > 1; i = fun(i)) { 
------->     // some O(1) expressions
-------> }
-----> See this for mathematical details. 
-----> How to combine the time complexities of consecutive loops? 
-----> When there are consecutive loops, we calculate time complexity as a sum of time complexities of individual loops. 
-------> for (int i = 1; i <=m; i += c) {  
------->      // some O(1) expressions
-------> }
-------> for (int i = 1; i <=n; i += c) {
------->      // some O(1) expressions
-------> }
-----> Time complexity of above code is O(m) + O(n) which is O(m+n)
-------> If m == n, the time complexity becomes O(2n) which is O(n).
-----> How to calculate time complexity when there are many if, else statements inside loops? 
-------> As discussed here, worst-case time complexity is the most useful among best, average and worst. Therefore we need to consider the worst case. We evaluate the situation when values in if-else conditions cause a maximum number of statements to be executed. 
-------> For example, consider the linear search function where we consider the case when an element is present at the end or not present at all. 
-------> When the code is too complex to consider all if-else cases, we can get an upper bound by ignoring if-else and other complex control statements. 
-------> How to calculate the time complexity of recursive functions? 
-------> The time complexity of a recursive function can be written as a mathematical recurrence relation. To calculate time complexity, we must know how to solve recurrences. We will soon be discussing recurrence solving techniques as a separate post. 



-> Analysis of Recurrences:

---> In the previous post, we discussed the analysis of loops. 
-----> Many algorithms are recursive. 
-----> When we analyze them, we get a recurrence relation for time complexity. 
-----> We get running time on an input of size n as a function of n and the running time on inputs of smaller sizes. 
-----> For example in Merge Sort, to sort a given array, we divide it into two halves and recursively repeat the process for the two halves. 
-----> Finally, we merge the results. 
-----> Time complexity of Merge Sort can be written as T(n) = 2T(n/2) + cn. 
-----> There are many other algorithms like Binary Search, Tower of Hanoi, etc.
---> There are mainly three ways of solving recurrences:

-----> (1) Substitution Method: 
-------> We make a guess for the solution and then we use mathematical induction to prove the guess is correct or incorrect. 
---------> For example consider the recurrence T(n) = 2T(n/2) + n
-------> We guess the solution as T(n) = O(nLogn). 
---------> Now we use induction to prove our guess.
-------> We need to prove that T(n) <= cnLogn. 
---------> We can assume that it is true for values smaller than n.
---------> T(n) = 2T(n/2) + n
--------->     <= 2cn/2Log(n/2) + n
--------->     =  cnLogn - cnLog2 + n
--------->     =  cnLogn - cn + n
--------->     <= cnLogn

-----> (2) Recurrence Tree Method: 
-------> In this method, we draw a recurrence tree and calculate the time taken by every level of the tree. 
---------> Finally, we sum the work done at all levels. 
---------> To draw the recurrence tree, we start from the given recurrence and keep drawing till we find a pattern among levels. 
---------> The pattern is typically arithmetic or geometric series.  
-------> NOTE: The number of leaves is equal to branches^depth ("number of branches of each node" ^ "depth of tree")

-------> Example 1: consider the recurrence relation:
---------> T(n) = T(n/4) + T(n/2) + cn^2
---------> The recursion tree will look like this:
----------->            cn^2
----------->          /      \
----------->      T(n/4)     T(n/2)
---------> If we further break down the expression T(n/4) and T(n/2),  we get the following recursion tree.
----------->                 cn^2
----------->            /           \      
----------->        c(n^2)/16      c(n^2)/4
----------->       /      \          /     \
----------->   T(n/16)     T(n/8)  T(n/8)    T(n/4) 
-----------> Breaking down further gives us following
----------->                  cn^2
----------->             /            \      
----------->        c(n^2)/16          c(n^2)/4
----------->        /      \            /      \
-----------> c(n^2)/256  c(n^2)/64  c(n^2)/64  c(n^2)/16
----------->  /    \      /    \    /    \       /    \  
---------> To know the value of T(n), we need to calculate the sum of tree nodes level by level. 
-----------> If we sum the above tree level by level, we get the following series:
-------------> T(n)  = [first level] c(n^2) + [second level] c(n^2)/16 + c(n^2)/4 + [third level] c(n^2)/256  + c(n^2)/64 + c(n^2)/64 + c(n^2)/16 + ...
-------------> T(n)  = c(n^2) * ( 1 + 5/16 + 25/256 + ... )
-----------> The above series is a geometrical progression with a ratio of 5/16.
-------------> To get an upper bound, we can sum the infinite series. 
-------------> Using integration for finite calulus and limits:
---------------> Value for each term: ((5/16)^x)
---------------> Integration via finite calculus: (((5/16)^x)/(-11/16))
-----------------> Note that: c^x = c^x / (c-1)
---------------> Substituting x+1: (-16*((5/16)^(x+1))/11)
---------------> Evaluating from 0 to x: (-16*((5/16)^(x+1))/11) - (-16*((5/16)^(0))/11) = (-16*((5/16)^(x+1))/11) + (16/11)
---------------> Getting limits at infinity: 0 + (16/11) = 16/11
-------------> Or just use the formula for geometric series (when multiplier is fractional): 
---------------> Summation = firstValue / (1 - commonMultiplier)  = 1 / (1 - 5/16) = 16/11
-----------> Total = (n^2)*(16/11) = O(n^2)

-------> Example 2: consider the recurrence relation:
---------> T(n) = 3*T(n/4) + cn^2
---------> The recursion tree will look like this:
-----------> cn^2
----------->   |                      \                             \
----------->   |                             \                              \
-----------> c(n/4)^2                     c(n/4)^2                       c(n/4)^2
----------->   |   \     \                      \      \     \                  \      \     \   
----------->   |     \      \                    \       \       \                \       \       \
-----------> c(n/16)^2 c(n/16)^2 c(n/16)^2   c(n/16)^2 c(n/16)^2 c(n/16)^2   c(n/16)^2 c(n/16)^2 c(n/16)^2
-----------> ...
---------> To know the value of T(n), we need to calculate the sum of tree nodes level by level. 
-----------> If we sum the above tree level by level, we get the following series:
-------------> T(n)  = [first level] c(n^2) + [second level] + c(n^2)*3/16 + [third level] c(n^2)*9/256 + ...
-------------> T(n)  = c(n^2) * ( 1 + 3/16 + 9/256 + ... )
-----------> The above series is a geometrical progression with a ratio of 3/16.
-------------> Just use the formula for geometric series (when multiplier is fractional): Summation = firstValue / (1 - commonMultiplier) 
---------------> Summation = firstValue / (1 - commonMultiplier)  = 1 / (1 - 3/16) = 16/13
-----------> Total = (n^2)*(16/13) = O(n^2)

-------> Example 3: consider the recurrence relation:
---------> T(n) = T(n/3) + T(n*2/3) + cn
---------> The recursion tree will look like this:
-----------> cn
----------->   |              \ 
----------->   |                \
-----------> c(n/3)           c(n*2/3)
----------->   |   \             \
----------->   |     \           \       \
-----------> c(n/9) c(n*2/9)  c(n*2/9) c(n*4/9)
-----------> ...
---------> To know the value of T(n), we need to calculate the sum of tree nodes level by level. 
-----------> If we sum the above tree level by level, we get the following series:
-------------> T(n)  = [first level] cn + [second level] + cn + [third level] cn + ..
-------------> We need to determine the height of the tree to know the sum of all the cns.
-----------> The longest simple path from the root to the leaf is n -> n*(2/3) -> n*(2/3)^2 -> ... -> 1
-------------> The formula for the last leaf is: n*(2/3)^depth = 1 -> (3/2)^depth = n -> log1.5(n) = depth
-----------> Total = cn*log1.5(n) = O(n lg n)
-------------> Note that lg(n) is an upper bound: O(log1.5(n)) = O(lg(n)) 

---------> Example 4:
-----------> This code counts all permutations of a string.
-----------> void permutation(String str) {
-------------> permutation(str, "");
-----------> }
-----------> void permutation(String str, String prefix) {
-------------> if (str.length() == 0) {
-----------------> System.out.println(prefix);
-------------> } else {
---------------> for (int i= 0; i < str.length(); i++) {
-----------------> String rem = str.substring(0, i) + str.substring(i + 1);
-----------------> permutation(rem, prefix + str.charAt(i));
---------------> }
-------------> }
-----------> }
-----------> This is a (very!) tricky one. 
-------------> We can think about this by looking at how many times permutation gets called and how long each call takes.
-------------> We'll aim for getting as tight of an upper bound as possible.
-----------> How many times does permutation get called in its base case?
-------------> If we were to generate a permutation, then we would need to pick characters for each "slot'.
-------------> Suppose we had 7 characters in the string. In the first slot, we have 7 choices. 
-------------> Once we pick the letter there, we have 6 choices for the next slot. 
-------------> (Note that this is 6 choices for each of the 7 choices earlier.) 
-------------> Then 5 choices for the next slot, and so on.
-------------> Therefore, the total number of options is 7 * 6 * 5 * 4 * 3 * 2 * 1, which is also expressed as 7! (7 factorial).
-------------> This tells us that there are n! permutations. 
-------------> Therefore, permutation is called n ! times in its base case (when prefix is the full permutation).
-----------> How many times does permutation get called before its base case?
-------------> But, of course, we also need to consider how many times lines 9 through 12 are hit. 
-------------> Picture a large call tree representing all the calls. 
-------------> There are n! leaves, as shown above. 
-------------> Each leaf is attached to a path of length n.
-------------> Therefore, we know there will be no more than n * n ! nodes (function calls) in this tree.
-----------> How long does each function call take?
-------------> Executing line 7 takes 0( n) time since each character needs to be printed.
-------------> Line 10 and line 11 will also take O ( n) time combined, due to the string concatenation. 
-------------> Observe that the sum of the lengths of rem, prefix,and str. charAt(i) will always be n.
-------------> Each node in our call tree therefore corresponds to 0( n) work.
-----------> What is the total runtime?
-------------> Since we are calling permutation 0(n*n!) times (as an upper bound), and each one takes 0(n) time, the total runtime will not exceed O (n^2 * n!) .
-------------> Through more complex mathematics, we can derive a tighter runtime equation (though not necessarily a nice closed-form expression). 
-------------> This would almost certainly be beyond the scope of any normal interview.

-----> (3) Master Method: 
-------> Master Method is a direct way to get the solution. 
-------> The master method works only for the following type of recurrences or for recurrences that can be transformed into the following type. 
---------> Let a >= 1 and b > 1,
---------> and let f(n) be a function,
---------> and let T(n) be defined on the non negative integers by the recurrence:
-----------> T(n) = aT(n/b) + f(n) 
-------> There are following three cases: 
---------> Case 1: If f(n) = O(n^c) where c < log(b)(a) then T(n) = Θ(n^(log(b)(a))) 
---------> Case 2: If f(n) = Θ(n^c) where c = log(b)(a) then T(n) = Θ(n^c * lg(n)) 
---------> Case 3: If f(n) = Ω(n^c) where c > log(b)(a) then T(n) = Θ(f(n))
-------> Stricter definition:
---------> Case 1: If f(n) = O( n^(log(b)(a)-e) ) for some constant e>0, then T(n) = Θ( n^(log(b)(a)) ) 
---------> Case 2: If f(n) = Θ( n^(log(b)(a)) ), then T(n) = Θ( n^(log(b)(a))*lg(n) ) 
---------> Case 3: If f(n) = Ω( n^(log(b)(a)+e) ) for some constant e>0
-----------> and if a*f(n/b) <= c*f(n) for some constant c<1 and all sufficiently large n
-----------> then T(n) = Θ(f(n))
-------> How does this work? 
---------> The master method is mainly derived from the recurrence tree method. 
---------> If we draw the recurrence tree of T(n) = aT(n/b) + f(n), 
-----------> we can see that the work done at the root is f(n), and work done at all leaves is Θ(n^c) where c is log(b)(a). 
---------> And the height of the recurrence tree is log(b)(n) 
-------> Master Theorem explanation
---------> In the recurrence tree method, we calculate the total work done. 
-----------> Case 1: The work done at leaves is polynomially more, then leaves are the dominant part, and our result becomes the work done at leaves. 
-----------> Case 2: The work done at leaves and root is asymptotically the same, then our result becomes height multiplied by work done at any level. 
-----------> Case 3: The work done at the root is asymptotically more, then our result becomes work done at the root.
-------> Technicalities:
---------> In the first case, not only must f(n) be smaller than n^(log(b)(a)), it must be polynomially smaller.
------------> that is, f(n) must be asymptotically smaller than n^(log(b)(a)) by a factor of n^e for some constant e>0.
---------> In the third case, ont only must f(n) be larger than n^(log(b)(a)), 
------------> it also must be polynomially larger and in addition satisfy the reularity condition that a*f(n/b) <= c*f(n).
------------> This condition is safisfied by most of the polynomially bounded functions that we shall encounter.
-------> Notes: 
---------> (1) Mote that thre cases do not cover all the possibilities for f(n). 
-----------> There is a gap between cases 1 and 2 when f(n) is smaller than n^(log(b)(a)) but not polynomially smaller.
-----------> Similarly there is a gap between 2 and 3 when f(n) is larger than n^(log(b)(a))  but no polynomially larger.
-----------> If the function f(n) falls into one of these gaps, or if the regularity condition in case 3 fails to hold,
-------------> you cannot use the master theorem to solve the reccurence.
-----------> For example, the recurrence T(n) = 2T(n/2) + n/log(n) cannot be solved using master method. 
-------------> This is gap between case 1 and 2. There is no e that can satisfies this:
---------------> In Case 1, If n/log(n) = O( n^(1-e) ) for some constant e>0
---------> (2) Case 2 can be extended for f(n) = Θ( n^c * log(k)(n) ) 
-----------> If f(n) = Θ( n^c * log(k)(n) ) for some constant k >= 0 and c = log(b)(a), then T(n) = Θ( n^c * log(k+1)(n) ) 

-------> Examples:

---------> Remember the form: T(n) = aT(n/b) + f(n) 

---------> Example 1: Merge Sort: T(n) = 2T(n/2) + Θ(n). 
-----------> Its in case 2 because c == log(b)(a) so its 0==log(2)(2) -> 1==1. 
-----------> So T(n) = Θ( n^(log(2)(2)) * lg(n) ) = Θ(n lg(n))

---------> Example 2: Binary Search: T(n) = T(n/2) + Θ(1). 
-----------> Its in case 2 because c == log(b)(a) so its 0==log(2)(1) -> 0==0. 
-----------> So T(n) = Θ( n^(log(2)(1)) * lg(n) ) = Θ(n^0 lg(n)) = Θ(lg(n))

---------> Example 3: T(n) = 9T(n/3) + n. 
-----------> Its in case 1 because c < log(b)(a) so its 1<log(3)(9) -> 1<2. 
-----------> So T(n) = Θ( n^(log(3)(9)) )  = Θ(n^2)

---------> Example 4: T(n) = T(n*2/3) + 1. 
-----------> Its in case 2 because c == log(b)(a) so its 0==log(3/2)(1) -> 0==0. 
-----------> So T(n) = Θ(n^c * lg(n)) = Θ(n^0 * lg(n)) = Θ(lg(n))

---------> Example 5: T(n) = 3T(n/4) + n lg(n). 
-----------> Although its n lg n, we can use c==1 because its case 3 (the root is asymptotically more) and n < n lg(n) 
--------------> It basically satisfies this: I
----------------> If f(n) = Ω( n^(log(b)(a)+e) ) for some constant e>0  ->  n lg n == Ω(n^(log(4)(3)+e))  ->  can be satisfied with e ~ 0.2
-----------> Its in case 3 because c > log(b)(a) so its 1>log(4)(3) -> 1>0.792. 
-----------> The regularity condition also holds:
-------------> if a*f(n/b) <= c*f(n) for some constant c<1 and all sufficiently large n
-------------> 3*n/4 * lg(n/4) <= c*n*lg(n), let c = 3/4 and the condition holds
-----------> So T(n) = Θ(f(n)) = Θ(n lg(n))

---------> Example 6: T(n) = 2T(n/2) + n lg(n). 
-----------> The master method cannot be used because none of the conditions can be satisfied:
-------------> Case 1: If f(n) = O( n^(log(b)(a)-e) ) for some constant e>0, ->  n lg n == O(n^(1-e))  ->  cannot be satisfied 
-------------> Case 2: If f(n) = Θ( n^(log(b)(a)) ), ->  n lg n == Θ(n^(1))  ->  cannot be satisfied 
-------------> Case 3: If f(n) = Ω( n^(log(b)(a)+e) ) for some constant e>0, ->  n lg n == Ω(n^(1+e))  ->  cannot be satisfied 

---------> Example 7: Strassen algorithm: T(n) = 7T(n/2) + Θ(n^2). 
-----------> Its in case 1 because c < log(b)(a) so its 2<log(2)(7) -> 2<lg(7). 
-----------> So T(n) = Θ( n^(log(2)(7)) )  = Θ(n^lg(7))

---------> Example 8: Van Emde Boas Trees (insert and successor operations): T(u) = T(u^(1/2)) + Θ(1). 
-----------> This is a little bit problematic because its sqrt of u so we cant use the master method directly.
-----------> We have to turn the recurrence from a recurrence in u to a recurrence in k = log U.
-------------> Define S(k) = T(2^k).
-------------> Since T(2) ≤ Θ(1) and T(u) ≤ T(U^(1/2)) + Θ(1)
-------------> S(1) ≤ Θ(1) and S(k) ≤ S(k / 2) + Θ(1)
-------------> Applying the master theorem:
---------------> Its in case 2 because c == log(b)(a) so its 0==log(2)(1) -> 0==0. 
---------------> S(k) = O(lg k)
-------------> T(u) = T(2^(lg U)) = S(lg U) = Θ(lg(lg(u))).
-------------> T(u) = Θ(lg(lg(u)))

-> Amortized Analysis

---> Amortized Analysis is used for algorithms where an occasional operation is very slow, but most of the other operations are faster. 
-----> In Amortized Analysis, we analyze a sequence of operations and guarantee a worst-case average time 
-----> that is lower than the worst-case time of a particularly expensive operation. 
-----> The example data structures whose operations are analyzed using Amortized Analysis are Hash Tables, Disjoint Sets, and Splay Trees. 
---> Let us consider an example of simple hash table insertions. 
-----> How do we decide on table size? 
-----> There is a trade-off between space and time, if we make hash-table size big, search time becomes low, but the space required becomes high. 
---> Dynamic Table
-----> The solution to this trade-off problem is to use Dynamic Table (or Arrays). 
-------> The idea is to increase the size of the table whenever it becomes full. 
-------> Following are the steps to follow when the table becomes full. 
---------> 1) Allocate memory for larger table size, typically twice the old table. 
---------> 2) Copy the contents of the old table to a new table. 
---------> 3) Free the old table. 
---> If the table has space available, we simply insert a new item in the available space. 
---> What is the time complexity of n insertions using the above scheme? 
-----> If we use simple analysis, the worst-case cost of insertion is O(n). 
-----> Therefore, the worst-case cost of n inserts is n * O(n) which is O(n2). 
-----> This analysis gives an upper bound, but not a tight upper bound for n insertions as all insertions don’t take Θ(n) time. 
---> Amortized Analysis
-----> So using Amortized Analysis, we could prove that the Dynamic Table scheme has O(1) insertion time which is a great result used in hashing. 
-------> Also, the concept of the dynamic table is used in vectors in C++ and ArrayList in Java. 
-----> Following are a few important notes. 
-------> 1) Amortized cost of a sequence of operations can be seen as expenses of a salaried person. 
---------> The average monthly expense of the person is less than or equal to the salary, 
---------> but the person can spend more money in a particular month by buying a car or something. 
---------> In other months, he or she saves money for the expensive month. 
-------> 2) The above Amortized Analysis was done for Dynamic Array example is called Aggregate Method. 
---------> There are two more powerful ways to do Amortized analysis called Accounting Method and Potential Method. 
-------> 3) The amortized analysis doesn’t involve probability. 
---------> There is also another different notion of average-case running time where algorithms use randomization 
---------> to make them faster and the expected running time is faster than the worst-case running time. 
---------> These algorithms are analyzed using Randomized Analysis. 
---------> Examples of these algorithms are Randomized Quick Sort, Quick Select and Hashing. 



-> Pseudo-polynomial Algorithms

---> What is a pseudo-polynomial algorithm? 
-----> A pseudo-polynomial algorithm is an algorithm whose worst-case time complexity is polynomial in the numeric value of input (not number of inputs).
-------> For example, consider the problem of counting frequencies of all elements in an array of positive numbers. 
-------> A pseudo-polynomial time solution for this is to first find the maximum value, then iterate from 1 to maximum value and for each value, find its frequency in array. 
-------> This solution requires time according to maximum value in input array, therefore pseudo-polynomial. 
-------> On the other hand, an algorithm whose time complexity is polynomial in the number of elements in array (not value) is considered as polynomial time algorithm. 
-----> A numeric algorithm runs in pseudo-polynomial time if its running time is a polynomial in the numeric value of the input 
-------> (the largest integer present in the input) — but not necessarily in the length of the input (the number of bits required to represent it), 
-------> which is the case for polynomial time algorithms.
-----> In general, the numeric value of the input is exponential in the input length, 
-------> which is why a pseudo-polynomial time algorithm does not necessarily run in polynomial time with respect to the input length.

---> Pseudo-polynomial and NP-Completeness 
-----> Some NP-Complete problems have pseudo-polynomial time solutions. 
-------> For example, Dynamic Programming Solutions of 0-1 Knapsack, Subset-Sum and Partition problems are Pseudo-Polynomial. 
-------> NP complete problems that can be solved using a pseudo-polynomial time algorithms are called weakly NP-complete. 
-----> An NP-complete problem with known pseudo-polynomial time algorithms is called weakly NP-complete. 
-------> An NP-complete problem is called strongly NP-complete if it is proven that it cannot be solved by a pseudo-polynomial time algorithm unless P = NP. 
-------> The strong/weak kinds of NP-hardness are defined analogously. 



-> NP-Completeness

---> We have been writing about efficient algorithms to solve complex problems, like shortest path, Euler graph, minimum spanning tree, etc. 
-----> Those were all success stories of algorithm designers. In this post, failure stories of computer science are discussed. 

---> Can all computational problems be solved by a computer? 
-----> There are computational problems that can not be solved by algorithms even with unlimited time. For example Turing Halting problem 
-----> (Given a program and an input, whether the program will eventually halt when run with that input, or will run forever). 
-----> Alan Turing proved that a general algorithm to solve the halting problem for all possible program-input pairs cannot exist. 
-----> A key part of the proof is, that the Turing machine was used as a mathematical definition of a computer and program (Source Halting Problem). 
-----> Status of NP-Complete problems is another failure story, NP-complete problems are problems whose status is unknown. 
-----> No polynomial-time algorithm has yet been discovered for any NP-complete problem, 
-----> nor has anybody yet been able to prove that no polynomial-time algorithm exists for any of them. 
-----> The interesting part is, that if any one of the NP-complete problems can be solved in polynomial time, then all of them can be solved. 

---> What are NP, P, NP-complete, and NP-Hard problems? 
-----> P is a set of problems that can be solved by a deterministic Turing machine in Polynomial-time. 
-----> NP is a set of decision problems that can be solved by a Non-deterministic Turing Machine in Polynomial-time. 
-------> P is a subset of NP (any problem that can be solved by a deterministic machine in polynomial time can also be solved by a non-deterministic machine in polynomial time). 
-------> Informally, NP is a set of decision problems that can be solved by a polynomial-time via a “Lucky Algorithm”, 
-------> a magical algorithm that always makes a right guess among the given set of choices. 
-----> NP-complete problems are the hardest problems in the NP set.  
-------> A decision problem L is NP-complete if: 
-------> 1) L is in NP (Any given solution for NP-complete problems can be verified quickly, but there is no efficient known solution). 
-------> 2) Every problem in NP is reducible to L in polynomial time (Reduction is defined below). 
-----> A problem is NP-Hard if it follows property 2 mentioned above, and doesn’t need to follow property 1. 
-------> Therefore, the NP-Complete set is also a subset of the NP-Hard set. 

---> NP, P, NP-complete and NP-Hard problems

-----> Decision vs Optimization Problems 
-------> NP-completeness applies to the realm of decision problems.  
-------> It was set up this way because it’s easier to compare the difficulty of decision problems than that of optimization problems. 
-------> In reality, though, being able to solve a decision problem in polynomial time will often permit us to solve the corresponding optimization problem in polynomial time
-------> (using a polynomial number of calls to the decision problem). 
-------> So, discussing the difficulty of decision problems is often really equivalent to discussing the difficulty of optimization problems. 
-------> For example, consider the vertex cover problem (Given a graph, find out the minimum sized vertex set that covers all edges). 
-------> It is an optimization problem. 
-------> The corresponding decision problem is, given undirected graph G and k, is there a vertex cover of size k? 

-----> What is Reduction? 
-------> Let L1 and L2 be two decision problems. Suppose algorithm A2 solves L2. 
---------> That is, if y is an input for L2 then algorithm A2 will answer Yes or No depending upon whether y belongs to L2 or not. 
---------> The idea is to find a transformation from L1 to L2 so that algorithm A2 can be part of an algorithm A1 to solve L1. 

-----> Learning reduction, in general, is very important. 
-------> For example, if we have library functions to solve certain problems and if we can reduce a new problem to one of the solved problems, we save a lot of time. 
-------> Consider the example of a problem where we have to find the minimum product path in a given directed graph 
-------> where the product of the path is the multiplication of weights of edges along the path. 
-------> If we have code for Dijkstra’s algorithm to find the shortest path, 
-------> we can take the log of all weights and use Dijkstra’s algorithm to find the minimum product path rather than writing a fresh code for this new problem. 

-----> How to prove that a given problem is NP-complete? 
-------> From the definition of NP-complete, it appears impossible to prove that a problem L is NP-Complete.  
-------> By definition, it requires us to that show every problem in NP is polynomial time reducible to L.   
-------> Fortunately, there is an alternate way to prove it.   The idea is to take a known NP-Complete problem and reduce it to L.  
-------> If a polynomial-time reduction is possible, 
-------> we can prove that L is NP-Complete by transitivity of reduction (If an NP-Complete problem is reducible to L in polynomial time, 
-------> then all problems are reducible to L in polynomial time). 

-----> What was the first problem proved as NP-Complete? 
-------> There must be some first NP-Complete problem proved by the definition of NP-Complete problems.  
-------> SAT (Boolean satisfiability problem) is the first NP-Complete problem proved by Cook (See CLRS book for proof). 

---> It is always useful to know about NP-Completeness even for engineers. 
-----> Suppose you are asked to write an efficient algorithm to solve an extremely important problem for your company. 
-----> After a lot of thinking, you can only come up exponential time approach which is impractical. 
-----> If you don’t know about NP-Completeness, you can only say that I could not come up with an efficient algorithm. 
-----> If you know about NP-Completeness and prove that the problem is NP-complete, you can proudly say that the polynomial-time solution is unlikely to exist. 
-----> If there is a polynomial-time solution possible, then that solution solves a big problem of computer science many scientists have been trying for years. 





-> List of Algorithms (including my notes)



-> Combinatorial algorithms

---> General combinatorial algorithms

-----> Brent's algorithm: 
-------> This finds a cycle in function value iterations using only two iterators.
-------> This is an alternative cycle detection algorithm that, like the tortoise and hare algorithm, requires only two pointers into the sequence.
-------> It has two advantages compared to the tortoise and hare algorithm: 
---------> It finds the correct length λ of the cycle directly, rather than needing to search for it in a subsequent stage, 
---------> and its steps involve only one evaluation of f rather than three (faster).
------->  Its much faster because it relies on power of two for changing the position of turtoise. 

-----> Floyd's cycle-finding algorithm: 
-------> This finds a cycle in function value iterations
-------> This is tortoise and hare algorithm
-------> This is a pointer algorithm that uses only two pointers, which move through the sequence at different speeds (typically the turtoise moves one and the hare moves two). 

-----> Gale–Shapley algorithm: 
-------> This solves the stable marriage problem
---------> The stable marriage problem is the problem of finding a stable matching between two equally sized sets of elements given an ordering of preferences for each element. 
---------> A matching is a bijection from the elements of one set to the elements of the other set. 
---------> A matching is not stable if:
-----------> There is an element A of the first matched set which prefers some given element B of the second matched set over the element to which A is already matched, and
-----------> B also prefers A over the element to which B is already matched.
-------> In other words, a matching is stable when there does not exist any pair (A, B) which both prefer each other to their current partner under the matching.
-------> The stable marriage problem has been stated as follows:
---------> Given n men and n women, where each person has ranked all members of the opposite sex in order of preference, 
---------> marry the men and women together such that there are no two people of opposite sex who would both rather have each other than their current partners.
---------> When there are no such pairs of people, the set of marriages is deemed stable.
-------> Applications: Algorithms for finding solutions to the stable marriage problem have applications in a variety of real-world situations, 
---------> perhaps the best known of these being in the assignment of graduating medical students to their first hospital appointments
-------> The Gale–Shapley algorithm (also known as the deferred acceptance algorithm or propose-and-reject algorithm) is an algorithm for finding a solution to the stable matching problem.
---------> It takes polynomial time, and the time is linear in the size of the input to the algorithm.
-------> The Gale–Shapley algorithm involves a number of "rounds" (or "iterations"):
---------> (1) In the first round, first 
-----------> a) each unengaged man proposes to the woman he prefers most, and then 
-----------> b) each woman replies "maybe" to her suitor she most prefers and "no" to all other suitors. 
-----------> She is then provisionally "engaged" to the suitor she most prefers so far, and that suitor is likewise provisionally engaged to her.
---------> (2) In each subsequent round, first 
-----------> a) each unengaged man proposes to the most-preferred woman to whom he has not yet proposed (regardless of whether the woman is already engaged), 
-----------> and then b) each woman replies "maybe" if she is currently not engaged or if she prefers this man over her current provisional partner 
-------------> (in this case, she rejects her current provisional partner who becomes unengaged). 
-----------> The provisional nature of engagements preserves the right of an already-engaged woman to "trade up" (and, in the process, to "jilt" her until-then partner).
---------> (3) This process is repeated until everyone is engaged.
-------> The runtime complexity of this algorithm is O(n^2) where n is the number of men or women. 
---------> Since the input preference lists also have size proportional to n^2, the runtime is linear in the input size.
-------> This algorithm guarantees that:
---------> Everyone gets married
-----------> At the end, there cannot be a man and a woman both unengaged, as he must have proposed to her at some point (since a man will eventually propose to everyone, if necessary) 
-------------> and, being proposed to, she would necessarily be engaged (to someone) thereafter.
---------> The marriages are stable
-----------> Let Alice and Bob both be engaged, but not to each other. 
-----------> Upon completion of the algorithm, it is not possible for both Alice and Bob to prefer each other over their current partners. 
-------------> If Bob prefers Alice to his current partner, he must have proposed to Alice before he proposed to his current partner. 
-------------> If Alice accepted his proposal, yet is not married to him at the end, she must have dumped him for someone she likes more, and therefore doesn't like Bob more than her current partner. 
-------------> If Alice rejected his proposal, she was already with someone she liked more than Bob.
-------> Note: The stable marriage problem is also called the stable matching problem or SMP.
 
-----> Pseudorandom number generators:

-------> ACORN generator
---------> The ACORN or ″Additive Congruential Random Number″ generators are a robust family of PRNGs (pseudorandom number generators) for sequences of uniformly distributed pseudo-random numbers, 
---------> The main advantages of ACORN are simplicity of concept and coding, speed of execution, long period length, and mathematically proven convergence.
---------> In testing, ACORN performs extremely well, for appropriate parameters. However in its present form, ACORN has not been shown to be suitable for cryptography.
---------> The ACORN was introduced in 1989 and still valid in 2019, thirty years later. 

-------> Blum Blum Shub
---------> Blum Blum Shub (B.B.S.) is a pseudorandom number generator takes the form:
-----------> x[n+1] = x[n]^2 mod M 
-----------> where M = pq is the product of two large primes p and q. 
-----------> At each step of the algorithm, some output is derived from xn+1; the output is commonly either the bit parity of xn+1 or one or more of the least significant bits of xn+1.
---------> The seed x0 should be an integer that is co-prime to M (i.e. p and q are not factors of x0) and not 1 or 0.
---------> The two primes, p and q, should both be congruent to 3 (mod 4) (this guarantees that each quadratic residue has one square root which is also a quadratic residue),
---------> and should be safe primes with a small gcd((p-3)/2, (q-3)/2) (this makes the cycle length large).
---------> An interesting characteristic of the Blum Blum Shub generator is the possibility to calculate any xi value directly (via Euler's theorem)
---------> Security: There is a proof reducing its security to the computational difficulty of factoring.
---------> Note: The Blum Blum Shub generator wasproposed in 1986 by Lenore Blum, Manuel Blum and Michael Shub that is derived from Michael O. Rabin's one-way function. 

-------> Lagged Fibonacci generator
---------> A Lagged Fibonacci generator (LFG or sometimes LFib) is an example of a pseudorandom number generator. 
---------> This class of random number generator is aimed at being an improvement on the 'standard' linear congruential generator.
---------> These are based on a generalisation of the Fibonacci sequence. 
---------> The Fibonacci sequence may be described by the recurrence relation:
-----------> S[n] = S[n−1] + S[n−2]
---------> Hence, the new term is the sum of the last two terms in the sequence. 
---------> This can be generalised to the sequence:
-----------> S[n] ≡ S[n−j] BinaryOperator S[n−k] ( mod m ), 0<j<k
---------> In which case, the new term is some combination of any two previous terms. 
---------> m is usually a power of 2 (m = 2M), often 2^32 or 2^64. 
---------> The operator denotes a general binary operation. 
---------> This may be either addition, subtraction, multiplication, or the bitwise exclusive-or operator (XOR). 
---------> The theory of this type of generator is rather complex, and it may not be sufficient simply to choose random values for j and k.
---------> These generators also tend to be very sensitive to initialization. 

-------> Linear congruential generator
---------> A linear congruential generator (LCG) is an algorithm that yields a sequence of pseudo-randomized numbers calculated with a discontinuous piecewise linear equation. 
---------> The method represents one of the oldest and best-known pseudorandom number generator algorithms. 
---------> The theory behind them is relatively easy to understand, and they are easily implemented and fast, especially on computer hardware which can provide modular arithmetic by storage-bit truncation.
---------> The generator is defined by the recurrence relation:
-----------> X[n+1] = ( a X[n] + c ) mod m
-----------> where X is the sequence of pseudo-random values, and
-------------> m , 0 < m — the "modulus"
-------------> a , 0 < a < m — the "multiplier"
-------------> c , 0 ≤ c < m  — the "increment"
-------------> X 0 , 0 ≤ X 0 < m  — the "seed" or "start value"
-------------> are integer constants that specify the generator. 
-------------> If c = 0, the generator is often called a multiplicative congruential generator (MCG), or Lehmer RNG. 
-------------> If c ≠ 0, the method is called a mixed congruential generator.[1]: 4- 
-------------> When c ≠ 0, a mathematician would call the recurrence an affine transformation, not a linear one, but the misnomer is well-established in computer science.

-------> Mersenne Twister
---------> The Mersenne Twister is a general-purpose pseudorandom number generator (PRNG)
---------> Its name derives from the fact that its period length is chosen to be a Mersenne prime.
---------> The Mersenne Twister was designed specifically to rectify most of the flaws found in older PRNGs.
---------> The most commonly used version of the Mersenne Twister algorithm is based on the Mersenne prime. 
---------> The standard implementation of that, MT19937, uses a 32-bit word length.
---------> There is another implementation that uses a 64-bit word length, MT19937-64; it generates a different sequence. 
---------> Note: C++ has this random generator.
---------> Note: This was developed in 1997 by Makoto Matsumoto [ja] (松本 眞) and Takuji Nishimura (西村 拓士). 

 

---> Graph algorithms

-----> Coloring algorithm: 
-------> In graph theory, graph coloring is a special case of graph labeling; it is an assignment of labels traditionally called "colors" to elements of a graph subject to certain constraints. 
-------> In its simplest form, it is a way of coloring the vertices of a graph such that no two adjacent vertices are of the same color; this is called a vertex coloring. 
-------> Similarly, an edge coloring assigns a color to each edge so that no two adjacent edges are of the same color, 
-------> and a face coloring of a planar graph assigns a color to each face or region so that no two faces that share a boundary have the same color. 

-----> Hopcroft–Karp algorithm: 
-------> This convert a bipartite graph to a maximum cardinality matching.
-------> The "Maximum cardinality matching" is a set of as many edges as possible with the property that no two edges share an endpoint. 
-------> This is an algorithm that takes a bipartite graph as input and produces a maximum cardinality matching as its output 
---------> It runs in O(|E|sqrt(|V|))) time in the worst case, 
-----------> where E  E is set of edges in the graph, V is set of vertices of the graph, and it is assumed that |E| = Ω (|V|)  .
-----------> In the case of dense graphs the time bound becomes O (|V|^2.5)), and for sparse random graphs it runs in time O (|E| log⁡ (|V|)).
-------> The algorithm may be expressed in the following pseudocode:
---------> Input: Bipartite graph G ( U ∪ V , E )
---------> Output: Matching M is subset of E 
---------> M ← ∅ 
---------> repeat
-----------> P ← { P 1 , P 2 , … , P k }  maximal set of vertex-disjoint shortest augmenting paths
-----------> M ← M ⊕ ( P 1 ∪ P 2 ∪ ⋯ ∪ P k )
---------> until P = nullset
-------> In more detail, let U and V be the two sets in the bipartition of G, and let the matching from U to V at any time be represented as the set M. 
-------> The algorithm is run in phases. 
-------> Each phase consists of the following steps:
-------> (1) A breadth-first search partitions the vertices of the graph into layers. 
---------> The free vertices in U are used as the starting vertices of this search and form the first layer of the partitioning. 
---------> At the first level of the search, there are only unmatched edges, since the free vertices in U are by definition not adjacent to any matched edges. 
---------> At subsequent levels of the search, the traversed edges are required to alternate between matched and unmatched. 
---------> That is, when searching for successors from a vertex in U, only unmatched edges may be traversed, while from a vertex in V only matched edges may be traversed. 
---------> The search terminates at the first layer k where one or more free vertices in V are reached.
-------> (2) All free vertices in V at layer k are collected into a set F. 
---------> That is, a vertex V is put into F if and only if it ends a shortest augmenting path.
-------> (3) The algorithm finds a maximal set of vertex disjoint augmenting paths of length k. 
---------> Note: Maximal means that no more such paths can be added. 
-----------> This is different from finding the maximum number of such paths, which would be harder to do. 
-----------> Fortunately, it is sufficient here to find a maximal set of paths.)
---------> This set may be computed by depth first search (DFS) from F to the free vertices in U, using the breadth first layering to guide the search: 
---------> the DFS is only allowed to follow edges that lead to an unused vertex in the previous layer, and paths in the DFS tree must alternate between matched and unmatched edges. 
---------> Once an augmenting path is found that involves one of the vertices in F, the DFS is continued from the next starting vertex. 
---------> Any vertex encountered during the DFS can immediately be marked as used, since if there is no path from it to U at the current point in the DFS, 
---------> then that vertex can't be used to reach U at any other point in the DFS.
---------> This ensures O (|E|)) running time for the DFS. 
---------> It is also possible to work in the other direction, from free vertices in U to those in V, which is the variant used in the pseudocode.
-------> (4) Every one of the paths found in this way is used to enlarge M.
-------> Note: This sometimes more accurately called the Hopcroft–Karp–Karzanov algorithm.
 
-----> Hungarian algorithm:
-------> This is an algorithm for finding a perfect matching.
-------> The Hungarian method is a combinatorial optimization algorithm that solves the assignment problem in polynomial time and which anticipated later primal–dual methods. 
-------> The assignment problem in its most general form, the problem is as follows:
---------> The problem instance has a number of agents and a number of tasks. 
---------> Any agent can be assigned to perform any task,  incurring some cost that may vary depending on the agent-task assignment. 
---------> It is required to perform as many tasks as possible by assigning at most one agent to each task and at most one task to each agent, in such a way that the total cost of the assignment is minimized.
---------> Alternatively, describing the problem using graph theory:
-----------> The assignment problem consists of finding, in a weighted bipartite graph, a matching of a given size, in which the sum of weights of the edges is minimum.
-------> The cost of each perfect matching is at least the value of each potential: 
---------> the total cost of the matching is the sum of costs of all edges; 
---------> the cost of each edge is at least the sum of potentials of its endpoints; since the matching is perfect, 
---------> each vertex is an endpoint of exactly one edge; hence the total cost is at least the total potential. 
-------> Example: 
---------> In this simple example there are three workers: Paul, Dave, and Chris. 
---------> One of them has to clean the bathroom, another sweep the floors and the third washes the windows, but they each demand different pay for the various tasks. 
---------> The problem is to find the lowest-cost way to assign the jobs. The problem can be represented in a matrix of the costs of the workers doing the jobs. 
---------> For example:
--------->          |  Clean bathroom | Sweep floors | Wash windows
---------> | Paul   |$2               | $3           | $3
---------> | Dave   |$3               | $2           | $3
---------> | Chris  |$3               | $3           | $2 
-------> Note: This was developed and published in 1955 by Harold Kuhn, who gave the name "Hungarian method" because of the two Hungarian mathematicians: Dénes Kőnig and Jenő Egerváry

----> Prüfer coding: conversion between a labeled tree and its Prüfer sequence
-----> In combinatorial mathematics, the Prüfer sequence (also Prüfer code or Prüfer numbers) of a labeled tree is a unique sequence associated with the tree. 
-----> The sequence for a tree on n vertices has length n − 2, and can be generated by a simple iterative algorithm. 
-----> This can be use to convert a graph into a Prüfer sequence and vice versa
-----> Algorithms:
-------> (1) Algorithm to convert a tree into a Prüfer sequence
---------> One can generate a labeled tree's Prüfer sequence by iteratively removing vertices from the tree until only two vertices remain. 
---------> Specifically, consider a labeled tree T with vertices {1, 2, ..., n}. At step i, remove the leaf with the smallest label and set the ith element of the Prüfer sequence to be the label of this leaf's neighbour.
---------> The Prüfer sequence of a labeled tree is unique and has length n − 2.
---------> Both coding and decoding can be reduced to integer radix sorting and parallelized.[2] 
-------> (2) Algorithm to convert a Prüfer sequence into a tree
---------> Let {a[1], a[2], ..., a[n]} be a Prüfer sequence:
---------> The tree will have n+2 nodes, numbered from 1 to n+2. For each node set its degree to the number of times it appears in the sequence plus 1. For instance, in pseudo-code:
-----------> Convert-Prüfer-to-Tree(a)
----------->  1 n ← length[a]
----------->  2 T ← a graph with n + 2 isolated nodes, numbered 1 to n + 2
----------->  3 degree ← an array of integers
----------->  4 for each node i in T do
----------->  5     degree[i] ← 1
----------->  6 for each value i in a do
----------->  7     degree[i] ← degree[i] + 1
----------->  8 for each value i in a do
----------->  9     for each node j in T do
-----------> 10         if degree[j] = 1 then
-----------> 11             Insert edge[i, j] into T
-----------> 12             degree[i] ← degree[i] - 1
-----------> 13             degree[j] ← degree[j] - 1
-----------> 14             break
-----------> 15 u ← v ← 0
-----------> 16 for each node i in T
-----------> 17     if degree[i] = 1 then
-----------> 18         if u = 0 then
-----------> 19             u ← i
-----------> 20         else
-----------> 21             v ← i
-----------> 22             break
-----------> 23 Insert edge[u, v] into T
-----------> 24 degree[u] ← degree[u] - 1
-----------> 25 degree[v] ← degree[v] - 1
-----------> 26 return T
-----> Note: Prüfer sequences were first used by Heinz Prüfer to prove Cayley's formula in 1918

-----> Tarjan's off-line lowest common ancestors algorithm: 
-------> This computes lowest common ancestors for pairs of nodes in a tree
-------> This is an algorithm for computing lowest common ancestors for pairs of nodes in a tree, based on the union-find data structure. 
-------> The lowest common ancestor of two nodes d and e in a rooted tree T is the node g that is an ancestor of both d and e and that has the greatest depth in T. 
-------> Tarjan's algorithm is an offline algorithm; that is, unlike other lowest common ancestor algorithms, 
-------> it requires that all pairs of nodes for which the lowest common ancestor is desired must be specified in advance. 
-------> The simplest version of the algorithm uses the union-find data structure, 
-------> which unlike other lowest common ancestor data structures can take more than constant time per operation when the number of pairs of nodes is similar in magnitude to the number of nodes. 
-------> A later refinement by Gabow & Tarjan (1983) speeds the algorithm up to linear time. 
-------> The pseudocode below determines the lowest common ancestor of each pair in P, given the root r of a tree in which the children of node n are in the set n.children. 
---------> For this offline algorithm, the set P must be specified in advance. It uses the MakeSet, Find, and Union functions of a disjoint-set forest. 
---------> MakeSet(u) removes u to a singleton set, Find(u) returns the standard representative of the set containing u, and Union(u,v) merges the set containing u with the set containing v. 
---------> TarjanOLCA(r) is first called on the root r.
---------> function TarjanOLCA(u) is
--------->     MakeSet(u)
--------->     u.ancestor := u
--------->     for each v in u.children do
--------->         TarjanOLCA(v)
--------->         Union(u, v)
--------->         Find(u).ancestor := u
--------->     u.color := black
--------->     for each v such that {u, v} in P do
--------->         if v.color == black then
--------->             print "Tarjan's Lowest Common Ancestor of " + u +
--------->                   " and " + v + " is " + Find(v).ancestor + "."
-------> Note: This was named after Robert Tarjan, who discovered the technique in 1979. 

-----> Topological sort: 
-------> This finds linear order of nodes (e.g. jobs) based on their dependencies.
-------> A topological sort or topological ordering of a directed graph is a linear ordering of its vertices such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. 
-------> For instance, the vertices of the graph may represent tasks to be performed, and the edges may represent constraints that one task must be performed before another; 
-------> in this application, a topological ordering is just a valid sequence for the tasks. 
-------> Precisely, a topological sort is a graph traversal in which each node v is visited only after all its dependencies are visited. 
-------> A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). 
-------> Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time.



---> Graph drawing

-----> Force-based algorithms
-------> This also known as force-directed algorithms or spring-based algorithm.
---------> Force-directed graph drawing algorithms are a class of algorithms for drawing graphs in an aesthetically-pleasing way. 
---------> Their purpose is to position the nodes of a graph in two-dimensional or three-dimensional space 
---------> so that all the edges are of more or less equal length and there are as few crossing edges as possible, 
---------> by assigning forces among the set of edges and the set of nodes, based on their relative positions, 
---------> and then using these forces either to simulate the motion of the edges and nodes or to minimize their energy.

-----> Spectral layout
-------> Spectral layout is a class of algorithm for drawing graphs. 
---------> The layout uses the eigenvectors of a matrix, such as the Laplace matrix of the graph, as Cartesian coordinates of the graph's vertices.
-------> The idea of the layout is to compute the two largest (or smallest) eigenvalues and corresponding eigenvectors of the Laplacian matrix of the graph and then use those for actually placing the nodes. 
---------> Usually nodes are placed in the 2 dimensional plane. 
---------> An embedding into more dimensions can be found by using more eigenvectors. 
---------> In the 2-dimensional case, for a given node which corresponds to the row/column i in the (symmetric) Laplacian matrix x of the graph, 
---------> the x and y-coordinates are the i-th entries of the first and second eigenvectors of x, respectively. 




---> Network theory

-----> Network analysis

-------> Link analysis

---------> Girvan–Newman algorithm: 
-----------> This detect communities in complex systems.
-----------> The Girvan–Newman algorithm detects communities by progressively removing edges from the original network. 
-----------> The connected components of the remaining network are the communities. 
-----------> Instead of trying to construct a measure that tells us which edges are the most central to communities, 
-----------> the Girvan–Newman algorithm focuses on edges that are most likely "between" communities.
-----------> The algorithm's steps for community detection are summarized below:
-------------> (1) The betweenness of all existing edges in the network is calculated first.
-------------> (2) The edge(s) with the highest betweenness are removed.
-------------> (3) The betweenness of all edges affected by the removal is recalculated.
-------------> (4) Steps 2 and 3 are repeated until no edges remain.
-----------> The end result of the Girvan–Newman algorithm is a dendrogram. 
-----------> As the Girvan–Newman algorithm runs, the dendrogram is produced from the top down (i.e. the network splits up into different communities with the successive removal of links). 
-----------> The leaves of the dendrogram are individual nodes. 
 
 
 
 
---------> Web link analysis

-----------> Hyperlink-Induced Topic Search (HITS)
-------------> Hyperlink-Induced Topic Search (HITS; also known as hubs and authorities) is a link analysis algorithm that rates Web pages, developed by Jon Kleinberg. 
-------------> The idea behind Hubs and Authorities stemmed from a particular insight into the creation of web pages when the Internet was originally forming; 
-------------> that is, certain web pages, known as hubs, served as large directories that were not actually authoritative in the information that they held,
-------------> but were used as compilations of a broad catalog of information that led users direct to other authoritative pages. 
-------------> In other words, a good hub represents a page that pointed to many other pages, while a good authority represents a page that is linked by many different hubs.
-------------> The scheme therefore assigns two scores for each page: its authority, which estimates the value of the content of the page, and its hub value, which estimates the value of its links to other pages. 
-------------> Algorithm
---------------> In the HITS algorithm, the first step is to retrieve the most relevant pages to the search query. 
-----------------> This set is called the root set and can be obtained by taking the top pages returned by a text-based search algorithm. 
-----------------> A base set is generated by augmenting the root set with all the web pages that are linked from it and some of the pages that link to it. 
-----------------> The web pages in the base set and all hyperlinks among those pages form a focused subgraph. 
-----------------> The HITS computation is performed only on this focused subgraph. 
-----------------> According to Kleinberg the reason for constructing a base set is to ensure that most (or many) of the strongest authorities are included.
---------------> Authority and hub values are defined in terms of one another in a mutual recursion. 
-----------------> An authority value is computed as the sum of the scaled hub values that point to that page. 
-----------------> A hub value is the sum of the scaled authority values of the pages it points to. 
-----------------> Some implementations also consider the relevance of the linked pages.
---------------> The algorithm performs a series of iterations, each consisting of two basic steps:
-----------------> (1) Authority update: 
-------------------> Update each node's authority score to be equal to the sum of the hub scores of each node that points to it. 
-------------------> That is, a node is given a high authority score by being linked from pages that are recognized as Hubs for information.
-----------------> (2) Hub update: 
-------------------> Update each node's hub score to be equal to the sum of the authority scores of each node that it points to. 
-------------------> That is, a node is given a high hub score by linking to nodes that are considered to be authorities on the subject.
---------------> The Hub score and Authority score for a node is calculated with the following algorithm:
-----------------> Start with each node having a hub score and authority score of 1.
-----------------> Run the authority update rule
-----------------> Run the hub update rule
-----------------> Normalize the values by dividing each Hub score by square root of the sum of the squares of all Hub scores, 
-----------------> and dividing each Authority score by square root of the sum of the squares of all Authority scores.
-----------------> Repeat from the second step as necessary.
---------------> HITS, like Page and Brin's PageRank, is an iterative algorithm based on the linkage of the documents on the web.
-----------------> However it does have some major differences:
-------------------> It is query dependent, that is, the (Hubs and Authority) scores resulting from the link analysis are influenced by the search terms;
-------------------> As a corollary, it is executed at query time, not at indexing time, with the associated hit on performance that accompanies query-time processing.
-------------------> It is not commonly used by search engines. (Though a similar algorithm was said to be used by Teoma, which was acquired by Ask Jeeves/Ask.com.)
-------------------> It computes two scores per document, hub and authority, as opposed to a single score;
-------------------> It is processed on a small subset of ‘relevant’ documents (a 'focused subgraph' or base set), not all documents as was the case with PageRank.

-----------> PageRank
-------------> PageRank (PR) is an algorithm used by Google Search to rank web pages in their search engine results. 
---------------> It is named after both the term "web page" and co-founder Larry Page. 
---------------> PageRank is a way of measuring the importance of website pages.
-------------> According to Google:
---------------> PageRank works by counting the number and quality of links to a page to determine a rough estimate of how important the website is. 
---------------> The underlying assumption is that more important websites are likely to receive more links from other websites.
-------------> Currently, PageRank is not the only algorithm used by Google to order search results, 
---------------> but it is the first algorithm that was used by the company, and it is the best known.
---------------> As of September 24, 2019, PageRank and all associated patents are expired.
-------------> Description:
---------------> The size of each face is proportional to the total size of the other faces which are pointing to it.
---------------> PageRank is a link analysis algorithm and it assigns a numerical weighting to each element of a hyperlinked set of documents, 
-----------------> such as the World Wide Web, with the purpose of "measuring" its relative importance within the set.
-----------------> The algorithm may be applied to any collection of entities with reciprocal quotations and references. 
-----------------> The numerical weight that it assigns to any given element E is referred to as the PageRank of E and denoted by PR(E).
---------------> A PageRank results from a mathematical algorithm based on the webgraph, created by all World Wide Web pages as nodes and hyperlinks as edges, 
-----------------> taking into consideration authority hubs such as cnn.com or mayoclinic.org. 
-----------------> The rank value indicates an importance of a particular page. A hyperlink to a page counts as a vote of support. 
-----------------> The PageRank of a page is defined recursively and depends on the number and PageRank metric of all pages that link to it ("incoming links"). 
-----------------> A page that is linked to by many pages with high PageRank receives a high rank itself.
---------------> Numerous academic papers concerning PageRank have been published since Page and Brin's original paper. 
-----------------> In practice, the PageRank concept may be vulnerable to manipulation. 
-----------------> Research has been conducted into identifying falsely influenced PageRank rankings. 
-----------------> The goal is to find an effective means of ignoring links from documents with falsely influenced PageRank.
-------------> Algorithm:
---------------> The PageRank algorithm outputs a probability distribution used to represent the likelihood that a person randomly clicking on links will arrive at any particular page. 
-----------------> PageRank can be calculated for collections of documents of any size. 
-----------------> It is assumed in several research papers that the distribution is evenly divided among all documents in the collection at the beginning of the computational process. 
-----------------> The PageRank computations require several passes, called "iterations", through the collection to adjust approximate PageRank values to more closely reflect the theoretical true value.
---------------> A probability is expressed as a numeric value between 0 and 1. 
-----------------> A 0.5 probability is commonly expressed as a "50% chance" of something happening. 
-----------------> Hence, a document with a PageRank of 0.5 means there is a 50% chance that a person clicking on a random link will be directed to said document. 
-------------> Simplified algorithm
---------------> Assume a small universe of four web pages: A, B, C, and D. Links from a page to itself are ignored. 
-----------------> Multiple outbound links from one page to another page are treated as a single link. PageRank is initialized to the same value for all pages. 
-----------------> In the original form of PageRank, the sum of PageRank over all pages was the total number of pages on the web at that time, so each page in this example would have an initial value of 1.
-----------------> However, later versions of PageRank, and the remainder of this section, assume a probability distribution between 0 and 1. 
-----------------> Hence the initial value for each page in this example is 0.25.
---------------> The PageRank transferred from a given page to the targets of its outbound links upon the next iteration is divided equally among all outbound links.
---------------> If the only links in the system were from pages B, C, and D to A, each link would transfer 0.25 PageRank to A upon the next iteration, for a total of 0.75.
-----------------> PR(A) = PR(B) + PR(C) + PR(D).
---------------> Suppose instead that page B had a link to pages C and A, page C had a link to page A, and page D had links to all three pages. 
---------------> Thus, upon the first iteration, page B would transfer half of its existing value, or 0.125, to page A and the other half, or 0.125, to page C. 
---------------> Page C would transfer all of its existing value, 0.25, to the only page it links to, A. 
---------------> Since D had three outbound links, it would transfer one third of its existing value, or approximately 0.083, to A.
---------------> At the completion of this iteration, page A will have a PageRank of approximately 0.458.
-----------------> PR(A) = PR(B)/2 + PR(C)/1 + PR(D)/3.
---------------> In other words, the PageRank conferred by an outbound link is equal to the document's own PageRank score divided by the number of outbound links L( ).
-----------------> PR(A) = PR(B)/L(B) + PR(C)/L(C) + PR(D)/L(D). 
---------------> In the general case, the PageRank value for any page u can be expressed as:
-----------------> PR(u) = ∑ v PR(v)/L(v) 
---------------> i.e. the PageRank value for a page u is dependent on the PageRank values for each page v 
-----------------> contained in the set Bu (the set containing all pages linking to page u), divided by the number L(v) of links from page v. 

-----------> TrustRank
-------------> TrustRank is an algorithm that conducts link analysis to separate useful webpages from spam and helps search engine rank pages in SERPs (Search Engine Results Pages).
---------------> It is semi-automated process which means that it needs some human assistance in order to function properly. 
---------------> Search engines have many different algorithms and ranking factors that they use when measuring the quality of webpages. 
---------------> TrustRank is one of them.
-------------> Because manual review of the Internet is impractical and very expensive, TrustRank was introduced in order to help achieve this task much faster and cheaper. 
---------------> It was first introduced by researchers Zoltan Gyongyi and Hector Garcia-Molina of Stanford University and Jan Pedersen of Yahoo! in their paper "Combating Web Spam with TrustRank" in 2004.
---------------> Today, this algorithm is a part of major web search engines like Yahoo! and Google.
-------------> One of the most important factors that help web search engine determine the quality of a web page when returning results are backlinks. 
---------------> Search engines take a number and quality of backlinks into consideration when assigning a place to a certain web page in SERPs. 
---------------> Many web spam pages are created only with the intention of misleading search engines. 
---------------> These pages, chiefly created for commercial reasons, use various techniques to achieve higher-than-deserved rankings in the search engines' result pages. 
---------------> While human experts can easily identify spam, search engines are still being improved daily in order to do it without help of humans.
-------------> One popular method for improving rankings is to increase the perceived importance of a document through complex linking schemes. 
---------------> Google's PageRank and other search ranking algorithms have been subjected to such manipulation.
-------------> TrustRank seeks to combat spam by filtering the web based upon reliability. 
---------------> The method calls for selecting a small set of seed pages to be evaluated by an expert.
---------------> Once the reputable seed pages are manually identified, a crawl extending outward from the seed set seeks out similarly reliable and trustworthy pages. 
---------------> TrustRank's reliability diminishes with increased distance between documents and the seed set.
-------------> The logic works in the opposite way as well, which is called Anti-Trust Rank. 
---------------> The closer a site is to spam resources, the more likely it is to be spam as well.
-------------> The researchers who proposed the TrustRank methodology have continued to refine their work by evaluating related topics, such as measuring spam mass. 




-----> Flow networks

-------> Dinic's algorithm:
---------> This is a strongly polynomial algorithm for computing the maximum flow in a flow network.
---------> Dinic's algorithm or Dinitz's algorithm is a strongly polynomial algorithm for computing the maximum flow in a flow network.
---------> The algorithm runs in O(V^2*E) time and is similar to the Edmonds–Karp algorithm, which runs in O (V*E^2) time, in that it uses shortest augmenting paths. 
---------> The introduction of the concepts of the level graph and blocking flow enable Dinic's algorithm to achieve its performance.
---------> Dinic's Algorithm
-----------> Terms: G is graph, Gf is residual graph, GL is the level graph, e is an edge, E is the set of all edges, f is the flow function, s is start, t is target, dist() is the shortest path 
-----------> Input: A network G = ((V, E), c, s, t).
-----------> Output: An s–t flow f of maximum value.
-------------> (1) Set f(e) = 0 for each e is an element of E.
-------------> (2) Construct GL from Gf of G. If dist ⁡ (t) = ∞, stop and output f.
-------------> (3) Find a blocking flow f' in GL.
-------------> (4) Add augment flow f by f' and go back to step 2.
---------> Analysis
-----------> It can be shown that the number of layers in each blocking flow increases by at least 1 each time and thus there are at most |V|-1 blocking flows in the algorithm. 
-------------> For each of them:
---------------> the level graph GL can be constructed by breadth-first search in O(E) time
---------------> a blocking flow in the level graph GL can be found in O(VE) time
-------------> with total running time O ( E + V E ) = O ( V E ) for each layer. 
-------------> As a consequence, the running time of Dinic's algorithm is O(V^2*E).
-----------> Using a data structure called dynamic trees, the running time of finding a blocking flow in each phase can be reduced to O(E log ⁡ V) 
-------------> and therefore the running time of Dinic's algorithm can be improved to O(V*E*log⁡ V). 
---------> Note: This was conceived in 1970 by Israeli (formerly Soviet) computer scientist Yefim (Chaim) A. Dinitz.

-------> Edmonds–Karp algorithm: 
---------> This is an implementation of Ford–Fulkerson
-----------> This is an implementation of the Ford–Fulkerson method for computing the maximum flow in a flow network in O (|V||E|^2) time.
-----------> Dinic's algorithm includes additional techniques that reduce the running time to O(|V|^2*|E|) 
---------> Algorithm
-----------> The algorithm is identical to the Ford–Fulkerson algorithm, except that the search order when finding the augmenting path is defined. 
-----------> The path found must be a shortest path that has available capacity. 
-----------> This can be found by a breadth-first search, where we apply a weight of 1 to each edge. 
-----------> The running time of O(|V||E|^2) is found by showing that each augmenting path can be found in O(|E|) time, 
-----------> that every time at least one of the E edges becomes saturated (an edge which has the maximum possible flow), 
-----------> that the distance from the saturated edge to the source along the augmenting path must be longer than last time it was saturated, and that the length is at most |V|.
-----------> Another property of this algorithm is that the length of the shortest augmenting path increases monotonically.
---------> Note: The algorithm was first published by Yefim Dinitz (whose name is also transliterated "E. A. Dinic", notably as author of his early papers) in 1970 
-----------> and independently published by Jack Edmonds and Richard Karp in 1972. 

-------> Ford–Fulkerson algorithm: 
---------> This computes the maximum flow in a graph
-----------> The Ford–Fulkerson method or Ford–Fulkerson algorithm (FFA) is a greedy algorithm that computes the maximum flow in a flow network. 
-----------> It is sometimes called a "method" instead of an "algorithm" as the approach to finding augmenting paths in a residual graph is not fully specified 
-----------> or it is specified in several implementations with different running times.
---------> Algorithm Ford–Fulkerson
-----------> Inputs Given a Network G=(V,E) with flow capacity c, a source node s, and a sink node t
-----------> Output Compute a flow f from s to t of maximum value
-------------> f(u,v) ← 0  for all edges (u,v)
-------------> While there is a path p from s to t in Gf, such that cf(u,v) > 0 for all edges (u,v) is an element of p:
------------->     Find cf(p) = min { cf(u, v) : (u, v) is an element of p }  (find the augmenting path)
------------->     For each edge (u , v) is an element of p
------------->         f(u, v) ← f(u, v) + cf(p) (Send flow along the path)
------------->         f(v, u) ← f(v, u) − cf(p) (The flow might be "returned" later)
-------------> 

-------> Karger's algorithm: 
---------> This is a Monte Carlo method to compute the minimum cut of a connected graph.
-----------> The idea of the algorithm is based on the concept of contraction of an edge(u,v) in an undirected graph G=(V, E). 
-------------> Informally speaking, the contraction of an edge merges the nodes u and v into one, reducing the total number of nodes of the graph by one. 
-------------> All other edges connecting either u or v are "reattached" to the merged node, effectively producing a multigraph. 
-------------> Karger's basic algorithm iteratively contracts randomly chosen edges until only two nodes remain; those nodes represent a cut in the original graph. 
-------------> By iterating this basic algorithm a sufficient number of times, a minimum cut can be found with high probability. 
---------> Note: This was invented by David Karger and first published in 1993.

-------> Push–relabel algorithm: 
---------> This computes a maximum flow in a graph.
-----------> In mathematical optimization, the push–relabel algorithm is an algorithm for computing maximum flows in a flow network. 
-----------> The name "push–relabel" comes from the two basic operations used in the algorithm. 
-----------> Throughout its execution, the algorithm maintains a "preflow" and gradually converts it into a maximum flow by moving flow locally between neighboring nodes
-----------> using push operations under the guidance of an admissible network maintained by relabel operations. 
-----------> In comparison, the Ford–Fulkerson algorithm performs global augmentations that send flow following paths from the source all the way to the sink.[1]
---------> The push–relabel algorithm is considered one of the most efficient maximum flow algorithms. 
-----------> The generic algorithm has a strongly polynomial O(V^2E) time complexity, which is asymptotically more efficient than the O(VE^2) Edmonds–Karp algorithm.
-----------> Specific variants of the algorithms achieve even lower time complexities. 
-----------> The variant based on the highest label node selection rule has O(V*2√E) time complexity and is generally regarded as the benchmark for maximum flow algorithms.
-----------> Subcubic O(V*E*log(V^2/E)) time complexity can be achieved using dynamic trees, although in practice it is less efficient
---------> The push–relabel algorithm has been extended to compute minimum cost flows.
-----------> The idea of distance labels has led to a more efficient augmenting path algorithm, 
-----------> which in turn can be incorporated back into the push–relabel algorithm to create a variant with even higher empirical performance.
---------> This is alternatively called the preflow–push algorithm.



---> Routing for graphs

-----> Edmonds' algorithm: 
-------> This finds the maximum or minimum branchings.
-------> This is an algorithm for finding a spanning arborescence of minimum weight (sometimes called an optimum branching). 
---------> An arborescence is an directed-graph form of a rooted tree of an undirected graph.
---------> It is the directed analog of the minimum spanning tree problem. 
-------> Algorithm Description
---------> The algorithm takes as input a directed graph D = <V, E>, where V is the set of nodes and E is the set of directed edges,
-----------> a distinguished vertex r is an element of V called the root, 
-----------> and a real-valued weight w(e) for each edge e is an element of E. 
-----------> It returns a spanning arborescence A rooted at r of minimum weight, 
-----------> where the weight of an arborescence is defined to be the sum of its edge weights, w(A) = ∑ w(e) where e is an element of A.
---------> The algorithm has a recursive description. 
-----------> Let f(D, r, w) denote the function which returns a spanning arborescence rooted at r of minimum weight. 
-----------> We first remove any edge from E whose destination is r. 
-----------> We may also replace any set of parallel edges (edges between the same pair of vertices in the same direction) 
-------------> by a single edge with weight equal to the minimum of the weights of these parallel edges.
---------> Now, for each node V other than the root, find the edge incoming to V of lowest weight (with ties broken arbitrarily). 
-----------> Denote the source of this edge by π(v)
-----------> If the set of edges P = {(π(v), v) | v is an element of V \ {r} } does not contain any cycles, then f(D, r, w) = P.
---------> Otherwise, P contains at least one cycle. 
-----------> Arbitrarily choose one of these cycles and call it C. 
-----------> We now define a new weighted directed graph D' = <V', E'>, in which the cycle C is "contracted" into one node as follows:
---------> The nodes of V' are the nodes of V not in C plus a new node denoted vC.
-----------> If (u, v) is an edge in E with u is not an element of C and v is an element of C (an edge coming into the cycle), 
-------------> then include in E' a new edge e = (u, vC), and define w′(e) = w(u, v) − w(π(v), v) .
-----------> If (u, v) is an edge in E with u is an element of C and v is not an element of C (an edge going away from the cycle), 
-------------> then include in E' a new edge e = (vC, v), and define w′(e) = w(u, v)
-----------> If (u, v) is an edge in E with u is not an element of C and v is not an element of C (an edge unrelated to the cycle), t
-------------> then include in E' a new edge e=(u, v), and define w′(e) = w(u, v)
---------> For each edge in E', we remember which edge in E it corresponds to.
---------> Now find a minimum spanning arborescence A' of D' using a call to f(D′, r, w′). 
-----------> Since A' is a spanning arborescence, each vertex has exactly one incoming edge. 
-----------> Let (u, vC) be the unique incoming edge to vC in A'. 
-----------> This edge corresponds to an edge (u, v) is an element of E with v is an element of C. 
-----------> Remove the edge (π(v), v) from C, breaking the cycle. 
-----------> Mark each remaining edge in C. 
-----------> For each edge in A', mark its corresponding edge in E. 
-----------> Now we define f(D, r, w) to be the set of marked edges, which form a minimum spanning arborescence.
---------> Observe that f(D, r, w) is defined in terms of f(D′, r, w′), with D' having strictly fewer vertices than D. 
-----------> Finding f(D, r, w) for a single-vertex graph is trivial (it is just D itself), so the recursive algorithm is guaranteed to terminate.
-----------> Running time
-----------> The running time of this algorithm is O(E*V).
-----------> A faster implementation of the algorithm due to Robert Tarjan runs in time O(E*logV)  for sparse graphs and O(V^2) for dense graphs. 
-----------> This is as fast as Prim's algorithm for an undirected minimum spanning tree. 
-----------> In 1986, Gabow, Galil, Spencer, and Tarjan produced a faster implementation, with running time O(E + V \log V). 
-------> Note: This is also known as Chu–Liu/Edmonds' algorithm.
---------> The algorithm was proposed independently first by Yoeng-Jin Chu and Tseng-Hong Liu (1965) and then by Jack Edmonds (1967). 

-----> Euclidean minimum spanning tree: 
-------> These algorithms for computing the minimum spanning tree of a set of points in the plane
-------> A Euclidean minimum spanning tree of a finite set of points in the Euclidean plane 
---------> or higher-dimensional Euclidean space connects the points by a system of line segments with the points as endpoints, 
---------> minimizing the total length of the selected segments. 
---------> In it, any two points can reach each other along a path through the line segments. 
---------> It can be found as the minimum spanning tree of a complete graph with the points as vertices and the Euclidean distances between points as edge weights.
-------> The edges of the minimum spanning tree meet at angles of at least 60°, at most six to a vertex. 
---------> In higher dimensions, the number of edges per vertex is controlled by the kissing number of tangent unit spheres. 
---------> The total length of the edges, for points in a unit square, is at most proportional to the square root of the number of points. 
---------> Each edge lies in an empty region of the plane, 
---------> and these regions can be used to prove that the Euclidean minimum spanning tree a subgraph of other geometric graphs including the relative neighborhood graph and Delaunay triangulation. 
---------> By constructing the Delaunay triangulation and then applying a graph minimum spanning tree algorithm, 
---------> a Euclidean minimum spanning tree for n given planar points may be found in time O(n log ⁡ n), as expressed in Big O notation. 
---------> Faster randomized algorithms are known for points with integer coordinates. 
---------> For points in higher dimensions, finding an optimal algorithm remains an open problem. 

-----> Longest path problem: 
-------> This finds a simple path of maximum length in a given graph.
-------> The longest path problem is the problem of finding a simple path of maximum length in a given graph. 
---------> A path is called simple if it does not have any repeated vertices.
---------> The length of a path may either be measured by its number of edges, or (in weighted graphs) by the sum of the weights of its edges. 
---------> In contrast to the shortest path problem, which can be solved in polynomial time in graphs without negative-weight cycles, 
---------> the longest path problem is NP-hard and the decision version of the problem, which asks whether a path exists of at least some given length, is NP-complete. 
---------> This means that the decision problem cannot be solved in polynomial time for arbitrary graphs unless P = NP. 
---------> Stronger hardness results are also known showing that it is difficult to approximate. 
-------> However, it has a linear time solution for directed acyclic graphs, which has important applications in finding the critical path in scheduling problems. 




-----> Minimum spanning tree
-------> A minimum spanning tree (MST) or minimum weight spanning tree is a subset of the edges of a connected, 
---------> edge-weighted undirected graph that connects all the vertices together, without any cycles and with the minimum possible total edge weight. 
---------> That is, it is a spanning tree whose sum of edge weights is as small as possible.
---------> More generally, any edge-weighted undirected graph (not necessarily connected) has a "Minimum Spanning Forest", which is a union of the minimum spanning trees for its connected components.
-------> There are many use cases for minimum spanning trees. 
---------> One example is a telecommunications company trying to lay cable in a new neighborhood. 
---------> If it is constrained to bury the cable only along certain paths (e.g. roads), then there would be a graph containing the points (e.g. houses) connected by those paths. 
---------> Some of the paths might be more expensive, because they are longer, or require the cable to be buried deeper; these paths would be represented by edges with larger weights. 
---------> Currency is an acceptable unit for edge weight – there is no requirement for edge lengths to obey normal rules of geometry such as the triangle inequality. 
---------> A spanning tree for that graph would be a subset of those paths that has no cycles but still connects every house; there might be several spanning trees possible. 
---------> A minimum spanning tree would be one with the lowest total cost, representing the least expensive path for laying the cable. 

-------> Borůvka's algorithm
---------> Borůvka's algorithm is a greedy algorithm for finding a minimum spanning tree in a graph, or a minimum spanning forest in the case of a graph that is not connected.
---------> The algorithm begins by finding the minimum-weight edge incident to each vertex of the graph, and adding all of those edges to the forest. 
-----------> Then, it repeats a similar process of finding the minimum-weight edge from each tree constructed so far to a different tree, and adding all of those edges to the forest. 
-----------> Each repetition of this process reduces the number of trees, within each connected component of the graph, 
-----------> to at most half of this former value, so after logarithmically many repetitions the process finishes. 
-----------> When it does, the set of edges it has added forms the minimum spanning forest. 
---------> Algorithm
-----------> The following pseudocode illustrates a basic implementation of Borůvka's algorithm. 
-------------> In the conditional clauses, every edge uv is considered cheaper than "None". 
-------------> The purpose of the completed variable is to determine whether the forest F is yet a spanning forest.
-----------> If edges do not have distinct weights, then a consistent tie-breaking rule must be used, e.g. based on some total order on vertices or edges. 
-------------> This can be achieved by representing vertices as integers and comparing them directly; comparing their memory addresses; etc. 
-------------> A tie-breaking rule is necessary to ensure that the created graph is indeed a forest, that is, it does not contain cycles. 
-------------> For example, consider a triangle graph with nodes {a,b,c} and all edges of weight 1. 
-------------> Then a cycle could be created if we select ab as the minimal weight edge for {a}, bc for {b}, and ca for {c}. 
-------------> A tie-breaking rule which orders edges first by source, then by destination, will prevent creation of a cycle, resulting in the minimal spanning tree {ab, bc}.
-----------> algorithm Borůvka is
----------->     input: A weighted undirected graph G = (V, E).
----------->     output: F, a minimum spanning forest of G.
----------->     Initialize a forest F to (V, E') where E' = {}.
----------->     completed := false
----------->     while not completed do
----------->         Find the connected components of F and assign to each vertex its component
----------->         Initialize the cheapest edge for each component to "None"
----------->         for each edge u, v in E, where u and v are in different components of F:
----------->             let wx be the cheapest edge for the component of u
----------->             if is-preferred-over(uv, wx) then
----------->                 Set uv as the cheapest edge for the component of u
----------->             let yz be the cheapest edge for the component of v
----------->             if is-preferred-over(uv, yz) then
----------->                 Set uv as the cheapest edge for the component of v
----------->         if all components have cheapest edge set to "None" then
----------->             // no more trees can be merged -- we are finished
----------->             completed := true
----------->         else
----------->             completed := false
----------->             for each component whose cheapest edge is not "None" do
----------->                 Add its cheapest edge to E'
-----------> function is-preferred-over(edge1, edge2) is
----------->     return (edge2 is "None") or
----------->            (weight(edge1) < weight(edge2)) or
----------->            (weight(edge1) = weight(edge2) and tie-breaking-rule(edge1, edge2))
-----------> function tie-breaking-rule(edge1, edge2) is
----------->     The tie-breaking rule; returns true if and only if edge1
----------->     is preferred over edge2 in the case of a tie.
-----------> As an optimization, one could remove from G each edge that is found to connect two vertices in the same component, 
-------------> so that it does not contribute to the time for searching for cheapest edges in later components. 
---------> Note: This was first published in 1926 by Otakar Borůvka as a method of constructing an efficient electricity network for Moravia. 
-----------> The algorithm was rediscovered by Choquet in 1938;[4] again by Florek, Łukasiewicz, Perkal, Steinhaus, and Zubrzycki in 1951; and again by Georges Sollin in 1965.
-----------> This algorithm is frequently called Sollin's algorithm, especially in the parallel computing literature.

-------> Kruskal's algorithm
---------> Kruskal's algorithm finds a minimum spanning forest of an undirected edge-weighted graph. 
-----------> If the graph is connected, it finds a minimum spanning tree. 
-----------> For a disconnected graph, a minimum spanning forest is composed of a minimum spanning tree for each connected component. 
-----------> It is a greedy algorithm in graph theory as in each step it adds the next lowest-weight edge that will not form a cycle to the minimum spanning forest.
---------> Algorithm
-----------> create a forest F (a set of trees), where each vertex in the graph is a separate tree
-----------> create a set S containing all the edges in the graph
-----------> while S is nonempty and F is not yet spanning
----------->     remove an edge with minimum weight from S
----------->     if the removed edge connects two different trees then add it to the forest F, combining two trees into a single tree
---------> At the termination of the algorithm, the forest forms a minimum spanning forest of the graph. 
-----------> If the graph is connected, the forest has a single component and forms a minimum spanning tree.
---------> Algorithm Pseudocode
-----------> A demo for Kruskal's algorithm on a complete graph with weights based on Euclidean distance.
-----------> The following code is implemented with a disjoint-set data structure. 
-----------> Here, we represent our forest F as a set of edges, and use the disjoint-set data structure to efficiently determine whether two vertices are part of the same tree.
-----------> algorithm Kruskal(G) is
----------->     F:= nullset
----------->     for each v is an element of G.V do
----------->         MAKE-SET(v)
----------->     for each (u, v) in G.E ordered by weight(u, v), increasing do
----------->         if FIND-SET(u) ≠ FIND-SET(v) then
----------->             F:= F ∪ {(u, v)} ∪ {(v, u)}
----------->             UNION(FIND-SET(u), FIND-SET(v))
----------->     return F
---------> Note: This algorithm first appeared in Proceedings of the American Mathematical Society, pp. 48–50 in 1956, and was written by Joseph Kruskal. 
-----------> It was rediscovered by Loberman & Weinberger (1957).

-------> Prim's algorithm
---------> Prim's algorithm is a greedy algorithm that finds a minimum spanning tree for a weighted undirected graph. 
-----------> This means it finds a subset of the edges that forms a tree that includes every vertex, where the total weight of all the edges in the tree is minimized. 
-----------> The algorithm operates by building this tree one vertex at a time, from an arbitrary starting vertex, at each step adding the cheapest possible connection from the tree to another vertex.
---------> The algorithm may informally be described as performing the following steps:
-----------> (1) Initialize a tree with a single vertex, chosen arbitrarily from the graph.
-----------> (2) Grow the tree by one edge: of the edges that connect the tree to vertices not yet in the tree, find the minimum-weight edge, and transfer it to the tree.
-----------> (3) Repeat step 2 (until all vertices are in the tree).
---------> In more detail, it may be implemented following the pseudocode below.
-----------> (1) Associate with each vertex v of the graph a number C[v] (the cheapest cost of a connection to v) and an edge E[v] (the edge providing that cheapest connection). 
-------------> To initialize these values, set all values of C[v] to +∞ (or to any number larger than the maximum edge weight) 
-------------> and set each E[v] to a special flag value indicating that there is no edge connecting v to earlier vertices.
-----------> (2) Initialize an empty forest F and a set Q of vertices that have not yet been included in F (initially, all vertices).
-----------> (3) Repeat the following steps until Q is empty:
-------------> (3.1) Find and remove a vertex v from Q having the minimum possible value of C[v]
-------------> (3.2) Add v to F
-------------> (3.3) Loop over the edges vw connecting v to other vertices w. 
-------------> For each such edge, if w still belongs to Q and vw has smaller weight than C[w], perform the following steps:
---------------> (3.3.1) Set C[w] to the cost of edge vw
---------------> (3.3.2) Set E[w] to point to edge vw.
-----------> (4) Return F
---------> As described above, the starting vertex for the algorithm will be chosen arbitrarily, 
-----------> because the first iteration of the main loop of the algorithm will have a set of vertices in Q that all have equal weights, 
-----------> and the algorithm will automatically start a new tree in F when it completes a spanning tree of each connected component of the input graph. 
-----------> The algorithm may be modified to start with any particular vertex s by setting C[s] to be a number smaller than the other values of C (for instance, zero), 
-----------> and it may be modified to only find a single spanning tree rather than an entire spanning forest (matching more closely the informal description) 
-----------> by stopping whenever it encounters another vertex flagged as having no associated edge.
---------> Different variations of the algorithm differ from each other in how the set Q is implemented: 
-----------> as a simple linked list or array of vertices, or as a more complicated priority queue data structure. 
-----------> This choice leads to differences in the time complexity of the algorithm. 
-----------> In general, a priority queue will be quicker at finding the vertex v with minimum cost, but will entail more expensive updates when the value of C[w] changes. 
---------> Note: This is also known as Jarník's algorithm
 
-------> Reverse-delete algorithm
---------> The reverse-delete algorithm is an algorithm in graph theory used to obtain a minimum spanning tree from a given connected, edge-weighted graph. 
-----------> If the graph is disconnected, this algorithm will find a minimum spanning tree for each disconnected part of the graph. 
-----------> The set of these minimum spanning trees is called a minimum spanning forest, which contains every vertex in the graph.
---------> This algorithm is a greedy algorithm, choosing the best choice given any situation. 
-----------> It is the reverse of Kruskal's algorithm, which is another greedy algorithm to find a minimum spanning tree.
-----------> Kruskal’s algorithm starts with an empty graph and adds edges while the Reverse-Delete algorithm starts with the original graph and deletes edges from it. 
---------> The algorithm works as follows:
----------->     Start with graph G, which contains a list of edges E.
----------->     Go through E in decreasing order of edge weights.
----------->     For each edge, check if deleting the edge will further disconnect the graph.
----------->     Perform any deletion that does not lead to additional disconnection.
-----------> function ReverseDelete(edges[] E) is
----------->     sort E in decreasing order
----------->     Define an index i ← 0
----------->     while i < size(E) do
----------->         Define edge ← E[i]
----------->     delete E[i]
----------->     if graph is not connected then
----------->                 E[i] ← edge
----------->                 i ← i + 1
----------->     return edges[] E
---------> In the above the graph is the set of edges E with each edge containing a weight and connected vertices v1 and v2. 
---------> Note: This first appeared in Kruskal (1956), but it should not be confused with Kruskal's algorithm which appears in the same paper. 

-----> Nonblocking minimal spanning switch
-------> A nonblocking minimal spanning switch is a device that can connect N inputs to N outputs in any combination. 
---------> The most familiar use of switches of this type is in a telephone exchange. 
---------> The term "non-blocking" means that if it is not defective, it can always make the connection. 
---------> The term "minimal" means that it has the fewest possible components, and therefore the minimal expense.
-------> Historically, in telephone switches, connections between callers were arranged with large, expensive banks of electromechanical relays, Strowger switches. 
---------> The basic mathematical property of Strowger switches is that for each input to the switch, there is exactly one output. 
---------> Much of the mathematical switching circuit theory attempts to use this property to reduce the total number of switches needed to connect a combination of inputs to a combination of outputs.
-------> Note: In the 1940s and 1950s, engineers in Bell Lab began an extended series of mathematical investigations into methods 
---------> for reducing the size and expense of the "switched fabric" needed to implement a telephone exchange. 
---------> One early, successful mathematical analysis was performed by Charles Clos, and a switched fabric constructed of smaller switches is called a Clos network.



-----> Shortest path problem
-------> In graph theory, the shortest path problem is the problem of finding a path between two vertices (or nodes) in a graph such that the sum of the weights of its constituent edges is minimized.
-------> The problem of finding the shortest path between two intersections on a road map may be modeled as a special case of the shortest path problem in graphs, 
-------> where the vertices correspond to intersections and the edges correspond to road segments, each weighted by the length of the segment. 

-------> Bellman–Ford algorithm: 
---------> This computes shortest paths in a weighted graph (where some of the edge weights may be negative)
-----------> The Bellman–Ford algorithm is an algorithm that computes shortest paths from a single source vertex to all of the other vertices in a weighted digraph. 
-----------> It is slower than Dijkstra's algorithm for the same problem, but more versatile, as it is capable of handling graphs in which some of the edge weights are negative numbers. 
---------> Like Dijkstra's algorithm, Bellman–Ford proceeds by relaxation, in which approximations to the correct distance are replaced by better ones until they eventually reach the solution. 
-----------> In both algorithms, the approximate distance to each vertex is always an overestimate of the true distance, and is replaced by the minimum of its old value and the length of a newly found path. 
-----------> However, Dijkstra's algorithm uses a priority queue to greedily select the closest vertex that has not yet been processed, and performs this relaxation process on all of its outgoing edges; 
-----------> by contrast, the Bellman–Ford algorithm simply relaxes all the edges, and does this |V|-1 times, where |V| is the number of vertices in the graph. 
-----------> In each of these repetitions, the number of vertices with correctly calculated distances grows, from which it follows that eventually all vertices will have their correct distances. 
-----------> This method allows the Bellman–Ford algorithm to be applied to a wider class of inputs than Dijkstra. 
-----------> The intermediate answers depend on the order of edges relaxed, but the final answer remains the same.
---------> Bellman–Ford runs in O(|V|*|E|) time, where |V| and |E| are the number of vertices and edges respectively.
---------> Pseudocode:
-----------> function BellmanFord(list vertices, list edges, vertex source) is
----------->     // This implementation takes in a graph, represented as
----------->     // lists of vertices (represented as integers [0..n-1]) and edges,
----------->     // and fills two arrays (distance and predecessor) holding
----------->     // the shortest path from the source to each vertex
----------->     distance := list of size n
----------->     predecessor := list of size n
----------->     // Step 1: initialize graph
----------->     for each vertex v in vertices do
----------->         distance[v] := inf             // Initialize the distance to all vertices to infinity
----------->         predecessor[v] := null         // And having a null predecessor
----------->     distance[source] := 0              // The distance from the source to itself is, of course, zero
----------->     // Step 2: relax edges repeatedly
----------->     repeat |V|−1 times:
----------->          for each edge (u, v) with weight w in edges do
----------->              if distance[u] + w < distance[v] then
----------->                  distance[v] := distance[u] + w
----------->                  predecessor[v] := u
----------->     // Step 3: check for negative-weight cycles
----------->     for each edge (u, v) with weight w in edges do
----------->         if distance[u] + w < distance[v] then
----------->             // Step 4: find a negative-weight cycle
----------->             negativeloop := [v, u]
----------->             repeat |V|−1 times:
----------->                 u := negativeloop[0]
----------->                 for each edge (u, v) with weight w in edges do
----------->                     if distance[u] + w < distance[v] then
----------->                         negativeloop := concatenate([v], negativeloop)
----------->             find a cycle in negativeloop, let it be ncycle
----------->             // use any cycle detection algorithm here
----------->             error "Graph contains a negative-weight cycle", ncycle
----------->     return distance, predecessor
---------> Simply put, the algorithm initializes the distance to the source to 0 and all other nodes to infinity. 
-----------> Then for all edges, if the distance to the destination can be shortened by taking the edge, the distance is updated to the new lower value.
---------> The core of the algorithm is a loop that scans across all edges at every loop. 
-----------> For every i ≤ |V|-1, at the end of the i-th iteration, from any vertex v,
-----------> following the predecessor trail recorded in predecessor yields a path that has a total weight that is at most distance[v],
-----------> and further, distance[v] is a lower bound to the length of any path from source to v that uses at most i edges.
---------> Since the longest possible path without a cycle can be |V|-1 edges, the edges must be scanned |V|-1 times to ensure the shortest path has been found for all nodes. 
-----------> A final scan of all the edges is performed and if any distance is updated, then a path of length |V| edges has been found which can only occur if at least one negative cycle exists in the graph. 
---------> Negative edge weights are found in various applications of graphs, hence the usefulness of this algorithm.
-----------> If a graph contains a "negative cycle" (i.e. a cycle whose edges sum to a negative value) that is reachable from the source, 
-----------> then there is no cheapest path: any path that has a point on the negative cycle can be made cheaper by one more walk around the negative cycle. 
-----------> In such a case, the Bellman–Ford algorithm can detect and report the negative cycle.
---------> Note: The algorithm was first proposed by Alfonso Shimbel (1955), but is instead named after Richard Bellman and Lester Ford Jr., who published it in 1958 and 1956, respectively. 
-----------> Edward F. Moore also published a variation of the algorithm in 1959, and for this reason it is also sometimes called the Bellman–Ford–Moore algorithm.

-------> Dijkstra's algorithm: 
---------> This computes shortest paths in a graph with non-negative edge weights.
---------> The algorithm exists in many variants. 
-----------> Dijkstra's original algorithm found the shortest path between two given nodes, 
-----------> but a more common variant fixes a single node as the "source" node and finds shortest paths from the source to all other nodes in the graph, producing a shortest-path tree.
---------> For a given source node in the graph, the algorithm finds the shortest path between that node and every other.
-----------> It can also be used for finding the shortest paths from a single node to a single destination node by stopping the algorithm once the shortest path to the destination node has been determined. 
-----------> For example, if the nodes of the graph represent cities and edge path costs represent driving distances between pairs of cities connected by a direct road 
-----------> Dijkstra's algorithm can be used to find the shortest route between one city and all other cities. 
-----------> A widely used application of shortest path algorithms is network routing protocols, most notably IS-IS (Intermediate System to Intermediate System) and OSPF (Open Shortest Path First). 
-----------> It is also employed as a subroutine in other algorithms such as Johnson's.
---------> The Dijkstra algorithm uses labels that are positive integers or real numbers, which are totally ordered. 
-----------> It can be generalized to use any labels that are partially ordered, provided the subsequent labels (a subsequent label is produced when traversing an edge) are monotonically non-decreasing. 
-----------> This generalization is called the generic Dijkstra shortest-path algorithm.[8]
---------> Dijkstra's algorithm uses a data structure for storing and querying partial solutions sorted by distance from the start. 
-----------> While the original algorithm uses a min-priority queue and runs in time Θ((|V|+|E|)log |V|), it can also be implemented in Θ(|V|^{2}) using an array. 
-----------> The idea of this algorithm is also given in Leyzorek et al. 1957. Fredman & Tarjan 1984 propose using a Fibonacci heap min-priority queue to optimize the running time complexity to Θ(|E|+|V|log|V|). 
-----------> This is asymptotically the fastest known single-source shortest-path algorithm for arbitrary directed graphs with unbounded non-negative weights. 
-----------> However, specialized cases can indeed be improved further as detailed in Specialized variants. 
-----------> Additionally, if preprocessing is allowed algorithms such as contraction hierarchies can be up to seven orders of magnitude faster. 
---------> Algorithm
-----------> Let the node at which we are starting be called the initial node. 
-----------> Let the distance of node Y be the distance from the initial node to Y. 
-----------> Dijkstra's algorithm will initially start with infinite distances and will try to improve them step by step.
-------------> (1) Mark all nodes unvisited. 
---------------> Create a set of all the unvisited nodes called the unvisited set.
-------------> (2) Assign to every node a tentative distance value: set it to zero for our initial node and to infinity for all other nodes. 
---------------> During the run of the algorithm, the tentative distance of a node v is the length of the shortest path discovered so far between the node v and the starting node. 
---------------> Since initially no path is known to any other vertex than the source itself (which is a path of length zero), all other tentative distances are initially set to infinity. 
---------------> Set the initial node as current.[15]
-------------> (3) For the current node, consider all of its unvisited neighbors and calculate their tentative distances through the current node. 
---------------> Compare the newly calculated tentative distance to the one currently assigned to the neighbor and assign it the smaller one. 
---------------> For example, if the current node A is marked with a distance of 6, and the edge connecting it with a neighbor B has length 2, 
---------------> then the distance to B through A will be 6 + 2 = 8. 
---------------> If B was previously marked with a distance greater than 8 then change it to 8. 
---------------> Otherwise, the current value will be kept.
-------------> (4) When we are done considering all of the unvisited neighbors of the current node, mark the current node as visited and remove it from the unvisited set. 
---------------> A visited node will never be checked again (this is valid and optimal in connection with the behavior in step 6.
---------------> That the next nodes to visit will always be in the order of 'smallest distance from initial node first' so any visits after would have a greater distance).
-------------> (5) If the destination node has been marked visited (when planning a route between two specific nodes) 
---------------> or if the smallest tentative distance among the nodes in the unvisited set is infinity (when planning a complete traversal; 
---------------> occurs when there is no connection between the initial node and remaining unvisited nodes), then stop. 
---------------> The algorithm has finished.
-------------> (6) Otherwise, select the unvisited node that is marked with the smallest tentative distance, set it as the new current node, and go back to step 3.
-----------> When planning a route, it is actually not necessary to wait until the destination node is "visited" as above: 
-------------> the algorithm can stop once the destination node has the smallest tentative distance among all "unvisited" nodes (and thus could be selected as the next "current"). 
---------> Note: This was conceived by computer scientist Edsger W. Dijkstra in 1956 and published three years later.

-------> Floyd–Warshall algorithm: 
---------> This solves the all pairs shortest path problem in a weighted, directed graph.
---------> This an algorithm for finding shortest paths in a directed weighted graph with positive or negative edge weights (but with no negative cycles).
-----------> A single execution of the algorithm will find the lengths (summed weights) of shortest paths between all pairs of vertices. 
-----------> Although it does not return details of the paths themselves, it is possible to reconstruct the paths with simple modifications to the algorithm. 
-----------> Versions of the algorithm can also be used for finding the transitive closure of a relation R, or (in connection with the Schulze voting system) widest paths between all pairs of vertices in a weighted graph.
---------> Algorithm
-----------> The Floyd–Warshall algorithm compares all possible paths through the graph between each pair of vertices. 
-------------> It is able to do this with Θ(|V|^3) comparisons in a graph, even though there may be up to Ω(|V|^{2}) edges in the graph, and every combination of edges is tested. 
-------------> It does so by incrementally improving an estimate on the shortest path between two vertices, until the estimate is optimal.
-----------> Consider a graph G with vertices V numbered 1 through N. 
-------------> Further consider a function shortestPath(i, j, k) that returns the shortest possible path from i to j using vertices only from the set {1 , 2, …, k }  as intermediate points along the way. 
-------------> Now, given this function, our goal is to find the shortest path from each i to each j using any vertex in {1, 2, …, N}.
---------> For each of these pairs of vertices, the shortestPath(i, j, k) could be either
-----------> (1) a path that does not go through k (only uses vertices in the set {1, …, k−1}.)
-----------> or
-----------> (2) a path that does go through k (from i to k and then from k to j, both only using intermediate vertices in  {1, …, k−1})
---------> We know that the best path from i to j that only uses vertices 1 through k-1 is defined by shortestPath(i, j, k-1), 
-----------> and it is clear that if there was a better path from i to k to j, 
-----------> then the length of this path would be the concatenation of the shortest path from i to k (only using intermediate vertices in {1, …, k−1}) 
-----------> and the shortest path from k to j (only using intermediate vertices in  {1, …, k−1}).
---------> If w(i,j) is the weight of the edge between vertices i and j, we can define shortestPath(i, j, k) in terms of the following recursive formula: the base case is
-----------> shortestPath(i, j, 0)=w(i,j)
-----------> and the recursive case is
-----------> shortestPath(i, j, 0) = min(shortestPath(i, j, k-1), shortestPath(i, k, k-1) + shortestPath(k, j, k-1))
---------> This formula is the heart of the Floyd–Warshall algorithm. 
-----------> The algorithm works by first computing shortestPath(i, j, k) for all (i, j) pairs for k=1, then k=2, and so on. 
-----------> This process continues until k=N, and we have found the shortest path for all (i, j) pairs using any intermediate vertices. 
-----------> Pseudocode for this basic version follows:
-------------> let dist be a |V| × |V| array of minimum distances initialized to ∞ (infinity)
-------------> for each edge (u, v) do
------------->     dist[u][v] ← w(u, v)  // The weight of the edge (u, v)
-------------> for each vertex v do
------------->     dist[v][v] ← 0
-------------> for k from 1 to |V|
------------->     for i from 1 to |V|
------------->         for j from 1 to |V|
------------->             if dist[i][j] > dist[i][k] + dist[k][j] 
------------->                 dist[i][j] ← dist[i][k] + dist[k][j]
------------->             end if
-----------> Note: This is also known as Floyd's algorithm, the Roy–Warshall algorithm, the Roy–Floyd algorithm, or the WFI algorithm.

-------> Johnson's algorithm: 
---------> This provides an all pairs shortest path algorithm in sparse weighted directed graph
-----------> Johnson's algorithm is a way to find the shortest paths between all pairs of vertices in an edge-weighted directed graph. 
-----------> It allows some of the edge weights to be negative numbers, but no negative-weight cycles may exist. 
-----------> It works by using the Bellman–Ford algorithm to compute a transformation of the input graph that removes all negative weights, allowing Dijkstra's algorithm to be used on the transformed graph.
---------> A similar reweighting technique is also used in Suurballe's algorithm for finding two disjoint paths of minimum total length
-----------> between the same two vertices in a graph with non-negative edge weights.
---------> Johnson's algorithm consists of the following steps:
-----------> (1) First, a new node q is added to the graph, connected by zero-weight edges to each of the other nodes.
-----------> (2) Second, the Bellman–Ford algorithm is used, starting from the new vertex q, to find for each vertex v the minimum weight h(v) of a path from q to v. 
-------------> If this step detects a negative cycle, the algorithm is terminated.
-----------> (3) Next the edges of the original graph are reweighted using the values computed by the Bellman–Ford algorithm: 
-------------> an edge from u to v, having length w(u,v), is given the new length w(u,v) + h(u) − h(v).
-----------> (4) Finally, q is removed, and Dijkstra's algorithm is used to find the shortest paths from each node s to every other vertex in the reweighted graph. 
-------------> The distance in the original graph is then computed for each distance D(u , v), by adding h(v) − h(u) to the distance returned by Dijkstra's algorithm.
---------> Note: It is named after Donald B. Johnson, who first published the technique in 1977.



-----> Transitive closure problem
-------> This finds the transitive closure of a given binary relation
-------> In mathematics, the transitive closure of a binary relation R on a set X is the smallest relation on X that contains R and is transitive. 
---------> For finite sets, "smallest" can be taken in its usual sense, of having the fewest related pairs; for infinite sets it is the unique minimal transitive superset of R.
-------> For example, if X is a set of airports and x R y means "there is a direct flight from airport x to airport y" (for x and y in X), 
---------> then the transitive closure of R on X is the relation R+ such that x R+ y means "it is possible to fly from x to y in one or more flights". 
---------> Informally, the transitive closure gives you the set of all places you can get to from any starting place. 
-------> More formally, the transitive closure of a binary relation R on a set X is the transitive relation R+ on set X such that R+ contains R and R+ is minimal; see Lidl & Pilz (1998, p. 337).
---------> If the binary relation itself is transitive, then the transitive closure is that same binary relation; otherwise, the transitive closure is a different relation.
-------> Conversely, transitive reduction adduces a minimal relation S from a given relation R such that they have the same closure, that is, S+ = R+; however, many different S with this property may exist.
-------> Both transitive closure and transitive reduction are also used in the closely related area of graph theory. 



-----> Traveling salesman problem
-------> The travelling salesman problem (also called the travelling salesperson problem or TSP) asks the following question: 
---------> "Given a list of cities and the distances between each pair of cities, what is the shortest possible route that visits each city exactly once and returns to the origin city?" 
---------> It is an NP-hard problem in combinatorial optimization, important in theoretical computer science and operations research.
-------> The travelling purchaser problem and the vehicle routing problem are both generalizations of TSP.
-------> In the theory of computational complexity, the decision version of the TSP (where given a length L, 
---------> the task is to decide whether the graph has a tour of at most L) belongs to the class of NP-complete problems. 
---------> Thus, it is possible that the worst-case running time for any algorithm for the TSP increases superpolynomially (but no more than exponentially) with the number of cities.
-------> The problem was first formulated in 1930 and is one of the most intensively studied problems in optimization. 
---------> It is used as a benchmark for many optimization methods. 
---------> Even though the problem is computationally difficult, many heuristics and exact algorithms are known, 
---------> so that some instances with tens of thousands of cities can be solved completely and even problems with millions of cities can be approximated within a small fraction of 1%.
-------> The TSP has several applications even in its purest formulation, such as planning, logistics, and the manufacture of microchips. 
---------> Slightly modified, it appears as a sub-problem in many areas, such as DNA sequencing. 
---------> In these applications, the concept city represents, for example, customers, soldering points, or DNA fragments, 
-----------> and the concept distance represents travelling times or cost, or a similarity measure between DNA fragments. 
---------> The TSP also appears in astronomy, as astronomers observing many sources will want to minimize the time spent moving the telescope between the sources; 
-----------> in such problems, the TSP can be embedded inside an optimal control problem. 
---------> In many applications, additional constraints such as limited resources or time windows may be imposed.  

-------> Christofides algorithm
---------> The Christofides algorithm or Christofides–Serdyukov algorithm is an algorithm for finding approximate solutions to the travelling salesman problem, 
-----------> on instances where the distances form a metric space (they are symmetric and obey the triangle inequality). 
-----------> It is an approximation algorithm that guarantees that its solutions will be within a factor of 3/2 of the optimal solution length.
---------> This algorithm still stands as the best polynomial time approximation algorithm that has been thoroughly peer-reviewed 
-----------> by the relevant scientific community for the traveling salesman problem on general metric spaces. 
---------> Algorithm
-----------> Let G = (V,w) be an instance of the travelling salesman problem. 
-----------> That is, G is a complete graph on the set V of vertices, and the function w assigns a nonnegative real weight to every edge of G. 
-----------> According to the triangle inequality, for every three vertices u, v, and x, it should be the case that w(uv) + w(vx) ≥ w(ux).
-----------> Then the algorithm can be described in pseudocode as follows.
-------------> (1) Create a minimum spanning tree T of G.
-------------> (2) Let O be the set of vertices with odd degree in T. 
---------------> By the handshaking lemma, O has an even number of vertices.
-------------> (3) Find a minimum-weight perfect matching M in the induced subgraph given by the vertices from O.
---------------> A perfect matching in a graph is a matching that covers every vertex of the graph.
-------------> (4) Combine the edges of M and T to form a connected multigraph H in which each vertex has even degree.
-------------> (5) Form an Eulerian circuit in H.
-------------> (6) Make the circuit found in previous step into a Hamiltonian circuit by skipping repeated vertices (shortcutting).
---------> The steps 5 and 6 do not necessarily yield only one result. 
---------> As such the heuristic can give several different paths. 
---------> Note: This named after Nicos Christofides and Anatoliy I. Serdyukov, who discovered it independently in 1976.
---------> Note: In July 2020, Karlin, Klein, and Gharan released a preprint in which they introduced a novel approximation algorithm and claimed that its approximation ratio is 1.5 − 10−36. 
-----------> Their method follows similar principles to Christofides' algorithm, but uses a randomly chosen tree from a carefully chosen random distribution in place of the minimum spanning tree. 
-----------> The paper was published at STOC'21[7] where it received a best paper award.

-------> Nearest neighbour algorithm
---------> The nearest neighbour algorithm was one of the first algorithms used to solve the travelling salesman problem approximately. 
-----------> In that problem, the salesman starts at a random city and repeatedly visits the nearest city until all have been visited.
-----------> The algorithm quickly yields a short tour, but usually not the optimal one.
---------> Algorithm
-----------> These are the steps of the algorithm:
-------------> (1) Initialize all vertices as unvisited.
-------------> (2) Select an arbitrary vertex, set it as the current vertex u. Mark u as visited.
-------------> (3) Find out the shortest edge connecting the current vertex u and an unvisited vertex v.
-------------> (4) Set v as the current vertex u. Mark v as visited.
-------------> (5) If all the vertices in the domain are visited, then terminate. Else, go to step 3.
-----------> The sequence of the visited vertices is the output of the algorithm.
---------> The nearest neighbour algorithm is easy to implement and executes quickly, 
-----------> but it can sometimes miss shorter routes which are easily noticed with human insight, due to its "greedy" nature. 
-----------> As a general guide, if the last few stages of the tour are comparable in length to the first stages, 
-----------> then the tour is reasonable; if they are much greater, then it is likely that much better tours exist. 
-----------> Another check is to use an algorithm such as the lower bound algorithm to estimate if this tour is good enough.
---------> In the worst case, the algorithm results in a tour that is much longer than the optimal tour. 
-----------> To be precise, for every constant r there is an instance of the traveling salesman problem 
-----------> such that the length of the tour computed by the nearest neighbour algorithm is greater than r times the length of the optimal tour. 
-----------> Moreover, for each number of cities there is an assignment of distances between the cities for which the nearest neighbor heuristic produces the unique worst possible tour. 
-----------> (If the algorithm is applied on every vertex as the starting vertex, the best path found will be better than at least N/2-1 other tours, where N is the number of vertices.)
---------> The nearest neighbour algorithm may not find a feasible tour at all, even when one exists.
 
-----> Warnsdorff's rule: 
-------> This is a heuristic method for solving the Knight's tour problem.
-------> A knight's tour is a sequence of moves of a knight on a chessboard such that the knight visits every square exactly once. 
---------> If the knight ends on a square that is one knight's move from the beginning square (so that it could tour the board again immediately, following the same path), 
---------> the tour is closed (or re-entrant); otherwise, it is open.
-------> Warnsdorff's rule is a heuristic for finding a single knight's tour. 
---------> The knight is moved so that it always proceeds to the square from which the knight will have the fewest onward moves. 
---------> When calculating the number of onward moves for each candidate square, we do not count moves that revisit any square already visited.
---------> It is possible to have two or more choices for which the number of onward moves is equal; there are various methods for breaking such ties, 
---------> including one devised by Pohl[18] and another by Squirrel and Cull.
-------> This rule may also more generally be applied to any graph. 
---------> In graph-theoretic terms, each move is made to the adjacent vertex with the least degree. 
---------> Although the Hamiltonian path problem is NP-hard in general, on many graphs that occur in practice this heuristic is able to successfully locate a solution in linear time.
---------> The knight's tour is such a special case.
-------> The heuristic was first described in "Des Rösselsprungs einfachste und allgemeinste Lösung" by H. C. von Warnsdorff in 1823.[21]
---------> A computer program that finds a knight's tour was written by Gordon Horsington and published in 1984 in the book Century/Acorn User Book of Computer Puzzles.



---> Graph search

-----> A*
-------> This special case of best-first search that uses heuristics to improve speed.
-------> A* (pronounced "A-star") is a graph traversal and path search algorithm, 
---------> which is often used in many fields of computer science due to its completeness, optimality, and optimal efficiency. 
---------> One major practical drawback is its O ( b d ) {\displaystyle O(b^{d})} O(b^d) space complexity, as it stores all generated nodes in memory. 
---------> Thus, in practical travel-routing systems, it is generally outperformed by algorithms which can pre-process the graph to attain better performance, as well as memory-bounded approaches; 
---------> however, A* is still the best solution in many cases.
-------> Compared to Dijkstra's algorithm, the A* algorithm only finds the shortest path from a specified source to a specified goal, 
---------> and not the shortest-path tree from a specified source to all possible goals. 
---------> This is a necessary trade-off for using a specific-goal-directed heuristic. 
---------> For Dijkstra's algorithm, since the entire shortest-path tree is generated, every node is a goal, and there can be no specific-goal-directed heuristic. 
-------> Algorithm:
---------> A* is an informed search algorithm, or a best-first search, meaning that it is formulated in terms of weighted graphs: 
-----------> starting from a specific starting node of a graph, it aims to find a path to the given goal node having the smallest cost (least distance travelled, shortest time, etc.). 
-----------> It does this by maintaining a tree of paths originating at the start node and extending those paths one edge at a time until its termination criterion is satisfied.
---------> At each iteration of its main loop, A* needs to determine which of its paths to extend. 
-----------> It does so based on the cost of the path and an estimate of the cost required to extend the path all the way to the goal. 
-----------> Specifically, A* selects the path that minimizes:
-------------> f(n)=g(n)+h(n)
-----------> where n is the next node on the path, g(n) is the cost of the path from the start node to n, 
-----------> and h(n) is a heuristic function that estimates the cost of the cheapest path from n to the goal. 
-----------> A* terminates when the path it chooses to extend is a path from start to goal or if there are no paths eligible to be extended. 
-----------> The heuristic function is problem-specific. 
-----------> If the heuristic function is admissible – meaning that it never overestimates the actual cost to get to the goal –, 
-----------> A* is guaranteed to return a least-cost path from start to goal.
---------> Typical implementations of A* use a priority queue to perform the repeated selection of minimum (estimated) cost nodes to expand. 
-----------> This priority queue is known as the open set or fringe. 
-----------> At each step of the algorithm, the node with the lowest f(x) value is removed from the queue, 
-----------> the f and g values of its neighbors are updated accordingly, and these neighbors are added to the queue. 
-----------> The algorithm continues until a removed node (thus the node with the lowest f value out of all fringe nodes) is a goal node. 
-----------> The f value of that goal is then also the cost of the shortest path, since h at the goal is zero in an admissible heuristic.
---------> The algorithm described so far gives us only the length of the shortest path. 
-----------> To find the actual sequence of steps, the algorithm can be easily revised so that each node on the path keeps track of its predecessor. 
-----------> After this algorithm is run, the ending node will point to its predecessor, and so on, until some node's predecessor is the start node.
---------> As an example, when searching for the shortest route on a map, h(x) might represent the straight-line distance to the goal, 
-----------> since that is physically the smallest possible distance between any two points. 
-----------> For a grid map from a video game, using the Manhattan distance or the octile distance becomes better depending on the set of movements available (4-way or 8-way).
---------> If the heuristic h satisfies the additional condition h(x) ≤ d(x, y) + h(y) 
-----------> for every edge (x, y) of the graph (where d denotes the length of that edge), then h is called monotone, or consistent. 
-----------> With a consistent heuristic, A* is guaranteed to find an optimal path without processing any node more than once 
-----------> and A* is equivalent to running Dijkstra's algorithm with the reduced cost d'(x, y) = d(x, y) + h(y) − h(x).
---------> Pseudocode
-----------> function reconstruct_path(cameFrom, current)
----------->     total_path := {current}
----------->     while current in cameFrom.Keys:
----------->         current := cameFrom[current]
----------->         total_path.prepend(current)
----------->     return total_path
-----------> // A* finds a path from start to goal.
-----------> // h is the heuristic function. h(n) estimates the cost to reach goal from node n.
-----------> function A_Star(start, goal, h)
----------->     // The set of discovered nodes that may need to be (re-)expanded.
----------->     // Initially, only the start node is known.
----------->     // This is usually implemented as a min-heap or priority queue rather than a hash-set.
----------->     openSet := {start}
----------->     // For node n, cameFrom[n] is the node immediately preceding it on the cheapest path from start
----------->     // to n currently known.
----------->     cameFrom := an empty map
----------->     // For node n, gScore[n] is the cost of the cheapest path from start to n currently known.
----------->     gScore := map with default value of Infinity
----------->     gScore[start] := 0
----------->     // For node n, fScore[n] := gScore[n] + h(n). fScore[n] represents our current best guess as to
----------->     // how cheap a path could be from start to finish if it goes through n.
----------->     fScore := map with default value of Infinity
----------->     fScore[start] := h(start)
----------->     while openSet is not empty
----------->         // This operation can occur in O(Log(N)) time if openSet is a min-heap or a priority queue
----------->         current := the node in openSet having the lowest fScore[] value
----------->         if current = goal
----------->             return reconstruct_path(cameFrom, current)
----------->         openSet.Remove(current)
----------->         for each neighbor of current
----------->             // d(current,neighbor) is the weight of the edge from current to neighbor
----------->             // tentative_gScore is the distance from start to the neighbor through current
----------->             tentative_gScore := gScore[current] + d(current, neighbor)
----------->             if tentative_gScore < gScore[neighbor]
----------->                 // This path to neighbor is better than any previous one. Record it!
----------->                 cameFrom[neighbor] := current
----------->                 gScore[neighbor] := tentative_gScore
----------->                 fScore[neighbor] := tentative_gScore + h(neighbor)
----------->                 if neighbor not in openSet
----------->                     openSet.add(neighbor)
----------->     // Open set is empty but goal was never reached
----------->     return failure
---------> Remark: In this pseudocode, if a node is reached by one path, removed from openSet, 
-----------> and subsequently reached by a cheaper path, it will be added to openSet again. 
-----------> This is essential to guarantee that the path returned is optimal if the heuristic function is admissible but not consistent. 
-----------> If the heuristic is consistent, when a node is removed from openSet the path to it is guaranteed to be optimal 
-----------> so the test ‘tentative_gScore < gScore[neighbor]’ will always fail if the node is reached again. 
-------> Note: Peter Hart, Nils Nilsson and Bertram Raphael of Stanford Research Institute (now SRI International) first published the algorithm in 1968.
---------> It can be seen as an extension of Dijkstra's algorithm. 
---------> A* achieves better performance by using heuristics to guide its search.

-----> B*: 
-------> This is a best-first graph search algorithm that finds the least-cost path from a given initial node to any goal node (out of one or more possible goals).
-------> The algorithm stores intervals for nodes of the tree as opposed to single point-valued estimates. 
---------> Then, leaf nodes of the tree can be searched until one of the top level nodes has an interval which is clearly "best." 
-------> Note: First published by Hans Berliner in 1979, it is related to the A* search algorithm. 

-----> Backtracking: 
-------> This abandons partial solutions when they are found not to satisfy a complete solution
-------> Backtracking is a general algorithm for finding solutions to some computational problems, notably constraint satisfaction problems, 
---------> that incrementally builds candidates to the solutions, and abandons a candidate ("backtracks") 
---------> as soon as it determines that the candidate cannot possibly be completed to a valid solution.
-------> The classic textbook example of the use of backtracking is the eight queens puzzle, 
---------> that asks for all arrangements of eight chess queens on a standard chessboard so that no queen attacks any other. 
---------> In the common backtracking approach, the partial candidates are arrangements of k queens in the first k rows of the board, all in different rows and columns. 
---------> Any partial solution that contains two mutually attacking queens can be abandoned.
-------> Backtracking can be applied only for problems which admit the concept of a "partial candidate solution" 
---------> and a relatively quick test of whether it can possibly be completed to a valid solution. 
---------> It is useless, for example, for locating a given value in an unordered table. 
---------> When it is applicable, however, backtracking is often much faster than brute-force enumeration of all complete candidates, since it can eliminate many candidates with a single test.
-------> Backtracking is an important tool for solving constraint satisfaction problems, such as crosswords, verbal arithmetic, Sudoku, and many other puzzles. 
---------> It is often the most convenient technique for parsing, for the knapsack problem and other combinatorial optimization problems. 
---------> It is also the basis of the so-called logic programming languages such as Icon, Planner and Prolog.
-------> Backtracking depends on user-given "black box procedures" that define the problem to be solved, 
---------> the nature of the partial candidates, and how they are extended into complete candidates. 
---------> It is therefore a metaheuristic rather than a specific algorithm – although, unlike many other meta-heuristics, 
---------> it is guaranteed to find all solutions to a finite problem in a bounded amount of time.
-------> Algorithm:
---------> The backtracking algorithm enumerates a set of partial candidates that, in principle, 
---------> could be completed in various ways to give all the possible solutions to the given problem. 
-----------> The completion is done incrementally, by a sequence of candidate extension steps.
---------> Conceptually, the partial candidates are represented as the nodes of a tree structure, the potential search tree. 
-----------> Each partial candidate is the parent of the candidates that differ from it by a single extension step; 
-----------> the leaves of the tree are the partial candidates that cannot be extended any further.
---------> The backtracking algorithm traverses this search tree recursively, from the root down, in depth-first order. 
-----------> At each node c, the algorithm checks whether c can be completed to a valid solution. 
-----------> If it cannot, the whole sub-tree rooted at c is skipped (pruned). 
-----------> Otherwise, the algorithm (1) checks whether c itself is a valid solution, and if so reports it to the user; 
-----------> and (2) recursively enumerates all sub-trees of c. 
-----------> The two tests and the children of each node are defined by user-given procedures.
---------> Therefore, the actual search tree that is traversed by the algorithm is only a part of the potential tree. 
-----------> The total cost of the algorithm is the number of nodes of the actual tree times the cost of obtaining and processing each node. 
-----------> This fact should be considered when choosing the potential search tree and implementing the pruning test.
-----------> Pseudocode:
-----------> In order to apply backtracking to a specific class of problems, one must provide the data P for the particular instance of the problem that is to be solved, 
-------------> and six procedural parameters, root, reject, accept, first, next, and output. 
-------------> These procedures should take the instance data P as a parameter and should do the following:
---------------> root(P): return the partial candidate at the root of the search tree.
---------------> reject(P,c): return true only if the partial candidate c is not worth completing.
---------------> accept(P,c): return true if c is a solution of P, and false otherwise.
---------------> first(P,c): generate the first extension of candidate c.
---------------> next(P,s): generate the next alternative extension of a candidate, after the extension s.
---------------> output(P,c): use the solution c of P, as appropriate to the application.
-----------> The backtracking algorithm reduces the problem to the call backtrack(root(P)), where backtrack is the following recursive procedure:
-------------> procedure backtrack(c) is
------------->     if reject(P, c) then return
------------->     if accept(P, c) then output(P, c)
------------->     s ← first(P, c)
------------->     while s ≠ NULL do
------------->         backtrack(s)
------------->         s ← next(P, s)
-----------> Usage considerations
-------------> The reject procedure should be a boolean-valued function that returns true only if it is certain that no possible extension of c is a valid solution for P. 
---------------> If the procedure cannot reach a definite conclusion, it should return false. 
---------------> An incorrect true result may cause the backtrack procedure to miss some valid solutions. 
---------------> The procedure may assume that reject(P,t) returned false for every ancestor t of c in the search tree.
-------------> On the other hand, the efficiency of the backtracking algorithm depends on reject returning true for candidates that are as close to the root as possible. 
---------------> If reject always returns false, the algorithm will still find all solutions, but it will be equivalent to a brute-force search.
-------------> The accept procedure should return true if c is a complete and valid solution for the problem instance P, and false otherwise. 
---------------> It may assume that the partial candidate c and all its ancestors in the tree have passed the reject test.
-------------> The general pseudo-code above does not assume that the valid solutions are always leaves of the potential search tree. 
---------------> In other words, it admits the possibility that a valid solution for P can be further extended to yield other valid solutions.
-------------> The first and next procedures are used by the backtracking algorithm to enumerate the children of a node c of the tree, 
---------------> that is, the candidates that differ from c by a single extension step. 
---------------> The call first(P,c) should yield the first child of c, in some order; and the call next(P,s) should return the next sibling of node s, in that order. 
---------------> Both functions should return a distinctive "NULL" candidate, if the requested child does not exist.
-------------> Together, the root, first, and next functions define the set of partial candidates and the potential search tree. 
---------------> They should be chosen so that every solution of P occurs somewhere in the tree, and no partial candidate occurs more than once. 
---------------> Moreover, they should admit an efficient and effective reject predicate.
-----------> Early stopping variants
-------------> The pseudo-code above will call output for all candidates that are a solution to the given instance P. 
---------------> The algorithm can be modified to stop after finding the first solution, or a specified number of solutions; 
---------------> or after testing a specified number of partial candidates, or after spending a given amount of CPU time. 
-------> Note: The term "backtrack" was coined by American mathematician D. H. Lehmer in the 1950s.[4] 
---------> The pioneer string-processing language SNOBOL (1962) may have been the first to provide a built-in general backtracking facility. 

-----> Beam search: 
-------> This is a heuristic search algorithm that is an optimization of best-first search that reduces its memory requirement
-------> The beam search is a heuristic search algorithm that explores a graph by expanding the most promising node in a limited set. 
---------> Beam search is an optimization of best-first search that reduces its memory requirements. 
---------> Best-first search is a graph search which orders all partial solutions (states) according to some heuristic.
---------> But in beam search, only a predetermined number of best partial solutions are kept as candidates.
---------> It is thus a greedy algorithm.
-------> Algorithm:
---------> Beam search uses breadth-first search to build its search tree.
-----------> At each level of the tree, it generates all successors of the states at the current level, sorting them in increasing order of heuristic cost.
-----------> However, it only stores a predetermined number, β, of best states at each level (called the beam width). 
-----------> Only those states are expanded next. 
-----------> greater the beam width, the fewer states are pruned. 
-----------> With an infinite beam width, no states are pruned and beam search is identical to breadth-first search. 
-----------> The beam width bounds the memory required to perform the search. 
-----------> Since a goal state could potentially be pruned, beam search sacrifices completeness (the guarantee that an algorithm will terminate with a solution, if one exists). 
-----------> Beam search is not optimal (that is, there is no guarantee that it will find the best solution).
-------> Applications:
---------> A beam search is most often used to maintain tractability in large systems with insufficient amount of memory to store the entire search tree.
-----------> For example, it has been used in many machine translation systems. (The state of the art now primarily uses neural machine translation based methods.) 
-----------> To select the best translation, each part is processed, and many different ways of translating the words appear. 
-----------> The top best translations according to their sentence structures are kept, and the rest are discarded. 
-----------> The translator then evaluates the translations according to a given criterion, choosing the translation which best keeps the goals. 
-------> Note: The term "beam search" was coined by Raj Reddy of Carnegie Mellon University in 1977.[2] 
-------> Note: The first use of a beam search was in the Harpy Speech Recognition System, CMU 1976.[7] 

-----> Beam stack search: 
-------> This integrates backtracking with beam search
-------> Beam stack search is a search algorithm that combines chronological backtracking (that is, depth-first search) with beam search and is similar to depth-first beam search.
---------> Both search algorithms are anytime algorithms that find good but likely sub-optimal solutions quickly, 
---------> like beam search, then backtrack and continue to find improved solutions until convergence to an optimal solution. 
-------> Implementation
---------> Beam stack search uses the beam stack as a data structure to integrate chronological backtracking with beam search 
-----------> and can be combined with the divide and conquer algorithm technique, 
-----------> resulting in divide-and-conquer beam-stack search. 
-------> Alternatives
---------> Beam search using limited discrepancy backtracking (BULB) is a search algorithm that combines limited discrepancy search with beam search 
-----------> and thus performs non-chronological backtracking, which often outperforms the chronological backtracking done by beam stack search and depth-first beam search. 

-----> Best-first search: 
-------> This traverses a graph in the order of likely importance using a priority queue.
---------> Best-first search is a class of search algorithms, which explore a graph by expanding the most promising node chosen according to a specified rule.
-------> Some authors have used "best-first search" to refer specifically to a search with a heuristic that attempts to predict how close the end of a path is to a solution (or, goal),
---------> so that paths which are judged to be closer to a solution (or, goal) are extended first. 
---------> This specific type of search is called greedy best-first search or pure heuristic search.
-------> Efficient selection of the current best candidate for extension is typically implemented using a priority queue.
-------> The A* search algorithm is an example of a best-first search algorithm, as is B*. 
---------> Best-first algorithms are often used for path finding in combinatorial search. 
---------> Neither A* nor B* is a greedy best-first search, as they incorporate the distance from the start in addition to estimated distances to the goal. 
-------> Greedy BFS
---------> Using a greedy algorithm, expand the first successor of the parent.
---------> After a successor is generated:
-----------> If the successor's heuristic is better than its parent, 
-------------> the successor is set at the front of the queue (with the parent reinserted directly behind it), and the loop restarts.
-----------> Else, the successor is inserted into the queue (in a location determined by its heuristic value). 
-------------> The procedure will evaluate the remaining successors (if any) of the parent.
---------> Below is a pseudocode example of this algorithm, where queue represents a priority queue which orders nodes based on their heuristic distances from the goal. 
---------> This implementation keeps track of visited nodes, and can therefore be used for undirected graphs. It can be modified to retrieve the path.
-----------> procedure GBS(start, target) is:
----------->   mark start as visited
----------->   add start to queue
----------->   while queue is not empty do:
----------->     current_node ← vertex of queue with min distance to target
----------->     remove current_node from queue
----------->     foreach neighbor n of current_node do:
----------->       if n not in visited then:
----------->         if n is target:
----------->           return n
----------->         else:
----------->           mark n as visited
----------->           add n to queue
----------->   return failure 
-------> Note: Judea Pearl described the best-first search as estimating the promise of node n by a "heuristic evaluation function f(n) which, in general, 
---------> may depend on the description of n, the description of the goal, 
---------> the information gathered by the search up to that point, and most importantly, on any extra knowledge about the problem domain."


-----> Bidirectional search: 
-------> This find the shortest path from an initial vertex to a goal vertex in a directed graph.
---------> Bidirectional search is a graph search algorithm that finds a shortest path from an initial vertex to a goal vertex in a directed graph. 
---------> It runs two simultaneous searches: one forward from the initial state, and one backward from the goal, stopping when the two meet. 
---------> The reason for this approach is that in many cases it is faster: 
-----------> for instance, in a simplified model of search problem complexity in which both searches expand a tree with branching factor b,
-----------> and the distance from start to goal is d, each of the two searches has complexity O(bd/2) (in Big O notation), 
-----------> and the sum of these two search times is much less than the O(bd) complexity that would result from a single search from the beginning to the goal.
-------> As in A* search, bi-directional search can be guided by a heuristic estimate of the remaining distance to the goal (in the forward tree) or from the start (in the backward tree).
-------> Description
---------> A Bidirectional Heuristic Search is a state space search from some state s to another state t, searching from s to t and from t to s simultaneously. 
-----------> It returns a valid list of operators that if applied to s will give us t.
---------> While it may seem as though the operators have to be invertible for the reverse search, 
-----------> it is only necessary to be able to find, given any node n, 
-----------> the set of parent nodes of n such that there exists some valid operator from each of the parent nodes to n. 
-----------> This has often been likened to a one-way street in the route-finding domain: 
-----------> it is not necessary to be able to travel down both directions, but it is necessary when standing at the end of the street 
-----------> to determine the beginning of the street as a possible route.
---------> Similarly, for those edges that have inverse arcs (i.e. arcs going in both directions) it is not necessary that each direction be of equal cost. 
-----------> The reverse search will always use the inverse cost (i.e. the cost of the arc in the forward direction). 
-----------> More formally, if n is a node with parent p, then k1(p,n)=k2(n,p), defined as being the cost from p to n.
-------> Note: Andrew Goldberg and others explained the correct termination conditions for the bidirectional version of Dijkstra’s Algorithm.
---------> Ira Pohl (1971) was the first one to design and implement a bi-directional heuristic search algorithm.
---------> Search trees emanating from the start and goal nodes failed to meet in the middle of the solution space. 
---------> The BHFFA algorithm fixed this defect Champeaux (1977).
-------> Note: A solution found by the uni-directional A* algorithm using an admissible heuristic has a shortest path length; 
---------> the same property holds for the BHFFA2 bidirectional heuristic version described in de Champeaux (1983). 
---------> BHFFA2 has, among others, more careful termination conditions than BHFFA. 

-----> Breadth-first search: 
-------> This traverses a graph level by level.
-------> Breadth-first search (BFS) is an algorithm for searching a tree data structure for a node that satisfies a given property. 
---------> It starts at the tree root and explores all nodes at the present depth prior to moving on to the nodes at the next depth level. 
---------> Extra memory, usually a queue, is needed to keep track of the child nodes that were encountered but not yet explored.
-------> For example, in a chess endgame a chess engine may build the game tree from the current position by applying all possible moves, 
---------> and use breadth-first search to find a win position for white.
---------> Implicit trees (such as game trees or other problem-solving trees) may be of infinite size; 
---------> breadth-first search is guaranteed to find a solution node[1] if one exists.
-------> In contrast, (plain) depth-first search, which explores the node branch as far as possible before backtracking and expanding other nodes,
---------> may get lost in an infinite branch and never make it to the solution node. 
---------> Iterative deepening depth-first search avoids the latter drawback at the price of exploring the tree's top parts over and over again. 
---------> On the other hand, both depth-first algorithms get along without extra memory.
-------> Breadth-first search can be generalized to graphs, when the start node (sometimes referred to as a 'search key') is explicitly given, 
---------> and precautions are taken against following a vertex twice.
-------> Pseudocode
---------> Input: A graph G and a starting vertex root of G
---------> Output: Goal state. The parent links trace the shortest path back to root[8]
--------->  1  procedure BFS(G, root) is
--------->  2      let Q be a queue
--------->  3      label root as explored
--------->  4      Q.enqueue(root)
--------->  5      while Q is not empty do
--------->  6          v := Q.dequeue()
--------->  7          if v is the goal then
--------->  8              return v
--------->  9          for all edges from v to w in G.adjacentEdges(v) do
---------> 10              if w is not labeled as explored then
---------> 11                  label w as explored
---------> 12                  Q.enqueue(w)
-------> More details
---------> This non-recursive implementation is similar to the non-recursive implementation of depth-first search, but differs from it in two ways:
-----------> it uses a queue (First In First Out) instead of a stack and
-----------> it checks whether a vertex has been explored before enqueueing the vertex rather than delaying this check until the vertex is dequeued from the queue.
---------> If G is a tree, replacing the queue of this breadth-first search algorithm with a stack will yield a depth-first search algorithm. 
-----------> For general graphs, replacing the stack of the iterative depth-first search implementation 
-----------> with a queue would also produce a breadth-first search algorithm, although a somewhat nonstandard one.
---------> The Q queue contains the frontier along which the algorithm is currently searching.
-----------> Nodes can be labelled as explored by storing them in a set, or by an attribute on each node, depending on the implementation.
-----------> Note that the word node is usually interchangeable with the word vertex.
-----------> The parent attribute of each node is useful for accessing the nodes in a shortest path, 
-----------> for example by backtracking from the destination node up to the starting node, once the BFS has been run, 
-----------> and the predecessors nodes have been set.
---------> Breadth-first search produces a so-called breadth first tree. 
-------> BFS and its application in finding connected components of graphs were invented in 1945 by Konrad Zuse, 
---------> in his (rejected) Ph.D. thesis on the Plankalkül programming language, but this was not published until 1972. 
---------> It was reinvented in 1959 by Edward F. Moore, who used it to find the shortest path out of a maze,
---------> and later developed by C. Y. Lee into a wire routing algorithm (published 1961).



-----> Brute-force search: 
-------> This An exhaustive and reliable search method, but computationally inefficient in many applications.
-------> Brute-force search or exhaustive search, also known as generate and test, 
---------> is a very general problem-solving technique and algorithmic paradigm that consists of systematically enumerating all possible candidates for the solution 
---------> and checking whether each candidate satisfies the problem's statement.
-------> A brute-force algorithm that finds the divisors of a natural number n would enumerate all integers from 1 to n, 
---------> and check whether each of them divides n without remainder.
---------> A brute-force approach for the eight queens puzzle would examine all possible arrangements of 8 pieces on the 64-square chessboard 
---------> and for each arrangement, check whether each (queen) piece can attack any other.
-------> While a brute-force search is simple to implement and will always find a solution if it exists, 
---------> implementation costs are proportional to the number of candidate solutions,
---------> which in many practical problems tends to grow very quickly as the size of the problem increases (Combinatorial explosion).
---------> Therefore, brute-force search is typically used when the problem size is limited, 
---------> or when there are problem-specific heuristics that can be used to reduce the set of candidate solutions to a manageable size. 
---------> The method is also used when the simplicity of implementation is more important than speed.
-------> This is the case, for example, in critical applications where any errors in the algorithm would have very serious consequences or when using a computer to prove a mathematical theorem. 
---------> Brute-force search is also useful as a baseline method when benchmarking other algorithms or metaheuristics. 
---------> Indeed, brute-force search can be viewed as the simplest metaheuristic. 
---------> Brute force search should not be confused with backtracking, 
---------> where large sets of solutions can be discarded without being explicitly enumerated (as in the textbook computer solution to the eight queens problem above). 
---------> The brute-force method for finding an item in a table – namely, check all entries of the latter, sequentially – is called linear search. 
-------> Basic algorithm
---------> In order candidate for P after the current one c.
-----------> valid (P, c): check whether candidate c is a solution for P.
-----------> output (P, c): use the solution c of P as appropriate to the application.
---------> The next procedure must also tell when there are no more candidates for the instance P, after the current one c.
---------> A convenient way to do that is to return a "null candidate", some conventional data value Λ that is distinct from any real candidate. 
---------> Likewise the first procedure should return Λ if there are no candidates at all for the instance P. 
---------> The brute-force method is then expressed by the algorithm:
-----------> c ← first(P)
-----------> while c ≠ Λ do
----------->     if valid(P,c) then
----------->         output(P, c)
----------->     c ← next(P, c)
-----------> end while
---------> For example, when looking for the divisors of an integer n, the instance data P is the number n. 
-----------> The call first(n) should return the integer 1 if n ≥ 1, or Λ otherwise; the call next(n,c) should return c + 1 if c < n, and Λ otherwise; 
-----------> and valid(n,c) should return true if and only if c is a divisor of n. (In fact, if we choose Λ to be n + 1, the tests n ≥ 1 and c < n are unnecessary.)
-----------> The brute-force search algorithm above will call output for every candidate that is a solution to the given instance P. 
-----------> The algorithm is easily modified to stop after finding the first solution, or a specified number of solutions; 
-----------> or after testing a specified number of candidates, or after spending a given amount of CPU time. 

-----> D*: 
-------> This an incremental heuristic search algorithm.
-------> D* (pronounced "D star") is any one of the following three related incremental search algorithms:
---------> The original D*,[1] by Anthony Stentz, is an informed incremental search algorithm.
---------> Focused D*[2] is an informed incremental heuristic search algorithm by Anthony Stentz that combines ideas of A* and the original D*. 
-----------> Focused D* resulted from a further development of the original D*.
---------> D* Lite[4] is an incremental heuristic search algorithm by Sven Koenig and Maxim Likhachev that builds on LPA*,
-----------> an incremental heuristic search algorithm that combines ideas of A* and Dynamic SWSF-FP.[6]
-------> All three search algorithms solve the same assumption-based path planning problems, 
---------> including planning with the freespace assumption, 
---------> where a robot has to navigate to given goal coordinates in unknown terrain. 
-------> It makes assumptions about the unknown part of the terrain (for example: that it contains no obstacles) 
---------> and finds a shortest path from its current coordinates to the goal coordinates under these assumptions. 
---------> The robot then follows the path. 
---------> When it observes new map information (such as previously unknown obstacles), it adds the information to its map 
---------> and, if necessary, replans a new shortest path from its current coordinates to the given goal coordinates. 
---------> It repeats the process until it reaches the goal coordinates or determines that the goal coordinates cannot be reached. 
---------> When traversing unknown terrain, new obstacles may be discovered frequently, so this replanning needs to be fast. 
---------> Incremental (heuristic) search algorithms speed up searches for sequences of similar search problems by
---------> using experience with the previous problems to speed up the search for the current one. 
---------> Assuming the goal coordinates do not change, all three search algorithms are more efficient than repeated A* searches.
-------> D* and its variants have been widely used for mobile robot and autonomous vehicle navigation.
---------> Current systems are typically based on D* Lite rather than the original D* or Focused D*. 
---------> In fact, even Stentz's lab uses D* Lite rather than D* in some implementations. 
---------> Such navigation systems include a prototype system tested on the Mars rovers Opportunity 
---------> and Spirit and the navigation system of the winning entry in the DARPA Urban Challenge, both developed at Carnegie Mellon University.
-------> Operation
---------> The basic operation of D* is outlined below.
---------> Like Dijkstra's algorithm and A*, D* maintains a list of nodes to be evaluated, known as the "OPEN list". 
---------> Nodes are marked as having one of several states:
-----------> NEW, meaning it has never been placed on the OPEN list
-----------> OPEN, meaning it is currently on the OPEN list
-----------> CLOSED, meaning it is no longer on the OPEN list
-----------> RAISE, indicating its cost is higher than the last time it was on the OPEN list
-----------> LOWER, indicating its cost is lower than the last time it was on the OPEN list
-------> Note: The original D* was introduced by Anthony Stentz in 1994. 
---------> The name D* comes from the term "Dynamic A*", because the algorithm behaves like A* except that the arc costs can change as the algorithm runs. 

-----> Depth-first search: 
-------> This traverses a graph branch by branch.
-------> Depth-first search (DFS) is an algorithm for traversing or searching tree or graph data structures. 
---------> The algorithm starts at the root node (selecting some arbitrary node as the root node in the case of a graph) 
---------> and explores as far as possible along each branch before backtracking. 
---------> Extra memory, usually a stack, is needed to keep track of the nodes discovered so far along a specified branch which helps in backtracking of the graph.
-------> Properties
-------> The time and space analysis of DFS differs according to its application area. 
---------> DFS is typically used to traverse an entire graph, and takes time O(|V|+|E|),[4] where |V| is the number of vertices and |E| the number of edges. 
---------> This is linear in the size of the graph. 
---------> In these applications it also uses space O(|V|) in the worst case to store the stack of vertices 
---------> on the current search path as well as the set of already-visited vertices. 
---------> Thus, in this setting, the time and space bounds are the same as for breadth-first search
---------> and the choice of which of these two algorithms to use depends less on their complexity 
---------> and more on the different properties of the vertex orderings the two algorithms produce.
-------> For applications of DFS in relation to specific domains, such as searching for solutions in artificial intelligence or web-crawling, 
---------> the graph to be traversed is often either too large to visit in its entirety or infinite (DFS may suffer from non-termination). 
---------> In such cases, search is only performed to a limited depth; due to limited resources, such as memory or disk space, 
---------> one typically does not use data structures to keep track of the set of all previously visited vertices. 
---------> When search is performed to a limited depth, the time is still linear in terms of the number of expanded vertices and edges 
---------> (although this number is not the same as the size of the entire graph because some vertices may be searched more than once and others not at all) 
---------> but the space complexity of this variant of DFS is only proportional to the depth limit,
---------> and as a result, is much smaller than the space needed for searching to the same depth using breadth-first search. 
---------> For such applications, DFS also lends itself much better to heuristic methods for choosing a likely-looking branch. 
---------> When an appropriate depth limit is not known a priori, iterative deepening depth-first search applies DFS repeatedly with a sequence of increasing limits. 
---------> In the artificial intelligence mode of analysis, with a branching factor greater than one, 
---------> iterative deepening increases the running time by only a constant factor over the case in which the correct depth limit is known due to the geometric growth of the number of nodes per level.
-------> DFS may also be used to collect a sample of graph nodes. 
---------> However, incomplete DFS, similarly to incomplete BFS, is biased towards nodes of high degree. 
-------> Note: A version of depth-first search was investigated in the 19th century by French mathematician Charles Pierre Trémaux[1] as a strategy for solving mazes.

-----> Dijkstra's algorithm:
-------> This is special case of A* for which no heuristic function is used
-------> Dijkstra's algorithm is discussed above.

-----> General Problem Solver: 
-------> This a seminal theorem-proving algorithm intended to work as a universal problem solver machine.
-------> General Problem Solver (GPS) is a computer program  intended to work as a universal problem solver machine. 
---------> In contrast to the former Logic Theorist project, the GPS works with means–ends analysis.[
---------> Any problem that can be expressed as a set of well-formed formulas (WFFs) or Horn clauses, 
---------> and that constitute a directed graph with one or more sources (that is, axioms) and sinks (that is, desired conclusions), can be solved, in principle, by GPS. 
---------> Proofs in the predicate logic and Euclidean geometry problem spaces are prime examples of the domain the applicability of GPS. 
---------> It was based on Simon and Newell's theoretical work on logic machines.
---------> GPS was the first computer program which separated its knowledge of problems (rules represented as input data) from its strategy of how to solve problems (a generic solver engine). 
---------> GPS was implemented in the third-order programming language, IPL.
-------> While GPS solved simple problems such as the Towers of Hanoi that could be sufficiently formalized, 
---------> it could not solve any real-world problems because search was easily lost in the combinatorial explosion. 
---------> Put another way, the number of "walks" through the inferential digraph became computationally untenable. 
---------> (In practice, even a straightforward state space search such as the Towers of Hanoi can become computationally infeasible, 
---------> albeit judicious prunings of the state space can be achieved by such elementary AI techniques as A* and IDA*).
-------> The user defined objects and operations that could be done on the objects, and GPS generated heuristics by means-ends analysis in order to solve problems. 
---------> It focused on the available operations, finding what inputs were acceptable and what outputs were generated. It then created subgoals to get closer and closer to the goal.
-------> The GPS paradigm eventually evolved into the Soar architecture for artificial intelligence. 
-------> Note: This was created in 1959 by Herbert A. Simon, J. C. Shaw, and Allen Newell (RAND Corporation)

-----> Iterative deepening depth-first search (IDDFS): 
-------> This a state space search strategy.
-------> Iterative deepening search or more specifically iterative deepening depth-first search (IDS or IDDFS) 
---------> is a state space/graph search strategy in which a depth-limited version of depth-first search is run repeatedly with increasing depth limits until the goal is found. 
---------> IDDFS is optimal like breadth-first search, but uses much less memory; at each iteration, it visits the nodes in the search tree in the same order as depth-first search, 
---------> but the cumulative order in which nodes are first visited is effectively breadth-first.
-------> Algorithm for directed graphs
---------> The following pseudocode shows IDDFS implemented in terms of a recursive depth-limited DFS (called DLS) for directed graphs. 
---------> This implementation of IDDFS does not account for already-visited nodes and therefore does not work for undirected graphs.
-----------> function IDDFS(root) is
----------->     for depth from 0 to ∞ do
----------->         found, remaining ← DLS(root, depth)
----------->         if found ≠ null then
----------->             return found
----------->         else if not remaining then
----------->             return null
-----------> function DLS(node, depth) is
----------->     if depth = 0 then
----------->         if node is a goal then
----------->             return (node, true)
----------->         else
----------->             return (null, true)    (Not found, but may have children)
----------->     else if depth > 0 then
----------->         any_remaining ← false
----------->         foreach child of node do
----------->             found, remaining ← DLS(child, depth−1)
----------->             if found ≠ null then
----------->                 return (found, true)   
----------->             if remaining then
----------->                 any_remaining ← true    (At least one node found at depth, let IDDFS deepen)
----------->         return (null, any_remaining)
---------> If the goal node is found, then DLS unwinds the recursion returning with no further iterations. 
-----------> Otherwise, if at least one node exists at that level of depth, the remaining flag will let IDDFS continue.
---------> 2-tuples are useful as return value to signal IDDFS to continue deepening or stop, in case tree depth and goal membership are unknown a priori. 
-----------> Another solution could use sentinel values instead to represent not found or remaining level results. 

-----> Jump point search:
-------> This an optimization to A* which may reduce computation time by an order of magnitude using further heuristics. 
-------> Jump point search (JPS) is an optimization to the A* search algorithm for uniform-cost grids. 
---------> It reduces symmetries in the search procedure by means of graph pruning,
---------> eliminating certain nodes in the grid based on assumptions that can be made about the current node's neighbors, 
---------> as long as certain conditions relating to the grid are satisfied. 
---------> As a result, the algorithm can consider long "jumps" along straight (horizontal, vertical and diagonal) lines in the grid, 
---------> rather than the small steps from one grid position to the next that ordinary A* considers.
-------> Jump point search preserves A*'s optimality, while potentially reducing its running time by an order of magnitude.

-----> Lexicographic breadth-first search (also known as Lex-BFS): 
-------> This is a linear time algorithm for ordering the vertices of a graph.
-------> Lexicographic breadth-first search or Lex-BFS is a linear time algorithm for ordering the vertices of a graph. 
---------> The algorithm is different from a breadth-first search, but it produces an ordering that is consistent with breadth-first search.
-------> Background
---------> The breadth-first search algorithm is commonly defined by the following process:
-----------> Initialize a queue of graph vertices, with the starting vertex of the graph as the queue's only element.
-----------> While the queue is non-empty, remove (dequeue) a vertex v from the queue, and add to the queue (enqueue) 
---------> all the other vertices that can be reached by an edge from v that have not already been added in earlier steps.
---------> However, rather than defining the vertex to choose at each step in an imperative way as the one produced by the dequeue operation of a queue, 
---------> one can define the same sequence of vertices declaratively by the properties of these vertices. 
---------> That is, a standard breadth-first search is just the result of repeatedly applying this rule:
-----------> Repeatedly output a vertex v, choosing at each step a vertex v that has not already been chosen 
-----------> and that has a predecessor (a vertex that has an edge to v) as early in the output as possible.
---------> In some cases, this ordering of vertices by the output positions of their predecessors may have ties
-----------> two different vertices have the same earliest predecessor. 
---------> In this case, the order in which those two vertices are chosen may be arbitrary. 
---------> The output of lexicographic breadth-first search differs from a standard breadth-first search in having a consistent rule for breaking such ties. 
---------> In lexicographic breadth-first search, the output ordering is the order that would be produced by the rule:
-----------> Repeatedly output a vertex v, choosing at each step a vertex v that has not already been chosen 
---------> and whose entire set of already-output predecessors is as small as possible in lexicographic order.
---------> So, when two vertices v and w have the same earliest predecessor, earlier than any other unchosen vertices, 
-----------> the standard breadth-first search algorithm will order them arbitrarily. 
-----------> Instead, in this case, the LexBFS algorithm would choose between v and w by the output ordering of their second-earliest predecessors. 
-----------> If only one of them has a second-earliest predecessor that has already been output, that one is chosen. 
-----------> If both v and w have the same second-earliest predecessor, then the tie is broken by considering their third-earliest predecessors, and so on.
---------> Applying this rule directly by comparing vertices according to this rule would lead to an inefficient algorithm. 
-----------> Instead, the lexicographic breadth-first search uses a set partitioning data structure in order to produce the same ordering more efficiently, 
-----------> just as a standard breadth-first search uses a queue data structure to produce its ordering efficiently.
-------> Algorithm
---------> The lexicographic breadth-first search algorithm replaces 
---------> the queue of vertices of a standard breadth-first search with an ordered sequence of sets of vertices. 
---------> The sets in the sequence form a partition of the remaining vertices. 
---------> At each step, a vertex v from the first set in the sequence is removed from that set, 
---------> and if that removal causes the set to become empty then the set is removed from the sequence. 
---------> Then, each set in the sequence is replaced by two subsets: the neighbors of v and the non-neighbors of v. 
---------> The subset of neighbors is placed earlier in the sequence than the subset of non-neighbors. 
---------> In pseudocode, the algorithm can be expressed as follows:
-----------> Initialize a sequence Σ of sets, to contain a single set containing all vertices.
-----------> Initialize the output sequence of vertices to be empty.
-----------> While Σ is non-empty:
----------->     Find and remove a vertex v from the first set in Σ
----------->     If the first set in Σ is now empty, remove it from Σ
----------->     Add v to the end of the output sequence.
----------->     For each edge v-w such that w still belongs to a set S in Σ:
----------->         If the set S containing w has not yet been replaced while processing v, 
----------->         create a new empty replacement set T and place it prior to S in the sequence; otherwise, let T be the set prior to S.
----------->         Move w from S to T, and if this causes S to become empty remove S from Σ.
-------> Each vertex is processed once, each edge is examined only when its two endpoints are processed, 
---------> and (with an appropriate representation for the sets in Σ that allows items to be moved from one set to another in constant time) 
---------> each iteration of the inner loop takes only constant time. 
---------> Therefore, like simpler graph search algorithms such as breadth-first search and depth first search, this algorithm takes linear time.
-------> The algorithm is called lexicographic breadth-first search because the order it produces is an ordering that could also have been produced by a breadth-first search, 
---------> and because if the ordering is used to index the rows and columns of an adjacency matrix of a graph then the algorithm sorts the rows and columns into lexicographical order. 
-------> Note: The lexicographic breadth-first search algorithm is based on the idea of partition refinement and was first developed by Donald J. Rose, Robert E. Tarjan, and George S. Lueker (1976).
---------> A more detailed survey of the topic is presented by Corneil (2004). 
---------> It has been used as a subroutine in other graph algorithms including the recognition of chordal graphs, and optimal coloring of distance-hereditary graphs. 

-----> Uniform-cost search: 
-------> This a tree search that finds the lowest-cost route where costs vary.
-------> In common presentations of Dijkstra's algorithm, initially all nodes are entered into the priority queue. 
---------> This is, however, not necessary: the algorithm can start with a priority queue that contains only one item, and insert new items as they are discovered 
---------> instead of doing a decrease-key, check whether the key is in the queue; if it is, decrease its key, otherwise insert it).
---------> This variant has the same worst-case bounds as the common variant, but maintains a smaller priority queue in practice, speeding up the queue operations.
-------> Moreover, not inserting all nodes in a graph makes it possible to extend the algorithm to find the shortest path from a single source 
---------> to the closest of a set of target nodes on infinite graphs or those too large to represent in memory. 
---------> The resulting algorithm is called uniform-cost search (UCS) in the artificial intelligence literature and can be expressed in pseudocode as
-----------> procedure uniform_cost_search(start) is
----------->     node ← start
----------->     frontier ← priority queue containing node only
----------->     expanded ← empty set
----------->     do
----------->         if frontier is empty then
----------->             return failure
----------->         node ← frontier.pop()
----------->         if node is a goal state then
----------->             return solution(node)
----------->         expanded.add(node)
----------->         for each of node's neighbors n do
----------->             if n is not in expanded and not in frontier then
----------->                 frontier.add(n)
----------->             else if n is in frontier with higher cost
----------->                 replace existing node with n
-------> The complexity of this algorithm can be expressed in an alternative way for very large graphs:
---------> when C* is the length of the shortest path from the start node to any node satisfying the "goal" predicate, each edge has cost at least ε, 
---------> and the number of neighbors per node is bounded by b, then the algorithm's worst-case time and space complexity are both in O(b1+⌊C* ⁄ ε⌋).
-------> Further optimizations of Dijkstra's algorithm for the single-target case include bidirectional variants, 
---------> goal-directed variants such as the A* algorithm (see § Related problems and algorithms), 
---------> graph pruning to determine which nodes are likely to form the middle segment of shortest paths (reach-based routing), 
---------> and hierarchical decompositions of the input graph that reduce s–t routing to connecting s and t to their respective "transit nodes" 
---------> followed by shortest-path computation between these transit nodes using a "highway".
---------> Combinations of such techniques may be needed for optimal practical performance on specific problems.

-----> SSS*: 
-------> This state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm.
-------> SSS* is a search algorithm that conducts a state space search traversing a game tree in a best-first fashion similar to that of the A* search algorithm. 
-------> SSS* is based on the notion of solution trees. 
-------> Informally, a solution tree can be formed from any arbitrary game tree by pruning the number of branches at each MAX node to one. 
---------> Such a tree represents a complete strategy for MAX, since it specifies exactly one MAX action for every possible sequence of moves made by the opponent. 
---------> Given a game tree, SSS* searches through the space of partial solution trees, gradually analyzing larger and larger subtrees, 
---------> eventually producing a single solution tree with the same root and Minimax value as the original game tree. 
---------> SSS* never examines a node that alpha-beta pruning would prune, and may prune some branches that alpha-beta would not. 
---------> Stockman speculated that SSS* may therefore be a better general algorithm than alpha-beta. 
---------> However, Igor Roizen and Judea Pearl have shown that the savings in the number of positions that SSS*
---------> evaluates relative to alpha/beta is limited and generally not enough to compensate for the increase in other resources 
---------> (e.g., the storing and sorting of a list of nodes made necessary by the best-first nature of the algorithm). 
---------> However, Aske Plaat, Jonathan Schaeffer, Wim Pijls and Arie de Bruin have shown that a sequence of null-window alpha-beta calls is equivalent to SSS* 
---------> (i.e., it expands the same nodes in the same order) when alpha-beta is used with a transposition table, as is the case in all game-playing programs for chess, checkers, etc. 
---------> Now the storing and sorting of the OPEN list were no longer necessary. 
---------> This allowed the implementation of (an algorithm equivalent to) SSS* in tournament quality game-playing programs. 
---------> Experiments showed that it did indeed perform better than Alpha-Beta in practice, but that it did not beat NegaScout.
-------> The reformulation of a best-first algorithm as a sequence of depth-first calls prompted 
---------> the formulation of a class of null-window alpha-beta algorithms, of which MTD(f) is the best known example. 
-------> Algorithm
---------> There is a priority queue OPEN that stores states (J, s, h) or the nodes, 
---------> where J - node identificator (Dot-decimal notation is used to identify nodes, ϵ is a root), 
---------> s is an element of {L,S} - state of the node J (L - the node is live, which means it's not solved yet and S - the node is solved), 
---------> h ∈ (− ∞ , ∞) - value of the solved node. 
---------> Items in OPEN queue are sorted descending by their h value. 
---------> If more than one node has the same value of h, a node left-most in the tree is chosen.
-----------> OPEN := { (e, L, inf) }
-----------> while true do   // repeat until stopped
----------->     pop an element p=(J, s, h) from the head of the OPEN queue
----------->     if J = e and s = S then
----------->         STOP the algorithm and return h as a result
----------->     else
----------->         apply Gamma operator for p
---------> Γ/Gamma operator for p=(J, s, h) is defined in the following way:
-----------> if s = L then
----------->     if J is a terminal node then
----------->         (1.) add (J, S, min(h, value(J))) to OPEN
----------->     else if J is a MIN node then
----------->         (2.) add (J.1, L, h) to OPEN
----------->     else
----------->         (3.) for j=1..number_of_children(J) add (J.j, L, h) to OPEN
-----------> else
----------->     if J is a MIN node then
----------->         (4.) add (parent(J), S, h) to OPEN
----------->              remove from OPEN all the states that are associated with the children of parent(J)
----------->     else if is_last_child(J) then   // if J is the last child of parent(J)
----------->         (5.) add (parent(J), S, h) to OPEN
----------->     else
----------->         (6.) add (parent(J).(k+1), L, h) to OPEN   // add state associated with the next child of parent(J) to OPEN
-------> Note: This was introduced by George Stockman in 1979



---> Subgraphs

-----> Cliques
-------> A clique (/ˈkliːk/ or /ˈklɪk/) is a subset of vertices of an undirected graph such that every TWO distinct vertices in the clique are adjacent. 
---------> Me: So basically every vertex is adjacent to any other vertex.
---------> That is, a clique of a graph G is an induced subgraph of G that is complete. 
---------> Cliques are one of the basic concepts of graph theory and are used in many other mathematical problems and constructions on graphs. 
---------> Task of finding whether there is a clique of a given size in a graph (the clique problem) is NP-complete, 
---------> but despite this hardness result, many algorithms for finding cliques have been studied.
-------> Although the study of complete subgraphs goes back at least to the graph-theoretic reformulation of Ramsey theory by Erdős & Szekeres (1935), 
---------> the term clique comes from Luce & Perry (1949), who used complete subgraphs in social networks to model cliques of people; t
---------> hat is, groups of people all of whom know each other. 
---------> Cliques have many other applications in the sciences and particularly in bioinformatics. 
-------> Definitions
---------> A clique, C, in an undirected graph G = (V, E) is a subset of the vertices, C is an element of V, such that every two distinct vertices are adjacent. 
-----------> This is equivalent to the condition that the induced subgraph of G induced by C is a complete graph. 
-----------> In some cases, the term clique may also refer to the subgraph directly.
---------> A maximal clique is a clique that cannot be extended by including one more adjacent vertex, 
-----------> that is, a clique which does not exist exclusively within the vertex set of a larger clique. 
-----------> Some authors define cliques in a way that requires them to be maximal, and use other terminology for complete subgraphs that are not maximal.
---------> A maximum clique of a graph, G, is a clique, such that there is no clique with more vertices. 
-----------> Moreover, the clique number ω(G) of a graph G is the number of vertices in a maximum clique in G.
---------> The intersection number of G is the smallest number of cliques that together cover all edges of G.
---------> The clique cover number of a graph G is the smallest number of cliques of G whose union covers the set of vertices V of the graph.
---------> A maximum clique transversal of a graph is a subset of vertices with the property that each maximum clique of the graph contains at least one vertex in the subset.
---------> The opposite of a clique is an independent set, in the sense that every clique corresponds to an independent set in the complement graph.
-----------> The clique cover problem concerns finding as few cliques as possible that include every vertex in the graph.
---------> A related concept is a biclique, a complete bipartite subgraph. 
-----------> The bipartite dimension of a graph is the minimum number of bicliques needed to cover all the edges of the graph. 

-------> Bron–Kerbosch algorithm: 
---------> This a technique for finding maximal cliques in an undirected graph.
---------> The Bron–Kerbosch algorithm is an enumeration algorithm for finding all maximal cliques in an undirected graph. 
-----------> That is, it lists all subsets of vertices with the two properties that each pair of vertices in one of the listed subsets is connected by an edge, 
-----------> and no listed subset can have any additional vertices added to it while preserving its complete connectivity. 
-----------> The Bron–Kerbosch algorithm was designed by Dutch scientists Coenraad Bron and Joep Kerbosch, who published its description in 1973.
---------> Although other algorithms for solving the clique problem have running times that are, 
-----------> in theory, better on inputs that have few maximal independent sets, the Bron–Kerbosch algorithm
-----------> and subsequent improvements to it are frequently reported as being more efficient in practice than the alternatives. 
-----------> It is well-known and widely used in application areas of graph algorithms such as computational chemistry.
---------> Without pivoting
-----------> The basic form of the Bron–Kerbosch algorithm is a recursive backtracking algorithm that searches for all maximal cliques in a given graph G. 
-------------> More generally, given three disjoint sets of vertices R, P, and X, 
-------------> it finds the maximal cliques that include all of the vertices in R, some of the vertices in P, and none of the vertices in X. 
-------------> In each call to the algorithm, P and X are disjoint sets whose union consists of those vertices that form cliques when added to R. 
-------------> In other words, P ∪ X is the set of vertices which are joined to every element of R. 
-------------> When P and X are both empty there are no further elements that can be added to R, so R is a maximal clique and the algorithm outputs R.
-----------> The recursion is initiated by setting R and X to be the empty set and P to be the vertex set of the graph. 
-------------> Within each recursive call, the algorithm considers the vertices in P in turn; if there are no such vertices, it either reports R as a maximal clique (if X is empty), or backtracks. 
-------------> For each vertex v chosen from P, it makes a recursive call in which v is added to R and in which P and X are restricted to the neighbor set N(v) of v, 
-------------> which finds and reports all clique extensions of R that contain v. 
-------------> Then, it moves v from P to X to exclude it from consideration in future cliques and continues with the next vertex in P.
-----------> That is, in pseudocode, the algorithm performs the following steps:
-------------> algorithm BronKerbosch1(R, P, X) is
------------->     if P and X are both empty then
------------->         report R as a maximal clique
------------->     for each vertex v in P do
------------->         BronKerbosch1(R union {v}, P intersection N(v), X intersection N(v))
------------->         P := P \ {v}
------------->         X := X union {v}
---------> With pivoting
-----------> The basic form of the algorithm, described above, is inefficient in the case of graphs with many non-maximal cliques: it makes a recursive call for every clique, maximal or not. 
-----------> To save time and allow the algorithm to backtrack more quickly in branches of the search that contain no maximal cliques, 
-----------> Bron and Kerbosch introduced a variant of the algorithm involving a "pivot vertex" u, chosen from P (or more generally, as later investigators realized,[4] from P union of X). 
-----------> Any maximal clique must include either u or one of its non-neighbors, for otherwise the clique could be augmented by adding u to it. 
-----------> Therefore, only u and its non-neighbors need to be tested as the choices for the vertex v that is added to R in each recursive call to the algorithm. 
-----------> In pseudocode:
-------------> algorithm BronKerbosch2(R, P, X) is
------------->     if P and X are both empty then
------------->         report R as a maximal clique
------------->     choose a pivot vertex u in P union X
------------->     for each vertex v in P \ N(u) do
------------->         BronKerbosch2(R union {v}, P intersection N(v), X intersection N(v))
------------->         P := P \ {v}
------------->         X := X union {v}
-----------> If the pivot is chosen to minimize the number of recursive calls made by the algorithm, 
-----------> the savings in running time compared to the non-pivoting version of the algorithm can be significant.
---------> With vertex ordering
-----------> An alternative method for improving the basic form of the Bron–Kerbosch algorithm involves forgoing pivoting at the outermost level of recursion, 
-------------> and instead choosing the ordering of the recursive calls carefully in order to minimize the sizes of the sets P of candidate vertices within each recursive call.
-----------> The degeneracy of a graph G is the smallest number d such that every subgraph of G has a vertex with degree d or less. 
-------------> Every graph has a degeneracy ordering, an ordering of the vertices such that each vertex has d or fewer neighbors that come later in the ordering; 
-------------> a degeneracy ordering may be found in linear time by repeatedly selecting the vertex of minimum degree among the remaining vertices. 
-------------> If the order of the vertices v that the Bron–Kerbosch algorithm loops through is a degeneracy ordering, 
-------------> then the set P of candidate vertices in each call (the neighbors of v that are later in the ordering) will be guaranteed to have size at most d. 
-------------> The set X of excluded vertices will consist of all earlier neighbors of v, and may be much larger than d. 
-------------> In recursive calls to the algorithm below the topmost level of the recursion, the pivoting version can still be used.
-----------> In pseudocode, the algorithm performs the following steps:
-------------> algorithm BronKerbosch3(G) is
------------->     P = V(G)
------------->     R = X = empty
------------->     for each vertex v in a degeneracy ordering of G do
------------->         BronKerbosch2({v}, P is an element of N(v), X is an element of N(v))
------------->         P := P \ {v}
------------->         X := X union {v}
-----------> This variant of the algorithm can be proven to be efficient for graphs of small degeneracy,
-------------> and experiments show that it also works well in practice for large sparse social networks and other real-world graphs.
---------> Note: A contemporaneous algorithm of Akkoyunlu (1973), although presented in different terms, 
-----------> can be viewed as being the same as the Bron–Kerbosch algorithm, as it generates the same search tree.

-------> MaxCliqueDyn maximum clique algorithm: 
---------> This finds a maximum clique in an undirected graph.
---------> The MaxCliqueDyn algorithm is an algorithm for finding a maximum clique in an undirected graph. 
-----------> It is based on a basic algorithm (MaxClique algorithm) which finds a maximum clique of bounded size. 
-----------> The bound is found using improved coloring algorithm. 
-----------> The MaxCliqueDyn extends MaxClique algorithm to include dynamically varying bounds. 
-----------> In comparison to earlier algorithms described in the published article the MaxCliqueDyn algorithm 
-------------> is improved by an improved approximate coloring algorithm (ColorSort algorithm) 
-----------> and by applying tighter, more computationally expensive upper bounds on a fraction of the search space. 
-----------> Both improvements reduce time to find maximum clique. 
-----------> In addition to reducing time improved coloring algorithm also reduces the number of steps needed to find a maximum clique.
---------> MaxClique algorithm
-----------> The MaxClique algorithm is the basic algorithm of MaxCliqueDyn algorithm. 
-----------> The pseudo code of the algorithm is:
-----------> procedure MaxClique(R, C) is
----------->     Q = Ø; Qmax = Ø;
----------->     while R ≠ Ø do
----------->         choose a vertex p with a maximum color C(p) from set R;
----------->         R := R\{p};
----------->         if |Q| + C(p)>|Qmax| then
----------->             Q := Q union {p};
----------->             if R intersection Γ(p) ≠ Ø then
----------->                 obtain a vertex-coloring C' of G(R intersection Γ(p));
----------->                 MaxClique(R ⋂ Γ(p), C');
----------->             else if |Q|>|Qmax| then Qmax:=Q;
----------->             Q := Q\{p};
----------->         else
----------->             return
----------->     end while
-----------> where Q is a set of vertices of the currently growing clique, 
-----------> Qmax is a set of vertices of the largest clique currently found, R is a set of candidate vertices and C its corresponding set of color classes. 
-----------> The MaxClique algorithm recursively searches for maximum clique by adding and removing vertices to and from Q. 
---------> Note: This algorithm was designed by Janez Konc and description was published in 2007. 

-------> Strongly connected components
---------> In the mathematical theory of directed graphs, a graph is said to be strongly connected if every vertex is reachable from every other vertex. 
-----------> The strongly connected components of an arbitrary directed graph form a partition into subgraphs that are themselves strongly connected. 
-----------> It is possible to test the strong connectivity of a graph, or to find its strongly connected components, in linear time (that is, Θ(V + E)).
---------> A directed graph is called strongly connected if there is a path in each direction between each pair of vertices of the graph. 
-----------> That is, a path exists from the first vertex in the pair to the second, and another path exists from the second vertex to the first. 
-----------> In a directed graph G that may not itself be strongly connected, a pair of vertices u and v are said to be strongly connected to each other if there is a path in each direction between them.
---------> The binary relation of being strongly connected is an equivalence relation, and the induced subgraphs of its equivalence classes are called strongly connected components. 
-----------> Equivalently, a strongly connected component of a directed graph G is a subgraph that is strongly connected, and is maximal with this property: 
-----------> no additional edges or vertices from G can be included in the subgraph without breaking its property of being strongly connected. 
-----------> The collection of strongly connected components forms a partition of the set of vertices of G. 
-----------> A strongly connected component C is called trivial when C consists of a single vertex which is not connected to itself with an edge and non-trivial otherwise.
---------> If each strongly connected component is contracted to a single vertex, the resulting graph is a directed acyclic graph, the condensation of G. 
-----------> A directed graph is acyclic if and only if it has no strongly connected subgraphs with more than one vertex, 
-----------> because a directed cycle is strongly connected and every non-trivial strongly connected component contains at least one directed cycle. 
---------> Algorithms
-----------> DFS-based linear-time algorithms
-------------> Several algorithms based on depth first search compute strongly connected components in linear time.
-------------> (1) Kosaraju's algorithm uses two passes of depth first search. 
---------------> The first, in the original graph, is used to choose the order in which the outer loop of the second depth first search
---------------> tests vertices for having been visited already and recursively explores them if not. 
---------------> The second depth first search is on the transpose graph of the original graph, and each recursive exploration finds a single new strongly connected component.[2][3] 
---------------> It is named after S. Rao Kosaraju, who described it (but did not publish his results) in 1978; Micha Sharir later published it in 1981.[4]
-------------> (2) Tarjan's strongly connected components algorithm, published by Robert Tarjan in 1972, performs a single pass of depth first search. 
---------------> It maintains a stack of vertices that have been explored by the search but not yet assigned to a component, 
---------------> and calculates "low numbers" of each vertex (an index number of the highest ancestor reachable in one step from a descendant of the vertex) 
-------------> which it uses to determine when a set of vertices should be popped off the stack into a new component.
-------------> (3) The path-based strong component algorithm uses a depth first search, like Tarjan's algorithm, but with two stacks. 
---------------> One of the stacks is used to keep track of the vertices not yet assigned to components, while the other keeps track of the current path in the depth first search tree. 
---------------> The first linear time version of this algorithm was published by Edsger W. Dijkstra in 1976.[6]
-------------> Although Kosaraju's algorithm is conceptually simple, Tarjan's and the path-based algorithm require only one depth-first search rather than two.
-----------> Reachability-based algorithms
-------------> Previous linear-time algorithms are based on depth-first search which is generally considered hard to parallelize. 
---------------> Fleischer et al.[7] in 2000 proposed a divide-and-conquer approach based on reachability queries, 
---------------> and such algorithms are usually called reachability-based SCC algorithms. 
---------------> The idea of this approach is to pick a random pivot vertex and apply forward and backward reachability queries from this vertex. 
---------------> The two queries partition the vertex set into 4 subsets: vertices reached by both, either one, or none of the searches. 
---------------> One can show that a strongly connected component has to be contained in one of the subsets. 
---------------> The vertex subset reached by both searches forms a strongly connected component, and the algorithm then recurses on the other 3 subsets.
-------------> The expected sequential running time of this algorithm is shown to be O(n log n), a factor of O(log n) more than the classic algorithms. 
---------------> The parallelism comes from: (1) the reachability queries can be parallelized more easily (e.g. by a breadth-first search (BFS), 
---------------> and it can be fast if the diameter of the graph is small); and (2) the independence between the subtasks in the divide-and-conquer process. 
---------------> This algorithm performs well on real-world graphs,[3] but does not have theoretical guarantee on the parallelism 
---------------> (consider if a graph has no edges, the algorithm requires O(n) levels of recursions).
-------------> Blelloch et al.[8] in 2016 shows that if the reachability queries are applied in a random order, the cost bound of O(n log n) still holds. 
---------------> Furthermore, the queries then can be batched in a prefix-doubling manner (i.e. 1, 2, 4, 8 queries) and run simultaneously in one round. 
---------------> The overall span of this algorithm is log2 n reachability queries, which is probably the optimal parallelism that can be achieved using the reachability-based approach
-----------> Generating random strongly connected graphs
-------------> Peter M. Maurer describes an algorithm for generating random strongly connected graphs, 
-------------> based on a modification of an algorithm for strong connectivity augmentation, 
-------------> the problem of adding as few edges as possible to make a graph strongly connected. 
-------------> When used in conjunction with the Gilbert or Erdős-Rényi models with node relabelling, 
-------------> the algorithm is capable of generating any strongly connected graph on n nodes, 
-------------> without restriction on the kinds of structures that can be generated. 

---------> Path-based strong component algorithm
-----------> In graph theory, the strongly connected components of a directed graph may be found using an algorithm that uses depth-first search in combination with two stacks, 
-------------> one to keep track of the vertices in the current component and the second to keep track of the current search path.
-------------> The algorithm performs a depth-first search of the given graph G, maintaining as it does two stacks S and P (in addition to the normal call stack for a recursive function). 
-------------> Stack S contains all the vertices that have not yet been assigned to a strongly connected component, in the order in which the depth-first search reaches the vertices. 
-------------> Stack P contains vertices that have not yet been determined to belong to different strongly connected components from each other. 
-------------> It also uses a counter C of the number of vertices reached so far, which it uses to compute the preorder numbers of the vertices.
-----------> When the depth-first search reaches a vertex v, the algorithm performs the following steps:
-------------> Set the preorder number of v to C, and increment C.
-------------> Push v onto S and also onto P.
-------------> For each edge from v to a neighboring vertex w:
------------->     If the preorder number of w has not yet been assigned (the edge is a tree edge), recursively search w;
------------->     Otherwise, if w has not yet been assigned to a strongly connected component (the edge is a forward/back/cross edge):
------------->         Repeatedly pop vertices from P until the top element of P has a preorder number less than or equal to the preorder number of w.
-------------> If v is the top element of P:
------------->     Pop vertices from S until v has been popped, and assign the popped vertices to a new component.
------------->     Pop v from P.
-----------> The overall algorithm consists of a loop through the vertices of the graph, 
-------------> calling this recursive search on each vertex that does not yet have a preorder number assigned to it. 

---------> Kosaraju's algorithm
-----------> Kosaraju-Sharir's algorithm  is a linear time algorithm to find the strongly connected components of a directed graph. 
-------------> Kosaraju suggested it in 1978 but did not publish it, while Sharir independently discovered it and published it in 1981. 
-------------> It makes use of the fact that the transpose graph (the same graph with the direction of every edge reversed) has exactly the same strongly connected components as the original graph. 
-----------> The primitive graph operations that the algorithm uses are to enumerate the vertices of the graph, 
-------------> to store data per vertex (if not in the graph data structure itself, then in some table that can use vertices as indices), 
-------------> to enumerate the out-neighbours of a vertex (traverse edges in the forward direction), 
-------------> and to enumerate the in-neighbours of a vertex (traverse edges in the backward direction); 
-------------> however the last can be done without,
-------------> at the price of constructing a representation of the transpose graph during the forward traversal phase. 
-------------> The only additional data structure needed by the algorithm is an ordered list L of graph vertices, that will grow to contain each vertex once.
-----------> If strong components are to be represented by appointing a separate root vertex for each component, 
-----------> and assigning to each vertex the root vertex of its component, then Kosaraju's algorithm can be stated as follows:
-------------> For each vertex u of the graph, mark u as unvisited. Let L be empty.
-------------> For each vertex u of the graph do Visit(u), where Visit(u) is the recursive subroutine:
------------->     If u is unvisited then:
------------->         Mark u as visited.
------------->         For each out-neighbour v of u, if v is unvisited, do Visit(v).
------------->         Prepend u to L.
------------->     Otherwise do nothing.
-------------> For each element u of L in order, do Assign(u,u) where Assign(u,root) is the recursive subroutine:
------------->     If u has not been assigned to a component then:
------------->         Assign u as belonging to the component whose root is root.
------------->         For each in-neighbour v of u, do Assign(v,root).
------------->     Otherwise do nothing.
-----------> Note: Versions of this algorithm have been proposed by Purdom (1970), Munro (1971), Dijkstra (1976), Cheriyan & Mehlhorn (1996), and Gabow (2000); of these, 
-------------> Dijkstra's version was the first to achieve linear time.
-----------> Note: This also known as Kosaraju's algorithm.
-----------> Note: Aho, Hopcroft and Ullman credit it to S. Rao Kosaraju and Micha Sharir. 

---------> Tarjan's strongly connected components algorithm
-----------> Tarjan's strongly connected components algorithm is an algorithm in graph theory for finding the strongly connected components (SCCs) of a directed graph. 
-------------> It runs in linear time, matching the time bound for alternative methods including Kosaraju's algorithm and the path-based strong component algorithm. 
-----------> The algorithm takes a directed graph as input, and produces a partition of the graph's vertices into the graph's strongly connected components. 
-------------> Each vertex of the graph appears in exactly one of the strongly connected components. 
-------------> Any vertex that is not on a directed cycle forms a strongly connected component all by itself: 
-------------> for example, a vertex whose in-degree or out-degree is 0, or any vertex of an acyclic graph.
-----------> The basic idea of the algorithm is this: 
-------------> A depth-first search (DFS) begins from an arbitrary start node (and subsequent depth-first searches are conducted on any nodes that have not yet been found). 
-------------> As usual with depth-first search, the search visits every node of the graph exactly once, declining to revisit any node that has already been visited. 
-------------> Thus, the collection of search trees is a spanning forest of the graph. 
-------------> The strongly connected components will be recovered as certain subtrees of this forest. 
-------------> The roots of these subtrees are called the "roots" of the strongly connected components. 
-------------> Any node of a strongly connected component might serve as a root, if it happens to be the first node of a component that is discovered by search. 
-----------> Note: The algorithm is named for its inventor, Robert Tarjan.

-------> Subgraph isomorphism problem
---------> The subgraph isomorphism problem is a computational task in which two graphs G and H are given as input, 
-----------> and one must determine whether G contains a subgraph that is isomorphic (me: this means the same) to H. 
-----------> Subgraph isomorphism is a generalization of both the maximum clique problem and the problem of testing whether a graph contains a Hamiltonian cycle, and is therefore NP-complete. 
-----------> However certain other cases of subgraph isomorphism may be solved in polynomial time.
---------> Sometimes the name subgraph matching is also used for the same problem. This name puts emphasis on finding such a subgraph as opposed to the bare decision problem. 
---------> Decision problem and computational complexity
-----------> To prove subgraph isomorphism is NP-complete, it must be formulated as a decision problem. 
-----------> The input to the decision problem is a pair of graphs G and H. The answer to the problem is positive if H is isomorphic to a subgraph of G, and negative otherwise.
-----------> Formal question:
-------------> Let G = (V, E), H = (V′, E′) be graphs. 
---------------> Is there a subgraph G0 = (V0, E0) ∣ V0 is a subset of V , E0 is a subset of E ∩ (V0 × V0) G0 congruent with H? 
---------------> I. e., does there exist a bijection f : V0 → V′ such that {v1, v2} an element of E0 ⟺ {f(v1), f(v2)} an element of E′ ?
-------------> The proof of subgraph isomorphism being NP-complete is simple and based on reduction of the clique problem, 
---------------> an NP-complete decision problem in which the input is a single graph G and a number k, and the question is whether G contains a complete subgraph with k vertices. 
---------------> To translate this to a subgraph isomorphism problem, simply let H be the complete graph Kk; 
---------------> then the answer to the subgraph isomorphism problem for G and H is equal to the answer to the clique problem for G and k. 
---------------> Since the clique problem is NP-complete, this polynomial-time many-one reduction shows that subgraph isomorphism is also NP-complete.
-------------> An alternative reduction from the Hamiltonian cycle problem translates a graph G which is to be tested for Hamiltonicity into the pair of graphs G and H, 
---------------> where H is a cycle having the same number of vertices as G. 
---------------> Because the Hamiltonian cycle problem is NP-complete even for planar graphs, this shows that subgraph isomorphism remains NP-complete even in the planar case.
-------------> Subgraph isomorphism is a generalization of the graph isomorphism problem, which asks whether G is isomorphic to H: 
---------------> the answer to the graph isomorphism problem is true if and only if G and H both have the same numbers of vertices and edges and the subgraph isomorphism problem for G and H is true. 
---------------> However the complexity-theoretic status of graph isomorphism remains an open question.
-------------> In the context of the Aanderaa–Karp–Rosenberg conjecture on the query complexity of monotone graph properties, 
---------------> Gröger (1992) showed that any subgraph isomorphism problem has query complexity Ω(n3/2); 
---------------> that is, solving the subgraph isomorphism requires an algorithm to check the presence or absence in the input of Ω(n3/2) different edges in the graph.
---------> Algorithms
-----------> Ullmann (1976) describes a recursive backtracking procedure for solving the subgraph isomorphism problem. 
-------------> Although its running time is, in general, exponential, it takes polynomial time for any fixed choice of H (with a polynomial that depends on the choice of H). 
-------------> When G is a planar graph (or more generally a graph of bounded expansion) and H is fixed, the running time of subgraph isomorphism can be reduced to linear time.[2]
-----------> Ullmann (2010) is a substantial update to the 1976 subgraph isomorphism algorithm paper.
-----------> Cordella (2004) proposed in 2004 another algorithm based on Ullmann's, VF2, which improves the refinement process using different heuristics and uses significantly less memory.
-----------> Bonnici (2013) proposed a better algorithm, which improves the initial order of the vertices using some heuristics.
-----------> The current state of the art solver for moderately-sized, hard instances is the Glasgow Subgraph Solver (McCreesh (2020)).
-------------> This solver adopts a constraint programming approach, using bit-parallel data structures and specialized propagation algorithms for performance. 
-------------> It supports most common variations of the problem and is capable of counting or enumerating solutions as well as deciding whether one exists.
-----------> For large graphs, state-of-the art algorithms include CFL-Match and Turboiso, and extensions thereupon such as DAF by Han (2019). 



---> Sequence algorithms

-----> Approximate sequence matching

-----> Bitap algorithm: 
-------> This is a fuzzy algorithm that determines if strings are approximately equal.
-------> The bitap algorithm (also known as the shift-or, shift-and or Baeza-Yates–Gonnet algorithm) is an approximate string matching algorithm. 
---------> The algorithm tells whether a given text contains a substring which is "approximately equal" to a given pattern, 
---------> where approximate equality is defined in terms of Levenshtein distance,
---------> if the substring and pattern are within a given distance k of each other, then the algorithm considers them equal. 
---------> The algorithm begins by precomputing a set of bitmasks containing one bit for each element of the pattern. 
---------> Then it is able to do most of the work with bitwise operations, which are extremely fast.
-------> The bitap algorithm is perhaps best known as one of the underlying algorithms of the Unix utility agrep, written by Udi Manber, Sun Wu, and Burra Gopal. 
---------> Manber and Wu's original paper gives extensions of the algorithm to deal with fuzzy matching of general regular expressions.
-------> Due to the data structures required by the algorithm, it performs best on patterns less than a constant length (typically the word length of the machine in question), 
---------> and also prefers inputs over a small alphabet. 
---------> Once it has been implemented for a given alphabet and word length m, however, its running time is completely predictable, 
---------> it runs in O(mn) operations, no matter the structure of the text or the pattern.
-------> The bitap algorithm for exact string searching was invented by Bálint Dömölki in 1964 and extended by R. K. Shyamasundar in 1977, 
---------> before being reinvented by Ricardo Baeza-Yates and Gaston Gonnet in 1989 (one chapter of first author's PhD thesis) 
---------> which also extended it to handle classes of characters, wildcards, and mismatches. 
---------> In 1991, it was extended by Manber and Wu [6][7] to handle also insertions and deletions (full fuzzy string searching). 
---------> This algorithm was later improved by Baeza-Yates and Navarro in 1996.
-------> Exact searching
---------> The bitap algorithm for exact string searching, in full generality, looks like this in pseudocode:
-----------> algorithm bitap_search is
----------->     input: text as a string.
----------->            pattern as a string.
----------->     output: string
----------->     m := length(pattern)
----------->     if m = 0 then
----------->         return text
----------->     /* Initialize the bit array R. */
----------->     R := new array[m+1] of bit, initially all 0
----------->     R[0] := 1
-----------> 
----------->     for i := 0; i < length(text); i += 1 do
----------->         /* Update the bit array. */
----------->         for k := m; k ≥ 1; k -= 1 do
----------->             R[k] := R[k - 1] & (text[i] = pattern[k - 1])
-----------> 
----------->         if R[m] then
----------->             return (text + i - m) + 1
-----------> 
----------->     return null
-------> Fuzzy searching
---------> To perform fuzzy string searching using the bitap algorithm, it is necessary to extend the bit array R into a second dimension. 
-----------> Instead of having a single array R that changes over the length of the text, we now have k distinct arrays R1..k. Array Ri 
-----------> holds a representation of the prefixes of pattern that match any suffix of the current string with i or fewer errors. 
-----------> In this context, an "error" may be an insertion, deletion, or substitution; see Levenshtein distance for more information on these operations.
---------> The implementation below performs fuzzy matching (returning the first match with up to k errors) using the fuzzy bitap algorithm. 
-----------> However, it only pays attention to substitutions, not to insertions or deletions – in other words, a Hamming distance of k. 
-----------> As before, the semantics of 0 and 1 are reversed from their conventional meanings.
-------------> #include <stdlib.h>
-------------> #include <string.h>
-------------> #include <limits.h>
-------------> const char *bitap_fuzzy_bitwise_search(const char *text, const char *pattern, int k)
-------------> {
------------->     const char *result = NULL;
------------->     int m = strlen(pattern);
------------->     unsigned long *R;
------------->     unsigned long pattern_mask[CHAR_MAX+1];
------------->     int i, d;
-------------> 
------------->     if (pattern[0] == '\0') return text;
------------->     if (m > 31) return "The pattern is too long!";
-------------> 
------------->     /* Initialize the bit array R */
------------->     R = malloc((k+1) * sizeof *R);
------------->     for (i=0; i <= k; ++i)
------------->         R[i] = ~1;
-------------> 
------------->     /* Initialize the pattern bitmasks */
------------->     for (i=0; i <= CHAR_MAX; ++i)
------------->         pattern_mask[i] = ~0;
------------->     for (i=0; i < m; ++i)
------------->         pattern_mask[pattern[i]] &= ~(1UL << i);
-------------> 
------------->     for (i=0; text[i] != '\0'; ++i) {
------------->         /* Update the bit arrays */
------------->         unsigned long old_Rd1 = R[0];
-------------> 
------------->         R[0] |= pattern_mask[text[i]];
------------->         R[0] <<= 1;
-------------> 
------------->         for (d=1; d <= k; ++d) {
------------->             unsigned long tmp = R[d];
------------->             /* Substitution is all we care about */
------------->             R[d] = (old_Rd1 & (R[d] | pattern_mask[text[i]])) << 1;
------------->             old_Rd1 = tmp;
------------->         }
-------------> 
------------->         if (0 == (R[k] & (1UL << m))) {
------------->             result = (text+i - m) + 1;
------------->             break;
------------->         }
------------->     }
-------------> 
------------->     free(R);
------------->     return result;
-------------> }



-------> Phonetic algorithms
---------> A phonetic algorithm is an algorithm for indexing of words by their pronunciation. 
---------> Most phonetic algorithms were developed for English and are not useful for indexing words in other languages. 
---------> Because English spelling varies significantly depending on multiple factors, such as the word's origin and usage over time and borrowings from other languages, 
---------> phonetic algorithms necessarily take into account numerous rules and exceptions.
---------> Algorithms
-----------> Among the best-known phonetic algorithms are:
-------------> (1) Soundex, which was developed to encode surnames for use in censuses. 
---------------> Soundex codes are four-character strings composed of a single letter followed by three numbers.
-------------> (2) Daitch–Mokotoff Soundex, which is a refinement of Soundex designed to better match surnames of Slavic and Germanic origin. 
---------------> Daitch–Mokotoff Soundex codes are strings composed of six numeric digits.
-------------> (3) Cologne phonetics: This is similar to Soundex, but more suitable for German words.
-------------> (4) Metaphone and Double Metaphone which are suitable for use with most English words, not just names. 
---------------> Metaphone algorithms are the basis for many popular spell checkers.
-------------> (5) New York State Identification and Intelligence System (NYSIIS), which maps similar phonemes to the same letter. 
---------------> The result is a string that can be pronounced by the reader without decoding.
-------------> (6) Match Rating Approach developed by Western Airlines in 1977 - this algorithm has an encoding and range comparison technique.
-------------> (7) Caverphone, created to assist in data matching between late 19th century and early 20th century electoral rolls, optimized for accents present in parts of New Zealand.
-----------> Common uses
-------------> (1) Spell checkers can often contain phonetic algorithms. 
---------------> The Metaphone algorithm, for example, can take an incorrectly spelled word and create a code. 
---------------> The code is then looked up in directory for words with the same or similar Metaphone. 
---------------> Words that have the same or similar Metaphone become possible alternative spellings.
-------------> (2) Search functionality will often use phonetic algorithms to find results that don't match exactly the term(s) used in the search. 
---------------> Searching for names can be difficult as there are often multiple alternative spellings for names. 
---------------> An example is the name Claire. 
---------------> It has two alternatives, Clare/Clair, which are both pronounced the same. 
---------------> Searching for one spelling wouldn't show results for the two others. 
---------------> Using Soundex all three variations produce the same Soundex code, C460. 
---------------> By searching names based on the Soundex code all three variations will be returned.

---------> Daitch–Mokotoff Soundex: 
-----------> This is a Soundex refinement which allows matching of Slavic and Germanic surnames.
-----------> Daitch–Mokotoff Soundex (D–M Soundex) is a phonetic algorithm invented in 1985 by Jewish genealogists Gary Mokotoff and Randy Daitch. 
-----------> It is a refinement of the Russell and American Soundex algorithms designed to allow greater accuracy in matching of Slavic and Yiddish surnames with similar pronunciation but differences in spelling.
-----------> Daitch–Mokotoff Soundex is sometimes referred to as "Jewish Soundex" and "Eastern European Soundex", 
-----------> although the authors discourage use of these nicknames for the algorithm because the algorithm itself is independent of the fact that the motivation 
-----------> for creating the new system was the poor result of predecessor systems when dealing with Slavic and Yiddish surnames. 
-----------> Improvements over the older Soundex algorithms include:
-------------> (1) Coded names are six digits long, resulting in greater search precision (traditional Soundex uses four characters)
-------------> (2) The initial character of the name is coded.
-------------> (3) Several rules in the algorithm encode multiple character n-grams as single digits (American and Russell Soundex do not handle multi-character n-grams)
-------------> (4) Multiple possible encodings can be returned for a single name (traditional Soundex returns only one encoding, even if the spelling of a name could potentially have multiple pronunciations)

---------> Double Metaphone: 
-----------> This is an improvement on Metaphone.
-----------> The Double Metaphone phonetic encoding algorithm is the second generation of this algorithm. 
-------------> Its implementation was described in the June 2000 issue of C/C++ Users Journal. 
-------------> It makes a number of fundamental design improvements over the original Metaphone algorithm.
-----------> It is called "Double" because it can return both a primary and a secondary code for a string; 
-------------> this accounts for some ambiguous cases as well as for multiple variants of surnames with common ancestry. 
-------------> For example, encoding the name "Smith" yields a primary code of SM0 and a secondary code of XMT, 
-------------> while the name "Schmidt" yields a primary code of XMT and a secondary code of SMT—both have XMT in common.
-----------> Double Metaphone tries to account for myriad irregularities in English of Slavic, Germanic, Celtic, Greek, French, Italian, Spanish, Chinese, and other origins. 
-------------> Thus it uses a much more complex ruleset for coding than its predecessor; for example, it tests for approximately 100 different contexts of the use of the letter C alone. 

---------> Match rating approach:
-----------> This is a phonetic algorithm developed by Western Airlines.
-------------> The match rating approach (MRA) is a phonetic algorithm for indexing of words by their pronunciation 
-------------> developed by Western Airlines in 1977 for the indexation and comparison of homophonous names.
-----------> The algorithm itself has a simple set of encoding rules but a more lengthy set of comparison rules. 
-------------> The main mechanism is the similarity comparison, which calculates the number of unmatched characters 
-------------> by comparing the strings from left to right and then from right to left, and removing identical characters. 
-------------> This value is subtracted from 6 and then compared to a minimum threshold. 
-------------> The minimum threshold is defined in table A and is dependent upon the length of the strings.
-----------> The encoded name is known (perhaps incorrectly) as a personal numeric identifier (PNI). 
-------------> The encoded name can never contain more than 6 alpha only characters.
-----------> The match rating approach performs well with names containing the letter "y", unlike the original flavor of the NYSIIS algorithm; 
-------------> for example, the surnames "Smith" and "Smyth" are successfully matched. 
-------------> However, MRA does not perform well with encoded names that differ in length by more than 2. 
-----------> Encoding rules
-------------> (1) Delete all vowels unless the vowel begins the word
-------------> (2) Remove the second consonant of any double consonants present
-------------> (3) Reduce codex to 6 letters by joining the first 3 and last 3 letters only
-----------> Comparison rules
-------------> In this section, the words "string(s)" and "name(s)" mean "encoded string(s)" and "encoded name(s)".
---------------> (1) If the length difference between the encoded strings is 3 or greater, then no similarity comparison is done.
---------------> (2) Obtain the minimum rating value by calculating the length sum of the encoded strings and using table A
---------------> (3) Process the encoded strings from left to right and remove any identical characters found from both strings respectively.
---------------> (4) Process the unmatched characters from right to left and remove any identical characters found from both names respectively.
---------------> (5) Subtract the number of unmatched characters from 6 in the longer string. This is the similarity rating.
---------------> (6) If the similarity rating equal to or greater than the minimum rating then the match is considered good.

---------> Metaphone: 
-----------> This is an algorithm for indexing words by their sound, when pronounced in English
-----------> Metaphone is a phonetic algorithm, published by Lawrence Philips in 1990, for indexing words by their English pronunciation. 
-------------> It fundamentally improves on the Soundex algorithm by using information about variations and inconsistencies in English spelling and pronunciation to produce a more accurate encoding, 
-------------> which does a better job of matching words and names which sound similar. As with Soundex, similar-sounding words should share the same keys. M
-------------> etaphone is available as a built-in operator in a number of systems.
-----------> Philips later produced a new version of the algorithm, which he named Double Metaphone. 
-------------> Contrary to the original algorithm whose application is limited to English only, this version takes into account spelling peculiarities of a number of other languages. 
-------------> In 2009 Philips released a third version, called Metaphone 3, which achieves an accuracy of approximately 99% for English words, non-English words familiar to Americans, 
-------------> and first names and family names commonly found in the United States, having been developed according to modern engineering standards against a test harness of prepared correct encodings. 
-----------> Procedure
-------------> Original Metaphone codes use the 16 consonant symbols 0BFHJKLMNPRSTWXY. 
---------------> The '0' represents "th" (as an ASCII approximation of Θ), 'X' represents "sh" or "ch", and the others represent their usual English pronunciations. 
---------------> The vowels AEIOU are also used, but only at the beginning of the code. 
---------------> This table summarizes most of the rules in the original implementation:
-----------------> Drop duplicate adjacent letters, except for C.
-----------------> If the word begins with 'KN', 'GN', 'PN', 'AE', 'WR', drop the first letter.
-----------------> Drop 'B' if after 'M' at the end of the word.
-----------------> 'C' transforms to 'X' if followed by 'IA' or 'H' (unless in latter case, it is part of '-SCH-', in which case it transforms to 'K').
-----------------> 'C' transforms to 'S' if followed by 'I', 'E', or 'Y'. Otherwise, 'C' transforms to 'K'.
-----------------> 'D' transforms to 'J' if followed by 'GE', 'GY', or 'GI'. Otherwise, 'D' transforms to 'T'.
-----------------> Drop 'G' if followed by 'H' and 'H' is not at the end or before a vowel. Drop 'G' if followed by 'N' or 'NED' and is at the end.
-----------------> 'G' transforms to 'J' if before 'I', 'E', or 'Y', and it is not in 'GG'. Otherwise, 'G' transforms to 'K'.
-----------------> Drop 'H' if after vowel and not before a vowel.
-----------------> 'CK' transforms to 'K'.
-----------------> 'PH' transforms to 'F'.
-----------------> 'Q' transforms to 'K'.
-----------------> 'S' transforms to 'X' if followed by 'H', 'IO', or 'IA'.
-----------------> 'T' transforms to 'X' if followed by 'IA' or 'IO'. 'TH' transforms to '0'. Drop 'T' if followed by 'CH'.
-----------------> 'V' transforms to 'F'.
-----------------> 'WH' transforms to 'W' if at the beginning. Drop 'W' if not followed by a vowel.
-----------------> 'X' transforms to 'S' if at the beginning. Otherwise, 'X' transforms to 'KS'.
-----------------> Drop 'Y' if not followed by a vowel.
-----------------> 'Z' transforms to 'S'.
---------------> Drop all vowels unless it is the beginning.
-------------> This table does not constitute a complete description of the original Metaphone algorithm, and the algorithm cannot be coded correctly from it. 
---------------> Original Metaphone contained many errors and was superseded by Double Metaphone, 
---------------> and in turn Double Metaphone and original Metaphone were superseded by Metaphone 3, 
---------------> which corrects thousands of miscodings that will be produced by the first two versions.
-------------> To implement Metaphone without purchasing a (source code) copy of Metaphone 3, the reference implementation of Double Metaphone can be used.
---------------> Alternatively, version 2.1.3 of Metaphone 3, an earlier 2009 version without a number of encoding corrections made in the current version, version 2.5.4, 
---------------> has been made available under the terms of the BSD License via the OpenRefine project.

---------> NYSIIS: 
-----------> This is a phonetic algorithm, improves on Soundex.
-----------> The New York State Identification and Intelligence System Phonetic Code, commonly known as NYSIIS, 
-------------> is a phonetic algorithm devised in 1970 as part of the New York State Identification and Intelligence System 
-------------> (now a part of the New York State Division of Criminal Justice Services). 
-------------> It features an accuracy increase of 2.7% over the traditional Soundex algorithm.
-----------> Procedure
-------------> The algorithm, as described in Name Search Techniques,[2] is:
---------------> If the first letters of the name are
--------------->     'MAC' then change these letters to 'MCC'
--------------->     'KN' then change these letters to 'NN'
--------------->     'K' then change this letter to 'C'
--------------->     'PH' then change these letters to 'FF'
--------------->     'PF' then change these letters to 'FF'
--------------->     'SCH' then change these letters to 'SSS'
---------------> If the last letters of the name are[3]
--------------->     'EE' then change these letters to 'Y␢'
--------------->     'IE' then change these letters to 'Y␢'
--------------->     'DT' or 'RT' or 'RD' or 'NT' or 'ND' then change these letters to 'D␢'
---------------> The first character of the NYSIIS code is the first character of the name.
---------------> In the following rules, a scan is performed on the characters of the name. 
---------------> This is described in terms of a program loop. 
---------------> A pointer is used to point to the current position under consideration in the name. 
---------------> Step 4 is to set this pointer to point to the second character of the name.
---------------> Considering the position of the pointer, only one of the following statements can be executed.
--------------->     If blank then go to rule 7.
--------------->     If the current position is a vowel (AEIOU) then if equal to 'EV' then change to 'AF' otherwise change current position to 'A'.
--------------->     If the current position is the letter
--------------->         'Q' then change the letter to 'G'
--------------->         'Z' then change the letter to 'S'
--------------->         'M' then change the letter to 'N'
--------------->     If the current position is the letter 'K' then if the next letter is 'N' then replace the current position by 'N' otherwise replace the current position by 'C'
--------------->     If the current position points to the letter string
--------------->         'SCH' then replace the string with 'SSS'
--------------->         'PH' then replace the string with 'FF'
--------------->     If the current position is the letter 'H' and either the preceding or following letter is not a vowel (AEIOU) then replace the current position with the preceding letter.
--------------->     If the current position is the letter 'W' and the preceding letter is a vowel then replace the current position with the preceding position.
--------------->     If none of these rules applies, then retain the current position letter value.
---------------> If the current position letter is equal to the last letter placed in the code then set the pointer to point to the next letter and go to step 5.
---------------> The next character of the NYSIIS code is the current position letter.
---------------> Increment the pointer to point at the next letter.
---------------> Go to step 5.
---------------> If the last character of the NYSIIS code is the letter 'S' then remove it.
---------------> If the last two characters of the NYSIIS code are the letters 'AY' then replace them with the single character 'Y'.
---------------> If the last character of the NYSIIS code is the letter 'A' then remove this letter.

---------> Soundex: 
-----------> This is a phonetic algorithm for indexing names by sound, as pronounced in English.
-------------> Soundex is a phonetic algorithm for indexing names by sound, as pronounced in English. 
-------------> The goal is for homophones to be encoded to the same representation so that they can be matched despite minor differences in spelling. 
-------------> The algorithm mainly encodes consonants; a vowel will not be encoded unless it is the first letter. 
-------------> Soundex is the most widely known of all phonetic algorithms (in part because it is a standard feature of popular database software 
-------------> such as IBM Db2, PostgreSQL,[2] MySQL,[3] SQLite,[4] Ingres, MS SQL Server,[5] Oracle.[6] and SAP ASE.[7]) 
-------------> Improvements to Soundex are the basis for many modern phonetic algorithms.



-------> String metrics:
---------> This is computes a similarity or dissimilarity (distance) score between two pairs of text strings.
---------> A string metric (also known as a string similarity metric or string distance function)
-----------> is a metric that measures distance ("inverse similarity") between two text strings for approximate string matching or comparison and in fuzzy string searching. 
-----------> A requirement for a string metric (e.g. in contrast to string matching) is fulfillment of the triangle inequality. 
-----------> For example, the strings "Sam" and "Samuel" can be considered to be close. 
-----------> A string metric provides a number indicating an algorithm-specific indication of distance.
---------> The most widely known string metric is a rudimentary one called the Levenshtein distance (also known as edit distance). 
-----------> It operates between two input strings, returning a number equivalent to the number of substitutions and deletions needed in order to transform one input string into another. 
-----------> Simplistic string metrics such as Levenshtein distance have expanded to include phonetic, token, grammatical and character-based methods of statistical comparisons.
---------> String metrics are used heavily in information integration and are currently used in areas including fraud detection, 
-----------> fingerprint analysis, plagiarism detection, ontology merging, DNA analysis, RNA analysis, image analysis, evidence-based machine learning, 
-----------> database data deduplication, data mining, incremental search, data integration, Malware Detection, and semantic knowledge integration. 
---------> There are other popular measures of edit distance, which are calculated using a different set of allowable edit operations. For instance,
-----------> The Levenshtein distance allows deletion, insertion and substitution;
-----------> The Damerau–Levenshtein distance allows insertion, deletion, substitution, and the transposition of two adjacent characters;
-----------> The longest common subsequence (LCS) distance allows only insertion and deletion, not substitution;
-----------> The Hamming distance allows only substitution, hence, it only applies to strings of the same length.

---------> Damerau–Levenshtein distance: 
-----------> This is computes a distance measure between two strings, improves on Levenshtein distance.
-----------> The Damerau–Levenshtein distance is a string metric for measuring the edit distance between two sequences. 
-------------> Informally, the Damerau–Levenshtein distance between two words is the minimum number of operations 
-------------> (consisting of insertions, deletions or substitutions of a single character, or transposition of two adjacent characters) required to change one word into the other.
-----------> The Damerau–Levenshtein distance differs from the classical Levenshtein distance by including transpositions 
-------------> among its allowable operations in addition to the three classical single-character edit operations (insertions, deletions and substitutions).
-----------> In his seminal paper,[5] Damerau stated that in an investigation of spelling errors for an information-retrieval system, 
-------------> more than 80% were a result of a single error of one of the four types. 
-------------> Damerau's paper considered only misspellings that could be corrected with at most one edit operation. 
-------------> While the original motivation was to measure distance between human misspellings to improve applications such as spell checkers, 
-------------> Damerau–Levenshtein distance has also seen uses in biology to measure the variation between protein sequences.
-----------> Note: This named after Frederick J. Damerau and Vladimir I. Levenshtein
 
---------> Dice's coefficient (also known as the Dice coefficient): 
-----------> This is a similarity measure related to the Jaccard index.
-------------> The Jaccard index, also known as the Jaccard similarity coefficient, is a statistic used for gauging the similarity and diversity of sample sets.
-----------> The Sørensen–Dice coefficient (see below for other names) is a statistic used to gauge the similarity of two samples. 
-----------> Note: It was independently developed by the botanists Thorvald Sørensen[1] and Lee Raymond Dice,[2] who published in 1948 and 1945 respectively. 

---------> Hamming distance: 
-----------> Definition: The Hamming distance between two equal-length strings of symbols is the number of positions at which the corresponding symbols are different.
-----------> This is sum number of positions which are different.
-----------> The Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. 
-------------> In other words, it measures the minimum number of substitutions required to change one string into the other, 
-------------> or the minimum number of errors that could have transformed one string into the other. 
-------------> In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. 
-------------> string metrics for measuring the edit distance between two sequences. It is named after the American mathematician Richard Hamming.
-----------> A major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field. 
-----------> Examples
-------------> The symbols may be letters, bits, or decimal digits, among other possibilities. For example, the Hamming distance between:
------------->     "karolin" and "kathrin" is 3.
------------->     "karolin" and "kerstin" is 3.
------------->     "kathrin" and "kerstin" is 4.
------------->     0000 and 1111 is 4.
------------->     2173896 and 2233796 is 3.
-----------> Algorithm
-------------> The following function, written in Python 3, returns the Hamming distance between two strings:
---------------> def hamming_distance(string1, string2):
--------------->     dist_counter = 0
--------------->     for n in range(len(string1)):
--------------->         if string1[n] != string2[n]:
--------------->             dist_counter += 1
--------------->     return dist_counter
-------------> Or, in a shorter expression: 
---------------> sum(xi != yi for xi, yi in zip(x, y))
-----------> Note: This is named after the American mathematician Richard Hamming.

---------> Jaro–Winkler distance: 
-----------> This is a measure of similarity between two strings.
-----------> The Jaro–Winkler distance is a string metric measuring an edit distance between two sequences.
-----------> The Jaro–Winkler distance uses a prefix scale p which gives more favourable ratings to strings that match from the beginning for a set prefix length ℓ.
-----------> The lower the Jaro–Winkler distance for two strings is, the more similar the strings are. The score is normalized such that 1 means an exact match and 0 means there is no similarity. 
-------------> The original paper actually defined the metric in terms of similarity, so the distance is defined as the inversion of that value (distance = 1 − similarity).
-----------> Although often referred to as a distance metric, the Jaro–Winkler distance is not a metric in the mathematical sense of that term because it does not obey the triangle inequality. 
-----------> Note: This is a variant proposed in 1990 by William E. Winkler of the Jaro distance metric (1989, Matthew A. Jaro).

---------> Levenshtein edit distance: 
-----------> This computes a metric for the amount of difference between two sequences.
-------------> In information theory, linguistics, and computer science, the Levenshtein distance is a string metric for measuring the difference between two sequences. 
-------------> Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other. 
-------------> It is named after the Soviet mathematician Vladimir Levenshtein, who considered this distance in 1965.
-----------> Levenshtein distance may also be referred to as edit distance, although that term may also denote a larger family of distance metrics known collectively as edit distance.
-------------> It is closely related to pairwise string alignments. 
-----------> Algorithm: Check the APRG codes

-------> Trigram search: 
---------> This search for text when the exact syntax or spelling of the target object is not precisely known
---------> Trigram search is a method of searching for text when the exact syntax or spelling of the target object is not precisely known or when queries may be regular expressions. 
-----------> It finds objects which match the maximum number of three consecutive character strings (i.e. trigrams) in the entered search terms, which are generally near matches. 
-----------> Two strings with many shared trigrams can be expected to be very similar. 
-----------> Trigrams also allow for efficiently creating indexes for searches that are regular expressions or match the text inexactly. 
-----------> Indexes can significantly accelerate searches.
-----------> A threshold for number of trigram matches can be specified as a cutoff point, after which a result is no longer considered a match.
---------> Using trigrams for accelerating searches is a technique used in some systems for code searching, 
-----------> where queries that are regular expressions may be useful, search engines such as Elasticsearch, as well as in databases such as PostgreSQL.
-------> Examples
---------> Consider the string "alice". The trigrams of the string would be "ali", "lic", and "ice," not including spaces.
-----------> Searching for this string in a database with a trigram-based index would involve finding which objects contain as many of the three trigrams as possible.
---------> As a concrete example of using trigram search to search for a regular expression query, consider searching for the string ab[cd]e, 
-----------> where the brackets denote that the third character in the string being searched for could be c or d. 
-----------> In this situation, one could query the index for objects that have the two trigrams abc and bce or the two trigrams abd and bde. 
-----------> Thus, finding this query would involve no string matching, and could just query the index directly, which can be faster in practice.


-----> Selection algorithms

-------> Quickselect
---------> Quickselect is a selection algorithm to find the kth smallest element in an unordered list. 
-----------> It is related to the quicksort sorting algorithm. 
-----------> Like quicksort, it was developed by Tony Hoare, and thus is also known as Hoare's selection algorithm.
-----------> Like quicksort, it is efficient in practice and has good average-case performance, but has poor worst-case performance. 
-----------> Quickselect and its variants are the selection algorithms most often used in efficient real-world implementations.
---------> Quickselect uses the same overall approach as quicksort, choosing one element as a pivot and partitioning the data in two based on the pivot, accordingly as less than or greater than the pivot. 
-----------> However, instead of recursing into both sides, as in quicksort, quickselect only recurses into one side – the side with the element it is searching for. 
-----------> This reduces the average complexity from Θ(n log n) to Θ(n), with a worst case of O(n^2).
---------> As with quicksort, quickselect is generally implemented as an in-place algorithm, and beyond selecting the kth element, it also partially sorts the data. 
-----------> See selection algorithm for further discussion of the connection with sorting. 
---------> Algorithm
-----------> In quicksort, there is a subprocedure called partition that can, in linear time, 
-------------> group a list (ranging from indices left to right) into two parts: those less than a certain element, and those greater than or equal to the element. 
-------------> Here is pseudocode that performs a partition about the element list[pivotIndex]:
---------------> function partition(list, left, right, pivotIndex) is
--------------->     pivotValue := list[pivotIndex]
--------------->     swap list[pivotIndex] and list[right]  // Move pivot to end
--------------->     storeIndex := left
--------------->     for i from left to right − 1 do
--------------->         if list[i] < pivotValue then
--------------->             swap list[storeIndex] and list[i]
--------------->             increment storeIndex
--------------->     swap list[right] and list[storeIndex]  // Move pivot to its final place
--------------->     return storeIndex
-----------> This is known as the Lomuto partition scheme, which is simpler but less efficient than Hoare's original partition scheme.
-----------> In quicksort, we recursively sort both branches, leading to best-case Ω(n log ⁡ n) time. 
-------------> However, when doing selection, we already know which partition our desired element lies in, since the pivot is in its final sorted position,
-------------> with all those preceding it in an unsorted order and all those following it in an unsorted order. 
-------------> Therefore, a single recursive call locates the desired element in the correct partition, and we build upon this for quickselect:
---------------> // Returns the k-th smallest element of list within left..right inclusive
---------------> // (i.e. left <= k <= right).
---------------> function select(list, left, right, k) is
--------------->     if left = right then   // If the list contains only one element,
--------------->         return list[left]  // return that element
--------------->     pivotIndex  := ...     // select a pivotIndex between left and right,
--------------->                            // e.g., left + floor(rand() % (right − left + 1))
--------------->     pivotIndex  := partition(list, left, right, pivotIndex)
--------------->     // The pivot is in its final sorted position
--------------->     if k = pivotIndex then
--------------->         return list[k]
--------------->     else if k < pivotIndex then
--------------->         return select(list, left, pivotIndex − 1, k)
--------------->     else
--------------->         return select(list, pivotIndex + 1, right, k) 
-----------> Note the resemblance to quicksort: just as the minimum-based selection algorithm is a partial selection sort, 
-------------> this is a partial quicksort, generating and partitioning only O(log n) of its O(n) partitions. 
-------------> This simple procedure has expected linear performance, and, like quicksort, has quite good performance in practice. 
-------------> It is also an in-place algorithm, requiring only constant memory overhead if tail call optimization is available, or if eliminating the tail recursion with a loop:
---------------> function select(list, left, right, k) is
--------------->     loop
--------------->         if left = right then
--------------->             return list[left]
--------------->         pivotIndex := ...     // select pivotIndex between left and right
--------------->         pivotIndex := partition(list, left, right, pivotIndex)
--------------->         if k = pivotIndex then
--------------->             return list[k]
--------------->         else if k < pivotIndex then
--------------->             right := pivotIndex − 1
--------------->         else
--------------->             left := pivotIndex + 1
---------> Time complexity
------------> Like quicksort, quickselect has good average performance, but is sensitive to the pivot that is chosen. 
--------------> If good pivots are chosen, meaning ones that consistently decrease the search set by a given fraction, 
--------------> then the search set decreases in size exponentially and by induction (or summing the geometric series) one sees that performance is linear, 
--------------> as each step is linear and the overall time is a constant times this (depending on how quickly the search set reduces). 
--------------> However, if bad pivots are consistently chosen, such as decreasing by only a single element each time, then worst-case performance is quadratic: O(n^2). 
--------------> This occurs for example in searching for the maximum element of a set, using the first element as the pivot, and having sorted data. 
--------------> The probability of the worst-case occurring decreases exponentially with n , {\displaystyle n,} {\displaystyle n,} so quickselect has almost certain O(n) time complexity.
---------> Variants
-----------> The easiest solution is to choose a random pivot, which yields almost certain linear time. 
-------------> Deterministically, one can use median-of-3 pivot strategy (as in the quicksort), which yields linear performance on partially sorted data, as is common in the real world. 
-------------> However, contrived sequences can still cause worst-case complexity; 
-------------> David Musser describes a "median-of-3 killer" sequence that allows an attack against that strategy, which was one motivation for his introselect algorithm.
-----------> One can assure linear performance even in the worst case by using a more sophisticated pivot strategy; this is done in the median of medians algorithm. 
-------------> However, the overhead of computing the pivot is high, and thus this is generally not used in practice. 
-------------> One can combine basic quickselect with median of medians as fallback to get both fast average case performance and linear worst-case performance; this is done in introselect.
-----------> Finer computations of the average time complexity yield a worst case of n(2 + 2log2 + o(1)) ≤ 3.4n + o(n) for random pivots (in the case of the median; other k are faster). 
-------------> The constant can be improved to 3/2 by a more complicated pivot strategy, yielding the Floyd–Rivest algorithm, which has average complexity of 1.5n + Θ(n^(1/2)) for median, with other k being faster. 

-------> Introselect
---------> Introselect (short for "introspective selection") is a selection algorithm that is a hybrid of quickselect 
-----------> and median of medians which has fast average performance and optimal worst-case performance. 
-----------> Introselect is related to the introsort sorting algorithm: these are analogous refinements of the basic quickselect and quicksort algorithms, 
-----------> in that they both start with the quick algorithm, which has good average performance and low overhead, 
-----------> but fall back to an optimal worst-case algorithm (with higher overhead) if the quick algorithm does not progress rapidly enough. 
-----------> Both algorithms were introduced by David Musser in (Musser 1997), with the purpose of providing generic algorithms 
-----------> for the C++ Standard Library that have both fast average performance and optimal worst-case performance, thus allowing the performance requirements to be tightened. 
-----------> However, in most C++ Standard Library implementations that use introselect, another "introselect" algorithm is used, 
-----------> which combines quickselect and heapselect, and has a worst-case running time of O(n log n).
---------> Algorithms
-----------> Introsort achieves practical performance comparable to quicksort while preserving O(n log n) worst-case behavior by creating a hybrid of quicksort and heapsort. 
-------------> Introsort starts with quicksort, so it achieves performance similar to quicksort if quicksort works, 
-------------> and falls back to heapsort (which has optimal worst-case performance) if quicksort does not progress quickly enough. 
-------------> Similarly, introselect combines quickselect with median of medians to achieve worst-case linear selection with performance similar to quickselect.
-----------> Introselect works by optimistically starting out with quickselect and only switching to a worst-case linear-time selection algorithm 
-------------> (the Blum-Floyd-Pratt-Rivest-Tarjan median of medians algorithm) if it recurses too many times without making sufficient progress. 
-------------> The switching strategy is the main technical content of the algorithm. 
-------------> Simply limiting the recursion to constant depth is not good enough, since this would make the algorithm switch on all sufficiently large lists. 
-------------> Musser discusses a couple of simple approaches:
---------------> Keep track of the list of sizes of the subpartitions processed so far. 
-----------------> If at any point k recursive calls have been made without halving the list size, for some small positive k, switch to the worst-case linear algorithm.
---------------> Sum the size of all partitions generated so far. If this exceeds the list size times some small positive constant k, switch to the worst-case linear algorithm. 
-----------------> This sum is easy to track in a single scalar variable.
-----------> Both approaches limit the recursion depth to k(log n) = O(log n) and the total running time to O(n).
-----------> Note: The paper suggested that more research on introselect was forthcoming, but the author retired in 2007 without having published any such further research. 

-----> Sequence search

-------> Linear search:
---------> This locates an item in an unsorted sequence.
---------> A linear search or sequential search is a method for finding an element within a list. 
-----------> It sequentially checks each element of the list until a match is found or the whole list has been searched.
---------> A linear search runs in at worst linear time and makes at most n comparisons, where n is the length of the list. 
-----------> If each element is equally likely to be searched, then linear search has an average case of n+1/2 comparisons, 
-----------> but the average case can be affected if the search probabilities for each element vary. 
-----------> Linear search is rarely practical because other search algorithms and schemes, such as the binary search algorithm and hash tables, 
-----------> allow significantly faster searching for all but short lists.
---------> Algorithm
-----------> A linear search sequentially checks each element of the list until it finds an element that matches the target value. 
-------------> If the algorithm reaches the end of the list, the search terminates unsuccessfully.
-----------> Basic algorithm
-------------> Given a list L of n elements with values or records L0 .... Ln−1, and target value T, 
-------------> the following subroutine uses linear search to find the index of the target T in L.
---------------> Set i to 0.
---------------> If Li = T, the search terminates successfully; return i.
---------------> Increase i by 1.
---------------> If i < n, go to step 2. Otherwise, the search terminates unsuccessfully.
-----------> With a sentinel (Me: Who would do this?)
-------------> The basic algorithm above makes two comparisons per iteration: one to check if Li equals T,
---------------> and the other to check if i still points to a valid index of the list. 
---------------> By adding an extra record Ln to the list (a sentinel value) that equals the target, 
---------------> the second comparison can be eliminated until the end of the search, making the algorithm faster. 
---------------> The search will reach the sentinel if the target is not contained within the list.[4]
-----------------> Set i to 0.
-----------------> If Li = T, go to step 4.
-----------------> Increase i by 1 and go to step 2.
-----------------> If i < n, the search terminates successfully; return i. Else, the search terminates unsuccessfully.
-----------> In an ordered table
-------------> If the list is ordered such that L0 ≤ L1 ... ≤ Ln−1, the search can establish 
---------------> the absence of the target more quickly by concluding the search once Li exceeds the target. 
---------------> This variation requires a sentinel that is greater than the target.
-----------------> Set i to 0.
-----------------> If Li ≥ T, go to step 4.
-----------------> Increase i by 1 and go to step 2.
-----------------> If Li = T, the search terminates successfully; return i. Else, the search terminates unsuccessfully.

-------> Selection algorithm:
---------> This finds the kth largest item in a sequence.
---------> A selection algorithm is an algorithm for finding the kth smallest number in a list or array; such a number is called the kth order statistic. 
-----------> This includes the cases of finding the minimum, maximum, and median elements. 
-----------> There are O(n)-time (worst-case linear time) selection algorithms, 
-----------> and sublinear performance is possible for structured data; in the extreme, O(1) for an array of sorted data. 
-----------> Selection is a subproblem of more complex problems like the nearest neighbor and shortest path problems. 
-----------> Many selection algorithms are derived by generalizing a sorting algorithm, and conversely some sorting algorithms can be derived as repeated application of selection.
---------> The simplest case of a selection algorithm is finding the minimum (or maximum) element by iterating through the list, 
-----------> keeping track of the running minimum – the minimum so far – (or maximum) and can be seen as related to the selection sort. 
---------> Conversely, the hardest case of a selection algorithm is finding the median. 
-----------> In fact, a specialized median-selection algorithm can be used to build a general selection algorithm, as in median of medians. 
---------> The best-known selection algorithm is Quickselect, which is related to Quicksort; like Quicksort, 
-----------> it has (asymptotically) optimal average performance, but poor worst-case performance, though it can be modified to give optimal worst-case performance as well. 

-------> Ternary search: 
---------> This a technique for finding the minimum or maximum of a function that is either strictly increasing and then strictly decreasing or vice versa.
-----------> A ternary search algorithm is a technique in computer science for finding the minimum or maximum of a unimodal function. 
-----------> A ternary search determines either that the minimum or maximum cannot be in the first third of the domain 
-----------> or that it cannot be in the last third of the domain, then repeats on the remaining two thirds. 
-----------> A ternary search is an example of a divide and conquer algorithm (see search algorithm).
-----------> The function
-------------> Assume we are looking for a maximum of f(x) and that we know the maximum lies somewhere between A and B. 
-------------> For the algorithm to be applicable, there must be some value X such that
---------------> for all a,b with A ≤ a < b ≤ x, we have f(a)<f(b), and
---------------> for all a,b with x ≤ a < b ≤ B, we have f(a)>f(b).
-----------> Algorithm
-----------> Let f(x) be a unimodal function on some interval [l;r]. Take any two points m1 and m2 in this segment: l < m1 < m2 < r. 
-----------> Then there are three possibilities:
-------------> if f(m1) < f(m2), then the required maximum can not be located on the left side – [l;m1]. 
---------------> It means that the maximum further makes sense to look only in the interval [m1;r]
-------------> if f(m1) > f(m2), that the situation is similar to the previous, up to symmetry. 
---------------> Now, the required maximum can not be in the right side – [m2;r], so go to the segment [l;m2]
-------------> if f(m1) = f(m2), then the search should be conducted in [m1;m2], 
---------------> but this case can be attributed to any of the previous two (in order to simplify the code). 
---------------> Sooner or later the length of the segment will be a little less than a predetermined constant, and the process can be stopped.
-----------> choice points m1 and m2:
----------->     m1 = l + (r−l)/3
----------->     m2 = r − (r−l)/3
-----------> Run time order
----------->     T ( n ) = T ( 2 n / 3 ) + 1 = Θ ( log ⁡ n )
-----------> Recursive algorithm
-------------> def ternary_search(f, left, right, absolute_precision) -> float:
------------->     """Left and right are the current bounds;
------------->     the maximum is between them.
------------->     """
------------->     if abs(right - left) < absolute_precision:
------------->         return (left + right) / 2
-------------> 
------------->     left_third = (2*left + right) / 3
------------->     right_third = (left + 2*right) / 3
-------------> 
------------->     if f(left_third) < f(right_third):
------------->         return ternary_search(f, left_third, right, absolute_precision)
------------->     else:
------------->         return ternary_search(f, left, right_third, absolute_precision)
-----------> Iterative algorithm
-------------> def ternary_search(f, left, right, absolute_precision) -> float:
------------->     """Find maximum of unimodal function f() within [left, right].
------------->     To find the minimum, reverse the if/else statement or reverse the comparison.
------------->     """
------------->     while abs(right - left) >= absolute_precision:
------------->         left_third = left + (right - left) / 3
------------->         right_third = right - (right - left) / 3
-------------> 
------------->         if f(left_third) < f(right_third):
------------->             left = left_third
------------->         else:
------------->             right = right_third
-------------> 
------------->      # Left and right are the current bounds; the maximum is between them
------------->      return (left + right) / 2

-------> Sorted lists

---------> Binary search algorithm: 
-----------> This locates an item in a sorted sequence.
-------------> Binary search is a search algorithm that finds the position of a target value within a sorted array.
-------------> Binary search compares the target value to the middle element of the array. 
-------------> If they are not equal, the half in which the target cannot lie is eliminated and the search continues on the remaining half, 
-------------> again taking the middle element to compare to the target value, and repeating this until the target value is found. 
-------------> If the search ends with the remaining half being empty, the target is not in the array.
-----------> Binary search runs in logarithmic time in the worst case, making O(log ⁡ n) comparisons, where n is the number of elements in the array.
-------------> Binary search is faster than linear search except for small arrays. 
-------------> However, the array must be sorted first to be able to apply binary search. 
-------------> There are specialized data structures designed for fast searching, such as hash tables, that can be searched more efficiently than binary search. 
-------------> However, binary search can be used to solve a wider range of problems, 
-------------> such as finding the next-smallest or next-largest element in the array relative to the target even if it is absent from the array.
-----------> There are numerous variations of binary search. 
-------------> In particular, fractional cascading speeds up binary searches for the same value in multiple arrays. 
-------------> Fractional cascading efficiently solves a number of search problems in computational geometry and in numerous other fields. 
-------------> Exponential search extends binary search to unbounded lists. The binary search tree and B-tree data structures are based on binary search. 
-------------> Algorithm
---------------> Binary search works on sorted arrays. 
-----------------> Binary search begins by comparing an element in the middle of the array with the target value. 
-----------------> If the target value matches the element, its position in the array is returned. 
-----------------> If the target value is less than the element, the search continues in the lower half of the array. 
-----------------> If the target value is greater than the element, the search continues in the upper half of the array. 
-----------------> By doing this, the algorithm eliminates the half in which the target value cannot lie in each iteration.
---------------> Procedure
-----------------> Given an array A of n elements with values or records A0, A1, A2,…, An−1 sorted such that A0 ≤ A1 ≤ A2 ≤ ⋯ ≤ An−1, and target value T, 
-------------------> the following subroutine uses binary search to find the index of T in A.
---------------------> Set L to 0 and R to  n-1.
---------------------> If L > R, the search terminates as unsuccessful.
---------------------> Set  m (the position of the middle element) to the floor of (L + R)/2 , which is the greatest integer less than or equal to (L + R)/2.
---------------------> If A m < T , set L to m + 1 {\displaystyle m+1} m+1 and go to step 2.
---------------------> If A m > T , set R to m − 1 {\displaystyle m-1} m-1 and go to step 2.
---------------------> Now A m = T , the search is done; return m.
-----------------> This iterative procedure keeps track of the search boundaries with the two variables L and R. 
-------------------> The procedure may be expressed in pseudocode as follows, where the variable names and types remain the same as above, floor is the floor function,
-------------------> and unsuccessful refers to a specific value that conveys the failure of the search.
---------------------> function binary_search(A, n, T) is
--------------------->     L := 0
--------------------->     R := n − 1
--------------------->     while L ≤ R do
--------------------->         m := floor((L + R) / 2)
--------------------->         if A[m] < T then
--------------------->             L := m + 1
--------------------->         else if A[m] > T then
--------------------->             R := m − 1
--------------------->         else:
--------------------->             return m
--------------------->     return unsuccessful
---------------> Alternatively, the algorithm may take the ceiling of (L + R)/2. 
-----------------> This may change the result if the target value appears more than once in the array. 
-----------> Note: This is also known as half-interval search, logarithmic search, or binary chop.

---------> Fibonacci search technique: 
-----------> This search a sorted sequence using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers.
-------------> The Fibonacci search technique is a method of searching a sorted array using a divide and conquer algorithm that narrows down possible locations with the aid of Fibonacci numbers. 
-------------> Compared to binary search where the sorted array is divided into two equal-sized parts, one of which is examined further, 
-------------> Fibonacci search divides the array into two parts that have sizes that are consecutive Fibonacci numbers. 
-------------> On average, this leads to about 4% more comparisons to be executed, 
-------------> but it has the advantage that one only needs addition and subtraction to calculate the indices of the accessed array elements, 
-------------> while classical binary search needs bit-shift (see Bitwise operation), division or multiplication,
-------------> operations that were less common at the time Fibonacci search was first published. 
-------------> Fibonacci search has an average- and worst-case complexity of O(log n) (see Big O notation).
-----------> The Fibonacci sequence has the property that a number is the sum of its two predecessors. 
-------------> Therefore the sequence can be computed by repeated addition. 
-------------> The ratio of two consecutive numbers approaches the Golden ratio, 1.618... Binary search works by dividing the seek area in equal parts (1:1).
------------->  Fibonacci search can divide it into parts approaching 1:1.618 while using the simpler operations.
-----------> If the elements being searched have non-uniform access memory storage (i. e., the time needed to access a storage location varies depending on the location accessed), 
-------------> the Fibonacci search may have the advantage over binary search in slightly reducing the average time needed to access a storage location. 
-------------> If the machine executing the search has a direct mapped CPU cache, binary search may lead to more cache misses because the elements that are accessed often tend to gather in only a few cache lines; 
-------------> this is mitigated by splitting the array in parts that do not tend to be powers of two. If the data is stored on a magnetic tape where seek time depends on the current head position, 
-------------> a tradeoff between longer seek time and more comparisons may lead to a search algorithm that is skewed similarly to Fibonacci search.
-------------> Algorithm
---------------> Let k be defined as an element in F, the array of Fibonacci numbers. n = Fm is the array size. 
---------------> If n is not a Fibonacci number, let Fm be the smallest number in F that is greater than n.
---------------> The array of Fibonacci numbers is defined where Fk+2 = Fk+1 + Fk, when k ≥ 0, F1 = 1, and F0 = 1.
---------------> To test whether an item is in the list of ordered numbers, follow these steps:
-----------------> (1) Set k = m.
-----------------> (2) If k = 0, stop. There is no match; the item is not in the array.
-----------------> (3) Compare the item against element in Fk−1.
-----------------> (4) If the item matches, stop.
-----------------> (5) If the item is less than entry Fk−1, discard the elements from positions Fk−1 + 1 to n. Set k = k − 1 and return to step 2.
-----------------> (6) If the item is greater than entry Fk−1, discard the elements from positions 1 to Fk−1. Renumber the remaining elements from 1 to Fk−2, set k = k − 2, and return to step 2.
---------------> Alternative implementation (from "Sorting and Searching" by Knuth[4]):
---------------> Given a table of records R1, R2, ..., RN whose keys are in increasing order K1 < K2 < ... < KN, the algorithm searches for a given argument K. Assume N+1 = Fk+1
-----------------> Step 1. [Initialize] i ← Fk, p ← Fk-1, q ← Fk-2 (throughout the algorithm, p and q will be consecutive Fibonacci numbers)
-----------------> Step 2. [Compare] If K < Ki, go to Step 3; if K > Ki go to Step 4; and if K = Ki, the algorithm terminates successfully.
-----------------> Step 3. [Decrease i] If q=0, the algorithm terminates unsuccessfully. 
-------------------> Otherwise set (i, p, q) ← (i - q, q, p - q) (which moves p and q one position back in the Fibonacci sequence); then return to Step 2
-----------------> Step 4. [Increase i] If p=1, the algorithm terminates unsuccessfully. 
-------------------> Otherwise set (i, p, q) ← (i + q, p - q, 2q - p) (which moves p and q two positions back in the Fibonacci sequence); and return to Step 2
---------------> The two variants of the algorithm presented above always divide the current interval into a larger and a smaller subinterval. 
---------------> The original algorithm,[1] however, would divide the new interval into a smaller and a larger subinterval in Step 4. 
---------------> This has the advantage that the new i is closer to the old i and is more suitable for accelerating searching on magnetic tape. 
-----------> Note: Fibonacci search is derived from Golden section search, an algorithm by Jack Kiefer (1953) to search for the maximum or minimum of a unimodal function in an interval.

---------> Jump search (or block search): 
-----------> This linear search on a smaller subset of the sequence.
-------------> This works by first checking all items Lkm, where k is an element of N and m is the block size, until an item is found that is larger than the search key. 
-------------> To find the exact position of the search key in the list a linear search is performed on the sublist L[(k-1)m, km].
-----------> The optimal value of m is √n, where n is the length of the list L. 
-------------> Because both steps of the algorithm look at, at most, √n items the algorithm runs in O(√n) time. 
-------------> This is better than a linear search, but worse than a binary search. 
-------------> The advantage over the latter is that a jump search only needs to jump backwards once, while a binary can jump backwards up to log n times. 
-------------> This can be important if a jumping backwards takes significantly more time than jumping forward.
-----------> The algorithm can be modified by performing multiple levels of jump search on the sublists, before finally performing the linear search. 
-------------> For an k-level jump search the optimum block size ml for the l th level (counting from 1) is n(k-l)/k. The modified algorithm will perform k backward jumps and runs in O(kn1/(k+1)) time.
-----------> Algorithm
-------------> algorithm JumpSearch is
------------->     input: An ordered list L, its length n and a search key s.
------------->     output: The position of s in L, or nothing if s is not in L.
------------->     a ← 0
------------->     b ← ⌊√n⌋
------------->     while Lmin(b,n)-1 < s do
------------->         a ← b
------------->         b ← b + ⌊√n⌋
------------->         if a ≥ n then
------------->             return nothing
------------->     while La < s do
------------->         a ← a + 1
------------->         if a = min(b, n)
------------->             return nothing
------------->     if La = s then
------------->         return a
------------->     else
------------->         return nothing

---------> Predictive search (interpolation search): 
-----------> This binary-like search which factors in magnitude of search term versus the high and low values in the search.
-----------> Interpolation search is an algorithm for searching for a key in an array that has been ordered by numerical values assigned to the keys (key values). 
-------------> Interpolation search resembles the method by which people search a telephone directory for a name (the key value by which the book's entries are ordered): 
-------------> in each step the algorithm calculates where in the remaining search space the sought item might be, 
-------------> based on the key values at the bounds of the search space and the value of the sought key, usually via a linear interpolation. 
-------------> The key value actually found at this estimated position is then compared to the key value being sought. 
-------------> If it is not equal, then depending on the comparison, the remaining search space is reduced to the part before or after the estimated position. 
-------------> This method will only work if calculations on the size of differences between key values are sensible.
-----------> By comparison, binary search always chooses the middle of the remaining search space, discarding one half or the other, 
-------------> depending on the comparison between the key found at the estimated position and the key sought — it does not require numerical values for the keys, just a total order on them. 
-------------> The remaining search space is reduced to the part before or after the estimated position. 
-------------> The linear search uses equality only as it compares elements one-by-one from the start, ignoring any sorting.
-----------> On average the interpolation search makes about log(log(n)) comparisons (if the elements are uniformly distributed), 
-------------> where n is the number of elements to be searched. In the worst case (for instance where the numerical values of the keys increase exponentially) it can make up to O(n) comparisons.
-----------> In interpolation-sequential search, interpolation is used to find an item near the one being searched for, then linear search is used to find the exact item. 
-----------> Performance
-------------> Using big-O notation, the performance of the interpolation algorithm on a data set of size n is O(n); 
---------------> however under the assumption of a uniform distribution of the data on the linear scale used for interpolation, 
---------------> the performance can be shown to be O(log log n). 
---------------> However, Dynamic Interpolation Search is possible in o(log log n) time using a novel data structure.
-------------> Practical performance of interpolation search depends on whether the reduced number of probes is outweighed by the more complicated calculations needed for each probe. 
---------------> It can be useful for locating a record in a large sorted file on disk, where each probe involves a disk seek and is much slower than the interpolation arithmetic.
-------------> Index structures like B-trees also reduce the number of disk accesses, and are more often used to index on-disk data in part because they can index many types of data and can be updated online. 
---------------> Still, interpolation search may be useful when one is forced to search certain sorted but unindexed on-disk datasets. 
-----------> Note: This was first described by W. W. Peterson in 1957. 
-----------> Note: This is sometimes called dictionary search or interpolated search.
-----------> Sample implementation
-------------> The following C++ code example is a simple implementation. 
-------------> At each stage it computes a probe position then as with the binary search, moves either the upper or lower bound in to define a smaller interval containing the sought value. 
-------------> Unlike the binary search which guarantees a halving of the interval's size with each stage, a misled interpolation may reduce/i-case efficiency of O(n).
---------------> template <typename T>
---------------> int interpolation_search(T arr[], int size, T key)
---------------> {
--------------->     int low = 0;
--------------->     int high = size - 1;
--------------->     int mid;
--------------->     while ((arr[high] != arr[low]) && (key >= arr[low]) && (key <= arr[high])) {
--------------->         mid = low + ((key - arr[low]) * (high - low) / (arr[high] - arr[low]));
--------------->         if (arr[mid] < key)
--------------->             low = mid + 1;
--------------->         else if (key < arr[mid])
--------------->             high = mid - 1;
--------------->         else
--------------->             return mid;
--------------->     }
--------------->     if (key == arr[low])
--------------->         return low;
--------------->     else
--------------->         return -1;
---------------> }

---------> Uniform binary search: 
-----------> This an optimization of the classic binary search algorithm
-----------> Uniform binary search is an optimization of the classic binary search algorithm invented by Donald Knuth and given in Knuth's The Art of Computer Programming. 
-------------> It uses a lookup table to update a single array index, rather than taking the midpoint of an upper and a lower bound on each iteration; 
-------------> therefore, it is optimized for architectures (such as Knuth's MIX) on which:
---------------> a table lookup is generally faster than an addition and a shift, and
---------------> many searches will be performed on the same array, or on several arrays of the same length
-----------> Algorithm
-------------> The uniform binary search algorithm looks like this, when implemented in C.
---------------> #define LOG_N 4
---------------> static int delta[LOG_N];
---------------> void make_delta(int N)
---------------> {
--------------->     int power = 1;
--------------->     int i = 0;
--------------->     do {
--------------->         int half = power;
--------------->         power <<= 1;
--------------->         delta[i] = (N + half) / power;
--------------->     } while (delta[i++] != 0);
---------------> }
---------------> int unisearch(int *a, int key)
---------------> {
--------------->     int i = delta[0] - 1;  /* midpoint of array */
--------------->     int d = 0;
--------------->     while (1) {
--------------->         if (key == a[i]) {
--------------->             return i;
--------------->         } else if (delta[d] == 0) {
--------------->             return -1;
--------------->         } else {
--------------->             if (key < a[i]) {
--------------->                 i -= delta[++d];
--------------->             } else {
--------------->                 i += delta[++d];
--------------->             }
--------------->         }
--------------->     }
---------------> }
---------------> /* Example of use: */
---------------> #define N 10
---------------> int main(void)
---------------> {
--------------->     int a[N] = {1, 3, 5, 6, 7, 9, 14, 15, 17, 19};
--------------->     make_delta(N);
--------------->     for (int i = 0; i < 20; ++i)
--------------->         printf("%d is at index %d\n", i, unisearch(a, i));
--------------->     return 0;
---------------> }


-----> Sequence merging

-------> Merge algorithms are a family of algorithms that take multiple sorted lists as input and produce a single list as output, 
---------> containing all the elements of the inputs lists in sorted order. 
---------> These algorithms are used as subroutines in various sorting algorithms, most famously merge sort.
-------> The merge algorithm plays a critical role in the merge sort algorithm, a comparison-based sorting algorithm. 
---------> Conceptually, the merge sort algorithm consists of two steps:
-----------> (1) Recursively divide the list into sublists of (roughly) equal length, until each sublist contains only one element, 
-------------> or in the case of iterative (bottom up) merge sort, consider a list of n elements as n sub-lists of size 1. 
-------------> A list containing a single element is, by definition, sorted.
-----------> (2) Repeatedly merge sublists to create a new sorted sublist until the single list contains all elements. 
-------------> The single list is the sorted list.
-------> The merge algorithm is used repeatedly in the merge sort algorithm.
-------> An example merge sort is given in the illustration. It starts with an unsorted array of 7 integers. 
---------> The array is divided into 7 partitions; each partition contains 1 element and is sorted. 
---------> The sorted partitions are then merged to produce larger, sorted, partitions, until 1 partition, the sorted array, is left.


-------> Merging two lists
---------> Merging two sorted lists into one can be done in linear time and linear or constant space (depending on the data access model). 
-----------> The following pseudocode demonstrates an algorithm that merges input lists (either linked lists or arrays) A and B into a new list C.[1][2]:
-----------> The function head yields the first element of a list; "dropping" an element means removing it from its list, typically by incrementing a pointer or index.
-------------> algorithm merge(A, B) is
------------->     inputs A, B : list
------------->     returns list
------------->     C := new empty list
------------->     while A is not empty and B is not empty do
------------->         if head(A) ≤ head(B) then
------------->             append head(A) to C
------------->             drop the head of A
------------->         else
------------->             append head(B) to C
------------->             drop the head of B
------------->     // By now, either A or B is empty. It remains to empty the other input list.
------------->     while A is not empty do
------------->         append head(A) to C
------------->         drop the head of A
------------->     while B is not empty do
------------->         append head(B) to C
------------->         drop the head of B
------------->     return C
---------> When the inputs are linked lists, this algorithm can be implemented to use only a constant amount of working space; 
-----------> the pointers in the lists' nodes can be reused for bookkeeping and for constructing the final merged list.
---------> In the merge sort algorithm, this subroutine is typically used to merge two sub-arrays A[lo..mid], A[mid+1..hi] of a single array A. 
-----------> This can be done by copying the sub-arrays into a temporary array, then applying the merge algorithm above.
-----------> The allocation of a temporary array can be avoided, but at the expense of speed and programming ease. 
-----------> Various in-place merge algorithms have been devised, sometimes sacrificing the linear-time bound to produce an O(n log n) algorithm.

-------> K-way merging
---------> k-way merging generalizes binary merging to an arbitrary number k of sorted input lists. 
-----------> Applications of k-way merging arise in various sorting algorithms, including patience sorting 
-----------> and an external sorting algorithm that divides its input into k = 1/M−1 blocks that fit in memory, sorts these one by one, then merges these blocks.
---------> Several solutions to this problem exist. 
-----------> A naive solution is to do a loop over the k lists to pick off the minimum element each time, and repeat this loop until all lists are empty:
----------->     Input: a list of k lists.
----------->     While any of the lists is non-empty:
----------->         Loop over the lists to find the one with the minimum first element.
----------->         Output the minimum element and remove it from its list.
---------> In the worst case, this algorithm performs (k−1)(n−k/2) element comparisons to perform its work if there are a total of n elements in the lists.
-----------> It can be improved by storing the lists in a priority queue (min-heap) keyed by their first element:
----------->     Build a min-heap h of the k lists, using the first element as the key.
----------->     While any of the lists is non-empty:
----------->         Let i = find-min(h).
----------->         Output the first element of list i and remove it from its list.
----------->         Re-heapify h.
---------> Searching for the next smallest element to be output (find-min) and restoring heap order can now be done in O(log k) time (more specifically, 2⌊log k⌋ comparisons[6]), 
-----------> and the full problem can be solved in O(n log k) time (approximately 2n log(k) comparisons).
---------> A third algorithm for the problem is a divide and conquer solution that builds on the binary merge algorithm:
----------->     If k = 1, output the single input list.
----------->     If k = 2, perform a binary merge.
----------->     Else, recursively merge the first lower(k/2) lists and the final upper(k/2) lists, then binary merge these.
---------> When the input lists to this algorithm are ordered by length, shortest first, it requires fewer than n⌈log k⌉ comparisons, 
-----------> i.e., less than half the number used by the heap-based algorithm; in practice, 
-----------> it may be about as fast or slow as the heap-based algorithm.

-------> Parallel merge
---------> A parallel version of the binary merge algorithm can serve as a building block of a parallel merge sort. 
-----------> The following pseudocode demonstrates this algorithm in a parallel divide-and-conquer style (adapted from Cormen et al.). 
-----------> It operates on two sorted arrays A and B and writes the sorted output to array C. 
-----------> The notation A[i...j] denotes the part of A from index i through j, exclusive.
-------------> algorithm merge(A[i...j], B[k...ℓ], C[p...q]) is
------------->     inputs A, B, C : array
------------->            i, j, k, ℓ, p, q : indices
------------->     let m = j - i,
------------->         n = ℓ - k
------------->     if m < n then
------------->         swap A and B  // ensure that A is the larger array: i, j still belong to A; k, ℓ to B
------------->         swap m and n
------------->     if m ≤ 0 then
------------->         return  // base case, nothing to merge
------------->     let r = floor((i + j)/2)
------------->     let s = binary-search(A[r], B[k...ℓ])
------------->     let t = p + (r - i) + (s - k)
------------->     C[t] = A[r]
------------->     in parallel do
------------->         merge(A[i...r], B[k...s], C[p...t])
------------->         merge(A[r+1...j], B[s...ℓ], C[t+1...q])
---------> The algorithm operates by splitting either A or B, whichever is larger, into (nearly) equal halves. 
-----------> It then splits the other array into a part with values smaller than the midpoint of the first, and a part with larger or equal values. 
-----------> (The binary search subroutine returns the index in B where A[r] would be, if it were in B; that this always a number between k and ℓ.) 
-----------> Finally, each pair of halves is merged recursively, and since the recursive calls are independent of each other, they can be done in parallel. 
-----------> Hybrid approach, where serial algorithm is used for recursion base case has been shown to perform well in practice 
---------> The work performed by the algorithm for two arrays holding a total of n elements, i.e., the running time of a serial version of it, is O(n). 
-----------> This is optimal since n elements need to be copied into C. To calculate the span of the algorithm, it is necessary to derive a Recurrence relation. 
-----------> Since the two recursive calls of merge are in parallel, only the costlier of the two calls needs to be considered. 
-----------> In the worst case, the maximum number of elements in one of the recursive calls is at most 3/4 n  since the array with more elements is perfectly split in half. 
-----------> Adding the Θ ( log ⁡ ( n ) )  cost of the Binary Search, we obtain this recurrence as an upper bound:
-------------> T ∞ merge(n) = T ∞ merge(3/4 n) + Θ(log⁡ (n)) 
-------------> The solution is T ∞ merge(n) = Θ(log⁡ (n)2) , meaning that it takes that much time on an ideal machine with an unbounded number of processors.
---------> Note: The routine is not stable: if equal items are separated by splitting A and B, 
-----------> they will become interleaved in C; also swapping A and B will destroy the order, 
-----------> if equal items are spread among both input arrays. 
-----------> As a result, when used for sorting, this algorithm produces a sort that is not stable. 



-----> Sequence permutations
-------> In mathematics, a permutation of a set is, loosely speaking, an arrangement of its members into a sequence or linear order, or if the set is already ordered, a rearrangement of its elements. 
---------> The word "permutation" also refers to the act or process of changing the linear order of an ordered set.[
-------> Permutations differ from combinations, which are selections of some members of a set regardless of order. 
---------> For example, written as tuples, there are six permutations of the set {1, 2, 3}, namely (1, 2, 3), (1, 3, 2), (2, 1, 3), (2, 3, 1), (3, 1, 2), and (3, 2, 1). 
---------> These are all the possible orderings of this three-element set. Anagrams of words whose letters are different are also permutations:
---------> the letters are already ordered in the original word, and the anagram is a reordering of the letters. The study of permutations of finite sets is an important topic in the fields of combinatorics and group theory.
-------> Permutations are used in almost every branch of mathematics, and in many other fields of science. 
---------> In computer science, they are used for analyzing sorting algorithms; in quantum physics, for describing states of particles; and in biology, for describing RNA sequences.
-------> The number of permutations of n distinct objects is n factorial, usually written as n!, which means the product of all positive integers less than or equal to n.
-------> Technically, a permutation of a set S is defined as a bijection from S to itself. 
---------> That is, it is a function from S to S for which every element occurs exactly once as an image value. 
---------> This is related to the rearrangement of the elements of S in which each element s is replaced by the corresponding f(s). 
---------> For example, the permutation (3, 1, 2) mentioned above is described by the function α defined as:
-----------> α(1) = 3 , α(2) = 1 , α(3) = 2.
-------> The collection of all permutations of a set form a group called the symmetric group of the set. 
---------> The group operation is the composition (performing two given rearrangements in succession), which results in another rearrangement. 
---------> As properties of permutations do not depend on the nature of the set elements, it is often the permutations of the set { 1 , 2 , … , n } that are considered for studying permutations.
-------> In elementary combinatorics, the k-permutations, or partial permutations, are the ordered arrangements of k distinct elements selected from a set. 
---------> When k is equal to the size of the set, these are the permutations of the set. 

-------> Fisher–Yates shuffle 
---------> This is (also known as the Knuth shuffle): randomly shuffle a finite set
---------> The Fisher–Yates shuffle is an algorithm for generating a random permutation of a finite sequence—in plain terms, the algorithm shuffles the sequence. 
-----------> The algorithm effectively puts all the elements into a hat; it continually determines the next element by randomly drawing an element from the hat until no elements remain. 
-----------> The algorithm produces an unbiased permutation: every permutation is equally likely. 
-----------> The modern version of the algorithm is efficient: it takes time proportional to the number of items being shuffled and shuffles them in place.
---------> The Fisher–Yates shuffle is named after Ronald Fisher and Frank Yates, who first described it, and is also known as the Knuth shuffle after Donald Knuth. 
-----------> A variant of the Fisher–Yates shuffle, known as Sattolo's algorithm, may be used to generate random cyclic permutations of length n instead of random permutations.
---------> Comparison with other shuffling algorithms
-----------> The asymptotic time and space complexity of the Fisher–Yates shuffle are optimal. 
-------------> Combined with a high-quality unbiased random number source, it is also guaranteed to produce unbiased results. 
-------------> Compared to some other solutions, it also has the advantage that, if only part of the resulting permutation is needed, 
-------------> it can be stopped halfway through, or even stopped and restarted repeatedly, generating the permutation incrementally as needed.
-----------> Naïve method
-------------> The naïve method of swapping each element with another element chosen randomly from all elements is biased and fundamentally broken. 
---------------> Different permutations will have different probabilities of being generated, for every n > 2 {\displaystyle n>2} n>2, because the number of different permutations, n!, 
---------------> does not evenly divide the number of random outcomes of the algorithm, n^n. 
---------------> In particular, by Bertrand's postulate there will be at least one prime number between n / 2  and n , and this number will divide n !  but not divide n^n.
-----------------> from random import randrange
-----------------> def naive_shuffle(items) -> None:
----------------->     """A naive method. This is an example of what not to do -- use Fisher-Yates instead."""
----------------->     n = len(items)
----------------->     for i in range(n):
----------------->         j = randrange(n)  # 0 <= j <= n-1
----------------->         items[j], items[i] = items[i], items[j]
---------> Fisher and Yates' original method
-----------> Their description of the algorithm used pencil and paper; a table of random numbers provided the randomness. 
-------------> The basic method given for generating a random permutation of the numbers 1 through N goes as follows:
---------------> Write down the numbers from 1 through N.
---------------> Pick a random number k between one and the number of unstruck numbers remaining (inclusive).
---------------> Counting from the low end, strike out the kth number not yet struck out, and write it down at the end of a separate list.
---------------> Repeat from step 2 until all the numbers have been struck out.
---------------> The sequence of numbers written down in step 3 is now a random permutation of the original numbers.
-----------> Provided that the random numbers picked in step 2 above are truly random and unbiased, so will be the resulting permutation. 
-------------> Fisher and Yates took care to describe how to obtain such random numbers in any desired range from the supplied tables in a manner which avoids any bias. 
-------------> They also suggested the possibility of using a simpler method — picking random numbers from one to N and discarding any duplicates—to generate the first half of the permutation, 
-------------> and only applying the more complex algorithm to the remaining half, where picking a duplicate number would otherwise become frustratingly common. 
-----------> The Fisher–Yates shuffle, in its original form, was described in 1938 by Ronald Fisher and Frank Yates in their book Statistical tables for biological, agricultural and medical research.
---------> The modern algorithm
-----------> The algorithm described by Durstenfeld differs from that given by Fisher and Yates in a small but significant way. 
-------------> Whereas a naïve computer implementation of Fisher and Yates' method would spend needless time counting the remaining numbers in step 3 above, 
-------------> Durstenfeld's solution is to move the "struck" numbers to the end of the list by swapping them with the last unstruck number at each iteration. 
-------------> This reduces the algorithm's time complexity to O(n) compared to O(n^2) for the naïve implementation. 
-------------> This change gives the following algorithm (for a zero-based array).
---------------> -- To shuffle an array a of n elements (indices 0..n-1):
---------------> for i from n−1 downto 1 do
--------------->      j ← random integer such that 0 ≤ j ≤ i
--------------->      exchange a[j] and a[i]
---------------> An equivalent version which shuffles the array in the opposite direction (from lowest index to highest) is:
---------------> -- To shuffle an array a of n elements (indices 0..n-1):
---------------> for i from 0 to n−2 do
--------------->      j ← random integer such that i ≤ j < n
--------------->      exchange a[i] and a[j]
-----------> The modern version of the Fisher–Yates shuffle, designed for computer use, was introduced by Richard Durstenfeld in 1964 
-------------> and popularized by Donald E. Knuth in The Art of Computer Programming as "Algorithm P (Shuffling)".
-----------> Neither Durstenfeld's article nor Knuth's first edition of The Art of Computer Programming acknowledged the work of Fisher and Yates; they may not have been aware of it.
-------------> Subsequent editions of Knuth's The Art of Computer Programming mention Fisher and Yates' contribution.


-------> Schensted algorithm:
---------> This constructs a pair of Young tableaux from a permutation
---------> The Robinson–Schensted correspondence is a bijective correspondence between permutations and pairs of standard Young tableaux of the same shape. 
-----------> It has various descriptions, all of which are of algorithmic nature, it has many remarkable properties, and it has applications in combinatorics and other areas such as representation theory. 
-----------> The correspondence has been generalized in numerous ways, notably by Knuth to what is known as the Robinson–Schensted–Knuth correspondence, and a further generalization to pictures by Zelevinsky.
---------> The simplest description of the correspondence is using the Schensted algorithm (Schensted 1961), 
-----------> a procedure that constructs one tableau by successively inserting the values of the permutation according to a specific rule, 
-----------> while the other tableau records the evolution of the shape during construction. 
-----------> The correspondence had been described, in a rather different form, much earlier by Robinson (Robinson 1938), in an attempt to prove the Littlewood–Richardson rule. 
-----------> The correspondence is often referred to as the Robinson–Schensted algorithm, 
-----------> although the procedure used by Robinson is radically different from the Schensted algorithm, and almost entirely forgotten. 
-----------> Other methods of defining the correspondence include a nondeterministic algorithm in terms of jeu de taquin. 
---------> The Schensted algorithm
-----------> The Schensted algorithm starts from the permutation σ written in two-line notation:
------------->     σ = ( 1, 2, 3, ...   n) 
------------->       = (σ1,σ2,σ3, ...  σn) 
-------------> where σi = σ(i), and proceeds by constructing sequentially a sequence of (intermediate) ordered pairs of Young tableaux of the same shape:
------------->     (P0, Q0), (P1, Q1), … , (Pn, Qn) ,
-------------> where P0 = Q0 are empty tableaux. 
-----------> The output tableaux are P = Pn and Q = Qn. Once Pi−1 is constructed, one forms Pi by inserting σi into Pi−1, 
-------------> and then Qi by adding an entry i to Qi−1 in the square added to the shape by the insertion (so that Pi and Qi have equal shapes for all i). 
-------------> Because of the more passive role of the tableaux Qi, the final one Qn, which is part of the output and from which the previous Qi are easily read off, 
-------------> is called the recording tableau; by contrast the tableaux Pi are called insertion tableaux.
-----------> Insertion
-------------> The basic procedure used to insert each σi is called Schensted insertion or row-insertion (to distinguish it from a variant procedure called column-insertion). 
---------------> Its simplest form is defined in terms of "incomplete standard tableaux": 
---------------> like standard tableaux they have distinct entries, forming increasing rows and columns, 
---------------> but some values (still to be inserted) may be absent as entries. 
---------------> The procedure takes as arguments such a tableau T and a value x not present as entry of T; 
---------------> it produces as output a new tableau denoted T ← x and a square s by which its shape has grown. 
---------------> The value x appears in the first row of T ← x, either having been added at the end (if no entries larger than x were present), 
---------------> or otherwise replacing the first entry y > x in the first row of T. In the former case s is the square where x is added, and the insertion is completed; 
---------------> in the latter case the replaced entry y is similarly inserted into the second row of T, and so on, until at some step the first case applies (which certainly happens if an empty row of T is reached).
---------------> More formally, the following pseudocode describes the row-insertion of a new value x into T.
-----------------> Set i = 1 and j to one more than the length of the first row of T.
-----------------> While j > 1 and x < Ti, j−1, decrease j by 1. (Now (i, j) is the first square in row i with either an entry larger than x in T, or no entry at all.)
-----------------> If the square (i, j) is empty in T, terminate after adding x to T in square (i, j) and setting s = (i, j).
-----------------> Swap the values x and Ti, j. (This inserts the old x into row i, and saves the value it replaces for insertion into the next row.)
-----------------> Increase i by 1 and return to step 2.
---------------> The shape of T grows by exactly one square, namely s.
-----------> Correctness
-------------> The fact that T ← x has increasing rows and columns, if the same holds for T, is not obvious from this procedure (entries in the same column are never even compared). 
---------------> It can however be seen as follows. 
---------------> At all times except immediately after step 4, the square (i, j) is either empty in T or holds a value greater than x; 
---------------> step 5 re-establishes this property because (i, j) now is the square immediately below the one that originally contained x in T. 
---------------> Thus the effect of the replacement in step 4 on the value Ti, j is to make it smaller; in particular it cannot become greater than its right or lower neighbours. 
---------------> On the other hand the new value is not less than its left neighbour (if present) either, as is ensured by the comparison that just made step 2 terminate. 
---------------> Finally to see that the new value is larger than its upper neighbour Ti−1, j if present, observe that Ti−1, j holds after step 5, and that decreasing j in step 2 only decreases the corresponding value Ti−1, j.
-----------> Constructing the tableaux
-------------> The full Schensted algorithm applied to a permutation σ proceeds as follows.
---------------> Set both P and Q to the empty tableau
---------------> For i increasing from 1 to n compute P ← σi and the square s by the insertion procedure; then replace P by P ← σi and add the entry i to the tableau Q in the square s.
---------------> Terminate, returning the pair (P, Q).
-------------> The algorithm produces a pair of standard Young tableaux.
---------------> Invertibility of the construction
---------------> It can be seen that given any pair (P, Q) of standard Young tableaux of the same shape, there is an inverse procedure that produces a permutation that will give rise to (P, Q) by the Schensted algorithm. 
---------------> It essentially consists of tracing steps of the algorithm backwards, each time using an entry of Q to find the square where the inverse insertion should start, 
---------------> moving the corresponding entry of P to the preceding row, and continuing upwards through the rows until an entry of the first row is replaced, 
---------------> which is the value inserted at the corresponding step of the construction algorithm. These two inverse algorithms define a bijective correspondence between permutations of n on one side, 
---------------> and pairs of standard Young tableaux of equal shape and containing n squares on the other side. 

-------> Steinhaus–Johnson–Trotter algorithm 
---------> This (also known as the Johnson–Trotter algorithm): generates permutations by transposing elements.
-----------> This generates all of the permutations of n elements. Each permutation in the sequence that it generates differs from the previous permutation by swapping two adjacent elements of the sequence. 
-----------> Equivalently, this algorithm finds a Hamiltonian cycle in the permutohedron.
---------> This method was known already to 17th-century English change ringers, and Sedgewick (1977) calls it "perhaps the most prominent permutation enumeration algorithm". 
-----------> A version of the algorithm can be implemented in such a way that the average time per permutation is constant. 
-----------> As well as being simple and computationally efficient, 
-----------> this algorithm has the advantage that subsequent computations on the permutations that it generates may be sped up because of the similarity between consecutive permutations that it generates.
-----------> Algorithm
-------------> The sequence of permutations generated by the Steinhaus–Johnson–Trotter algorithm has a natural recursive structure, that can be generated by a recursive algorithm. 
-------------> However the actual Steinhaus–Johnson–Trotter algorithm does not use recursion, instead computing the same sequence of permutations by a simple iterative method. 
-------------> A later improvement allows it to run in constant average time per permutation.
-------------> Recursive structure
---------------> The sequence of permutations for a given number n can be formed from the sequence of permutations for n-1 by placing the number n into each possible position in each of the shorter permutations. 
-----------------> The Steinhaus–Johnson–Trotter algorithm follows this structure: the sequence of permutations it generates consists of (n-1)! blocks of permutations, 
-----------------> so that within each block the permutations agree on the ordering of the numbers from 1 to n-1 and differ only in the position of n. 
-----------------> The blocks themselves are ordered recursively, according to the Steinhaus–Johnson–Trotter algorithm for one less element. 
-----------------> Within each block, the positions in which n is placed occur either in descending or ascending order, and the blocks alternate between these two orders: 
-----------------> the placements of n in the first block are in descending order, in the second block they are in ascending order, in the third block they are in descending order, and so on.
---------------> Thus, from the single permutation on one element,
-----------------> 1
---------------> one may place the number 2 in each possible position in descending order to form a list of two permutations on two elements,
-----------------> 1 2
-----------------> 2 1
---------------> Then, one may place the number 3 in each of three different positions for these two permutations, in descending order for the first permutation 1 2, and then in ascending order for the permutation 2 1:
-----------------> 1 2 3
-----------------> 1 3 2
-----------------> 3 1 2
-----------------> 3 2 1
-----------------> 2 3 1
-----------------> 2 1 3
---------------> The same placement pattern, alternating between descending and ascending placements of n, applies for any larger value of n. 
-----------------> In sequences of permutations with this recursive structure, each permutation differs from the previous one either by the single-position-at-a-time motion of n, 
-----------------> or by a change of two smaller numbers inherited from the previous sequence of shorter permutations. 
-----------------> In either case this difference is just the transposition of two adjacent elements. 
-----------------> When n>1 the first and final elements of the sequence, also, differ in only two adjacent elements (the positions of the numbers 1 and 2), as may be proven by induction.
---------------> This sequence may be generated by a recursive algorithm that constructs the sequence of smaller permutations 
-----------------> and then performs all possible insertions of the largest number into the recursively-generated sequence.
-----------------> The same ordering of permutations can also be described equivalently as the ordering generated by the following greedy algorithm.
-----------------> Start with the identity permutation 1 2 … n. 
-----------------> Now repeatedly transpose the largest possible entry with the entry to its left or right, such that in each step, 
-----------------> a new permutation is created that has not been encountered in the list of permutations before. 
-----------------> For example, in the case n=3 the sequence starts with 1 2 3, then flips 3 with its left neighbor to get 1 3 2. 
-----------------> From this point, flipping 3 with its right neighbor 2 would yield the initial permutation 1 2 3, so the sequence instead flips 3 with its left neighbor 1 and arrives at 3 1 2, etc. 
-----------------> The direction of the transposition (left or right) is always uniquely determined in this algorithm. 
-----------------> However, the actual Steinhaus–Johnson–Trotter algorithm does not use recursion, and does not need to keep track of the permutations that it has already encountered. 
-----------------> Instead, it computes the same sequence of permutations by a simple iterative method. 
-----------> The Steinhaus–Johnson–Trotter algorithm or Johnson–Trotter algorithm, also called plain changes, is an algorithm named after Hugo Steinhaus, Selmer M. Johnson and Hale F. Trotter.

-------> Heap's permutation generation algorithm: 
---------> This  interchange elements to generate next permutation
-----------> Not to be confused with heapsort.
---------> Heap's algorithm generates all possible permutations of n objects. 
-----------> It was first proposed by B. R. Heap in 1963.
-----------> The algorithm minimizes movement: it generates each permutation from the previous one by interchanging a single pair of elements; the other n−2 elements are not disturbed. 
-----------> In a 1977 review of permutation-generating algorithms, Robert Sedgewick concluded that it was at that time the most effective algorithm for generating permutations by computer.
---------> The sequence of permutations of n objects generated by Heap's algorithm is the beginning of the sequence of permutations of n+1 objects. 
-----------> So there is one infinite sequence of permutations generated by Heap's algorithm (sequence A280318 in the OEIS). 
-----------> Algorithm
-------------> For a collection C containing n different elements, 
---------------> Heap found a systematic method for choosing at each step a pair of elements to switch in order to produce every possible permutation of these elements exactly once.
-------------> Described recursively as a decrease and conquer method, Heap's algorithm operates at each step on the k initial elements of the collection. 
---------------> Initially k=n and thereafter k<n. 
---------------> Each step generates the k! permutations that end with the same n-k final elements. 
---------------> It does this by calling itself once with the kth  element unaltered and then k-1 times with the ( k th ) element exchanged for each of the initial k-1 elements. 
---------------> The recursive calls modify the initial k-1 elements and a rule is needed at each iteration to select which will be exchanged with the last. 
---------------> Heap's method says that this choice can be made by the parity of the number of elements operated on at this step. 
---------------> If k is even, then the final element is iteratively exchanged with each element index. If k is odd, the final element is always exchanged with the first.
-----------------> procedure generate(k : integer, A : array of any):
----------------->     if k = 1 then
----------------->         output(A)
----------------->     else
----------------->         // Generate permutations with k-th unaltered
----------------->         // Initially k = length(A)
----------------->         generate(k - 1, A)
-----------------> 
----------------->         // Generate permutations for k-th swapped with each k-1 initial
----------------->         for i := 0; i < k-1; i += 1 do
----------------->             // Swap choice dependent on parity of k (even or odd)
----------------->             if k is even then
----------------->                 swap(A[i], A[k-1]) // zero-indexed, the k-th is at k-1
----------------->             else
----------------->                 swap(A[0], A[k-1])
----------------->             end if
----------------->             generate(k - 1, A)
----------------->         end for
----------------->     end if
-------------> One can also write the algorithm in a non-recursive format.
-----------------> procedure generate(n : integer, A : array of any):
----------------->     // c is an encoding of the stack state. c[k] encodes the for-loop counter for when generate(k - 1, A) is called
----------------->     c : array of int
----------------->     for i := 0; i < n; i += 1 do
----------------->         c[i] := 0
----------------->     end for
----------------->     output(A)
----------------->     // i acts similarly to a stack pointer
----------------->     i := 1;
----------------->     while i < n do
----------------->         if  c[i] < i then
----------------->             if i is even then
----------------->                 swap(A[0], A[i])
----------------->             else
----------------->                 swap(A[c[i]], A[i])
----------------->             end if
----------------->             output(A)
----------------->             // Swap has occurred ending the for-loop. Simulate the increment of the for-loop counter
----------------->             c[i] += 1
----------------->             // Simulate recursive call reaching the base case by bringing the pointer to the base case analog in the array
----------------->             i := 1
----------------->         else
----------------->             // Calling generate(i+1, A) has ended as the for-loop terminated. Reset the state and simulate popping the stack by incrementing the pointer.
----------------->             c[i] := 0
----------------->             i += 1
----------------->         end if
----------------->     end while



-----> Sequence combinations

------->  Combination
---------> In mathematics, a combination is a selection of items from a set that has distinct members, such that the order of selection does not matter (unlike permutations). 
-----------> For example, given three fruits, say an apple, an orange and a pear, 
-----------> there are three combinations of two that can be drawn from this set: an apple and a pear; an apple and an orange; or a pear and an orange. 
-----------> More formally, a k-combination of a set S is a subset of k distinct elements of S. 
-----------> So, two combinations are identical if and only if each combination has the same members. (The arrangement of the members in each set does not matter.) 
-----------> If the set has n elements, the number of k-combinations, denoted as C k n {\displaystyle C_{k}^{n}} C_{k}^{n}, is equal to the binomial coefficient
-------------> (n,k) = n(n−1) ⋯ (n−k+1) k (k−1) ⋯ 1 , 
-----------> which can be written using factorials as n! / k!(n−k)!  whenever k ≤ n, and which is zero when k > n. 
-----------> This formula can be derived from the fact that each k-combination of a set S of n members has k! permutations so P(n,k) = C(n,k) × k! or C(n,k) = P(n,k)/k!. 
-----------> The set of all k-combinations of a set S is often denoted by (S,k).
---------> A combination is a combination of n things taken k at a time without repetition. 
-----------> To refer to combinations in which repetition is allowed, the terms k-selection,[2] k-multiset,[3] or k-combination with repetition are often used. 
-----------> If, in the above example, it were possible to have two of any one kind of fruit there would be 3 more 2-selections: one with two apples, one with two oranges, and one with two pears.
---------> Although the set of three fruits was small enough to write a complete list of combinations, this becomes impractical as the size of the set increases. 
-----------> For example, a poker hand can be described as a 5-combination (k = 5) of cards from a 52 card deck (n = 52). 
-----------> The 5 cards of the hand are all distinct, and the order of cards in the hand does not matter. 
-----------> There are 2,598,960 such combinations, and the chance of drawing any one hand at random is 1 / 2,598,960. 



-----> Sequence alignment

-------> Dynamic time warping: 
---------> This measure similarity between two sequences which may vary in time or speed.
---------> In time series analysis, dynamic time warping (DTW) is an algorithm for measuring similarity between two temporal sequences, which may vary in speed. 
-----------> For instance, similarities in walking could be detected using DTW, even if one person was walking faster than the other, or if there were accelerations and decelerations during the course of an observation. 
-----------> DTW has been applied to temporal sequences of video, audio, and graphics data — indeed, any data that can be turned into a linear sequence can be analyzed with DTW. 
-----------> A well-known application has been automatic speech recognition, to cope with different speaking speeds. Other applications include speaker recognition and online signature recognition. 
-----------> It can also be used in partial shape matching applications.
---------> In general, DTW is a method that calculates an optimal match between two given sequences (e.g. time series) with certain restriction and rules:
-----------> (1) Every index from the first sequence must be matched with one or more indices from the other sequence, and vice versa
-----------> (2) The first index from the first sequence must be matched with the first index from the other sequence (but it does not have to be its only match)
-----------> (3) The last index from the first sequence must be matched with the last index from the other sequence (but it does not have to be its only match)
-----------> (4) The mapping of the indices from the first sequence to indices from the other sequence must be monotonically increasing, and vice versa, 
-------------> i.e. if j>i are indices from the first sequence, then there must not be two indices l>k  in the other sequence, 
-------------> such that index i is matched with index l and index j is matched with index k, and vice versa
---------> The optimal match is denoted by the match that satisfies all the restrictions and the rules and that has the minimal cost, 
-----------> where the cost is computed as the sum of absolute differences, for each matched pair of indices, between their values.
---------> The sequences are "warped" non-linearly in the time dimension to determine a measure of their similarity independent of certain non-linear variations in the time dimension. 
-----------> This sequence alignment method is often used in time series classification. Although DTW measures a distance-like quantity between two given sequences, it doesn't guarantee the triangle inequality to hold.
---------> In addition to a similarity measure between the two sequences, a so called "warping path" is produced, by warping according to this path the two signals may be aligned in time. 
-----------> The signal with an original set of points X(original), Y(original) is transformed to X(warped), Y(warped). 
-----------> This finds applications in genetic sequence and audio synchronisation. In a related technique sequences of varying speed may be averaged using this technique see the average sequence section.
---------> This is conceptually very similar to the Needleman–Wunsch algorithm. 
---------> Implementation
-----------> This example illustrates the implementation of the dynamic time warping algorithm when the two sequences s and t are strings of discrete symbols. 
-------------> For two symbols x and y, d(x, y) is a distance between the symbols, e.g. d(x, y) = | x − y | {\displaystyle |x-y|} | x - y |.
---------------> int DTWDistance(s: array [1..n], t: array [1..m]) {
--------------->     DTW := array [0..n, 0..m]
--------------->     for i := 0 to n
--------------->         for j := 0 to m
--------------->             DTW[i, j] := infinity
--------------->     DTW[0, 0] := 0
--------------->     for i := 1 to n
--------------->         for j := 1 to m
--------------->             cost := d(s[i], t[j])
--------------->             DTW[i, j] := cost + minimum(DTW[i-1, j  ],    // insertion
--------------->                                         DTW[i  , j-1],    // deletion
--------------->                                         DTW[i-1, j-1])    // match
--------------->     return DTW[n, m]
---------------> }
-------------> where DTW[i, j] is the distance between s[1:i] and t[1:j] with the best alignment.
-----------> We sometimes want to add a locality constraint. 
-------------> That is, we require that if s[i] is matched with t[j], then | i − j | is no larger than w, a window parameter.
-----------> We can easily modify the above algorithm to add a locality constraint (differences marked). 
-------------> However, the above given modification works only if | n − m | is no larger than w, i.e. the end point is within the window length from diagonal. 
-------------> In order to make the algorithm work, the window parameter w must be adapted so that | n − m | ≤ w (see the line marked with (*) in the code).
---------------> int DTWDistance(s: array [1..n], t: array [1..m], w: int) {
--------------->     DTW := array [0..n, 0..m]
--------------->     w := max(w, abs(n-m)) // adapt window size (*)
--------------->     for i := 0 to n
--------------->         for j:= 0 to m
--------------->             DTW[i, j] := infinity
--------------->     DTW[0, 0] := 0
--------------->     for i := 1 to n
--------------->         for j := max(1, i-w) to min(m, i+w)
--------------->             DTW[i, j] := 0
--------------->     for i := 1 to n
--------------->         for j := max(1, i-w) to min(m, i+w)
--------------->             cost := d(s[i], t[j])
--------------->             DTW[i, j] := cost + minimum(DTW[i-1, j  ],    // insertion
--------------->                                         DTW[i  , j-1],    // deletion
--------------->                                         DTW[i-1, j-1])    // match
--------------->     return DTW[n, m]
---------------> }

-------> Hirschberg's algorithm: 
---------> This finds the least cost sequence alignment between two sequences, as measured by their Levenshtein distance.
-----------> This is a dynamic programming algorithm that finds the optimal sequence alignment between two strings. 
-----------> Optimality is measured with the Levenshtein distance, defined to be the sum of the costs of insertions, 
-----------> replacements, deletions, and null actions needed to change one string into the other. 
-----------> Hirschberg's algorithm is simply described as a more space-efficient version of the Needleman–Wunsch algorithm that uses divide and conquer.
-----------> Hirschberg's algorithm is commonly used in computational biology to find maximal global alignments of DNA and protein sequences. 
---------> Algorithm information
-----------> Hirschberg's algorithm is a generally applicable algorithm for optimal sequence alignment. 
-------------> BLAST and FASTA are suboptimal heuristics. 
-------------> If x and y are strings, where length(x) = n and length(y) = m, the Needleman–Wunsch algorithm finds an optimal alignment in O(nm) time, using O(nm) space. 
-------------> Hirschberg's algorithm is a clever modification of the Needleman–Wunsch Algorithm, which still takes O(nm) time, but needs only O(min{n, m}) space and is much faster in practice. 
-------------> One application of the algorithm is finding sequence alignments of DNA or protein sequences. 
-------------> It is also a space-efficient way to calculate the longest common subsequence between two sets of data such as with the common diff tool.
-----------> The Hirschberg algorithm can be derived from the Needleman–Wunsch algorithm by observing that:
---------------> one can compute the optimal alignment score by only storing the current and previous row of the Needleman–Wunsch score matrix;
---------------> if ( Z , W ) = NW ⁡(X, Y)is the optimal alignment of(X, Y)is an arbitrary partition of X, 
---------------> there exists a partition Yl + Yr of Y such that NW ⁡(X, Y)= NW⁡ (Xl, Yl) + NW⁡ (Xr , Yr).
---------> Algorithm description
-----------> Xi denotes the i-th character of X, where 1 ⩽ i ⩽ length ⁡ ( X ).
-------------> X i : j, ranging from the i-th to the j-th character of X. rev ⁡ ( X ) is the reversed version of X.
-----------> X and Y are sequences to be aligned. 
-------------> Let X be a character from X, and Y be a character from Y. 
-------------> We assume that Del ⁡ ( x ), Ins ⁡ ( y )  and Sub ⁡(X, Y)are well defined integer-valued functions. 
-------------> These functions represent the cost of deleting X, inserting Y, and replacing X with Y, respectively.
-----------> We define NWScore ⁡ ( X , Y ), which returns the last line of the Needleman–Wunsch score matrix Score(i, j):
-------------> function NWScore(X, Y)
------------->     Score(0, 0) = 0 // 2 * (length(Y) + 1) array
------------->     for j = 1 to length(Y)
------------->         Score(0, j) = Score(0, j - 1) + Ins(Yj)
------------->     for i = 1 to length(X) // Init array
------------->         Score(1, 0) = Score(0, 0) + Del(Xi)
------------->         for j = 1 to length(Y)
------------->             scoreSub = Score(0, j - 1) + Sub(Xi, Yj)
------------->             scoreDel = Score(0, j) + Del(Xi)
------------->             scoreIns = Score(1, j - 1) + Ins(Yj)
------------->             Score(1, j) = max(scoreSub, scoreDel, scoreIns)
------------->         end
------------->         // Copy Score[1] to Score[0]
------------->         Score(0, :) = Score(1, :)
------------->     end
------------->     for j = 0 to length(Y)
------------->         LastLine(j) = Score(1, j)
------------->     return LastLine
-----------> Note that at any point, NWScore only requires the two most recent rows of the score matrix. 
-----------> Thus, NWScore is implemented in O ( min { length ⁡ ( X ) , length ⁡ ( Y ) } ) space.
-----------> The Hirschberg algorithm follows:
-------------> function Hirschberg(X, Y)
------------->     Z = ""
------------->     W = ""
------------->     if length(X) == 0
------------->         for i = 1 to length(Y)
------------->             Z = Z + '-'
------------->             W = W + Yi
------------->         end
------------->     else if length(Y) == 0
------------->         for i = 1 to length(X)
------------->             Z = Z + Xi
------------->             W = W + '-'
------------->         end
------------->     else if length(X) == 1 or length(Y) == 1
------------->         (Z, W) = NeedlemanWunsch(X, Y)
------------->     else
------------->         xlen = length(X)
------------->         xmid = length(X) / 2
------------->         ylen = length(Y)
-------------> 
------------->         ScoreL = NWScore(X1:xmid, Y)
------------->         ScoreR = NWScore(rev(Xxmid+1:xlen), rev(Y))
------------->         ymid = arg max ScoreL + rev(ScoreR)
-------------> 
------------->         (Z,W) = Hirschberg(X1:xmid, y1:ymid) + Hirschberg(Xxmid+1:xlen, Yymid+1:ylen)
------------->     end
------------->     return (Z, W)
-----------> In the context of observation (2), assume that Xl + Xr  is a partition of X. 
-------------> Index ymid is computed such that Yl = Y1:ymid and Yr = Yymid + 1:length⁡ (Y). 
---------> The Hirschberg's algorithm is named after its inventor(Dan Hirschberg).

-------> Needleman–Wunsch algorithm: 
---------> This find global alignment between two sequences.
-----------> The Needleman–Wunsch algorithm is an algorithm used in bioinformatics to align protein or nucleotide sequences. 
-----------> It was one of the first applications of dynamic programming to compare biological sequences. 
-----------> The algorithm was developed by Saul B. Needleman and Christian D. Wunsch and published in 1970.
-----------> The algorithm essentially divides a large problem (e.g. the full sequence) into a series of smaller problems, 
-----------> and it uses the solutions to the smaller problems to find an optimal solution to the larger problem.
-----------> It is also sometimes referred to as the optimal matching algorithm and the global alignment technique. 
-----------> The Needleman–Wunsch algorithm is still widely used for optimal global alignment, particularly when the quality of the global alignment is of the utmost importance. 
-----------> The algorithm assigns a score to every possible alignment, and the purpose of the algorithm is to find all possible alignments having the highest score. 

-------> Smith–Waterman algorithm: 
---------> This find local sequence alignment.
-----------> The Smith–Waterman algorithm performs local sequence alignment; that is, for determining similar regions between two strings of nucleic acid sequences or protein sequences. 
-----------> Instead of looking at the entire sequence, the Smith–Waterman algorithm compares segments of all possible lengths and optimizes the similarity measure.
---------> The algorithm was first proposed by Temple F. Smith and Michael S. Waterman in 1981. 
-----------> Like the Needleman–Wunsch algorithm, of which it is a variation, Smith–Waterman is a dynamic programming algorithm. 
-----------> As such, it has the desirable property that it is guaranteed to find the optimal local alignment with respect to the scoring system being used
-----------> (which includes the substitution matrix and the gap-scoring scheme). 
-----------> The main difference to the Needleman–Wunsch algorithm is that negative scoring matrix cells are set to zero, 
-----------> which renders the (thus positively scoring) local alignments visible. 
-----------> Traceback procedure starts at the highest scoring matrix cell and proceeds until a cell with score zero is encountered, 
-----------> yielding the highest scoring local alignment. Because of its quadratic complexity in time and space, 
-----------> it often cannot be practically applied to large-scale problems and is replaced in favor of less general 
-----------> but computationally more efficient alternatives such as (Gotoh, 1982), (Altschul and Erickson, 1986), and (Myers and Miller, 1988).
---------> Algorithm
-----------> Scoring method of the Smith–Waterman algorithm
-----------> Let A = a1a2...an and B = b1b2...bm be the sequences to be aligned, where n and m are the lengths of A and B respectively.
-------------> (1) Determine the substitution matrix and the gap penalty scheme.
---------------> s(a,b) - Similarity score of the elements that constituted the two sequences
---------------> Wk - The penalty of a gap that has length k
-------------> (2) Construct a scoring matrix H and initialize its first row and first column. 
---------------> The size of the scoring matrix is (n+1)*(m+1). The matrix uses 0-based indexing.
-----------------> Hk0 = H0l = 0 for 0 ≤ k ≤ n and 0 ≤ l ≤ m 
-------------> (3) Fill the scoring matrix using the equation below.
---------------> Hij = max {Hi−1,j−1 + s(ai,bj) , max k ≥ 1 {Hi−k, j−Wk} , max l ≥ 1 {Hi, j−l − Wl} , 0 (1≤i≤n, 1≤j≤m) 
-----------------> where
-----------------> Hi−1,j − 1 + s(ai, bj) is the score of aligning a i,
-----------------> Hi−k,j − Wk is the score if ai is at the end of a gap of length k,
-----------------> Hi,j−l − Wl is the score if bj is at the end of a gap of length l,
-----------------> 0 means there is no similarity up to ai and bj .
-------------> (4) Traceback. Starting at the highest score in the scoring matrix H and ending at a matrix cell that has a score of 0, 
---------------> traceback based on the source of each score recursively to generate the best local alignment.



-----> Sequence sorting

-------> A sorting algorithm is an algorithm that puts elements of a list into an order. 
---------> The most frequently used orders are numerical order and lexicographical order, and either ascending or descending. 
---------> Efficient sorting is important for optimizing the efficiency of other algorithms (such as search and merge algorithms) that require input data to be in sorted lists. 
---------> Sorting is also often useful for canonicalizing data and for producing human-readable output.
-------> Formally, the output of any sorting algorithm must satisfy two conditions:
---------> The output is in monotonic order (each element is no smaller/larger than the previous element, according to the required order).
---------> The output is a permutation (a reordering, yet retaining all of the original elements) of the input.
-------> For optimum efficiency, the input data should be stored in a data structure which allows random access rather than one that allows only sequential access. 
 
-------> Exchange sorts

---------> Bubble sort: 
-----------> This for each pair of indices, swap the items if out of order
-----------> Bubble sort, sometimes referred to as sinking sort, is a simple sorting algorithm that repeatedly steps through the input list element by element, 
-------------> comparing the current element with the one after it, swapping their values if needed. 
-------------> These passes through the list are repeated until no swaps had to be performed during a pass, meaning that the list has become fully sorted. 
-------------> The algorithm, which is a comparison sort, is named for the way the larger elements "bubble" up to the top of the list.
-----------> This simple algorithm performs poorly in real world use and is used primarily as an educational tool. 
-------------> More efficient algorithms such as quicksort, timsort, or merge sort are used by the sorting libraries built into popular programming languages such as Python and Java.
-------------> Analysis
-------------> An example of bubble sort. Starting from the beginning of the list, compare every adjacent pair, 
-------------> swap their position if they are not in the right order (the latter one is smaller than the former one). 
-------------> After each iteration, one less element (the last one) is needed to be compared until there are no more elements left to be compared.
-----------> Performance
-------------> Bubble sort has a worst-case and average complexity of O(n^2), 
---------------> where n is the number of items being sorted. 
---------------> Most practical sorting algorithms have substantially better worst-case or average complexity, often O(n log ⁡ n) . 
---------------> Even other O(n^2) sorting algorithms, such as insertion sort, generally run faster than bubble sort, and are no more complex. 
---------------> For this reason, bubble sort is rarely used in practice.
-------------> Like insertion sort, bubble sort is adaptive, giving it an advantage over algorithms like quicksort. 
---------------> This means that it may outperform those algorithms in cases where the list is already mostly sorted (having a small number of inversions), 
---------------> despite the fact that it has worse average-case time complexity. 
---------------> For example, bubble sort is  O(n) on a list that is already sorted, while quicksort would still perform its entire O(n log n) sorting process.
-------------> While any sorting algorithm can be made O(n) on a presorted list simply by checking the list before the algorithm runs, improved performance on almost-sorted lists is harder to replicate. 


---------> Cocktail shaker sort or bidirectional bubble sort, 
-----------> A bubble sort traversing the list alternately from front to back and back to front
-------------> Cocktail shaker sort, also known as bidirectional bubble sort, cocktail sort, shaker sort 
-------------> (which can also refer to a variant of selection sort), ripple sort, shuffle sort, or shuttle sort, is an extension of bubble sort. 
-------------> The algorithm extends bubble sort by operating in two directions. 
-------------> While it improves on bubble sort by more quickly moving items to the beginning of the list, it provides only marginal performance improvements.
-----------> Like most variants of bubble sort, cocktail shaker sort is used primarily as an educational tool. 
-------------> More performant algorithms such as timsort, or merge sort are used by the sorting libraries built into popular programming languages such as Python and Java.
-----------> Pseudocode
-------------> The simplest form goes through the whole list each time:
---------------> procedure cocktailShakerSort(A : list of sortable items) is
--------------->     do
--------------->         swapped := false
--------------->         for each i in 0 to length(A) − 2 do:
--------------->             if A[i] > A[i + 1] then // test whether the two elements are in the wrong order
--------------->                 swap(A[i], A[i + 1]) // let the two elements change places
--------------->                 swapped := true
--------------->             end if
--------------->         end for
--------------->         if not swapped then
--------------->             // we can exit the outer loop here if no swaps occurred.
--------------->             break do-while loop
--------------->         end if
--------------->         swapped := false
--------------->         for each i in length(A) − 2 to 0 do:
--------------->             if A[i] > A[i + 1] then
--------------->                 swap(A[i], A[i + 1])
--------------->                 swapped := true
--------------->             end if
--------------->         end for
--------------->     while swapped // if no elements have been swapped, then the list is sorted
---------------> end procedure
------------> The first rightward pass will shift the largest element to its correct place at the end, 
--------------> and the following leftward pass will shift the smallest element to its correct place at the beginning. 
--------------> The second complete pass will shift the second largest and second smallest elements to their correct places, and so on. 
--------------> After i passes, the first i and the last i elements in the list are in their correct positions, and do not need to be checked. 
--------------> By shortening the part of the list that is sorted each time, the number of operations can be halved (see bubble sort).
------------> This is an example of the algorithm in MATLAB/OCTAVE with the optimization of remembering the last swap index and updating the bounds.
--------------> function A = cocktailShakerSort(A)
--------------> % `beginIdx` and `endIdx` marks the first and last index to check
--------------> beginIdx = 1;
--------------> endIdx = length(A) - 1;
--------------> while beginIdx <= endIdx
-------------->     newBeginIdx = endIdx;
-------------->     newEndIdx = beginIdx;
-------------->     for ii = beginIdx:endIdx
-------------->         if A(ii) > A(ii + 1)
-------------->             [A(ii+1), A(ii)] = deal(A(ii), A(ii+1));
-------------->             newEndIdx = ii;
-------------->         end
-------------->     end
-------------->     % decreases `endIdx` because the elements after `newEndIdx` are in correct order
-------------->     endIdx = newEndIdx - 1;
--------------> 
-------------->     for ii = endIdx:-1:beginIdx
-------------->         if A(ii) > A(ii + 1)
-------------->             [A(ii+1), A(ii)] = deal(A(ii), A(ii+1));
-------------->             newBeginIdx = ii;
-------------->         end
-------------->     end
-------------->     % increases `beginIdx` because the elements before `newBeginIdx` are in correct order
-------------->     beginIdx = newBeginIdx + 1;
--------------> end
--------------> end

---------> Comb sort
-----------> Comb sort improves on bubble sort in the same way that Shellsort improves on insertion sort.
-----------> Algorithm
-------------> The basic idea is to eliminate turtles, or small values near the end of the list, since in a bubble sort these slow the sorting down tremendously. 
---------------> Rabbits, large values around the beginning of the list, do not pose a problem in bubble sort.
-------------> In bubble sort, when any two elements are compared, they always have a gap (distance from each other) of 1. 
---------------> The basic idea of comb sort is that the gap can be much more than 1. 
---------------> The inner loop of bubble sort, which does the actual swap, is modified such that the gap between swapped elements goes down 
---------------> (for each iteration of outer loop) in steps of a "shrink factor" k: [ n/k, n/k2, n/k3, ..., 1 ].
-------------> The gap starts out as the length of the list n being sorted divided by the shrink factor k (generally 1.3; see below) 
---------------> and one pass of the aforementioned modified bubble sort is applied with that gap. Then the gap is divided by the shrink factor again, 
---------------> the list is sorted with this new gap, and the process repeats until the gap is 1. At this point, comb sort continues using a gap of 1 until the list is fully sorted. 
---------------> The final stage of the sort is thus equivalent to a bubble sort, but by this time most turtles have been dealt with, so a bubble sort will be efficient.
-------------> The shrink factor has a great effect on the efficiency of comb sort. 
---------------> k = 1.3 has been suggested as an ideal shrink factor by the authors of the original article after empirical testing on over 200,000 random lists. 
---------------> A value too small slows the algorithm down by making unnecessarily many comparisons, whereas a value too large fails to effectively deal with turtles, making it require many passes with 1 gap size.
-------------> The pattern of repeated sorting passes with decreasing gaps is similar to Shellsort, 
---------------> but in Shellsort the array is sorted completely each pass before going on to the next-smallest gap. Comb sort's passes do not completely sort the elements. 
---------------> This is the reason that Shellsort gap sequences have a larger optimal shrink factor of about 2.2.
---------------> Pseudocode
-----------------> function combsort(array input) is
----------------->     gap := input.size // Initialize gap size
----------------->     shrink := 1.3 // Set the gap shrink factor
----------------->     sorted := false
----------------->     loop while sorted = false
----------------->         // Update the gap value for a next comb
----------------->         gap := floor(gap / shrink)
----------------->         if gap ≤ 1 then
----------------->             gap := 1
----------------->             sorted := true // If there are no swaps this pass, we are done
----------------->         end if
----------------->         // A single "comb" over the input list
----------------->         i := 0
----------------->         loop while i + gap < input.size // See Shell sort for a similar idea
----------------->             if input[i] > input[i+gap] then
----------------->                 swap(input[i], input[i+gap])
----------------->                 sorted := false
----------------->                 // If this assignment never happens within the loop,
----------------->                 // then there have been no swaps and the list is sorted.
----------------->              end if
----------------->              i := i + 1
----------------->          end loop
----------------->      end loop
-----------------> end function
-------------> Note: Comb sort is originally designed by Włodzimierz Dobosiewicz and Artur Borowy in 1980, later rediscovered (and given the name "Combsort") by Stephen Lacey and Richard Box in 1991.

---------> Gnome sort
-----------> Gnome sort (nicknamed stupid sort) is a variation of the insertion sort sorting algorithm that does not use nested loops. 
-------------> The sort was first called stupid sort[2] (not to be confused with bogosort), and then later described by Dick Grune and named gnome sort.
-----------> Gnome sort performs at least as many comparisons as insertion sort and has the same asymptotic runtime characteristics. 
-------------> Gnome sort works by building a sorted list one element at a time, getting each item to the proper place in a series of swaps. 
-------------> The average running time is O(n2) but tends towards O(n) if the list is initially almost sorted.[4][note 1]
-----------> Dick Grune described the sorting method with the following story:
-------------> Gnome Sort is based on the technique used by the standard Dutch Garden Gnome (Du.: tuinkabouter).
-------------> Here is how a garden gnome sorts a line of flower pots.
-------------> Basically, he looks at the flower pot next to him and the previous one; if they are in the right order he steps one pot forward, otherwise, he swaps them and steps one pot backward.
-------------> Boundary conditions: if there is no previous pot, he steps forwards; if there is no pot next to him, he is done.
-------------> "Gnome Sort - The Simplest Sort Algorithm". Dickgrune.com
-----------> Pseudocode
-------------> Here is pseudocode for the gnome sort using a zero-based array:
---------------> procedure gnomeSort(a[]):
--------------->     pos := 0
--------------->     while pos < length(a):
--------------->         if (pos == 0 or a[pos] >= a[pos-1]):
--------------->             pos := pos + 1
--------------->         else:
--------------->             swap a[pos] and a[pos-1]
--------------->             pos := pos - 1
-----------> Gnome sort was originally proposed by Iranian computer scientist Hamid Sarbazi-Azad (professor of Computer Science and Engineering at Sharif University of Technology) in 2000. 

---------> Odd–even sort
-----------> In computing, an odd–even sort or odd–even transposition sort (also known as brick sort[1][self-published source] or parity sort) 
-------------> is a relatively simple sorting algorithm, developed originally for use on parallel processors with local interconnections. 
-------------> It is a comparison sort related to bubble sort, with which it shares many characteristics. 
-------------> It functions by comparing all odd/even indexed pairs of adjacent elements in the list and, 
-------------> if a pair is in the wrong order (the first is larger than the second) the elements are switched. 
-------------> The next step repeats this for even/odd indexed pairs (of adjacent elements). 
-------------> Then it alternates between odd/even and even/odd steps until the list is sorted. 
-----------> Sorting on processor arrays
-------------> On parallel processors, with one value per processor and only local left–right neighbor connections, 
---------------> the processors all concurrently do a compare–exchange operation with their neighbors, alternating between odd–even and even–odd pairings. 
---------------> This algorithm was originally presented, and shown to be efficient on such processors, by Habermann in 1972.
-------------> The algorithm extends efficiently to the case of multiple items per processor. 
---------------> In the Baudet–Stevenson odd–even merge-splitting algorithm, each processor sorts its own sublist at each step,
---------------> using any efficient sort algorithm, and then performs a merge splitting, or transposition–merge, 
---------------> operation with its neighbor, with neighbor pairing alternating between odd–even and even–odd on each step.[3] 
-----------> Algorithm
-------------> The single-processor algorithm, like bubblesort, is simple but not very efficient. Here a zero-based index is assumed:
---------------> function oddEvenSort(list) {
--------------->   function swap(list, i, j) {
--------------->     var temp = list[i];
--------------->     list[i] = list[j];
--------------->     list[j] = temp;
--------------->   }
--------------->   var sorted = false;
--------------->   while (!sorted) {
--------------->     sorted = true;
--------------->     for (var i = 1; i < list.length - 1; i += 2) {
--------------->       if (list[i] > list[i + 1]) {
--------------->         swap(list, i, i + 1);
--------------->         sorted = false;
--------------->       }
--------------->     }
--------------->     for (var i = 0; i < list.length - 1; i += 2) {
--------------->       if (list[i] > list[i + 1]) {
--------------->         swap(list, i, i + 1);
--------------->         sorted = false;
--------------->       }
--------------->     }
--------------->   }
---------------> }
-----------> Batcher's odd–even mergesort
-------------> A related but more efficient sort algorithm is the Batcher odd–even mergesort, using compare–exchange operations and perfect-shuffle operations.
-------------> Batcher's method is efficient on parallel processors with long-range connections.

---------> Quicksort: 
-----------> This divide list into two, with all items on the first list coming before all items on the second list.; then sort the two lists.
-------------> When implemented well, it can be somewhat faster than merge sort and about two or three times faster than heapsort.
-----------> Quicksort is a divide-and-conquer algorithm. 
-------------> It works by selecting a 'pivot' element from the array and partitioning the other elements into two sub-arrays, according to whether they are less than or greater than the pivot. 
-------------> For this reason, it is sometimes called partition-exchange sort.[4] The sub-arrays are then sorted recursively. 
-------------> This can be done in-place, requiring small additional amounts of memory to perform the sorting.
-----------> Quicksort is a comparison sort, meaning that it can sort items of any type for which a "less-than" relation (formally, a total order) is defined. 
-------------> Efficient implementations of Quicksort are not a stable sort, meaning that the relative order of equal sort items is not preserved.
-----------> Mathematical analysis of quicksort shows that, on average, the algorithm takes O(n log n) comparisons to sort n items. 
-------------> In the worst case, it makes O(n^2) comparisons.
-----------> Algorithm
-------------> Quicksort is a type of divide and conquer algorithm for sorting an array, 
-------------> based on a partitioning routine; the details of this partitioning can vary somewhat, 
-------------> so that quicksort is really a family of closely related algorithms. 
-------------> Applied to a range of at least two elements, partitioning produces a division into two consecutive non empty sub-ranges, 
-------------> in such a way that no element of the first sub-range is greater than any element of the second sub-range. 
-------------> After applying this partition, quicksort then recursively sorts the sub-ranges, 
-------------> possibly after excluding from them an element at the point of division that is at this point known to be already in its final location. 
-------------> Due to its recursive nature, quicksort (like the partition routine) has to be formulated so as to be callable for a range within a larger array, 
-------------> even if the ultimate goal is to sort a complete array. 
-------------> The steps for in-place quicksort are:
---------------> (1) If the range has fewer than two elements, return immediately as there is nothing to do. 
-----------------> Possibly for other very short lengths a special-purpose sorting method is applied and the remainder of these steps skipped.
---------------> (2) Otherwise pick a value, called a pivot, that occurs in the range (the precise manner of choosing depends on the partition routine, and can involve randomness).
---------------> (3) Partition the range: reorder its elements, while determining a point of division, 
-----------------> so that all elements with values less than the pivot come before the division, 
-----------------> while all elements with values greater than the pivot come after it; elements that are equal to the pivot can go either way. 
-----------------> Since at least one instance of the pivot is present, 
-----------------> most partition routines ensure that the value that ends up at the point of division is equal to the pivot, 
-----------------> and is now in its final position (but termination of quicksort does not depend on this, as long as sub-ranges strictly smaller than the original are produced).
---------------> (4) Recursively apply the quicksort to the sub-range up to the point of division and to the sub-range after it, 
-----------------> possibly excluding from both ranges the element equal to the pivot at the point of division. 
-----------------> (If the partition produces a possibly larger sub-range near the boundary where all elements are known to be equal to the pivot, these can be excluded as well.)
-------------> The choice of partition routine (including the pivot selection) 
-------------> and other details not entirely specified above can affect the algorithm's performance, 
-------------> possibly to a great extent for specific input arrays. 
-----------> Note: Quicksort was developed by British computer scientist Tony Hoare in 1959 and published in 1961, it is still a commonly used algorithm for sorting. 

-------> Humorous or ineffective

---------> Bogosort
-----------> Bogosort (also known as permutation sort, stupid sort, or slowsort) is a sorting algorithm based on the generate and test paradigm. 
-------------> The function successively generates permutations of its input until it finds one that is sorted. 
-------------> It is not considered useful for sorting, but may be used for educational purposes, to contrast it with more efficient algorithms.
-----------> Two versions of this algorithm exist: a deterministic version that enumerates all permutations until it hits a sorted one,
-------------> and a randomized version that randomly permutes its input. 
-------------> An analogy for the working of the latter version is to sort a deck of cards by throwing the deck into the air, 
-------------> picking the cards up at random, and repeating the process until the deck is sorted.
-------------> Its name is a portmanteau of the words bogus and sort.
-----------> Algorithm
-------------> The following is a description of the randomized algorithm in pseudocode:
---------------> while not isInOrder(deck):
--------------->     shuffle(deck)
-------------> Here is the above pseudocode rewritten in Python 3:
---------------> from random import shuffle
---------------> def is_sorted(data) -> bool:
--------------->     """Determine whether the data is sorted."""
--------------->     return all(a <= b for a, b in zip(data, data[1:]))
---------------> def bogosort(data) -> list:
--------------->     """Shuffle data until sorted."""
--------------->     while not is_sorted(data):
--------------->         shuffle(data)
--------------->     return data
-------------> This code assumes that data is a simple, mutable datatype—like Python's built-in list—whose elements can be compared without issue. 

---------> Stooge sort
-----------> Stooge sort is a recursive sorting algorithm. 
-------------> It is notable for its exceptionally bad time complexity of O(nlog 3 / log 1.5 ) = O(n2.7095...). 
-------------> The running time of the algorithm is thus slower compared to reasonable sorting algorithms, 
-------------> and is slower than bubble sort, a canonical example of a fairly inefficient sort. 
-------------> It is however more efficient than Slowsort. The name comes from The Three Stooges.[1]
-----------> The algorithm is defined as follows:
-------------> If the value at the start is larger than the value at the end, swap them.
-------------> If there are 3 or more elements in the list, then:
---------------> Stooge sort the initial 2/3 of the list
---------------> Stooge sort the final 2/3 of the list
---------------> Stooge sort the initial 2/3 of the list again
-----------> It is important to get the integer sort size used in the recursive calls by rounding the 2/3 upwards, 
-------------> e.g. rounding 2/3 of 5 should give 4 rather than 3, as otherwise the sort can fail on certain data. 
-----------> Algorithm implementation
------------->  function stoogesort(array L, i = 0, j = length(L)-1){
------------->      if L[i] > L[j] then       // If the leftmost element is larger than the rightmost element
------------->          L[i] ↔ L[j]           // Swap the leftmost element and the rightmost element
------------->      if (j - i + 1) > 2 then       // If there are at least 3 elements in the array
------------->          t = floor((j - i + 1) / 3) // Rounding down
------------->          stoogesort(L, i  , j-t)  // Sort the first 2/3 of the array
------------->          stoogesort(L, i+t, j)    // Sort the last 2/3 of the array
------------->          stoogesort(L, i  , j-t)  // Sort the first 2/3 of the array again
------------->      return L
------------->  }
-------------> -- Not the best but equal to above 
-------------> stoogesort :: (Ord a) => [a] -> [a]
-------------> stoogesort [] = []
-------------> stoogesort src = innerStoogesort src 0 ((length src) - 1)
-------------> innerStoogesort :: (Ord a) => [a] -> Int -> Int -> [a]
-------------> innerStoogesort src i j 
------------->     | (j - i + 1) > 2 = src''''
------------->     | otherwise = src'
------------->     where 
------------->         src'    = swap src i j -- need every call
------------->         t = floor (fromIntegral (j - i + 1) / 3.0)
------------->         src''   = innerStoogesort src'   i      (j - t)
------------->         src'''  = innerStoogesort src'' (i + t)  j
------------->         src'''' = innerStoogesort src''' i      (j - t)
-------------> swap :: (Ord a) => [a] -> Int -> Int -> [a]
-------------> swap src i j 
------------->     | a > b     =  replaceAt (replaceAt src j a) i b
------------->     | otherwise = src
------------->     where 
------------->         a = src !! i
------------->         b = src !! j
-------------> 
-------------> replaceAt :: [a] -> Int -> a -> [a]
-------------> replaceAt (x:xs) index value
------------->     | index == 0 = value : xs
------------->     | otherwise  =  x : replaceAt xs (index - 1) value

-------> Hybrid

---------> Flashsort:
-----------> Flashsort is a distribution sorting algorithm showing linear computational complexity O(n) 
-------------> for uniformly distributed data sets and relatively little additional memory requirement. 
-----------> Concept
-------------> Flashsort is an efficient in-place implementation of histogram sort, itself a type of bucket sort. 
-------------> It assigns each of the n input elements to one of m buckets,
-------------> efficiently rearranges the input to place the buckets in the correct order, then sorts each bucket. 
-------------> The original algorithm sorts an input array A as follows:
---------------> (1) Using a first pass over the input or a priori knowledge, find the minimum and maximum sort keys.
---------------> (2) Linearly divide the range [Amin, Amax] into m buckets.
---------------> (3) Make one pass over the input, counting the number of elements Ai which fall into each bucket. 
---------------> (Neubert calls the buckets "classes" and the assignment of elements to their buckets "classification".)
---------------> (4) Convert the counts of elements in each bucket to a prefix sum, where Lb is the number of elements Ai in bucket b or less. (L0 = 0 and Lm = n.)
---------------> (5) Rearrange the input to all elements of each bucket b are stored in positions Ai where Lb−1 < i ≤ Lb.
---------------> (6) Sort each bucket using insertion sort.
-------------> Steps 1–3 and 6 are common to any bucket sort, and can be improved using techniques generic to bucket sorts. 
---------------> In particular, the goal is for the buckets to be of approximately equal size (n/m elements each),
---------------> with the ideal being division into m quantiles. 
---------------> While the basic algorithm is a linear interpolation sort, if the input distribution is known to be non-uniform, 
---------------> a non-linear division will more closely approximate this ideal. 
---------------> Likewise, the final sort can use any of a number of techniques, including a recursive flash sort.
-------------> What distinguishes flash sort is step 5: an efficient O(n) in-place algorithm for collecting the elements of each bucket together
---------------> in the correct relative order using only m words of additional memory. 
-----------> Note: The original work was published in 1998 by Karl-Dietrich Neubert

---------> Introsort: 
-----------> This begin with quicksort and switch to heapsort when the recursion depth exceeds a certain level
-----------> Introsort or introspective sort is a hybrid sorting algorithm that provides both fast average performance and (asymptotically) optimal worst-case performance. 
-------------> It begins with quicksort, it switches to heapsort when the recursion depth exceeds a level based on (the logarithm of) 
-------------> the number of elements being sorted and it switches to insertion sort when the number of elements is below some threshold. 
-------------> This combines the good parts of the three algorithms, with practical performance comparable to quicksort on typical data sets 
-------------> and worst-case O(n log n) runtime due to the heap sort. 
-------------> Since the three algorithms it uses are comparison sorts, it is also a comparison sort.
-----------> This uses introselect, a hybrid selection algorithm based on quickselect (a variant of quicksort), 
-------------> which falls back to median of medians and thus provides worst-case linear complexity, which is optimal. 
-------------> Both algorithms were introduced with the purpose of providing generic algorithms for the C++ Standard Library 
-------------> which had both fast average performance and optimal worst-case performance,
-------------> thus allowing the performance requirements to be tightened.
-----------> Introsort is in place and not stable.
-----------> Algorithm
-------------> If a heapsort implementation and partitioning functions of the type discussed in the quicksort article are available, the introsort can be described succinctly as
---------------> procedure sort(A : array):
--------------->     let maxdepth = ⌊log2(length(A))⌋ × 2
--------------->     introsort(A, maxdepth)
---------------> procedure introsort(A, maxdepth):
--------------->     n ← length(A)
--------------->     if n < 16:
--------------->         insertionsort(A)
--------------->     else if maxdepth = 0:
--------------->         heapsort(A)
--------------->     else:
--------------->         p ← partition(A)  // assume this function does pivot selection, p is the final position of the pivot
--------------->         introsort(A[1:p-1], maxdepth - 1)
--------------->         introsort(A[p+1:n], maxdepth - 1)
-------------> The factor 2 in the maximum depth is arbitrary; it can be tuned for practical performance. 
---------------> A[i:j] denotes the array slice of items i to j including both A[i] and A[j].
-----------> Analysis
-------------> In quicksort, one of the critical operations is choosing the pivot: the element around which the list is partitioned. 
---------------> The simplest pivot selection algorithm is to take the first or the last element of the list as the pivot, 
---------------> causing poor behavior for the case of sorted or nearly sorted input. 
---------------> Niklaus Wirth's variant uses the middle element to prevent these occurrences, degenerating to O(n2) for contrived sequences. 
---------------> The median-of-3 pivot selection algorithm takes the median of the first, middle, and last elements of the list; 
---------------> however, even though this performs well on many real-world inputs, it is still possible to contrive a median-of-3 killer list
---------------> that will cause dramatic slowdown of a quicksort based on this pivot selection technique.
-------------> Musser reported that on a median-of-3 killer sequence of 100,000 elements, 
---------------> introsort's running time was 1/200 that of median-of-3 quicksort. 
---------------> Musser also considered the effect on caches of Sedgewick's delayed small sorting, 
---------------> where small ranges are sorted at the end in a single pass of insertion sort. 
---------------> He reported that it could double the number of cache misses, but that its performance with 
---------------> double-ended queues was significantly better and should be retained for template libraries, 
---------------> in part because the gain in other cases from doing the sorts immediately was not great. 
-----------> Note: Introsort was invented by David Musser in Musser (1997).

---------> Timsort: 
-----------> This adaptative algorithm derived from merge sort and insertion sort. 
-------------> Timsort is a hybrid, stable sorting algorithm, derived from merge sort and insertion sort, designed to perform well on many kinds of real-world data. 
---------------> The algorithm finds subsequences of the data that are already ordered (runs) and uses them to sort the remainder more efficiently. 
---------------> This is done by merging runs until certain criteria are fulfilled. 
---------------> Timsort has been Python's standard sorting algorithm since version 2.3. 
---------------> It is also used to sort arrays of non-primitive type in Java SE 7, on the Android platform, in GNU Octave, on V8, Swift, and Rust.
-------------> Operation
---------------> Timsort was designed to take advantage of runs of consecutive ordered elements that already exist in most real-world data, natural runs. 
-----------------> It iterates over the data collecting elements into runs and simultaneously putting those runs in a stack. 
-----------------> Whenever the runs on the top of the stack match a merge criterion, they are merged. 
-----------------> This goes on until all data is traversed; then, all runs are merged two at a time and only one sorted run remains. 
-----------------> The advantage of merging ordered runs instead of merging fixed size sub-lists (as done by traditional mergesort) 
-----------------> is that it decreases the total number of comparisons needed to sort the entire list.
---------------> Each run has a minimum size, which is based on the size of the input and it is defined at the start of the algorithm.
-----------------> If a run is smaller than this minimum run size, insertion sort is used to add more elements to the run until the minimum run size is reached. 
---------------> Note: This was implemented by Tim Peters in 2002 for use in the Python programming language. 
-----------------> This is used in Python 2.3 and up, and Java SE 7.
-----------------> This uses techniques from Peter McIlroy's 1993 paper "Optimistic Sorting and Information Theoretic Complexity

-------> Insertion sorts

---------> Insertion sort: 
-----------> This determine where the current item belongs in the list of sorted ones, and insert it there.
-----------> Insertion sort is a simple sorting algorithm that builds the final sorted array (or list) one item at a time. 
-------------> It is much less efficient on large lists than more advanced algorithms such as quicksort, heapsort, or merge sort. However, insertion sort provides several advantages:
---------------> Simple implementation: Jon Bentley shows a three-line C++ version, and a five-line optimized version[1]
---------------> Efficient for (quite) small data sets, much like other quadratic sorting algorithms
---------------> More efficient in practice than most other simple quadratic (i.e., O(n2)) algorithms such as selection sort or bubble sort
---------------> Adaptive, i.e., efficient for data sets that are already substantially sorted: the time complexity is O(kn) when each element in the input is no more than k places away from its sorted position
---------------> Stable; i.e., does not change the relative order of elements with equal keys
---------------> In-place; i.e., only requires a constant amount O(1) of additional memory space
---------------> Online; i.e., can sort a list as it receives it
-----------> When people manually sort cards in a bridge hand, most use a method that is similar to insertion sort.
-----------> Algorithm:
-------------> The most common variant of insertion sort, which operates on arrays, can be described as follows:
---------------> (1) Suppose there exists a function called Insert designed to insert a value into a sorted sequence at the beginning of an array. 
-----------------> It operates by beginning at the end of the sequence and shifting each element one place to the right until a suitable position is found for the new element. 
-----------------> The function has the side effect of overwriting the value stored immediately after the sorted sequence in the array.
---------------> (2) To perform an insertion sort, begin at the left-most element of the array and invoke Insert to insert each element encountered into its correct position. 
-----------------> The ordered sequence into which the element is inserted is stored at the beginning of the array in the set of indices already examined. Each insertion overwrites a single value: the value being inserted.
-----------> Pseudocode
-------------> i ← 1
-------------> while i < length(A)
------------->     x ← A[i]
------------->     j ← i - 1
------------->     while j >= 0 and A[j] > x
------------->         A[j+1] ← A[j]
------------->         j ← j - 1
------------->     end while
------------->     A[j+1] ← x[3]
------------->     i ← i + 1
-------------> end while


---------> Library sort
-----------> Library sort, or gapped insertion sort is a sorting algorithm that uses an insertion sort, 
-------------> but with gaps in the array to accelerate subsequent insertions. 
-------------> The name comes from an analogy:
-----------> Suppose a librarian were to store their books alphabetically on a long shelf, starting with the As at the left end, 
-------------> and continuing to the right along the shelf with no spaces between the books until the end of the Zs. 
-------------> If the librarian acquired a new book that belongs to the B section, once they find the correct space in the B section, 
-------------> they will have to move every book over, from the middle of the Bs all the way down to the Zs in order to make room for the new book. 
-------------> This is an insertion sort. 
-------------> However, if they were to leave a space after every letter, as long as there was still space after B, 
-------------> they would only have to move a few books to make room for the new one. 
-------------> This is the basic principle of the Library Sort.
-----------> Like the insertion sort it is based on, library sort is a comparison sort; 
-------------> however, it was shown to have a high probability of running in O(n log n) time (comparable to quicksort), rather than an insertion sort's O(n2). 
-------------> There is no full implementation given in the paper, nor the exact algorithms of important parts, such as insertion and rebalancing. 
-------------> Further information would be needed to discuss how the efficiency of library sort compares to that of other sorting methods in reality.
-----------> Compared to basic insertion sort, the drawback of library sort is that it requires extra space for the gaps. 
-------------> The amount and distribution of that space would be implementation dependent. 
-------------> In the paper the size of the needed array is (1 + ε)n,[2] but with no further recommendations on how to choose ε. 
-------------> Moreover, it is neither adaptive nor stable. 
-------------> In order to warrant the with-high-probability time bounds, it requires to randomly permute the input, 
-------------> what changes the relative order of equal elements and shuffles any presorted input. 
-------------> Also, the algorithm uses binary search to find the insertion point for each element, which does not take profit of presorted input.
-----------> Another drawback is that it cannot be run as an online algorithm, because it is not possible to randomly shuffle the input. 
-------------> If used without this shuffling, it could easily degenerate into quadratic behaviour.
-----------> One weakness of insertion sort is that it may require a high number of swap operations and be costly if memory write is expensive. 
-------------> Library sort may improve that somewhat in the insertion step, as fewer elements need to move to make room, 
-------------> but is also adding an extra cost in the rebalancing step.
-------------> In addition, locality of reference will be poor compared to mergesort 
-------------> as each insertion from a random data set may access memory that is no longer in cache, especially with large data sets. 
-----------> Algorithm
-------------> Let us say we have an array of n elements. We choose the gap we intend to give. 
---------------> Then we would have a final array of size (1 + ε)n. The algorithm works in log n rounds. 
---------------> In each round we insert as many elements as there are in the final array already, before re-balancing the array. 
---------------> For finding the position of inserting, we apply Binary Search in the final array and then swap the following elements till we hit an empty space. 
---------------> Once the round is over, we re-balance the final array by inserting spaces between each element.
---------------> Following are three important steps of the algorithm:
----------------->  (1) Binary Search: Finding the position of insertion by applying binary search within the already inserted elements. 
------------------->  This can be done by linearly moving towards left or right side of the array if you hit an empty space in the middle element.
----------------->  (2) Insertion: Inserting the element in the position found and swapping the following elements by 1 position till an empty space is hit. 
------------------->  This is done in logarithmic time, with high probability.
----------------->  (3) Re-Balancing: Inserting spaces between each pair of elements in the array. 
------------------->  The cost of rebalancing is linear in the number of elements already inserted. 
------------------->  As these lengths increase with the powers of 2 for each round, the total cost of rebalancing is also linear.
-----------> Pseudocode
-------------> procedure rebalance(A, begin, end) is
------------->     r ← end
------------->     w ← end ÷ 2
------------->     while r ≥ begin do
------------->         A[w+1] ← gap
------------->         A[w] ← A[r]
------------->         r ← r − 1
------------->         w ← w − 2
-------------> procedure sort(A) is
------------->     n ← length(A)
------------->     S ← new array of n gaps
------------->     for i ← 1 to floor(log2(n) + 1) do
------------->         for j ← 2^i to 2^(i + 1) do
------------->             ins ← binarysearch(A[j], S, 2^(i − 1))
------------->             insert A[j] at S[ins]
-----------> Here, binarysearch(el, A, k) performs binary search in the first k elements of A, skipping over gaps, to find a place where to locate element el. 
-------------> Insertion should favor gaps over filled-in elements.  
-----------> Note: The algorithm was proposed by Michael A. Bender, Martín Farach-Colton, and Miguel Mosteiro in 2004 and was published in 2006.


---------> Patience sorting
---------> The algorithm's name derives from a simplified variant of the patience card game.
---------> Algorithm
-----------> The game begins with a shuffled deck of cards.
-----------> The cards are dealt one by one into a sequence of piles on the table, according to the following rules.[2]
-----------> 1) Initially, there are no piles.
-------------> The first card dealt forms a new pile consisting of the single card.
-----------> 2) Each subsequent card is placed on the leftmost existing pile whose top card has a value greater than or equal to
-----------> the new card's value,
-------------> or to the right of all of the existing piles, thus forming a new pile.
-----------> 3) When there are no more cards remaining to deal, the game ends.
-----------> This card game is turned into a two-phase sorting algorithm, as follows.
-------------> Given an array of n elements from some totally ordered domain, consider this array as a collection of cards and
-------------> simulate the patience sorting game. When the game is over, recover the sorted sequence by repeatedly picking off the
-------------> minimum visible card; in other words, perform a k-way merge of the p piles, each of which is internally sorted.
---------> Analysis
-----------> The first phase of patience sort, the card game simulation,
-------------> can be implemented to take O(n log n) comparisons in the worst case for an n-element input array:
-------------> there will be at most n piles, and by construction, the top cards of the piles form an increasing sequence from left
-------------> to right, so the desired pile can be found by binary search. The second phase, the merging of piles, can be done in
-------------> O(n log n) time as well using a priority queue.
-----------> When the input data contain natural "runs", i.e., non-decreasing subarrays, then performance can be strictly better.
-------------> In fact, when the input array is already sorted, all values form a single pile and both phases run in O(n) time.
-------------> The average-case complexity is still O(n log n):
-------------> any uniformly random sequence of values will produce an expected number of O(√n) piles,
-------------> which take O(n log √n) = O(n log n) time to produce and merge.
-----------> An evaluation of the practical performance of patience sort is given by Chandramouli and Goldstein,
-------------> who show that a naïve version is about ten to twenty times slower than a state-of-the-art quicksort on their
-------------> benchmark problem. They attribute this to the relatively small amount of research put into patience sort, and develop
-------------> several optimizations that bring its performance to within a factor two of that of quicksort.
-----------> If values of cards are in the range 1, . . . , n, there is an efficient implementation with O(n log n)
-------------> worst-case running time for putting the cards into piles, relying on a Van Emde Boas tree.

---------> Shell sort: 
-----------> This an attempt to improve insertion sort
-----------> Shellsort, also known as Shell sort or Shell's method, is an in-place comparison sort. 
-------------> It can be seen as either a generalization of sorting by exchange (bubble sort) or sorting by insertion (insertion sort). 
-------------> The method starts by sorting pairs of elements far apart from each other, then progressively reducing the gap between elements to be compared. 
-------------> By starting with far apart elements, it can move some out-of-place elements into position faster than a simple nearest neighbor exchange. 
-------------> The running time of Shellsort is heavily dependent on the gap sequence it uses. 
-------------> For many practical variants, determining their time complexity remains an open problem. 
-----------> Shellsort is an optimization of insertion sort that allows the exchange of items that are far apart. 
-------------> The idea is to arrange the list of elements so that, starting anywhere, taking every hth element produces a sorted list. 
-------------> Such a list is said to be h-sorted. 
-------------> It can also be thought of as h interleaved lists, each individually sorted. 
-------------> Beginning with large values of h allows elements to move long distances in the original list, 
-------------> reducing large amounts of disorder quickly, and leaving less work for smaller h-sort steps to do. 
-------------> If the list is then k-sorted for some smaller integer k, then the list remains h-sorted. 
-------------> Following this idea for a decreasing sequence of h values ending in 1 is guaranteed to leave a sorted list in the end.[6]
-----------> Note: Donald Shell published the first version of this sort in 1959.

---------> Tree sort (binary tree sort): 
-----------> This build binary tree, then traverse it to create sorted list
-----------> A tree sort is a sort algorithm that builds a binary search tree from the elements to be sorted, 
-------------> and then traverses the tree (in-order) so that the elements come out in sorted order. 
-------------> Its typical use is sorting elements online: after each insertion, the set of elements seen so far is available in sorted order.
-----------> Tree sort can be used as a one-time sort, but it is equivalent to quicksort as both recursively partition the elements based on a pivot, 
-------------> and since quicksort is in-place and has lower overhead, tree sort has few advantages over quicksort. 
-------------> It has better worst case complexity when a self-balancing tree is used, but even more overhead. 

---------> Cycle sort: 
-----------> This in-place with theoretically optimal number of writes
-------------> Cycle sort is an in-place, unstable sorting algorithm.
-------------> This comparison sort is theoretically optimal in terms of the total number of writes to the original array, 
-------------> unlike any other in-place sorting algorithm. 
-------------> It is based on the idea that the permutation to be sorted can be factored into cycles, which can individually be rotated to give a sorted result.
-----------> Unlike nearly every other sort, items are never written elsewhere in the array simply to push them out of the way of the action. 
-------------> Each value is either written zero times, if it's already in its correct position, or written one time to its correct position. 
-------------> This matches the minimal number of overwrites required for a completed in-place sort.
-----------> Minimizing the number of writes is useful when making writes to some huge data set is very expensive, 
-------------> such as with EEPROMs like Flash memory where each write reduces the lifespan of the memory.
-----------> Algorithm
-------------> To illustrate the idea of cycle sort, consider a list with distinct elements. 
-------------> Given an element x, we can find the index at which it will occur in the sorted list by simply counting the number of elements in the entire list that are smaller than x. Now
-------------> (1) If the element is already at the correct position, do nothing.
-------------> (2) If it is not, we will write it to its intended position. 
---------------> That position is inhabited by a different element y, which we then have to move to its correct position. 
---------------> This process of displacing elements to their correct positions continues until an element is moved to the original position of x. 
---------------> This completes a cycle.
-----------> Implementation
-------------> To create a working implementation from the above outline, two issues need to be addressed:
---------------> (1) When computing the correct positions, we have to make sure not to double-count the first element of the cycle.
---------------> (2) If there are duplicate elements present, when we try to move an element x to its correct position, that position might already be inhabited by an x. 
-----------------> Simply swapping these would cause the algorithm to cycle indefinitely. 
-----------------> Instead, we have to insert the element after any of its duplicates.
-------------> The following Python implementation performs cycle sort on an array, counting the number of writes to that array that were needed to sort it.
---------------> def cycle_sort(array) -> int:
--------------->     """Sort an array in place and return the number of writes."""
--------------->     writes = 0
--------------->     # Loop through the array to find cycles to rotate.
--------------->     # Note that the last item will already be sorted after the first n-1 cycles.
--------------->     for cycle_start in range(0, len(array) - 1):
--------------->         item = array[cycle_start]
--------------->         # Find where to put the item.
--------------->         pos = cycle_start
--------------->         for i in range(cycle_start + 1, len(array)):
--------------->             if array[i] < item:
--------------->                 pos += 1
--------------->         # If the item is already there, this is not a cycle.
--------------->         if pos == cycle_start:
--------------->             continue
--------------->         # Otherwise, put the item there or right after any duplicates.
--------------->         while item == array[pos]:
--------------->             pos += 1
--------------->         array[pos], item = item, array[pos]
--------------->         writes += 1
--------------->         # Rotate the rest of the cycle.
--------------->         while pos != cycle_start:
--------------->             # Find where to put the item.
--------------->             pos = cycle_start
--------------->             for i in range(cycle_start + 1, len(array)):
--------------->                 if array[i] < item:
--------------->                     pos += 1
--------------->             # Put the item there or right after any duplicates.
--------------->             while item == array[pos]:
--------------->                 pos += 1
--------------->             array[pos], item = item, array[pos]
--------------->             writes += 1
---------------> 
--------------->     return writes

-------> Merge sorts

---------> Merge sort: 
-----------> This sort the first and second half of the list separately, then merge the sorted lists
-----------> Merge sort (also commonly spelled as mergesort) is an efficient, general-purpose, and comparison-based sorting algorithm. 
-------------> Most implementations produce a stable sort, which means that the order of equal elements is the same in the input and output. 
-----------> Algorithm
-------------> Conceptually, a merge sort works as follows:
---------------> Divide the unsorted list into n sublists, each containing one element (a list of one element is considered sorted).
---------------> Repeatedly merge sublists to produce new sorted sublists until there is only one sublist remaining. This will be the sorted list.
-----------> Note: Merge sort is a divide-and-conquer algorithm that was invented by John von Neumann in 1945.
-------------> A detailed description and analysis of bottom-up merge sort appeared in a report by Goldstine and von Neumann as early as 1948.

---------> Slowsort
-----------> Slowsort is a sorting algorithm. 
-------------> It is of humorous nature and not useful. 
-------------> It is a reluctant algorithm based on the principle of multiply and surrender (a parody formed by taking the opposites of divide and conquer). 
-----------> Algorithm
-------------> Slowsort is a recursive algorithm.
---------------> It sorts in-place.
---------------> It is a stable sort. (It does not change the order of equal-valued keys.)
-------------> This is an implementation in pseudocode:
---------------> procedure slowsort(A[], i, j)          // Sort array range A[i ... j] in-place.
--------------->     if i ≥ j then
--------------->         return
--------------->     m := floor( (i+j)/2 )
--------------->     slowsort(A, i, m)                  // (1.1)
--------------->     slowsort(A, m+1, j)                // (1.2)
--------------->     if A[j] < A[m] then
--------------->         swap A[j] , A[m]               // (1.3)
--------------->     slowsort(A, i, j-1)                // (2)
-------------> Sort the first half, recursively. (1.1)
-------------> Sort the second half, recursively. (1.2)
-------------> Find the maximum of the whole array by comparing the results of 1.1 and 1.2, and place it at the end of the list. (1.3)
-------------> Sort the entire list (except for the maximum now at the end), recursively. 
-----------> Note: It was published in 1986 by Andrei Broder and Jorge Stolfi in their paper Pessimal Algorithms and Simplexity Analysis (a parody of optimal algorithms and complexity analysis). 
 
---------> Strand sort
-----------> Strand sort is a recursive sorting algorithm that sorts items of a list into increasing order. 
-------------> It has O(n2) worst time complexity which occurs when the input list is reverse sorted.[1] 
-------------> It has a best case time complexity of O(n) which occurs when the input is a list that is already sorted.[citation needed]
-----------> The algorithm first moves the first element of a list into a sub-list. 
-------------> It then compares the last element in the sub-list to each subsequent element in the original list.
-------------> Once there is an element in the original list that is greater than the last element in the sub-list, 
-------------> the element is removed from the original list and added to the sub-list.
-------------> This process continues until the last element in the sub-list is compared to the remaining elements in the original list.
-------------> The sub-list is then merged into a new list.
-------------> Repeat this process and merge all sub-lists until all elements are sorted.
-------------> This algorithm is called strand sort because there are strands of sorted elements within the unsorted elements that are removed one at a time.
-------------> This algorithm is also used in J Sort for fewer than 40 elements. 

-------> Non-comparison sorts

---------> Bead sort
-----------> Bead sort, also called gravity sort, is a natural sorting algorithm.
-----------> Both digital and analog hardware implementations of bead sort can achieve a sorting time of O(n); 
-------------> however, the implementation of this algorithm tends to be significantly slower in software and can only be used to sort lists of positive integers. 
-------------> Also, it would seem that even in the best case, the algorithm requires O(n2) space. 
-----------> Algorithm overview
-------------> The bead sort operation can be compared to the manner in which beads slide on parallel poles, such as on an abacus. 
---------------> However, each pole may have a distinct number of beads. Initially, it may be helpful to imagine the beads suspended on vertical poles. 
---------------> In Step 1, such an arrangement is displayed using n=5 rows of beads on m=4 vertical poles. 
---------------> The numbers to the right of each row indicate the number that the row in question represents; 
---------------> rows 1 and 2 are representing the positive integer 3 (because they each contain three beads) 
---------------> while the top row represents the positive integer 2 (as it only contains two beads).
-------------> If we then allow the beads to fall, the rows now represent the same integers in sorted order. 
---------------> Row 1 contains the largest number in the set, while row n contains the smallest. 
---------------> If the above-mentioned convention of rows containing a series of beads on poles 1..k and leaving poles k+1..m empty has been followed, it will continue to be the case here.
-------------> The action of allowing the beads to "fall" in our physical example has allowed the larger values from the higher rows to propagate to the lower rows. 
---------------> If the value represented by row a is smaller than the value contained in row a+1, some of the beads from row a+1 will fall into row a; this is certain to happen, 
---------------> as row a does not contain beads in those positions to stop the beads from row a+1 from falling.
-------------> The mechanism underlying bead sort is similar to that behind counting sort; 
---------------> the number of beads on each pole corresponds to the number of elements with value equal or greater than the index of that pole. 
-------------> Note: This was developed by Joshua J. Arulanandham, Cristian S. Calude and Michael J. Dinneen in 2002
-------------> Note: This was published in The Bulletin of the European Association for Theoretical Computer Science.

---------> Bucket sort
-----------> Bucket sort, or bin sort, is a sorting algorithm that works by distributing the elements of an array into a number of buckets. 
-------------> Each bucket is then sorted individually, either using a different sorting algorithm, or by recursively applying the bucket sorting algorithm. 
-------------> It is a distribution sort, a generalization of pigeonhole sort that allows multiple keys per bucket, and is a cousin of radix sort in the most-to-least significant digit flavor. 
-------------> Bucket sort can be implemented with comparisons and therefore can also be considered a comparison sort algorithm. 
-------------> The computational complexity depends on the algorithm used to sort each bucket, the number of buckets to use, and whether the input is uniformly distributed.
-----------> Bucket sort works as follows:
-------------> (1) Set up an array of initially empty "buckets".
-------------> (2) Scatter: Go over the original array, putting each object in its bucket.
-------------> (3) Sort each non-empty bucket.
-------------> (4) Gather: Visit the buckets in order and put all elements back into the original array.
-----------> Pseudocode
-------------> function bucketSort(array, k) is
------------->     buckets ← new array of k empty lists
------------->     M ← the maximum key value in the array
------------->     for i = 0 to length(array) do
------------->         insert array[i] into buckets[floor(k × array[i] / M)]
------------->     for i = 0 to k do 
------------->         nextSort(buckets[i])
------------->     return the concatenation of buckets[0], ...., buckets[k]
-----------> Let array denote the array to be sorted and k denote the number of buckets to use. 
-------------> One can compute the maximum key value linear time by iterating over all the keys once. 
-------------> The floor function must be used to convert a floating number to an integer ( and possibly casting of datatypes too ). 
-------------> The function nextSort is a sorting function used to sort each bucket. 
-------------> Conventionally, insertion sort is used, but other algorithms could be used as well, such as selection sort or merge sort. 
-------------> Using bucketSort itself as nextSort produces a relative of radix sort; in particular, the case n = 2 corresponds to quicksort (although potentially with poor pivot choices). 

---------> Burstsort: 
-----------> This build a compact, cache efficient burst trie and then traverse it to create sorted output.
-------------> Burstsort and its variants are cache-efficient algorithms for sorting strings. 
-------------> They are variants of the traditional radix sort but faster for large data sets of common strings, first published in 2003, with some optimizing versions published in later years.
-----------> Burstsort algorithms use a trie to store prefixes of strings, with growable arrays of pointers as end nodes containing sorted, unique, suffixes (referred to as buckets). 
-------------> Some variants copy the string tails into the buckets. As the buckets grow beyond a predetermined threshold, the buckets are "burst" into tries, giving the sort its name. 
-------------> A more recent variant uses a bucket index with smaller sub-buckets to reduce memory usage. 
-------------> Most implementations delegate to multikey quicksort, an extension of three-way radix quicksort, to sort the contents of the buckets. 
-------------> By dividing the input into buckets with common prefixes, the sorting can be done in a cache-efficient manner.
-----------> Burstsort was introduced as a sort that is similar to MSD radix sort, 
-------------> but is faster due to being aware of caching and related radixes being stored closer to each other due to specifics of trie structure. 
-------------> It exploits specifics of strings that are usually encountered in real world.
-------------> And although asymptotically it is the same as radix sort, with time complexity of O(wn) (w – word length and n – number of strings to be sorted), 
-------------> but due to better memory distribution it tends to be twice as fast on big data sets of strings. 
-------------> It has been billed as the "fastest known algorithm to sort large sets of strings".

---------> Counting sort
-----------> Counting sort is an algorithm for sorting a collection of objects according to keys 
-------------> that are small positive integers; that is, it is an integer sorting algorithm. 
-------------> It operates by counting the number of objects that possess distinct key values, 
-------------> and applying prefix sum on those counts to determine the positions of each key value in the output sequence. 
-------------> Its running time is linear in the number of items and the difference between the maximum key value and the minimum key value, 
-------------> so it is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items. 
-------------> It is often used as a subroutine in radix sort, another sorting algorithm, which can handle larger keys more efficiently.[1][2][3]
-----------> Counting sort is not a comparison sort; it uses key values as indexes into an array and the Ω(n log n) lower bound for comparison sorting will not apply.
-------------> Bucket sort may be used in lieu of counting sort, and entails a similar time analysis. 
-------------> However, compared to counting sort, bucket sort requires linked lists, dynamic arrays,
-------------> or a large amount of pre-allocated memory to hold the sets of items within each bucket, 
-------------> whereas counting sort stores a single number (the count of items) per bucket.
-----------> Pseudocode
-------------> In pseudocode, the algorithm may be expressed as:
---------------> function CountingSort(input, k)
--------------->     count ← array of k + 1 zeros
--------------->     output ← array of same length as input
--------------->     
--------------->     for i = 0 to length(input) - 1 do
--------------->         j = key(input[i])
--------------->         count[j] += 1
---------------> 
--------------->     for i = 1 to k do
--------------->         count[i] += count[i - 1]
---------------> 
--------------->     for i = length(input) - 1 downto 0 do
--------------->         j = key(input[i])
--------------->         count[j] -= 1
--------------->         output[count[j]] = input[i]
---------------> 
--------------->     return output

---------> Pigeonhole sort
-----------> Pigeonhole sorting is a sorting algorithm that is suitable for sorting lists of elements where the number of elements (n) 
-------------> and the length of the range of possible key values (N) are approximately the same.
-------------> It requires O(n + N) time. It is similar to counting sort, 
-------------> but differs in that it "moves items twice: once to the bucket array and again to the final destination [whereas] counting sort builds an auxiliary array 
-------------> then uses the array to compute each item's final destination and move the item there."
-------------> The pigeonhole algorithm works as follows:
---------------> Given an array of values to be sorted, set up an auxiliary array of initially empty "pigeonholes", one pigeonhole for each key in the range of the keys in the original array.
---------------> Going over the original array, put each value into the pigeonhole corresponding to its key, such that each pigeonhole eventually contains a list of all values with that key.
---------------> Iterate over the pigeonhole array in increasing order of keys, and for each pigeonhole, put its elements into the original array in increasing order.
-----------> Python implementation
-------------> from typing import List, Tuple, Any
-------------> def pigeonhole_sort(lst: List[Tuple[int, Any]]) -> List[Tuple[int, Any]]:
------------->     """
------------->     In-place sorts a list of (key, value) tuples by key.
------------->     :param lst: A list of tuples, each having a key as a number and a value that can be anything.
------------->     :return:    List[Tuple[int, Any]]
------------->     """
------------->     base = min(key for key, value in lst)
------------->     size = max(key for key, value in lst) - base + 1
------------->     # Create the empty list (of lists) to be filled
------------->     # It's a list of lists because the key can appear twice
------------->     pigeonholes: List[List[Tuple[int, Any]]] = [[] for _ in range(size)]
------------->     for key, value in lst:
------------->         pigeonholes[key - base].append((key, value))
------------->     return sum(pigeonhole,[]) # concatenate all containing lists the result

 
---------> Postman sort: 
-----------> This variant of Bucket sort which takes advantage of hierarchical structure
-------------> The Postman's sort is a variant of bucket sort that takes advantage of a hierarchical structure of elements, typically described by a set of attributes. 
-------------> This is the algorithm used by letter-sorting machines in post offices: mail is sorted first between domestic and international; 
-------------> then by state, province or territory; then by destination post office; then by routes, etc. 
-------------> Since keys are not compared against each other, sorting time is O(cn), where c depends on the size of the key and number of buckets. 
-------------> This is similar to a radix sort that works "top down," or "most significant digit first."

---------> Radix sort: 
-----------> This sorts strings letter by letter.
-----------> Radix sort is a non-comparative sorting algorithm. 
-------------> It avoids comparison by creating and distributing elements into buckets according to their radix. 
-------------> For elements with more than one significant digit, this bucketing process is repeated for each digit, 
-------------> while preserving the ordering of the prior step, until all digits have been considered. 
-------------> For this reason, radix sort has also been called bucket sort and digital sort.
-------------> Radix sort can be applied to data that can be sorted lexicographically, be they integers, words, punch cards, playing cards, or the mail. 

-------> Selection sorts

---------> Heapsort: 
-----------> This convert the list into a heap, keep removing the largest element from the heap and adding it to the end of the list
-------------> Heapsort is a comparison-based sorting algorithm. 
-------------> Heapsort can be thought of as an improved selection sort: like selection sort, 
-------------> heapsort divides its input into a sorted and an unsorted region, 
-------------> and it iteratively shrinks the unsorted region by extracting the largest element from it and inserting it into the sorted region. 
-------------> Unlike selection sort, heapsort does not waste time with a linear-time scan of the unsorted region; 
-------------> rather, heap sort maintains the unsorted region in a heap data structure to more quickly find the largest element in each step.
-----------> Although somewhat slower in practice on most machines than a well-implemented quicksort, 
-------------> it has the advantage of a more favorable worst-case O(n log n) runtime. 
-------------> Heapsort is an in-place algorithm, but it is not a stable sort.
-----------> Heapsort was invented by J. W. J. Williams in 1964.[2] This was also the birth of the heap, 
-------------> presented already by Williams as a useful data structure in its own right.[3] 
-------------> In the same year, Robert W. Floyd published an improved version that could sort an array in-place, continuing his earlier research into the treesort algorithm.[
-----------> Algorithm
-------------> The heapsort algorithm involves preparing the list by first turning it into a max heap. 
-------------> The algorithm then repeatedly swaps the first value of the list with the last value, 
-------------> decreasing the range of values considered in the heap operation by one, 
-------------> and sifting the new first value into its position in the heap. 
-------------> This repeats until the range of considered values is one value in length.
-------------> The steps are:
---------------> (1)Call the buildMaxHeap() function on the list. Also referred to as heapify(), this builds a heap from a list in  O(n) operations.
---------------> (2)Swap the first element of the list with the final element. Decrease the considered range of the list by one.
---------------> (3)Call the siftDown() function on the list to sift the new first element to its appropriate index in the heap.
---------------> (4)Go to step (2) unless the considered range of the list is one element.
-------------> The buildMaxHeap() operation is run once, and is O(n) in performance. The siftDown() function is O(log n), and is called n times. 
-------------> Therefore, the performance of this algorithm is O(n + n log n) = O(n log n). 

---------> Selection sort: 
-----------> This pick the smallest of the remaining elements, add it to the end of the sorted list
-------------> Delection sort is an in-place comparison sorting algorithm. It has an O(n2) time complexity, 
-------------> which makes it inefficient on large lists, and generally performs worse than the similar insertion sort. 
-------------> Selection sort is noted for its simplicity and has performance advantages over more complicated algorithms in certain situations, particularly where auxiliary memory is limited.
-----------> The algorithm divides the input list into two parts: a sorted sublist of items which is built up from left to right at the front (left) of the list 
-------------> and a sublist of the remaining unsorted items that occupy the rest of the list. 
-------------> Initially, the sorted sublist is empty and the unsorted sublist is the entire input list. 
-------------> The algorithm proceeds by finding the smallest (or largest, depending on sorting order) element in the unsorted sublist,
-------------> exchanging (swapping) it with the leftmost unsorted element (putting it in sorted order), and moving the sublist boundaries one element to the right.
-----------> The time efficiency of selection sort is quadratic, so there are a number of sorting techniques which have better time complexity than selection sort. 
-------------> One thing which distinguishes selection sort from other sorting algorithms is that it makes the minimum possible number of swaps, n − 1 in the worst case. 
-----------> Implementation
-------------> /* a[0] to a[aLength-1] is the array to sort */
-------------> int i,j;
-------------> int aLength; // initialise to a's length
-------------> /* advance the position through the entire array */
-------------> /*   (could do i < aLength-1 because single element is also min element) */
-------------> for (i = 0; i < aLength-1; i++)
-------------> {
------------->     /* find the min element in the unsorted a[i .. aLength-1] */
------------->     /* assume the min is the first element */
------------->     int jMin = i;
------------->     /* test against elements after i to find the smallest */
------------->     for (j = i+1; j < aLength; j++)
------------->     {
------------->         /* if this element is less, then it is the new minimum */
------------->         if (a[j] < a[jMin])
------------->         {
------------->             /* found new minimum; remember its index */
------------->             jMin = j;
------------->         }
------------->     }
------------->     if (jMin != i) 
------------->     {
------------->         swap(a[i], a[jMin]);
------------->     }
-------------> }

---------> Smoothsort
-----------> Smoothsort is a comparison-based sorting algorithm. 
-------------> A variant of heapsort, it was invented and published by Edsger Dijkstra in 1981.
-------------> Like heapsort, smoothsort is an in-place algorithm with an upper bound of O(n log n), but it is not a stable sort. 
-------------> The advantage of smoothsort is that it comes closer to O(n) time if the input is already sorted to some degree, 
-------------> whereas heapsort averages O(n log n) regardless of the initial sorted state. 

-------> Other

---------> Bitonic sorter
-----------> Bitonic mergesort is a parallel algorithm for sorting. It is also used as a construction method for building a sorting network. 
-------------> The algorithm was devised by Ken Batcher. The resulting sorting networks consist of O(n log(n)^2) comparators and have a delay of O(n log(n)^2), where n is the number of items to be sorted.[1]
-----------> A sorted sequence is a monotonically non-decreasing (or non-increasing) sequence. 
-------------> A bitonic sequence is a sequence with x0 ≤ ⋯ ≤ xk ≥ ⋯ ≥ xn−  for some k, 0 ≤ k < n, or a circular shift of such a sequence. 
-----------> Implementation
-------------> The following is a recursion-free implementation of the bitonic mergesort in C-like pseudocode:[2]
---------------> // given an array arr of length n, this code sorts it in place
---------------> // all indices run from 0 to n-1
---------------> for (k = 2; k <= n; k *= 2) // k is doubled every iteration
--------------->     for (j = k/2; j > 0; j /= 2) // j is halved at every iteration, with truncation of fractional parts
--------------->         for (i = 0; i < n; i++)
--------------->             l = bitwiseXOR (i, j); // in C-like languages this is "i ^ j"
--------------->             if (l > i)
--------------->                 if (  (bitwiseAND (i, k) == 0) AND (arr[i] > arr[l])
--------------->                    OR (bitwiseAND (i, k) != 0) AND (arr[i] < arr[l]) )
--------------->                       swap the elements arr[i] and arr[l]

---------> Pancake sorting
-----------> Pancake sorting is the mathematical problem of sorting a disordered stack of pancakes in order of size when a spatula can be inserted at any point in the stack and used to flip all pancakes above it. 
-------------> A pancake number is the minimum number of flips required for a given number of pancakes. 
-------------> A variant of the problem is concerned with burnt pancakes, where each pancake has a burnt side and all pancakes must, in addition, end up with the burnt side on bottom.
-----------> All sorting methods require pairs of elements to be compared. 
-------------> For the traditional sorting problem, the usual problem studied is to minimize the number of comparisons required to sort a list. 
-------------> The number of actual operations, such as swapping two elements, is then irrelevant. 
-------------> For pancake sorting problems, in contrast, the aim is to minimize the number of operations, where the only allowed operations are reversals of the elements of some prefix of the sequence. 
-------------> Now, the number of comparisons is irrelevant. 
---------> Algorithm
-----------> An example of the pancake sorting algorithm is given below in Python.
-------------> def flip(arr, k):
------------->     left = 0
------------->     while left < k:
------------->         arr[left], arr[k] = arr[k], arr[left]
------------->         k -= 1
------------->         left += 1
-------------> def max_index(arr, k):
------------->     index = 0
------------->     for i in range(k):
------------->         if arr[i] > arr[index]:
------------->             index = i
------------->     return index
-------------> def pancake_sort(arr):
------------->     n = len(arr)
------------->     while n > 1:
------------->         maxdex = max_index(arr, n)
------------->         flip(arr, maxdex)
------------->         flip(arr, n - 1)
------------->         n -= 1
-------------> arreglo = [15, 8, 9, 1, 78, 30, 69, 4, 10]
-------------> pancake_sort(arreglo)
-------------> print(arreglo)
-----------> Note: The problem was first discussed by American geometer Jacob E. Goodman. 

---------> Spaghetti sort
-----------> Spaghetti sort is a linear-time, analog algorithm for sorting a sequence of items.
-------------> This algorithm sorts a sequence of items requiring O(n) stack space in a stable manner. 
-------------> It requires a parallel processor. 
-----------> Note: This was introduced by A. K. Dewdney in his Scientific American column

---------> Topological sort
-----------> A topological sort or topological ordering of a directed graph is a linear ordering of its vertices
-------------> such that for every directed edge uv from vertex u to vertex v, u comes before v in the ordering. 
-------------> For instance, the vertices of the graph may represent tasks to be performed, 
-------------> and the edges may represent constraints that one task must be performed before another; 
-------------> in this application, a topological ordering is just a valid sequence for the tasks. 
-------------> Precisely, a topological sort is a graph traversal in which each node v is visited only after all its dependencies are visited. 
-------------> A topological ordering is possible if and only if the graph has no directed cycles, that is, if it is a directed acyclic graph (DAG). 
-------------> Any DAG has at least one topological ordering, and algorithms are known for constructing a topological ordering of any DAG in linear time. 
-------------> Topological sorting has many applications especially in ranking problems such as feedback arc set. 
-------------> Topological sorting is possible even when the DAG has disconnected components. 
-----------> Algorithms
-------------> The usual algorithms for topological sorting have running time linear in the number of nodes plus the number of edges, asymptotically, O(|V| + |E|).
-------------> Kahn's algorithm:
---------------> Not to be confused with Kuhn's algorithm.
---------------> One of these algorithms, first described by Kahn (1962), works by choosing vertices in the same order as the eventual topological sort. 
-----------------> First, find a list of "start nodes" which have no incoming edges and insert them into a set S; at least one such node must exist in a non-empty acyclic graph. 
-----------------> Then:
-------------------> L ← Empty list that will contain the sorted elements
-------------------> S ← Set of all nodes with no incoming edge
-------------------> while S is not empty do
------------------->     remove a node n from S
------------------->     add n to L
------------------->     for each node m with an edge e from n to m do
------------------->         remove edge e from the graph
------------------->         if m has no other incoming edges then
------------------->             insert m into S
-------------------> if graph has edges then
------------------->     return error   (graph has at least one cycle)
-------------------> else 
------------------->     return L   (a topologically sorted order)
---------------> If the graph is a DAG, a solution will be contained in the list L (the solution is not necessarily unique). 
-----------------> Otherwise, the graph must have at least one cycle and therefore a topological sort is impossible. 
---------------> Reflecting the non-uniqueness of the resulting sort, the structure S can be simply a set or a queue or a stack. 
-----------------> Depending on the order that nodes n are removed from set S, a different solution is created. 
-----------------> A variation of Kahn's algorithm that breaks ties lexicographically forms a key component of the Coffman–Graham algorithm for parallel scheduling and layered graph drawing.
-------------> Depth-first search
---------------> An alternative algorithm for topological sorting is based on depth-first search. 
-----------------> The algorithm loops through each node of the graph, in an arbitrary order, 
-----------------> initiating a depth-first search that terminates when it hits any node that has already been visited 
-----------------> since the beginning of the topological sort or the node has no outgoing edges (i.e. a leaf node):
-------------------> L ← Empty list that will contain the sorted nodes
-------------------> while exists nodes without a permanent mark do
------------------->     select an unmarked node n
------------------->     visit(n)
-------------------> function visit(node n)
------------------->     if n has a permanent mark then
------------------->         return
------------------->     if n has a temporary mark then
------------------->         stop   (not a DAG)
------------------->     mark n with a temporary mark
------------------->     for each node m with an edge from n to m do
------------------->         visit(m)
------------------->     remove temporary mark from n
------------------->     mark n with a permanent mark
------------------->     add n to head of L
-------------> Each node n gets prepended to the output list L only after considering all other nodes which depend on n (all descendants of n in the graph). 
---------------> Specifically, when the algorithm adds node n, we are guaranteed that all nodes which depend on n are already in the output list L: 
---------------> they were added to L either by the recursive call to visit() which ended before the call to visit n,
---------------> or by a call to visit() which started even before the call to visit n. Since each edge and node is visited once, the algorithm runs in linear time. 
---------------> This depth-first-search-based algorithm is the one described by Cormen et al. (2001);[3] it seems to have been first described in print by Tarjan in 1976.[4] 

-------> Unknown class

---------> Samplesort
-----------> Samplesort is a sorting algorithm that is a divide and conquer algorithm often used in parallel processing systems. 
-------------> Conventional divide and conquer sorting algorithms partitions the array into sub-intervals or buckets. 
-------------> The buckets are then sorted individually and then concatenated together. However, if the array is non-uniformly distributed, 
-------------> the performance of these sorting algorithms can be significantly throttled. 
-------------> Samplesort addresses this issue by selecting a sample of size s from the n-element sequence, 
-------------> and determining the range of the buckets by sorting the sample and choosing p−1 < s elements from the result. 
-------------> These elements (called splitters) then divide the array into p approximately equal-sized buckets.
-------------> Samplesort is described in the 1970 paper, "Samplesort: A Sampling Approach to Minimal Storage Tree Sorting", by W. D. Frazer and A. C. McKellar.[3] 
-----------> Algorithm
-------------> Samplesort is a generalization of quicksort. 
---------------> Where quicksort partitions its input into two parts at each step, based on a single value called the pivot, 
---------------> samplesort instead takes a larger sample from its input and divides its data into buckets accordingly. 
---------------> Like quicksort, it then recursively sorts the buckets.
-------------> To devise a samplesort implementation, one needs to decide on the number of buckets p. When this is done, the actual algorithm operates in three phases:[4]
---------------> Sample p−1 elements from the input (the splitters). Sort these; each pair of adjacent splitters then defines a bucket.
---------------> Loop over the data, placing each element in the appropriate bucket. (This may mean: send it to a processor, in a multiprocessor system.)
---------------> Sort each of the buckets.
-------------> The full sorted output is the concatenation of the buckets.
-------------> A common strategy is to set p equal to the number of processors available. 
---------------> The data is then distributed among the processors, which perform the sorting of buckets using some other, sequential, sorting algorithm. 



-----> Subsequences

-------> Kadane's algorithm: 
---------> This finds maximum sub-array of any size.
---------> Kadane's original algorithm solves the problem version when empty subarrays are admitted. 
-----------> It scans the given array A [1 … n] from left to right. 
-----------> In the jth step, it computes the subarray with the largest sum ending at j; this sum is maintained in variable current_sum.
-----------> Moreover, it computes the subarray with the largest sum anywhere in A [1 … j], 
-----------> maintained in variable best_sum, and easily obtained as the maximum of all values of current_sum seen so far, cf. line 7 of the algorithm.
---------> As a loop invariant, in the jth step, the old value of current_sum holds the maximum over all i is an element of {1, …, j}  of the sum A[i] + ⋯ + A[j−1].
-----------> Therefore, current_sum + A [ j ] is the maximum over all i is an element of {1, …, j}  of the sum A[i]+...+A[j]. 
-----------> To extend the latter maximum to cover also the case i = j + 1, it is sufficient to consider also the empty subarray A[j+1 … j]. 
-----------> This is done in line 6 by assigning max(0, current_sum + A[j]) as the new value of current_sum, which after that holds the maximum over all i is an element of {1, …, j+1}  of the sum A[i]+...+A[j].
---------> Thus, the problem can be solved with the following code,[4][7] expressed here in Python:
-----------> def max_subarray(numbers):
----------->     """Find the largest sum of any contiguous subarray."""
----------->     best_sum = 0
----------->     current_sum = 0
----------->     for x in numbers:
----------->         current_sum = max(0, current_sum + x)
----------->         best_sum = max(best_sum, current_sum)
----------->     return best_sum

-------> Longest common subsequence problem: 
---------> This finds the longest subsequence common to all sequences in a set of sequences
---------> The longest common subsequence (LCS) problem is the problem of finding the longest subsequence common
-----------> to all sequences in a set of sequences (often just two sequences). 
-----------> It differs from the longest common substring problem: unlike substrings, 
-----------> subsequences are not required to occupy consecutive positions within the original sequences. 
-----------> The longest common subsequence problem is a classic computer science problem, 
-----------> the basis of data comparison programs such as the diff utility, and has applications in computational linguistics and bioinformatics. 
-----------> It is also widely used by revision control systems such as Git for reconciling multiple changes made to a revision-controlled collection of files.
---------> For example, consider the sequences (ABCD) and (ACBAD). 
-----------> They have 5 length-2 common subsequences: (AB), (AC), (AD), (BD), and (CD); 2 length-3 common subsequences: (ABD) and (ACD); and no longer common subsequences. 
-----------> So (ABD) and (ACD) are their longest common subsequences. 
---------> Computing the length of the LCS
-----------> The function below takes as input sequences X[1..m] and Y[1..n], 
-----------> computes the LCS between X[1..i] and Y[1..j] for all 1 ≤ i ≤ m and 1 ≤ j ≤ n, 
-----------> and stores it in C[i,j]. C[m,n] will contain the length of the LCS of X and Y.[7]
-------------> function LCSLength(X[1..m], Y[1..n])
------------->     C = array(0..m, 0..n)
------------->     for i := 0..m
------------->         C[i,0] = 0
------------->     for j := 0..n
------------->         C[0,j] = 0
------------->     for i := 1..m
------------->         for j := 1..n
------------->             if X[i] = Y[j]
------------->                 C[i,j] := C[i-1,j-1] + 1
------------->             else
------------->                 C[i,j] := max(C[i,j-1], C[i-1,j])
------------->     return C[m,n]

-------> Longest increasing subsequence problem: 
---------> This finds the longest increasing subsequence of a given sequence
-----------> The longest increasing subsequence problem is to find a subsequence of a given sequence in which the subsequence's elements are in sorted order, 
-------------> lowest to highest, and in which the subsequence is as long as possible. 
-------------> This subsequence is not necessarily contiguous, or unique. Longest increasing subsequences are studied in the context of various disciplines related to mathematics, 
-------------> including algorithmics, random matrix theory, representation theory, and physics.
-------------> The longest increasing subsequence problem is solvable in time O(n * log ⁡ n), where n denotes the length of the input sequence.[2]
-----------> Example
-------------> In the first 16 terms of the binary Van der Corput sequence
---------------> 0, 8, 4, 12, 2, 10, 6, 14, 1, 9, 5, 13, 3, 11, 7, 15
-------------> a longest increasing subsequence is
---------------> 0, 2, 6, 9, 11, 15.
-------------> This subsequence has length six; the input sequence has no seven-member increasing subsequences. The longest increasing subsequence in this example is not the only solution: for instance,
---------------> 0, 4, 6, 9, 11, 15
---------------> 0, 2, 6, 9, 13, 15
---------------> 0, 4, 6, 9, 13, 15
-------------> are other increasing subsequences of equal length in the same input sequence. 
-----------> Algorithm:
-------------> P = array of length N
-------------> M = array of length N + 1
-------------> M[0] = -1 // undefined so can be set to any value
-------------> 
-------------> L = 0
-------------> for i in range 0 to N-1:
------------->     // Binary search for the smallest positive l ≤ L
------------->     // such that X[M[l]] > X[i]
------------->     lo = 1
------------->     hi = L + 1
------------->     while lo < hi:
------------->         mid = lo + floor((hi-lo)/2) // lo <= mid < hi
------------->         if X[M[mid]] > X[i]
------------->             hi = mid
------------->         else: // if X[M[mid]] <= X[i]
------------->             lo = mid + 1
------------->     // After searching, lo == hi is 1 greater than the
------------->     // length of the longest prefix of X[i]
------------->     newL = lo
------------->     // The predecessor of X[i] is the last index of 
------------->     // the subsequence of length newL-1
------------->     P[i] = M[newL-1]
------------->     M[newL] = i
------------->     if newL > L:
------------->         // If we found a subsequence longer than any we've
------------->         // found yet, update L
------------->         L = newL
-------------> // Reconstruct the longest increasing subsequence
-------------> // It consists of the values of X at the L indices:
-------------> // ...,  P[P[M[L]]], P[M[L]], M[L]
-------------> S = array of length L
-------------> k = M[L]
-------------> for j in range L-1 to 0:
------------->     S[j] = X[k]
------------->     k = P[k]
-------------> return S

-------> Ruzzo–Tompa algorithm: 
---------> This finds all non-overlapping, contiguous, maximal scoring subsequences in a sequence of real numbers
----------> The Ruzzo–Tompa algorithm is a linear-time algorithm for finding all non-overlapping, contiguous, maximal scoring subsequences in a sequence of real numbers. 
-----------> This algorithm is an improvement over previously known quadratic time algorithms. 
-----------> The maximum scoring subsequence from the set produced by the algorithm is also a solution to the maximum subarray problem.
---------> The Ruzzo–Tompa algorithm has applications in bioinformatics, web scraping, and information retrieval.
---------> Algorithm
-----------> This animation shows the Ruzzo–Tompa algorithm running with an input sequence of 11 integers each represented by a line segment in the graph. 
-------------> Segments with bold lines represent maximal segments found so far.
-------------> The animation shows the state of I,R and L at each step. 
-------------> Below that it shows the current state the algorithm which correspond to steps 1–4 in the Algorithm section of this page. 
-------------> The red highlight shows the algorithm finding a value for j in steps 1 and 3. 
-------------> If the value of j satisfies the inequalities in those steps the highlight turns green. 
-------------> At the end of the animation the maximal subsequences will be bolded and displayed in I.
-----------> The standard implementation of the Ruzzo–Tompa algorithm runs in  O(n) time and uses O(n) space, where n is the length of the list of scores. 
-------------> The algorithm uses dynamic programming to progressively build the final solution by incrementally solving progressively larger subsets of the problem. 
-------------> The description of the algorithm provided by Ruzzo and Tompa is as follows:
---------------> Read the scores left to right and maintain the cumulative sum of the scores read. Maintain an ordered list I1, I2, …, Ij  of disjoint subsequences. 
-----------------> For each subsequence Ij, record the cumulative total Lj of all scores up to but not including the leftmost score of Ij, and the total Rj up to and including the rightmost score of Ij.
---------------> The lists are initially empty. Scores are read from left to right and are processed as follows. 
-----------------> Nonpositive scores require no special processing, so the next score is read. 
-----------------> A positive score is incorporated into a new sub-sequence Ik of length one that is then integrated into the list by the following process.
-----------------> (1) The list I is searched from right to left for the maximum value of j satisfying Lj < Lk
-----------------> (2) If there is no such j, then add Ik to the end of the list.
-----------------> (3) If there is such a j, and Rj ≥ Rk, then add Ik to the end of the list.
-----------------> (4) Otherwise (i.e., there is such a j, but Rj < Rk, extend the subsequence Ik to the left to encompass everything up to and including the leftmost score in Ij. 
-------------------> Delete subsequences Ij , Ij+1 , … , Ik−1 from the list, and append Ik to the end of the list. 
-------------------> Reconsider the newly extended subsequence Ik (now renumbered Ij) as in step 1.
---------------> Once the end of the input is reached, all subsequences remaining on the list I are maximal.
-----------> The following Python code implements the Ruzzo–Tompa algorithm:
-------------> def ruzzo_tompa(scores):
------------->     """Ruzzo–Tompa algorithm."""
------------->     k = 0
------------->     total = 0
------------->     # Allocating arrays of size n
------------->     I, L, R, Lidx = [[0] * len(scores) for _ in range(4)]
------------->     for i, s in enumerate(scores):
------------->         total += s
------------->         if s > 0:
------------->             # store I[k] by (start,end) indices of scores
------------->             I[k] = (i, i + 1)
------------->             Lidx[k] = i
------------->             L[k] = total - s
------------->             R[k] = total
------------->             while True:
------------->                 maxj = None
------------->                 for j in range(k - 1, -1, -1):
------------->                     if L[j] < L[k]:
------------->                         maxj = j
------------->                         break
------------->                 if maxj is not None and R[maxj] < R[k]:
------------->                     I[maxj] = (Lidx[maxj], i + 1)
------------->                     R[maxj] = total
------------->                     k = maxj
------------->                 else:
------------->                     k += 1
------------->                     break
------------->     # Getting maximal subsequences using stored indices
------------->     return [scores[I[l][0] : I[l][1]] for l in range(k)]

-------> Shortest common supersequence problem: 
---------> This finds the shortest supersequence that contains two or more sequences as subsequences
---------> The shortest common supersequence of two sequences X and Y is the shortest sequence which has X and Y as subsequences. 
-----------> This is a problem closely related to the longest common subsequence problem. 
-----------> Given two sequences X = < x1,...,xm > and Y = < y1,...,yn >, a sequence U = < u1,...,uk > is a common supersequence of X and Y if items can be removed from U to produce X and Y.
---------> A shortest common supersequence (SCS) is a common supersequence of minimal length. 
-----------> In the shortest common supersequence problem, two sequences X and Y are given, and the task is to find a shortest possible common supersequence of these sequences. 
-----------> In general, an SCS is not unique.
---------> For two input sequences, an SCS can be formed from a longest common subsequence (LCS) easily. 
-----------> For example, the longest common subsequence of X[1..m]=abcbdab and Y[1..n]=bdcaba is Z[1..L]=bcba}. 
-----------> By inserting the non-LCS symbols into Z while preserving their original order, we obtain a shortest common supersequence U[1..S]=abdcabdab. 
-----------> In particular, the equation L+S=m+n holds for any two input sequences.
---------> There is no similar relationship between shortest common supersequences and longest common subsequences of three or more input sequences. (In particular, LCS and SCS are not dual problems.) 
-----------> However, both problems can be solved in O(nk) time using dynamic programming, where k is the number of sequences, and n is their maximum length. 
-----------> For the general case of an arbitrary number of input sequences, the problem is NP-hard.


-----> Substrings

-------> Longest common substring problem: 
---------> This finds the longest string (or strings) that is a substring (or are substrings) of two or more strings
---------> The longest common substring problem is to find a longest string that is a substring of two or more strings. 
-----------> The problem may have multiple solutions. 
-----------> Applications include data deduplication and plagiarism detection. 
-------> Implementation using dynamic programming
---------> The following pseudocode finds the set of longest common substrings between two strings with dynamic programming:
-----------> function LCSubstr(S[1..r], T[1..n])
----------->     L := array(1..r, 1..n)
----------->     z := 0
----------->     ret := {}
-----------> 
----------->     for i := 1..r
----------->         for j := 1..n
----------->             if S[i] = T[j]
----------->                 if i = 1 or j = 1
----------->                     L[i, j] := 1
----------->                 else
----------->                     L[i, j] := L[i − 1, j − 1] + 1
----------->                 if L[i, j] > z
----------->                     z := L[i, j]
----------->                     ret := {S[i − z + 1..i]}
----------->                 else if L[i, j] = z
----------->                     ret := ret ∪ {S[i − z + 1..i]}
----------->             else
----------->                 L[i, j] := 0
----------->     return ret
---------> This algorithm runs in O ( n r ) {\displaystyle O(nr)} {\displaystyle O(nr)} time. 
-----------> The array L stores the longest common substring of the prefixes S[1..i] and T[1..j] which end at position S[i], T[j], resp. 
-----------> The variable z is used to hold the length of the longest common substring found so far. The set ret is used to hold the set of strings which are of length z. 
-----------> The set ret can be saved efficiently by just storing the index i, which is the last character of the longest common substring (of size z) instead of S[i-z+1..i]. 
-----------> Thus all the longest common substrings would be, for each i in ret, S[(ret[i]-z)..(ret[i])].
---------> The following tricks can be used to reduce the memory usage of an implementation:
-----------> Keep only the last and current row of the DP table to save memory O(\min(r,n)) instead of O(nr)
-----------> Store only non-zero values in the rows. This can be done using hash-tables instead of arrays. This is useful for large alphabets.

---------> Substring search

-----------> Aho–Corasick string matching algorithm: 
------------> This is trie based algorithm for finding all substring matches to any of a finite set of strings.
------------> It is a kind of dictionary-matching algorithm that locates elements of a finite set of strings (the "dictionary") within an input text. 
--------------> It matches all strings simultaneously. 
--------------> The complexity of the algorithm is linear in the length of the strings plus the length of the searched text plus the number of output matches. 
--------------> Note that because all matches are found, there can be a quadratic number of matches if every substring matches (e.g. dictionary = a, aa, aaa, aaaa and input string is aaaa).
------------> Informally, the algorithm constructs a finite-state machine that resembles a trie with additional links between the various internal nodes. 
--------------> These extra internal links allow fast transitions between failed string matches 
--------------> (e.g. a search for cat in a trie that does not contain cat, but contains cart, and thus would fail at the node prefixed by ca), 
--------------> to other branches of the trie that share a common prefix (e.g., in the previous case, a branch for attribute might be the best lateral transition). 
--------------> This allows the automaton to transition between string matches without the need for backtracking.
------------> When the string dictionary is known in advance (e.g. a computer virus database), 
--------------> the construction of the automaton can be performed once off-line and the compiled automaton stored for later use. 
--------------> In this case, its run time is linear in the length of the input plus the number of matched entries.
------------> The Aho–Corasick string-matching algorithm formed the basis of the original Unix command fgrep. 
------------> Note: The Aho–Corasick algorithm is a string-searching algorithm invented by Alfred V. Aho and Margaret J. Corasick in 1975. 

-----------> Boyer–Moore string-search algorithm: 
------------> This amortized linear (sublinear in most times) algorithm for substring search
------------> The Boyer–Moore string-search algorithm is an efficient string-searching algorithm that is the standard benchmark for practical string-search literature. 
--------------> The algorithm preprocesses the string being searched for (the pattern), but not the string being searched in (the text). 
--------------> It is thus well-suited for applications in which the pattern is much shorter than the text or where it persists across multiple searches. 
--------------> The Boyer–Moore algorithm uses information gathered during the preprocess step to skip sections of the text, 
--------------> resulting in a lower constant factor than many other string search algorithms. 
--------------> In general, the algorithm runs faster as the pattern length increases. 
--------------> The key features of the algorithm are to match on the tail of the pattern rather than the head, 
--------------> and to skip along the text in jumps of multiple characters rather than searching every single character in the text. 
------------> Note: This was developed by Robert S. Boyer and J Strother Moore in 1977. 
--------------> The original paper contained static tables for computing the pattern shifts without an explanation of how to produce them. 
--------------> The algorithm for producing the tables was published in a follow-on paper; this paper contained errors which were later corrected by Wojciech Rytter in 1980.

-----------> Boyer–Moore–Horspool algorithm: 
------------> This is simplification of Boyer–Moore
------------> The Boyer–Moore–Horspool algorithm or Horspool's algorithm is an algorithm for finding substrings in strings. 
------------> It is a simplification of the Boyer–Moore string search algorithm which is related to the Knuth–Morris–Pratt algorithm. 
--------------> The algorithm trades space for time in order to obtain an average-case complexity of O(n) on random text, 
--------------> although it has O(nm) in the worst case, where the length of the pattern is m and the length of the search string is n. 
--------------> Like Boyer–Moore, Boyer–Moore–Horspool preprocesses the pattern to produce a table containing, 
--------------> for each symbol in the alphabet, the number of characters that can safely be skipped. 
--------------> The preprocessing phase, in pseudocode, is as follows (for an alphabet of 256 symbols, i.e., bytes):
------------> Unlike the original, we use zero-based indices here.
--------------> function preprocess(pattern)
-------------->     T ← new table of 256 integers
-------------->     for i from 0 to 256 exclusive
-------------->         T[i] ← length(pattern)
-------------->     for i from 0 to length(pattern) - 1 exclusive
-------------->         T[pattern[i]] ← length(pattern) - 1 - i
-------------->     return T
------------> Pattern search proceeds as follows. The procedure search reports the index of the first occurrence of needle in haystack.
--------------> function same(str1, str2, len)              Compares two strings, up to the first len characters.
-------------->     i ← len - 1
-------------->     while str1[i] = str2[i]               Note: this is equivalent to !memcmp(str1, str2, len).
-------------->         if i = 0                            The original algorithm tries to play smart here: it checks for the
-------------->             return true                     last character, and then starts from the first to the second-last.
-------------->         i ← i - 1
-------------->     return false
--------------> function search(needle, haystack)
-------------->     T ← preprocess(needle)
-------------->     skip ← 0
-------------->     while length(haystack) - skip ≥ length(needle)
-------------->         haystack[skip:] -- substring starting with "skip". &haystack[skip] in C.
-------------->         if same(haystack[skip:], needle, length(needle))   
-------------->             return skip  
-------------->         skip ← skip + T[haystack[skip + length(needle) - 1]]
-------------->     return not-found
------------> Note: This was published by Nigel Horspool in 1980 as SBM.

-----------> Knuth–Morris–Pratt algorithm: 
------------> This substring search which bypasses reexamination of matched characters
--------------> The Knuth–Morris–Pratt string-searching algorithm (or KMP algorithm) searches for occurrences of a "word" W within a main "text string" S by employing the observation that when a mismatch occurs,
--------------> the word itself embodies sufficient information to determine where the next match could begin, thus bypassing re-examination of previously matched characters.
------------> Algorithm
--------------> For the moment, we assume the existence of a "partial match" table T, described below, 
----------------> which indicates where we need to look for the start of a new match when a mismatch is found. 
----------------> The entries of T are constructed so that if we have a match starting at S[m] that fails when comparing S[m + i] to W[i], 
----------------> then the next possible match will start at index m + i - T[i] in S (that is, T[i] is the amount of "backtracking" we need to do after a mismatch). 
----------------> This has two implications: first, T[0] = -1, which indicates that if W[0] is a mismatch, 
----------------> we cannot backtrack and must simply check the next character; and second, although the next possible match will begin at index m + i - T[i],
----------------> we need not actually check any of the T[i] characters after that, so that we continue searching from W[T[i]]. 
----------------> The following is a sample pseudocode implementation of the KMP search algorithm.
------------------> algorithm kmp_search:
------------------>     input:
------------------>         an array of characters, S (the text to be searched)
------------------>         an array of characters, W (the word sought)
------------------>     output:
------------------>         an array of integers, P (positions in S at which W is found)
------------------>         an integer, nP (number of positions)
------------------> 
------------------>     define variables:
------------------>         an integer, j ← 0 (the position of the current character in S)
------------------>         an integer, k ← 0 (the position of the current character in W)
------------------>         an array of integers, T (the table, computed elsewhere)
------------------>     let nP ← 0
------------------>     while j < length(S) do
------------------>         if W[k] = S[j] then
------------------>             let j ← j + 1
------------------>             let k ← k + 1
------------------>             if k = length(W) then
------------------>                 (occurrence found, if only first occurrence is needed, m ← j - k  may be returned here)
------------------>                 let P[nP] ← j - k, nP ← nP + 1
------------------>                 let k ← T[k] (T[length(W)] can't be -1)
------------------>         else
------------------>             let k ← T[k]
------------------>             if k < 0 then
------------------>                 let j ← j + 1
------------------>                 let k ← k + 1
--------------> The algorithm was conceived by James H. Morris and independently discovered by Donald Knuth "a few weeks later" from automata theory.
----------------> Morris and Vaughan Pratt published a technical report in 1970.
----------------> The three also published the algorithm jointly in 1977.
----------------> Independently, in 1969, Matiyasevich[4][5] di
----------------> scovered a similar algorithm, coded by a two-dimensional Turing machine, while studying a string-pattern-matching recognition problem over a binary alphabet. 
----------------> This was the first linear-time algorithm for string matching.

-----------> Rabin–Karp string search algorithm: 
------------> This searches multiple patterns efficiently
--------------> This uses hashing to find an exact match of a pattern string in a text. 
----------------> It uses a rolling hash to quickly filter out positions of the text that cannot match the pattern, and then checks for a match at the remaining positions. 
----------------> Generalizations of the same idea can be used to find more than one match of a single pattern, or to find matches for more than one pattern.
--------------> To find a single match of a single pattern, the expected time of the algorithm is linear in the combined length of the pattern and text, 
----------------> although its worst-case time complexity is the product of the two lengths. 
----------------> To find multiple matches, the expected time is linear in the input lengths, plus the combined length of all the matches, which could be greater than linear. 
----------------> In contrast, the Aho–Corasick algorithm can find all matches of multiple patterns in worst-case time 
----------------> and space linear in the input length and the number of matches (instead of the total length of the matches).
--------------> A practical application of the algorithm is detecting plagiarism. 
----------------> Given source material, the algorithm can rapidly search through a paper for instances of sentences from the source material, ignoring details such as case and punctuation. 
----------------> Because of the abundance of the sought strings, single-string searching algorithms are impractical. 
--------------> Algorithm
----------------> The algorithm is as shown:
------------------> function RabinKarp(string s[1..n], string pattern[1..m])
------------------>     hpattern := hash(pattern[1..m]);
------------------>     for i from 1 to n-m+1
------------------>         hs := hash(s[i..i+m-1])
------------------>         if hs = hpattern
------------------>             if s[i..i+m-1] = pattern[1..m]
------------------>                 return i
------------------>     return not found
----------------> Note: The Rabin–Karp algorithm or Karp–Rabin algorithm is a string-searching algorithm created by Richard M. Karp and Michael O. Rabin (1987).

-----------> Zhu–Takaoka string matching algorithm: 
------------> This a variant of Boyer–Moore.
------------> The Zhu–Takaoka string matching algorithm is a variant of the Boyer–Moore string search algorithm. 
--------------> It uses two consecutive text characters to compute the bad character shift. 
--------------> It is faster when the alphabet or pattern is small, but the skip table grows quickly, slowing the pre-processing phase. 

---------> Ukkonen's algorithm: 
------------> This a linear-time, online algorithm for constructing suffix trees
------------> This is online algorithm for constructing suffix trees.
--------------> The algorithm begins with an implicit suffix tree containing the first character of the string. 
--------------> Then it steps through the string, adding successive characters until the tree is complete. 
--------------> This order addition of characters gives Ukkonen's algorithm its "on-line" property. 
--------------> The original algorithm presented by Peter Weiner proceeded backward from the last character to the first one from the shortest to the longest suffix.
--------------> A simpler algorithm was found by Edward M. McCreight, going from the longest to the shortest suffix.[3]
------------> Note: This was proposed by Esko Ukkonen in 1995

---------> Matching wildcards
------------> This is an algorithm for matching wildcards (also known as globbing) is useful in comparing text strings that may contain wildcard syntax.
--------------> Common uses of these algorithms include command-line interfaces, 
--------------> e.g. the Bourne shell or Microsoft Windows command-line or text editor or file manager, as well as the interfaces for some search engines[4] and databases.
--------------> Wildcard matching is a subset of the problem of matching regular expressions and string matching in general.

-----------> Rich Salz' wildmat: 
------------> This a widely used open-source recursive algorithm
------------> Wildmat is a pattern matching library developed by Rich Salz. 
--------------> Based on the wildcard syntax already used in the Bourne shell, 
--------------> wildmat provides a uniform mechanism for matching patterns across applications with simpler syntax than that typically offered by regular expressions. 
--------------> Patterns are implicitly anchored at the beginning and end of each string when testing for a match.
------------> Note: In June 2019, Rich Salz released the original version of the now-defunct library on GitHub under a public domain dedication.

-----------> Krauss matching wildcards algorithm: 
------------> This an open-source non-recursive algorithm
------------> The Krauss wildcard-matching algorithm is a pattern matching algorithm. 
--------------> Based on the wildcard syntax in common use, e.g. in the Microsoft Windows command-line interface, 
--------------> the algorithm provides a non-recursive mechanism for matching patterns in software applications, 
--------------> based on syntax simpler than that typically offered by regular expressions. 



-> Computational mathematics



---> Chien search: 
-----> This a recursive algorithm for determining roots of polynomials defined over a finite field
-----> In abstract algebra, is a fast algorithm for determining roots of polynomials defined over a finite field. 
-------> Chien search is commonly used to find the roots of error-locator polynomials encountered in decoding Reed-Solomon codes and BCH codes. 
-----> Note: The Chien search, named after Robert Tienwen Chien

---> Schreier–Sims algorithm: 
-----> This is computing a base and strong generating set (BSGS) of a permutation group.
-----> The Schreier–Sims algorithm is an algorithm in computational group theory, named after the mathematicians Otto Schreier and Charles Sims. 
-------> This algorithm can find the order of a finite permutation group, test membership (is a given permutation contained in a group?), and many other tasks in polynomial time. 
-------> It was introduced by Sims in 1970, based on Schreier's subgroup lemma. 
-------> The timing was subsequently improved by Donald Knuth in 1991. 
-------> Later, an even faster randomized version of the algorithm was developed

---> Todd–Coxeter algorithm: 
-----> This is a procedure for generating cosets.
-----> This is an algorithm for solving the coset enumeration problem. 
-------> Given a presentation of a group G by generators and relations and a subgroup H of G, 
-------> the algorithm enumerates the cosets of H on G and describes the permutation representation of G on the space of the cosets (given by the left multiplication action). 
-------> If the order of a group G is relatively small and the subgroup H is known to be uncomplicated (for example, a cyclic group), 
-------> then the algorithm can be carried out by hand and gives a reasonable description of the group G. 
-------> Using their algorithm, Coxeter and Todd showed that certain systems of relations between generators of known groups are complete, i.e. constitute systems of defining relations.
-----> The Todd–Coxeter algorithm can be applied to infinite groups and is known to terminate in a finite number of steps, provided that the index of H in G is finite. 
-------> On the other hand, for a general pair consisting of a group presentation and a subgroup, 
-------> its running time is not bounded by any computable function of the index of the subgroup and the size of the input data. 
-----> Note: The Todd–Coxeter algorithm, created by J. A. Todd and H. S. M. Coxeter in 1936,

---> Computer algebra

-----> Buchberger's algorithm: 
-------> This finds a Gröbner basis.
-------> In the theory of multivariate polynomials, Buchberger's algorithm is a method for transforming a given set of polynomials into a Gröbner basis, 
---------> which is another set of polynomials that have the same common zeros and are more convenient for extracting information on these common zeros. 
---------> It was introduced by Bruno Buchberger simultaneously with the definition of Gröbner bases.
-------> Euclidean algorithm for polynomial Greatest common divisor computation 
---------> and Gaussian elimination of linear systems are special cases of Buchberger's algorithm 
---------> when the number of variables or the degrees of the polynomials are respectively equal to one.
-------> For other Gröbner basis algorithms, see Gröbner basis § Algorithms and implementations. 
---------> Algorithm
-----------> A crude version of this algorithm to find a basis for an ideal I of a polynomial ring R proceeds as follows:
-------------> Input A set of polynomials F that generates I
-------------> Output A Gröbner basis G for I
---------------> G := F
---------------> For every fi, fj in G, denote by gi the leading term of fi with respect to the given ordering, and by aij the least common multiple of gi and gj.
---------------> Choose two polynomials in G and let Sij = aij/ gi fi − aij/ gj fj (Note that the leading terms here will cancel by construction).
---------------> Reduce Sij, with the multivariate division algorithm relative to the set G until the result is not further reducible. If the result is non-zero, add it to G.
---------------> Repeat steps 2-4 until all possible pairs are considered, including those involving the new polynomials added in step 4.
---------------> Output G
---------> The polynomial Sij is commonly referred to as the S-polynomial, where S refers to subtraction (Buchberger) or Syzygy (others). 
-----------> The pair of polynomials with which it is associated is commonly referred to as critical pair.
---------> There are numerous ways to improve this algorithm beyond what has been stated above. 
-----------> For example, one could reduce all the new elements of F relative to each other before adding them. 
-----------> If the leading terms of fi and fj share no variables in common, 
-----------> then Sij will always reduce to 0 (if we use only fi and fj for reduction), so we needn't calculate it at all.
---------> The algorithm terminates because it is consistently increasing the size of the monomial ideal generated by the leading terms of our set F, 
-----------> and Dickson's lemma (or the Hilbert basis theorem) guarantees that any such ascending chain must eventually become constant. 

-----> Cantor–Zassenhaus algorithm: 
-------> This factors polynomials over finite fields
-------> In computational algebra, the Cantor–Zassenhaus algorithm is a method for factoring polynomials over finite fields (also called Galois fields).
-------> The algorithm consists mainly of exponentiation and polynomial GCD computations. It was invented by David G. Cantor and Hans Zassenhaus in 1981.
-------> It is arguably the dominant algorithm for solving the problem, having replaced the earlier Berlekamp's algorithm of 1967. 
---------> It is currently implemented in many computer algebra systems. 

-----> Faugère F4 algorithm: 
-------> This finds a Gröbner basis (also mentions the F5 algorithm)
-------> In computer algebra, the Faugère F4 algorithm, by Jean-Charles Faugère, 
---------> computes the Gröbner basis of an ideal of a multivariate polynomial ring. 
---------> The algorithm uses the same mathematical principles as the Buchberger algorithm, 
---------> but computes many normal forms in one go by forming a generally sparse matrix and using fast linear algebra to do the reductions in parallel.
-------> The Faugère F5 algorithm first calculates the Gröbner basis of a pair of generator polynomials of the ideal. 
---------> Then it uses this basis to reduce the size of the initial matrices of generators for the next larger basis:
-----------> If Gprev is an already computed Gröbner basis (f2, …, fm) and we want to compute a Gröbner basis of (f1) + Gprev 
-------------> then we will construct matrices whose rows are m f1 such that m is a monomial not divisible by the leading term of an element of Gprev.
-------> This strategy allows the algorithm to apply two new criteria based on what Faugère calls signatures of polynomials. 
---------> Thanks to these criteria, the algorithm can compute Gröbner bases for a large class of interesting polynomial systems, called regular sequences, 
---------> without ever simplifying a single polynomial to zero—the most time-consuming operation in algorithms that compute Gröbner bases. 
---------> It is also very effective for a large number of non-regular sequences. 

-----> Gosper's algorithm: 
-------> This finds sums of hypergeometric terms that are themselves hypergeometric terms
-------> Gosper's algorithm, due to Bill Gosper, is a procedure for finding sums of hypergeometric terms that are themselves hypergeometric terms. 
---------> That is: suppose one has a(1) + ... + a(n) = S(n) − S(0), where S(n) is a hypergeometric term (i.e., S(n + 1)/S(n) is a rational function of n); 
---------> then necessarily a(n) is itself a hypergeometric term, and given the formula for a(n) Gosper's algorithm finds that for S(n). 

-----> Knuth–Bendix completion algorithm: 
-------> This is for rewriting rule systems
-------> The Knuth–Bendix completion algorithm (named after Donald Knuth and Peter Bendix[1]) is a semi-decision algorithm 
---------> for transforming a set of equations (over terms) into a confluent term rewriting system. 
---------> When the algorithm succeeds, it effectively solves the word problem for the specified algebra.
-------> Buchberger's algorithm for computing Gröbner bases is a very similar algorithm. 
---------> Although developed independently, it may also be seen as the instantiation of Knuth–Bendix algorithm in the theory of polynomial rings. 

-----> Multivariate division algorithm: 
-------> This is for polynomials in several indeterminates.
-------> The concept of reduction, also called multivariate division or normal form computation, is central to Gröbner basis theory. 
---------> It is a multivariate generalization of the Euclidean division of univariate polynomials.
-------> In this section we suppose a fixed monomial ordering, which will not be defined explicitly.
-------> Given two polynomials f and g, one says that f is reducible by g if some monomial m in f is a multiple of the leading monomial lm(g) of g. 
---------> If m happens to be the leading monomial of f then one says that f is lead-reducible by g. If c is the coefficient of m in f and m = q lm(g), 
---------> the one-step reduction of f by g is the operation that associates to f the polynomial
-----------> red 1 ⁡ ( f , g ) = f − c lc ⁡ ( g ) q g.
-------> The main properties of this operation are that the resulting polynomial does not contain the monomial m
---------> and that the monomials greater than m (for the monomial ordering) remain unchanged. 
---------> This operation is not, in general, uniquely defined; if several monomials in f are multiples of lm(g), 
---------> then one may choose arbitrarily which one to reduce. In practice, it is better to choose the greatest one for the monomial ordering, 
---------> because otherwise subsequent reductions could reintroduce the monomial that has just been removed.
-------> Given a finite set G of polynomials, one says that f is reducible or lead-reducible by G if it is reducible or lead-reducible, respectively, by an element g of G. 
---------> If it is the case, then one defines red 1 ⁡ ( f , G ) = red 1 ⁡ ( f , g ) {\displaystyle \operatorname {red} _{1}(f,G)=\operatorname {red} _{1}(f,g)} \operatorname{red}_1(f,G)=\operatorname{red}_1(f,g).
---------> The (complete) reduction of f by G consists in applying iteratively this operator red1 until getting a polynomial red ⁡ (f, G) , which is irreducible by G. 
---------> It is called a normal form of f by G. In general this form is not uniquely defined (this is not a canonical form); this non-uniqueness is the starting point of Gröbner basis theory.
-------> For Gröbner basis computations, except at the end, it is not necessary to do a complete reduction: a lead-reduction is sufficient, which saves a large amount of computation.
---------> The definition of the reduction shows immediately that, if h is a normal form of f by G, then we have
--------->     f = h + summation of qg * g with g is an element of G
---------> where the qg are polynomials. 
-------> In the case of univariate polynomials, if G consists of a single element g, 
---------> then h is the remainder of the Euclidean division of f by g, qg is the quotient and the division algorithm is exactly the process of lead-reduction. 
---------> For this reason, some authors use the term multivariate division instead of reduction. 

-----> Pollard's kangaroo algorithm (also known as Pollard's lambda algorithm ): 
-------> This an algorithm for solving the discrete logarithm problem.
-------> In computational number theory and computational algebra, 
---------> Pollard's kangaroo algorithm (also Pollard's lambda algorithm, see Naming below) is an algorithm for solving the discrete logarithm problem. 
-------> Although Pollard described the application of his algorithm to the discrete logarithm problem in the multiplicative group of units modulo a prime p, 
---------> it is in fact a generic discrete logarithm algorithm—it will work in any finite cyclic group. 
-------> Note: The algorithm was introduced in 1978 by the number theorist J. M. Pollard, 
---------> in the same paper as his better-known Pollard's rho algorithm for solving the same problem. 

-----> Polynomial long division: 
-------> This an algorithm for dividing a polynomial by another polynomial of the same or lower degree
-------> In algebra, polynomial long division is an algorithm for dividing a polynomial by another polynomial of the same or lower degree, 
---------> a generalized version of the familiar arithmetic technique called long division. 
---------> It can be done easily by hand, because it separates an otherwise complex division problem into smaller ones. 
---------> Sometimes using a shorthand version called synthetic division is faster, with less writing and fewer calculations. 
---------> Another abbreviated method is polynomial short division (Blomqvist's method).
-------> Polynomial long division is an algorithm that implements the Euclidean division of polynomials, 
---------> which starting from two polynomials A (the dividend) and B (the divisor) produces, if B is not zero, a quotient Q and a remainder R such that
-----------> A = BQ + R,
---------> and either R = 0 or the degree of R is lower than the degree of B. These conditions uniquely define Q and R, 
---------> which means that Q and R do not depend on the method used to compute them.
-------> The result R = 0 occurs if and only if the polynomial A has B as a factor. 
---------> Thus long division is a means for testing whether one polynomial has another as a factor, and, if it does, for factoring it out. 
---------> For example, if a root r of A is known, it can be factored out by dividing A by (x – r). 

-----> Risch algorithm: 
-------> This an algorithm for the calculus operation of indefinite integration (i.e. finding antiderivatives)
-------> In symbolic computation, the Risch algorithm is a method of indefinite integration used in some computer algebra systems to find antiderivatives. 
-------> The algorithm transforms the problem of integration into a problem in algebra. 
---------> It is based on the form of the function being integrated and on methods for integrating rational functions, radicals, logarithms, and exponential functions. 
---------> Risch called it a decision procedure, because it is a method for deciding whether a function has an elementary function as an indefinite integral, and if it does, for determining that indefinite integral. 
---------> However, the algorithm does not always succeed in identifying whether or not the antiderivative of a given function in fact can be expressed in terms of elementary functions.
-------> The complete description of the Risch algorithm takes over 100 pages.
---------> The Risch–Norman algorithm is a simpler, faster, but less powerful variant that was developed in 1976 by Arthur Norman.
-------> Some significant progress has been made in computing the logarithmic part of a mixed transcendental-algebraic integral by Miller.
-------> Note: This is named after the American mathematician Robert Henry Risch, a specialist in computer algebra who developed it in 1968.

---> Geometry

-----> Closest pair problem: 
-------> This finds the pair of points (from a set of points) with the smallest distance between them
-------> The closest pair of points problem or closest pair problem is a problem of computational geometry: given n points in metric space, 
---------> find a pair of points with the smallest distance between them. 
---------> The closest pair problem for points in the Euclidean plane was among the first geometric problems that were treated at the origins 
---------> of the systematic study of the computational complexity of geometric algorithms. 

-----> Collision detection algorithms: 
-------> This checks for the collision or intersection of two given solids
-------> Collision detection is the computational problem of detecting the intersection of two or more objects. 
---------> Collision detection is a classic issue of computational geometry and has applications in various computing fields, 
---------> primarily in computer graphics, computer games, computer simulations, robotics and computational physics. 
---------> Collision detection algorithms can be divided into operating on 2D and 3D objects.

-----> Cone algorithm: 
-------> This identifies surface points.
---------> In computational geometry, the cone algorithm is an algorithm for identifying the particles that are near the surface of an object composed of discrete particles. 
---------> Its applications include computational surface science and computational nano science. 
---------> The cone algorithm was first described in a publication about nanogold in 2005.
-------> The cone algorithm works well with clusters in condensed phases, including solid and liquid phases. 
---------> It can handle the situations when one configuration includes multiple clusters or when holes exist inside clusters. 
---------> It can also be applied to a cluster iteratively to identify multiple sub-surface layers. 

-----> Convex hull algorithms: 
-------> This determines the convex hull of a set of points
-------> Algorithms that construct convex hulls of various objects have a broad range of applications in mathematics and computer science.
---------> In computational geometry, numerous algorithms are proposed for computing the convex hull of a finite set of points, with various computational complexities.
---------> Computing the convex hull means that a non-ambiguous and efficient representation of the required convex shape is constructed. 
---------> The complexity of the corresponding algorithms is usually estimated in terms of n, 
---------> the number of input points, and sometimes also in terms of h, the number of points on the convex hull. 

-------> Graham scan
---------> Graham's scan is a method of finding the convex hull of a finite set of points in the plane with time complexity O(n log n). 
-----------> It is named after Ronald Graham, who published the original algorithm in 1972.
-----------> The algorithm finds all vertices of the convex hull ordered along its boundary. 
-----------> It uses a stack to detect and remove concavities in the boundary efficiently.
---------> Pseudocode
-----------> The code below uses a function ccw: ccw > 0 if three points make a counter-clockwise turn, clockwise if ccw < 0, and collinear if ccw = 0. 
-------------> (In real applications, if the coordinates are arbitrary real numbers, 
-------------> the function requires exact comparison of floating-point numbers, 
-------------> and one has to beware of numeric singularities for "nearly" collinear points.)
-----------> Then let the result be stored in the stack.
-------------> let points be the list of points
-------------> let stack = empty_stack()
-------------> find the lowest y-coordinate and leftmost point, called P0
-------------> sort points by polar angle with P0, if several points have the same polar angle then only keep the farthest
-------------> for point in points:
------------->     # pop the last point from the stack if we turn clockwise to reach this point
------------->     while count stack > 1 and ccw(next_to_top(stack), top(stack), point) <= 0:
------------->         pop stack
------------->     push point to stack
-------------> end
-----------> Now the stack contains the convex hull, where the points are oriented counter-clockwise and P0 is the first point.
-----------> Here, next_to_top() is a function for returning the item one entry below the top of stack, without changing the stack, and similarly, top() for returning the topmost element. 

-------> Quickhull
---------> Quickhull is a method of computing the convex hull of a finite set of points in n-dimensional space. 
-----------> It uses a divide and conquer approach similar to that of quicksort, from which its name derives. 
-----------> Its worst case complexity for 2-dimensional and 3-dimensional space is considered to be O ( n log ⁡ ( r ) ) ,  w
-----------> here n is the number of input points and r is the number of processed points.
-----------> However, unlike quicksort, there is no obvious way to convert quickhull into a randomized algorithm. 
-----------> Nevertheless, there exist works from Smoothed Analysis which tell us that the 2-dimensional Quick hull algorithm has expected runtime O(n log(n)). 
-----------> Indeed, and related works show that the number of points on the convex hull of any randomly perturbed pointset with Gaussian noise is O(log ⁡ n) 
-----------> from which it follows that Quick hull (and many other algorithms) can only takes time O(n log ⁡ n) on any set of perturbed points.
---------> Algorithm
-----------> Under average circumstances the algorithm works quite well, but processing usually becomes slow in cases of high symmetry or points lying on the circumference of a circle. 
-------------> The algorithm can be broken down to the following steps:
---------------> (1) Find the points with minimum and maximum x coordinates, as these will always be part of the convex hull. 
-----------------> If many points with the same minimum/maximum x exist, use ones with minimum/maximum y correspondingly.
---------------> (2) Use the line formed by the two points to divide the set into two subsets of points, which will be processed recursively.
---------------> (3) Determine the point, on one side of the line, with the maximum distance from the line. 
-----------------> This point forms a triangle with those of the line.
---------------> (4) The points lying inside of that triangle cannot be part of the convex hull and can therefore be ignored in the next steps.
---------------> (5) Repeat the previous two steps on the two lines formed by the triangle (not the initial line).
---------------> (6) Keep on doing so until no more points are left, the recursion has come to an end and the points selected constitute the convex hull.
-----------> The problem is more complex in the higher-dimensional case, as the hull is built from many facets; 
-------------> the data structure needs to account for that and record the line/plane/hyperplane (ridge) shared by neighboring facets too. 
-------------> For d dimensions:[1]
---------------> (1) Pick d + 1 points from the set that do not share a plane or a hyperplane. This forms an initial hull with facets Fs[].
---------------> (2) For each F in Fs[], find all unassigned points that are "above" it, i.e. pointing away from the center of the hull, and add it to an "outside" set F.O associated with F.
---------------> (3) For each F with a non-empty F.O:
-----------------> (1) Find the point p with the maximum distance from F. We will add it to the hull.
-----------------> (2) Create a visible set V and initialize it to F. 
-------------------> Extend V in all directions for neighboring facets Fv until no further facets are visible from p. Fv being visible from p means that p is above Fv
-----------------> (3) The boundary of V then forms the set of horizon ridges H.
-----------------> (4) Let Fnew[] be the set of facets created from p and all ridges in H.
-----------------> (5) For each new facet in Fnew[], perform step (2) and initialize its own outside sets. 
-------------------> This time look only from points that are outside of a facet in V using their outside sets V[i].O, since we have only expanded in that direction.
-----------------> (6) Delete the now-internal facets in V from Fs[]. Add the new facets in Fnew[] to Fs[] and continue the iteration.
-----------> Pseudocode for 2D set of points
-------------> Input = a set S of n points 
-------------> Assume that there are at least 2 points in the input set S of points
-------------> function QuickHull(S) is
------------->     // Find convex hull from the set S of n points
------------->     Convex Hull := {} 
------------->     Find left and right most points, say A & B, and add A & B to convex hull 
------------->     Segment AB divides the remaining (n − 2) points into 2 groups S1 and S2 
------------->         where S1 are points in S that are on the right side of the oriented line from A to B, 
------------->         and S2 are points in S that are on the right side of the oriented line from B to A 
------------->     FindHull(S1, A, B) 
------------->     FindHull(S2, B, A) 
------------->     Output := Convex Hull
-------------> end function
-------------> function FindHull(Sk, P, Q) is
------------->     // Find points on convex hull from the set Sk of points 
------------->     // that are on the right side of the oriented line from P to Q
------------->     if Sk has no point then
------------->         return
------------->     From the given set of points in Sk, find farthest point, say C, from segment PQ 
------------->     Add point C to convex hull at the location between P and Q 
------------->     Three points P, Q, and C partition the remaining points of Sk into 3 subsets: S0, S1, and S2 
------------->         where S0 are points inside triangle PCQ, S1 are points on the right side of the oriented 
------------->         line from P to C, and S2 are points on the right side of the oriented line from C to Q. 
------------->     FindHull(S1, P, C) 
------------->     FindHull(S2, C, Q) 
-------------> end function
---------> N-dimensional Quickhull was invented in 1996 by C. Bradford Barber, David P. Dobkin, and Hannu Huhdanpaa.
-----------> It was an extension of Jonathan Scott Greenfield's 1990 planar Quickhull algorithm, although the 1996 authors did not know of his methods.
-----------> Instead, Barber et al describes it as a deterministic variant of Clarkson and Shor's 1989 algorithm.

-------> Gift wrapping algorithm or Jarvis march
-----------> In computational geometry, the gift wrapping algorithm is an algorithm for computing the convex hull of a given set of points. 
-----------> Algorithm
-------------> For the sake of simplicity, the description below assumes that the points are in general position, i.e., no three points are collinear. 
---------------> The algorithm may be easily modified to deal with collinearity, 
---------------> including the choice whether it should report only extreme points (vertices of the convex hull) or all points that lie on the convex hull. 
---------------> Also, the complete implementation must deal[how?] with degenerate cases when the convex hull has only 1 or 2 vertices,
---------------> as well as with the issues of limited arithmetic precision, both of computer computations and input data.
-------------> The gift wrapping algorithm begins with i=0 and a point p0 known to be on the convex hull, e.g., the leftmost point, 
---------------> and selects the point pi+1 such that all points are to the right of the line pi pi+1. 
---------------> This point may be found in O(n) time by comparing polar angles of all points with respect to point pi taken for the center of polar coordinates. 
---------------> Letting i=i+1, and repeating with until one reaches ph=p0 again yields the convex hull in h steps. 
---------------> In two dimensions, the gift wrapping algorithm is similar to the process of winding a string (or wrapping paper) around the set of points.
-------------> The approach can be extended to higher dimensions. 
-------------> Pseudocode
---------------> Jarvis's march computing the convex hull.
---------------> algorithm jarvis(S) is
--------------->     // S is the set of points
--------------->     // P will be the set of points which form the convex hull. Final set size is i.
--------------->     pointOnHull = leftmost point in S // which is guaranteed to be part of the CH(S)
--------------->     i := 0
--------------->     repeat
--------------->         P[i] := pointOnHull
--------------->         endpoint := S[0]      // initial endpoint for a candidate edge on the hull
--------------->         for j from 0 to |S| do
--------------->             // endpoint == pointOnHull is a rare case and can happen only when j == 1 and a better endpoint has not yet been set for the loop
--------------->             if (endpoint == pointOnHull) or (S[j] is on left of line from P[i] to endpoint) then
--------------->                 endpoint := S[j]   // found greater left turn, update endpoint
--------------->         i := i + 1
--------------->         pointOnHull = endpoint
--------------->     until endpoint = P[0]      // wrapped around to first hull point

-------> Chan's algorithm
---------> This is an optimal output-sensitive algorithm to compute the convex hull of a set P of n points, in 2- or 3-dimensional space. 
-----------> The algorithm takes O(n log h) time, where h is the number of vertices of the output (the convex hull). 
-----------> In the planar case, the algorithm combines an O(n log n) algorithm (Graham scan, for example) with Jarvis march O(nh)), in order to obtain an optimal O(n log h) time. 
-----------> Chan's algorithm is notable because it is much simpler than the Kirkpatrick–Seidel algorithm, and it naturally extends to 3-dimensional space. 
-----------> This paradigm has been independently developed by Frank Nielsen in his Ph.D. thesis.
-----------> Algorithm Overview
-------------> A single pass of the algorithm requires a parameter m which is between 0 and n (number of points of our set P). 
---------------> Ideally, m=h but h, the number of vertices in the output convex hull, is not known at the start.
--------------->  Multiple passes with increasing values of m are done which then terminates when m ≥ h(see below on choosing parameter m).
-------------> The algorithm starts by arbitrarily partitioning the set of points P into K = ceil( n / m) subsets (Qk) k = 1, 2, ... K  with at most m points each; notice that K = O(n/m).
-------------> For each subset Qk, it computes the convex hull, Ck, using an O(p log ⁡ p) algorithm (for example, Graham scan), 
---------------> where P is the number of points in the subset. As there are K  subsets of O(m) points each, this phase takes K ⋅ O ( m log ⁡ m ) = O ( n log ⁡ m ) time.
-------------> During the second phase, Jarvis's march is executed, making use of the precomputed (mini) convex hulls, (Ck) k = 1, 2, ... K. 
---------------> At each step in this Jarvis's march algorithm, we have a point pi in the convex hull (at the beginning, pi may be the point in P with the lowest y coordinate, 
---------------> which is guaranteed to be in the convex hull of P), and need to find a point pi+1 = f(pi, P) 
---------------> such that all other points of P are to the right of the line pi, pi+1, where the notation pi + 1 = f(pi , P)
---------------> simply means that the next point, that is pi + 1, is determined as a function of pi and P. 
---------------> The convex hull of the set Qk, Ck, is known and contains at most m points (listed in a clockwise or counter-clockwise order), 
---------------> which allows to compute f(pi, Qk) in O ( log ⁡ m ) time by binary search. 
---------------> Hence, the computation of f(pi, Qk) for all the k subsets can be done in O ( K log ⁡ m ) time. 
---------------> Then, we can determine f ( p i , P ) using the same technique as normally used in Jarvis's march, 
---------------> but only considering the points ( f ( p i , Q k ) ) 1 ≤ k ≤ K  (i.e. the points in the mini convex hulls) instead of the whole set P. 
---------------> For those points, one iteration of Jarvis's march is O ( K ) which is negligible compared to the computation for all subsets. 
---------------> Jarvis's march completes when the process has been repeated O(h) times (because, in the way Jarvis march works, 
---------------> after at most h iterations of its outermost loop, where h is the number of points in the convex hull of P, we must have found the convex hull), 
---------------> hence the second phase takes O(Kh log m) time, equivalent to O(n log h) time if m is close to h (see below the description of a strategy to choose m such that this is the case).
-----------> By running the two phases described above, the convex hull of n points is computed in O(n log h) time. 
---------> Note: This is called the Chan's algorithm named after Timothy M. Chan.

-------> Kirkpatrick–Seidel algorithm
---------> The Kirkpatrick–Seidel algorithm, proposed by its authors as a potential "ultimate planar convex hull algorithm".
-----------> This is an algorithm for computing the convex hull of a set of points in the plane, 
-----------> with O(n log ⁡ h)time complexity, where n is the number of input points and h is the number of points 
-----------> (non dominated or maximal points, as called in some texts) in the hull. 
-----------> Thus, the algorithm is output-sensitive: its running time depends on both the input size and the output size. 
-----------> Another output-sensitive algorithm, the gift wrapping algorithm, was known much earlier, 
-----------> but the Kirkpatrick–Seidel algorithm has an asymptotic running time that is significantly smaller and that always improves on the O(n log ⁡ n)  bounds of non-output-sensitive algorithms. 
---------> Although the algorithm is asymptotically optimal, it is not very practical for moderate-sized problems.
-----------> The basic idea of the algorithm is a kind of reversal of the divide-and-conquer algorithm 
-----------> for convex hulls of Preparata and Hong, dubbed "marriage-before-conquest" by the authors.
---------> The traditional divide-and-conquer algorithm splits the input points into two equal parts, e.g., by a vertical line, 
-----------> recursively finds convex hulls for the left and right subsets of the input, 
-----------> and then merges the two hulls into one by finding the "bridge edges", bitangents that connect the two hulls from above and below.
---------> The Kirkpatrick–Seidel algorithm splits the input as before, by finding the median of the x-coordinates of the input points. 
-----------> However, the algorithm reverses the order of the subsequent steps: its next step is to find the edges of the convex hull 
-----------> that intersect the vertical line defined by this median x-coordinate, which turns out to require linear time. 
-----------> The points on the left and right sides of the splitting line that cannot contribute to the eventual hull are discarded, 
-----------> and the algorithm proceeds recursively on the remaining points. 
-----------> In more detail, the algorithm performs a separate recursion for the upper and lower parts of the convex hull; 
-----------> in the recursion for the upper hull, the noncontributing points to be discarded are those below the bridge edge vertically, 
-----------> while in the recursion for the lower hull the points above the bridge edge vertically are discarded.
---------> At the ith level of the recursion, the algorithm solves at most 2^i subproblems, each of size at most n/2^i. 
-----------> The total number of subproblems considered is at most h, since each subproblem finds a new convex hull edge. 
-----------> The worst case occurs when no points can be discarded and the subproblems are as large as possible; that is, when there are exactly 2^i subproblems in each level of recursion up to level log2(h). 
-----------> For this worst case, there are O(log ⁡ h) levels of recursion and O(n) points considered within each level, 
-----------> so the total running time is O(n log ⁡ h) as stated. 
---------> The Kirkpatrick–Seidel algorithm is named after its inventors, David G. Kirkpatrick and Raimund Seidel.

-----> Euclidean distance transform: 
-------> This computes the distance between every point in a grid and a discrete collection of points.
-------> A distance transform, also known as distance map or distance field, is a derived representation of a digital image. 
---------> The choice of the term depends on the point of view on the object in question: whether the initial image is transformed into another representation, or it is simply endowed with an additional map or field.
-------> Distance fields can also be signed, in the case where it is important to distinguish whether the point is inside or outside of the shape.
---------> The map labels each pixel of the image with the distance to the nearest obstacle pixel. 
---------> A most common type of obstacle pixel is a boundary pixel in a binary image. 
-------> Usually the transform/map is qualified with the chosen metric. 
---------> For example, one may speak of Manhattan distance transform, if the underlying metric is Manhattan distance. 
---------> Common metrics are:
-----------> Euclidean distance
-----------> Taxicab geometry, also known as City block distance or Manhattan distance.
-----------> Chebyshev distance
-------> There are several algorithms to compute the distance transform for these different distance metrics, 
---------> however the computation of the exact Euclidean distance transform (EEDT) needs special treatment if it is computed on the image grid.
-------> Applications are digital image processing (e.g., blurring effects, skeletonizing), motion planning in robotics, medical image analysis for prenatal genetic testing, and even pathfinding. 
---------> Uniformly-sampled signed distance fields have been used for GPU-accelerated font smoothing, for example by Valve researchers.[4]
-------> Signed distance fields can also be used for (3D) solid modelling. Rendering on typical GPU hardware requires conversion to polygon meshes, e.g. by the marching cubes algorithm.[5] 

-----> Geometric hashing: 
-------> This is a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an affine transformation
-------> Geometric hashing is a method for efficiently finding two-dimensional objects represented by discrete points that have undergone an affine transformation, 
---------> though extensions exist to other object representations and transformations. In an off-line step, the objects are encoded by treating each pair of points as a geometric basis. 
---------> The remaining points can be represented in an invariant fashion with respect to this basis using two parameters. 
---------> For each point, its quantized transformed coordinates are stored in the hash table as a key, and indices of the basis points as a value. 
---------> Then a new pair of basis points is selected, and the process is repeated. In the on-line (recognition) step, randomly selected pairs of data points are considered as candidate bases. 
---------> For each candidate basis, the remaining data points are encoded according to the basis and possible correspondences from the object are found in the previously constructed table.
--------->  The candidate basis is accepted if a sufficiently large number of the data points index a consistent object basis.
-------> Geometric hashing was originally suggested in computer vision for object recognition in 2D and 3D, but later was applied to different problems such as structural alignment of proteins.[2][3] 

-----> Gilbert–Johnson–Keerthi distance algorithm: 
-------> This determines the smallest distance between two convex shapes.
-------> The Gilbert–Johnson–Keerthi distance algorithm is a method of determining the minimum distance between two convex sets. 
---------> Unlike many other distance algorithms, it does not require that the geometry data be stored in any specific format, 
---------> but instead relies solely on a support function to iteratively generate closer simplices to the correct answer 
---------> using the configuration space obstacle (CSO) of two convex shapes, more commonly known as the Minkowski difference.
-------> "Enhanced GJK" algorithms use edge information to speed up the algorithm by following edges when looking for the next simplex. 
---------> This improves performance substantially for polytopes with large numbers of vertices.
-------> GJK makes use of Johnson's distance subalgorithm, which computes in the general case the point of a tetrahedron closest to the origin, 
---------> but is known to suffer from numerical robustness problems. In 2017 Montanari, Petrinic, and Barbieri proposed a new subalgorithm 
---------> based on signed volumes which avoids the multiplication of potentially small quantities and achieved a speedup of 15% to 30%.
-------> GJK algorithms are often used incrementally in simulation systems and video games. 
---------> In this mode, the final simplex from a previous solution is used as the initial guess in the next iteration, or "frame". 
---------> If the positions in the new frame are close to those in the old frame, the algorithm will converge in one or two iterations. 
---------> This yields collision detection systems which operate in near-constant time.
-------> The algorithm's stability, speed, and small storage footprint make it popular for realtime collision detection, especially in physics engines for video games. 

-----> Jump-and-Walk algorithm: 
-------> This an algorithm for point location in triangulations
-------> Jump-and-Walk is an algorithm for point location in triangulations (though most of the theoretical analysis were performed in 2D and 3D random Delaunay triangulations). 
---------> Surprisingly, the algorithm does not need any preprocessing or complex data structures except some simple representation of the triangulation itself. 
---------> The predecessor of Jump-and-Walk was due to Lawson (1977) and Green and Sibson (1978), which picks a random starting point S 
---------> and then walks from S toward the query point Q one triangle at a time. But no theoretical analysis was known for these predecessors until after mid-1990s.
-------> Jump-and-Walk picks a small group of sample points and starts the walk from the sample point which is the closest to Q until the simplex containing Q is found. 
---------> The algorithm was a folklore in practice for some time, and the formal presentation of the algorithm 
---------> and the analysis of its performance on 2D random Delaunay triangulation was done by Devroye, Mucke and Zhu in mid-1990s (the paper appeared in Algorithmica, 1998). 
---------> The analysis on 3D random Delaunay triangulation was done by Mucke, Saias and Zhu (ACM Symposium of Computational Geometry, 1996). 
---------> In both cases, a boundary condition was assumed, namely, Q must be slightly away from the boundary of the convex domain where the vertices of the random Delaunay triangulation are drawn. 
---------> In 2004, Devroye, Lemaire and Moreau showed that in 2D the boundary condition can be withdrawn (the paper appeared in Computational Geometry: Theory and Applications, 2004).
-------> Jump-and-Walk has been used in many famous software packages, e.g., QHULL, Triangle and CGAL. 

-----> Laplacian smoothing: 
-------> This an algorithm to smooth a polygonal mesh
-------> Laplacian smoothing is an algorithm to smooth a polygonal mesh.
---------> For each vertex in a mesh, a new position is chosen based on local information (such as the position of neighbours) and the vertex is moved there. 
---------> In the case that a mesh is topologically a rectangular grid (that is, each internal vertex is connected to four neighbours) then this operation produces the Laplacian of the mesh.
-------> More formally, the smoothing operation may be described per-vertex as:
---------> xi = 1/N summation of xj from 1 to N
-------> Where N is the number of adjacent vertices to node i, xj is the position of the jth adjacent vertex and xi is the new position for node i.

-----> Line segment intersection: 

-------> Bentley–Ottmann algorithm
---------> In computational geometry, the Bentley–Ottmann algorithm is a sweep line algorithm for listing all crossings in a set of line segments, 
-----------> i.e. it finds the intersection points (or, simply, intersections) of line segments. 
-----------> It extends the Shamos–Hoey algorithm, a similar previous algorithm for testing whether or not a set of line segments has any crossings. 
-----------> For an input consisting of N line segments with k crossings (or intersections), the Bentley–Ottmann algorithm takes time O((n+k) log ⁡ n). 
-----------> In cases where k = o( n^2 log ⁡ n ), this is an improvement on a naïve algorithm that tests every pair of segments, which takes Θ(n^2).
---------> The algorithm was initially developed by Jon Bentley and Thomas Ottmann (1979); it is described in more detail in the textbooks Preparata & Shamos (1985), O'Rourke (1998), and de Berg et al. (2000).
-----------> Although asymptotically faster algorithms are now known by Chazelle & Edelsbrunner (1992) and Balaban (1995), 
-----------> the Bentley–Ottmann algorithm remains a practical choice due to its simplicity and low memory requirements[citation needed]. 
---------> Overall strategy
-----------> The main idea of the Bentley–Ottmann algorithm is to use a sweep line approach, in which a vertical line L moves from left to right (or, e.g., from top to bottom) across the plane, 
-------------> intersecting the input line segments in sequence as it moves.
-------------> The algorithm is described most easily in its general position, meaning:
---------------> No two line segment endpoints or crossings have the same x-coordinate
---------------> No line segment endpoint lies upon another line segment
---------------> No three line segments intersect at a single point.
-----------> In such a case, L will always intersect the input line segments in a set of points whose vertical ordering changes only at a finite set of discrete events. 
-------------> Specifically, a discrete event can either be associated with an endpoint (left or right) of a line-segment or intersection point of two line-segments. 
-------------> Thus, the continuous motion of L can be broken down into a finite sequence of steps, and simulated by an algorithm that runs in a finite amount of time.
-----------> There are two types of events that may happen during the course of this simulation. 
-------------> When L sweeps across an endpoint of a line segment s, the intersection of L with s is added to or removed from the vertically ordered set of intersection points. 
-------------> These events are easy to predict, as the endpoints are known already from the input to the algorithm. 
-------------> The remaining events occur when L sweeps across a crossing between (or intersection of) two line segments s and t. 
-------------> These events may also be predicted from the fact that, just prior to the event, 
-------------> the points of intersection of L with s and t are adjacent in the vertical ordering of the intersection points.
-----------> The Bentley–Ottmann algorithm itself maintains data structures representing the current vertical ordering 
-------------> of the intersection points of the sweep line with the input line segments, 
-------------> and a collection of potential future events formed by adjacent pairs of intersection points. 
-------------> It processes each event in turn, updating its data structures to represent the new set of intersection points. 
---------> Detailed algorithm
-----------> The Bentley–Ottmann algorithm performs the following steps.
-------------> (1) Initialize a priority queue Q of potential future events, each associated with a point in the plane and prioritized by the x-coordinate of the point. 
---------------> So, initially, Q contains an event for each of the endpoints of the input segments.
-------------> (2) Initialize a self-balancing binary search tree T of the line segments that cross the sweep line L, ordered by the y-coordinates of the crossing points. 
---------------> Initially, T is empty. (Even though the line sweep L is not explicitly represented, it may be helpful to imagine it as a vertical line which, initially, is at the left of all input segments.)
-------------> (3) While Q is nonempty, find and remove the event from Q associated with a point p with minimum x-coordinate. 
---------------> Determine what type of event this is and process it according to the following case analysis:
-----------------> If p is the left endpoint of a line segment s, insert s into T. Find the line-segments r and t that are respectively immediately above and below s in T (if they exist); 
-------------------> if the crossing of r and t (the neighbours of s in the status data structure) forms a potential future event in the event queue, 
-------------------> remove this possible future event from the event queue. If s crosses r or t, add those crossing points as potential future events in the event queue.
-----------------> If p is the right endpoint of a line segment s, remove s from T. 
-------------------> Find the segments r and t that (prior to the removal of s) were respectively immediately above and below it in T (if they exist). 
-------------------> If r and t cross, add that crossing point as a potential future event in the event queue.
-----------------> If p is the crossing point of two segments s and t (with s below t to the left of the crossing), 
-------------------> swap the positions of s and t in T. After the swap, find the segments r and u (if they exist) that are immediately below and above t and s, respectively. 
-------------------> Remove any crossing points rs (i.e. a crossing point between r and s) and tu (i.e. a crossing point between t and u) from the event queue, 
-------------------> and, if r and t cross or s and u cross, add those crossing points to the event queue.

-------> Shamos–Hoey algorithm

-----> Minimum bounding box algorithms: 
-------> This find the oriented minimum bounding box enclosing a set of points
---------> In computational geometry, the smallest enclosing box problem is that of finding the oriented minimum bounding box enclosing a set of points. 
---------> It is a type of bounding volume. "Smallest" may refer to volume, area, perimeter, etc. of the box.
-------> It is sufficient to find the smallest enclosing box for the convex hull of the objects in question. 
---------> It is straightforward to find the smallest enclosing box that has sides parallel to the coordinate axes; 
---------> the difficult part of the problem is to determine the orientation of the box. 

-----> Nearest neighbor search: 
-------> This find the nearest point or points to a query point
---------> Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. 
---------> Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values.
-------> Formally, the nearest-neighbor (NN) search problem is defined as follows: 
---------> given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. 
---------> Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, 
---------> referring to an application of assigning to a residence the nearest post office. 
---------> A direct generalization of this problem is a k-NN search, where we need to find the k closest points.
-------> Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. 
---------> Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, 
---------> Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.
-------> Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. 
---------> Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values.
-------> Formally, the nearest-neighbor (NN) search problem is defined as follows: 
---------> given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. 
---------> Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, 
---------> referring to an application of assigning to a residence the nearest post office. 
---------> A direct generalization of this problem is a k-NN search, where we need to find the k closest points.
-------> Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. 
---------> Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, 
---------> Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. 
---------> One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.[1] 

-----> Point in polygon algorithms: 
-------> This tests whether a given point lies within a given polygon
-------> In computational geometry, the point-in-polygon (PIP) problem asks whether a given point in the plane lies inside, outside, or on the boundary of a polygon. 
---------> It is a special case of point location problems and finds applications in areas that deal with processing geometrical data, 
---------> such as computer graphics, computer vision, geographic information systems (GIS), motion planning, and computer-aided design (CAD).
-------> An early description of the problem in computer graphics shows two common approaches (ray casting and angle summation) in use as early as 1974.
-------> An attempt of computer graphics veterans to trace the history of the problem and some tricks for its solution can be found in an issue of the Ray Tracing News.

-----> Point set registration algorithms: 
-------> This finds the transformation between two point sets to optimally align them.
-------> In computer vision, pattern recognition, and robotics, point-set registration, also known as point-cloud registration or scan matching, 
---------> is the process of finding a spatial transformation (e.g., scaling, rotation and translation) that aligns two point clouds. 
---------> The purpose of finding such a transformation includes merging multiple data sets into a globally consistent model (or coordinate frame), 
---------> and mapping a new measurement to a known data set to identify features or to estimate its pose. 
---------> Raw 3D point cloud data are typically obtained from Lidars and RGB-D cameras. 
---------> 3D point clouds can also be generated from computer vision algorithms such as triangulation, bundle adjustment, 
---------> and more recently, monocular image depth estimation using deep learning. 
---------> For 2D point set registration used in image processing and feature-based image registration, 
---------> a point set may be 2D pixel coordinates obtained by feature extraction from an image, for example corner detection. 
---------> Point cloud registration has extensive applications in autonomous driving, motion estimation and 3D reconstruction, 
---------> object detection and pose estimation, robotic manipulation, simultaneous localization and mapping (SLAM), panorama stitching, virtual and augmented reality, and medical imaging.
-------> As a special case, registration of two point sets that only differ by a 3D rotation (i.e., there is no scaling and translation), is called the Wahba Problem and also related to the orthogonal procrustes problem. 

-----> Rotating calipers: 
-------> This determines all antipodal pairs of points and vertices on a convex polygon or convex hull.
-------> In computational geometry, the method of rotating calipers is an algorithm design technique 
---------> that can be used to solve optimization problems including finding the width or diameter of a set of points.
-------> The method is so named because the idea is analogous to rotating a spring-loaded vernier caliper around the outside of a convex polygon. 
---------> Every time one blade of the caliper lies flat against an edge of the polygon, it forms an antipodal pair with the point or edge touching the opposite blade. 
---------> The complete "rotation" of the caliper around the polygon detects all antipodal pairs; the set of all pairs, viewed as a graph, forms a thrackle. 
---------> The method of rotating calipers can be interpreted as the projective dual of a sweep line algorithm in which the sweep is across slopes of lines rather than across x- or y-coordinates of points.

-----> Shoelace algorithm: 
-------> This determines the area of a polygon whose vertices are described by ordered pairs in the plane
-------> The shoelace formula, shoelace algorithm, or shoelace method (also known as Gauss's area formula and the surveyor's formula).
---------> This is a mathematical algorithm to determine the area of a simple polygon whose vertices are described by their Cartesian coordinates in the plane.
---------> It is called the shoelace formula because of the constant cross-multiplying for the coordinates making up the polygon, like threading shoelaces. 
---------> It has applications in surveying and forestry, among other areas.
-------> The formula was described by Albrecht Ludwig Friedrich Meister (1724–1788) in 1769[4] 
---------> and is based on the trapezoid formula which was described by Carl Friedrich Gauss and C.G.J. Jacobi. 
---------> The triangle form of the area formula can be considered to be a special case of Green's theorem.
-------> The area formula can also be applied to self-overlapping polygons since the meaning of area is still clear even though self-overlapping polygons are not generally simple. 
---------> Furthermore, a self-overlapping polygon can have multiple "interpretations" but the Shoelace formula can be used to show that the polygon's area is the same regardless of the interpretation.

-----> Triangulation
-------> In geometry, a triangulation is a subdivision of a planar object into triangles, 
---------> and by extension the subdivision of a higher-dimension geometric object into simplices. 
---------> Triangulations of a three-dimensional volume would involve subdividing it into tetrahedra packed together.
-------> In most instances, the triangles of a triangulation are required to meet edge-to-edge and vertex-to-vertex. 

-------> Delaunay triangulation
---------> In mathematics and computational geometry, a Delaunay triangulation (also known as a Delone triangulation) 
-----------> for a given set P of discrete points in a general position is a triangulation DT(P) such that no point in P is inside the circumcircle of any triangle in DT(P). 
-----------> Delaunay triangulations maximize the minimum of all the angles of the triangles in the triangulation; they tend to avoid sliver triangles. 
-----------> The triangulation is named after Boris Delaunay for his work on this topic from 1934.[1]
---------> For a set of points on the same line there is no Delaunay triangulation (the notion of triangulation is degenerate for this case). 
-----------> For four or more points on the same circle (e.g., the vertices of a rectangle) the Delaunay triangulation is not unique: 
-----------> each of the two possible triangulations that split the quadrangle into two triangles satisfies the "Delaunay condition", i.e., 
-----------> the requirement that the circumcircles of all triangles have empty interiors.
---------> By considering circumscribed spheres, the notion of Delaunay triangulation extends to three and higher dimensions. 
-----------> Generalizations are possible to metrics other than Euclidean distance. 
-----------> However, in these cases a Delaunay triangulation is not guaranteed to exist or be unique. 

---------> Ruppert's algorithm (also known as Delaunay refinement): 
-----------> This creates quality Delaunay triangulations
---------> Ruppert's algorithm takes a planar straight-line graph (or in dimension higher than two a piecewise linear system) 
-----------> and returns a conforming Delaunay triangulation of only quality triangles. A triangle is considered poor-quality if it has a circumradius to shortest edge ratio larger than some prescribed threshold. 
-----------> Discovered by Jim Ruppert in the early 1990s,[4] "Ruppert's algorithm for two-dimensional quality mesh generation is perhaps the first theoretically guaranteed meshing algorithm to be truly satisfactory in practice."
---------> Motivation
-----------> When doing computer simulations such as computational fluid dynamics, one starts with a model such as a 2D outline of a wing section. 
-------------> The input to a 2D finite element method needs to be in the form of triangles that fill all space, and each triangle to be filled with one kind of material – in this example, either "air" or "wing". 
-------------> Long, skinny triangles cannot be simulated accurately. The simulation time is generally proportional to the number of triangles, 
-------------> and so one wants to minimize the number of triangles, while still using enough triangles to give reasonably accurate results – typically by using an unstructured grid. 
-------------> The computer uses Ruppert's algorithm (or some similar meshing algorithm) to convert the polygonal model into triangles suitable for the finite element method.
---------> Algorithm
-----------> The algorithm begins with a Delaunay triangulation of the input vertices and then consists of two main operations.
-------------> The midpoint of a segment with non-empty diametral circles is inserted into the triangulation.
-------------> The circumcenter of a poor-quality triangle is inserted into the triangulation, 
---------------> unless this circumcenter lies in the diametral circle of some segment. In this case, the encroached segment is split instead.
-----------> These operations are repeated until no poor-quality triangles exist and all segments are not encroached.
-----------> Pseudocode
-------------> function Ruppert(points, segments, threshold) is
------------->     T := DelaunayTriangulation(points)
------------->     Q := the set of encroached segments and poor quality triangles
-------------> 
------------->     while Q is not empty:                 // The main loop
------------->         if Q contains a segment s:
------------->             insert the midpoint of s into T
------------->         else Q contains poor quality triangle t:
------------->             if the circumcenter of t encroaches a segment s:
------------->                 add s to Q;
------------->             else:
------------->                 insert the circumcenter of t into T
------------->             end if
------------->         end if
------------->         update Q
------------->     end while
-------------> 
------------->     return T
-------------> end Ruppert.

---------> Chew's second algorithm: 
-----------> This creates quality constrained Delaunay triangulations
-----------> Chew's second algorithm takes a piecewise linear system (PLS) and returns a constrained Delaunay triangulation of only quality triangles where quality is defined by the minimum angle in a triangle. 
-------------> Developed by L. Paul Chew for meshing surfaces embedded in three-dimensional space, Chew's second algorithm has been adopted as a two-dimensional mesh generator 
-------------> due to practical advantages over Ruppert's algorithm in certain cases and is the default quality mesh generator implemented in the freely available Triangle package.
-------------> Chew's second algorithm is guaranteed to terminate and produce a local feature size-graded meshes with minimum angle up to about 28.6 degrees.
-----------> The algorithm begins with a constrained Delaunay triangulation of the input vertices. 
-------------> At each step, the circumcenter of a poor-quality triangle is inserted into the triangulation with one exception: 
-------------> If the circumcenter lies on the opposite side of an input segment as the poor quality triangle, 
-------------> the midpoint of the segment is inserted. 
-------------> Moreover, any previously inserted circumcenters inside the diametral ball of the original segment (before it is split) are removed from the triangulation. 
-------------> Circumcenter insertion is repeated until no poor-quality triangles exist. 

-------> Marching triangles: 
---------> This reconstructs two-dimensional surface geometry from an unstructured point cloud
---------> In computer graphics, the problem of transforming a cloud of points on the surface of 
-----------> a three-dimensional object into a polygon mesh for the object can be solved by a technique called marching triangles. 
-----------> This provides a faster alternative to other methods for the same problem of surface reconstruction, based on Delaunay triangulation.

-------> Polygon triangulation algorithms: 
---------> This decomposes a polygon into a set of triangles
---------> In computational geometry, polygon triangulation is the partition of a polygonal area (simple polygon) P into a set of triangles,
-----------> i.e., finding a set of triangles with pairwise non-intersecting interiors whose union is P.
---------> Triangulations may be viewed as special cases of planar straight-line graphs. 
-----------> When there are no holes or added points, triangulations form maximal outerplanar graphs. 

-------> Voronoi diagrams, 
---------> This geometric dual of Delaunay triangulation
---------> In mathematics, a Voronoi diagram is a partition of a plane into regions close to each of a given set of objects. 
-----------> In the simplest case, these objects are just finitely many points in the plane (called seeds, sites, or generators). 
-----------> For each seed there is a corresponding region, called a Voronoi cell, consisting of all points of the plane closer to that seed than to any other. 
-----------> The Voronoi diagram of a set of points is dual to its Delaunay triangulation.
---------> The Voronoi diagram is named after Georgy Voronoy, 
-----------> and is also called a Voronoi tessellation, a Voronoi decomposition, a Voronoi partition, or a Dirichlet tessellation (after Peter Gustav Lejeune Dirichlet). 
-----------> Voronoi cells are also known as Thiessen polygons.
-----------> Voronoi diagrams have practical and theoretical applications in many fields, mainly in science and technology, but also in visual art.

---------> Bowyer–Watson algorithm: 
-----------> This creates voronoi diagram in any number of dimensions
-----------> In computational geometry, the Bowyer–Watson algorithm is a method for computing the Delaunay triangulation of a finite set of points in any number of dimensions. 
-------------> The algorithm can be also used to obtain a Voronoi diagram of the points, which is the dual graph of the Delaunay triangulation. 
-----------> Description
-------------> The Bowyer–Watson algorithm is an incremental algorithm. 
------------==-> It works by adding points, one at a time, to a valid Delaunay triangulation of a subset of the desired points. 
------------==-> After every insertion, any triangles whose circumcircles contain the new point are deleted, l
------------==-> eaving a star-shaped polygonal hole which is then re-triangulated using the new point. 
------------==-> By using the connectivity of the triangulation to efficiently locate triangles to remove, the algorithm can take O(N log N) operations to triangulate N points, 
------------==-> although special degenerate cases exist where this goes up to O(N2).
-----------> Pseudocode
-------------> The following pseudocode describes a basic implementation of the Bowyer-Watson algorithm. 
---------------> Its time complexity is O(n^2). 
---------------> Efficiency can be improved in a number of ways. 
---------------> For example, the triangle connectivity can be used to locate the triangles which contain the new point in their circumcircle, without having to check all of the triangles - 
---------------> by doing so we can decrease time complexity to O(n log n). 
---------------> Pre-computing the circumcircles can save time at the expense of additional memory usage. 
---------------> And if the points are uniformly distributed, sorting them along a space filling Hilbert curve prior to insertion can also speed point location.
-----------------> function BowyerWatson (pointList)
----------------->    // pointList is a set of coordinates defining the points to be triangulated
----------------->    triangulation := empty triangle mesh data structure
----------------->    add super-triangle to triangulation // must be large enough to completely contain all the points in pointList
----------------->    for each point in pointList do // add all the points one at a time to the triangulation
----------------->       badTriangles := empty set
----------------->       for each triangle in triangulation do // first find all the triangles that are no longer valid due to the insertion
----------------->          if point is inside circumcircle of triangle
----------------->             add triangle to badTriangles
----------------->       polygon := empty set
----------------->       for each triangle in badTriangles do // find the boundary of the polygonal hole
----------------->          for each edge in triangle do
----------------->             if edge is not shared by any other triangles in badTriangles
----------------->                add edge to polygon
----------------->       for each triangle in badTriangles do // remove them from the data structure
----------------->          remove triangle from triangulation
----------------->       for each edge in polygon do // re-triangulate the polygonal hole
----------------->          newTri := form a triangle from edge to point
----------------->          add newTri to triangulation
----------------->    for each triangle in triangulation // done inserting points, now clean up
----------------->       if triangle contains a vertex from original super-triangle
----------------->          remove triangle from triangulation
----------------->    return triangulation

---------> Fortune's Algorithm: 
-----------> This creates voronoi diagram.
-----------> Fortune's algorithm is a sweep line algorithm for generating a Voronoi diagram from a set of points in a plane using O(n log n) time and O(n) space.
-------------> It was originally published by Steven Fortune in 1986 in his paper "A sweepline algorithm for Voronoi diagrams."
-----------> Algorithm description
-------------> The algorithm maintains both a sweep line and a beach line, which both move through the plane as the algorithm progresses. 
---------------> The sweep line is a straight line, which we may by convention assume to be vertical and moving left to right across the plane. 
---------------> At any time during the algorithm, the input points left of the sweep line will have been incorporated into the Voronoi diagram, 
---------------> while the points right of the sweep line will not have been considered yet. 
---------------> The beach line is not a straight line, but a complicated, piecewise curve to the left of the sweep line, composed of pieces of parabolas; 
---------------> it divides the portion of the plane within which the Voronoi diagram can be known, regardless of what other points might be right of the sweep line, from the rest of the plane. 
---------------> For each point left of the sweep line, one can define a parabola of points equidistant from that point and from the sweep line; 
---------------> the beach line is the boundary of the union of these parabolas. 
---------------> As the sweep line progresses, the vertices of the beach line, at which two parabolas cross, trace out the edges of the Voronoi diagram. 
---------------> The beach line progresses by keeping each parabola base exactly half way between the points initially swept over with the sweep line, 
---------------> and the new position of the sweep line. Mathematically, this means each parabola is formed by using the sweep line as the directrix and the input point as the focus.
-------------> The algorithm maintains as data structures a binary search tree describing the combinatorial structure of the beach line, 
---------------> and a priority queue listing potential future events that could change the beach line structure. 
---------------> These events include the addition of another parabola to the beach line (when the sweep line crosses another input point) 
---------------> and the removal of a curve from the beach line (when the sweep line becomes tangent to a circle through some three input points whose parabolas form consecutive segments of the beach line). 
---------------> Each such event may be prioritized by the x-coordinate of the sweep line at the point the event occurs. 
---------------> The algorithm itself then consists of repeatedly removing the next event from the priority queue, finding the changes the event causes in the beach line, and updating the data structures.
-------------> As there are O(n) events to process (each being associated with some feature of the Voronoi diagram) 
---------------> and O(log n) time to process an event (each consisting of a constant number of binary search tree and priority queue operations) the total time is O(n log n). 

-------> Quasitriangulation
---------> A quasi-triangulation is a subdivision of a geometric object into simplices, where vertices are not points but arbitrary sloped line segments.
---------> This division is not a triangulation in the geometric sense. 
---------> It is a topological triangulation, however. 
---------> A quasi-triangulation may have some of the characteristics of a Delaunay triangulation. 



-> Number theoretic algorithms



---> Binary GCD algorithm: 
-----> This is an efficient way of calculating GCD.
-------> The binary GCD algorithm, also known as Stein's algorithm or the binary Euclidean algorithm, is an algorithm that computes the greatest common divisor of two nonnegative integers. 
-------> Stein's algorithm uses simpler arithmetic operations than the conventional Euclidean algorithm; it replaces division with arithmetic shifts, comparisons, and subtraction.
-----> Although the algorithm in its contemporary form was first published by the Israeli physicist and programmer Josef Stein in 1967, it may have been known by the 2nd century BCE, in ancient China.[4][5] 
-----> Algorithm
-------> The algorithm reduces the problem of finding the GCD of two nonnegative numbers v and u by repeatedly applying these identities:
---------> gcd(0, v) = v, because everything divides zero, and v is the largest number that divides v. Similarly, gcd(u, 0) = u.
---------> gcd(2u, 2v) = 2·gcd(u, v)
---------> gcd(2u, v) = gcd(u, v), if v is odd (2 is not a common divisor). Similarly, gcd(u, 2v) = gcd(u, v) if u is odd.
---------> gcd(u, v) = gcd(|u − v|, min(u, v)), if u and v are both odd.
-----> Implementation
-------> While the above description of the algorithm is mathematically-correct, performant software implementations typically differ from it in a few notable ways:
---------> eschewing trial division by 2 in favour of a single bitshift and the count trailing zeros primitive; 
-----------> this is functionally equivalent to repeatedly applying identity 3, but much faster;
---------> expressing the algorithm iteratively rather than recursively: the resulting implementation can be laid out to avoid repeated work, 
-----------> invoking identity 2 at the start and maintaining as invariant that both numbers are odd upon entering the loop, which only needs to implement identities 3 and 4;
---------> making the loop's body branch-free except for its exit condition (v == 0): in the example below,
-----------> the exchange of u and v (ensuring u ≤ v) compiles down to conditional moves;
-----------> hard-to-predict branches can have a large, negative impact on performance.[7][8]
-------> Following is an implementation of the algorithm in Rust exemplifying those differences, adapted from uutils:
---------> pub fn gcd(mut u: u64, mut v: u64) -> u64 {
--------->     use std::cmp::min;
--------->     use std::mem::swap;
--------->     // Base cases: gcd(n, 0) = gcd(0, n) = n
--------->     if u == 0 {
--------->         return v;
--------->     } else if v == 0 {
--------->         return u;
--------->     }
--------->     // Using identities 2 and 3:
--------->     // gcd(2ⁱ u, 2ʲ v) = 2ᵏ gcd(u, v) with u, v odd and k = min(i, j)
--------->     // 2ᵏ is the greatest power of two that divides both u and v
--------->     let i = u.trailing_zeros();  u >>= i;
--------->     let j = v.trailing_zeros();  v >>= j;
--------->     let k = min(i, j);
--------->     loop {
--------->         /// u and v are odd at the start of the loop
--------->         debug_assert!(u % 2 == 1, "u = {} is even", u);
--------->         debug_assert!(v % 2 == 1, "v = {} is even", v);
--------->         // Swap if necessary so u <= v
--------->         if u > v {
--------->             swap(&mut u, &mut v);
--------->         }
--------->         /// u and v are still both odd after (potentially) swapping
--------->         // Using identity 4 (gcd(u, v) = gcd(|v-u|, min(u, v))
--------->         v -= u;
--------->         /// v is now even, but u is unchanged (and odd)
--------->         // Identity 1: gcd(u, 0) = u
--------->         // The shift by k is necessary to add back the 2ᵏ factor that was removed before the loop
--------->         if v == 0 {
--------->             return u << k;
--------->         }
--------->         // Identity 3: gcd(u, 2ʲ v) = gcd(u, v) (u is known to be odd)
--------->         v >>= v.trailing_zeros();
--------->         /// v is now odd again
--------->     }
---------> }


---> Booth's multiplication algorithm
-----> Booth's multiplication algorithm is a multiplication algorithm that multiplies two signed binary numbers in two's complement notation. 
-------> The algorithm was invented by Andrew Donald Booth in 1950 while doing research on crystallography at Birkbeck College in Bloomsbury, London.
-------> Booth's algorithm is of interest in the study of computer architecture. 
-----> A typical implementation
-------> Booth's algorithm can be implemented by repeatedly adding (with ordinary unsigned binary addition) one of two predetermined values A and S to a product P, 
---------> then performing a rightward arithmetic shift on P. Let m and r be the multiplicand and multiplier, respectively; and let x and y represent the number of bits in m and r.
-----------> (1) Determine the values of A and S, and the initial value of P. All of these numbers should have a length equal to (x + y + 1).
-------------> (1) A: Fill the most significant (leftmost) bits with the value of m. Fill the remaining (y + 1) bits with zeros.
-------------> (2) S: Fill the most significant bits with the value of (−m) in two's complement notation. Fill the remaining (y + 1) bits with zeros.
-------------> (3) P: Fill the most significant x bits with zeros. To the right of this, append the value of r. Fill the least significant (rightmost) bit with a zero.
-----------> (2) Determine the two least significant (rightmost) bits of P.
-------------> (1) If they are 01, find the value of P + A. Ignore any overflow.
-------------> (2) If they are 10, find the value of P + S. Ignore any overflow.
-------------> (3) If they are 00, do nothing. Use P directly in the next step.
-------------> (4) If they are 11, do nothing. Use P directly in the next step.
-----------> (3) Arithmetically shift the value obtained in the 2nd step by a single place to the right. Let P now equal this new value.
-----------> (4) Repeat steps 2 and 3 until they have been done y times.
-----------> (5) Drop the least significant (rightmost) bit from P. This is the product of m and r.

---> Chakravala method: 
-----> This a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation.
-------> The chakravala method (Sanskrit: चक्रवाल विधि) is a cyclic algorithm to solve indeterminate quadratic equations, including Pell's equation. 
-------> It is commonly attributed to Bhāskara II, (c. 1114 – 1185 CE)[1][2] although some attribute it to Jayadeva (c. 950 ~ 1000 CE).
-------> Jayadeva pointed out that Brahmagupta's approach to solving equations of this type could be generalized, 
-------> and he then described this general method, which was later refined by Bhāskara II in his Bijaganita treatise. 
-------> He called it the Chakravala method: chakra meaning "wheel" in Sanskrit, a reference to the cyclic nature of the algorithm.
-------> Selenius held that no European performances at the time of Bhāskara, nor much later, exceeded its marvellous height of mathematical complexity.
-----> This method is also known as the cyclic method and contains traces of mathematical induction.[5] 

---> Discrete logarithm:
-----> In mathematics, for given real numbers a and b, the logarithm logb(a) is a number x such that b^x = a. 
-------> Analogously, in any group G, powers b^k can be defined for all integers k, and the discrete logarithm log(a) is an integer k such that b^k = a. 
-------> In number theory, the more commonly used term is index: we can write x = ind_r a (mod m) (read "the index of a to the base r modulo m") for rx ≡ a (mod m) if r is a primitive root of m and gcd(a,m) = 1.
-----> Discrete logarithms are quickly computable in a few special cases. 
-------> However, no efficient method is known for computing them in general. 
-------> Several important algorithms in public-key cryptography, 
-------> such as ElGamal base their security on the assumption that the discrete logarithm problem over carefully chosen groups has no efficient solution.
 
-----> Baby-step giant-step
-------> In group theory, a branch of mathematics, the baby-step giant-step is a meet-in-the-middle algorithm 
---------> for computing the discrete logarithm or order of an element in a finite abelian group by Daniel Shanks.
---------> The discrete log problem is of fundamental importance to the area of public key cryptography.
-------> Many of the most commonly used cryptography systems are based on the assumption that the discrete log is extremely difficult to compute; 
---------> the more difficult it is, the more security it provides a data transfer. One way to increase the difficulty of the discrete log problem is to base the cryptosystem on a larger group. 
-------> The algorithm
---------> Input: A cyclic group G of order n, having a generator α and an element β.
---------> Output: A value x satisfying α^x = β
--------->     m ← Ceiling(√n)
--------->     For all j where 0 ≤ j < m:
--------->         Compute αj and store the pair (j, αj) in a table. (See § In practice)
--------->     Compute α−m.
--------->     γ ← β. (set γ = β)
--------->     For all i where 0 ≤ i < m:
--------->         Check to see if γ is the second component (αj) of any pair in the table.
--------->         If so, return im + j.
--------->         If not, γ ← γ • α−m.

---> Index calculus algorithm
-----> In computational number theory, the index calculus algorithm is a probabilistic algorithm for computing discrete logarithms. 
-------> Dedicated to the discrete logarithm in (Z / qZ) where q is a prime, 
-------> index calculus leads to a family of algorithms adapted to finite fields and to some families of elliptic curves. 
-------> The algorithm collects relations among the discrete logarithms of small primes, computes them by a 
-------> linear algebra procedure and finally expresses the desired discrete logarithm with respect to the discrete logarithms of small primes. 
-----> The algorithm
-------> Input: Discrete logarithm generator g, modulus q and argument h. Factor base {−1,2,3,5,7,11,...,pr}, of length r + 1.
-------> Output: x such that gx ≡ h (mod q).
------->     relations ← empty_list
------->     for k = 1, 2, ...
------->         Using an integer factorization algorithm optimized for smooth numbers, try to factor g k mod q (Euclidean residue) using the factor base, i.e. find ei's such that g k mod q = ( − 1 ) e 0 2 e 1 3 e 2 ⋯ p r e r 
------->         Each time a factorization is found:
------->             Store k and the computed ei's as a vector ( e 0 , e 1 , e 2 , … , e r , k ) (this is a called a relation)
------->             If this relation is linearly independent to the other relations:
------->                 Add it to the list of relations
------->                 If there are at least r + 1 relations, exit loop
------->     Form a matrix whose rows are the relations
------->     Obtain the reduced echelon form of the matrix
------->         The first element in the last column is the discrete log of −1 and the second element is the discrete log of 2 and so on
------->     for s = 1, 2, ...
------->         Try to factor g s h mod q = ( − 1 ) f 0 2 f 1 3 f 2 ⋯ p r f r  over the factor base
------->         When a factorization is found:
------->             Output x = f 0 log g ⁡ ( − 1 ) + f 1 log g ⁡ 2 + ⋯ + f r log g ⁡ p r − s . .

---> Pollard's rho algorithm for logarithms
-----> Pollard's rho algorithm for logarithms is an algorithm introduced by John Pollard in 1978 to solve the discrete logarithm problem, analogous to Pollard's rho algorithm to solve the integer factorization problem.
-----> The goal is to compute γ such that α γ = β, where β belongs to a cyclic group G generated by α. 
-------> The algorithm computes integers A, B, A, and B such that α^a*β^b = α^A*β^B. 
-------> If the underlying group is cyclic of order n, by substituting β as a^γ and noting that two powers are equal if and only if the exponents are equivalent modulo the order of the base, 
-------> in this case modulo n, we get that γ {\displaystyle \gamma } \gamma is one of the solutions of the equation ( B − b ) γ = ( a − A ) ( mod n ). 
-------> Solutions to this equation are easily obtained using the extended Euclidean algorithm.
-----> To find the needed A, B, A, and B the algorithm uses Floyd's cycle-finding algorithm to find a cycle in the sequence x i = α^(a*i) β^(b*i), 
-------> where the function f : xi -> xi + 1 is assumed to be random-looking and thus is likely to enter into a loop of approximate length sqrt(π*n/8) after sqrt(π*n/8) steps. 
-------> One way to define such a function is to use the following rules: Divide G into three disjoint subsets of approximately equal size: S0, S1, and S2. 
-------> If xi is in S0 then double both A and B; if xi is an element of S1  then increment A, if xi is an element of S2 then increment B. 
-----> Algorithm
-------> Let G be a cyclic group of order n, and given α , β ∈ G, and a partition G = S0 ∪ S1 ∪ S2, let f : G → G be the map
------->     f ( x ) = { β x x ∈ S 0 x 2 x ∈ S 1 α x x ∈ S 2 
-------> and define maps g : G × Z → Z and h : G × Z → Z  by
------->     g ( x , k ) = { k x ∈ S 0 2 k ( mod n ) x ∈ S 1 k + 1 ( mod n ) x ∈ S 2 h ( x , k ) = { k + 1 ( mod n ) x ∈ S 0 2 k ( mod n ) x ∈ S 1 k x ∈ S 2 
-------> input: a: a generator of G
------->        b: an element of G
-------> output: An integer x such that ax = b, or failure
-------> Initialise a0 ← 0, b0 ← 0, x0 ← 1 ∈ G
-------> i ← 1
---------> loop
--------->     xi ← f(xi-1), 
--------->     ai ← g(xi-1, ai-1), 
--------->     bi ← h(xi-1, bi-1)
---------> 
--------->     x2i ← f(f(x2i-2)), 
--------->     a2i ← g(f(x2i-2), g(x2i-2, a2i-2)), 
--------->     b2i ← h(f(x2i-2), h(x2i-2, b2i-2))
---------> 
--------->     if xi = x2i then
--------->         r ← bi - b2i
--------->         if r = 0 return failure
--------->         x ← r−1(a2i - ai) mod n
--------->         return x
--------->     else // xi ≠ x2i
--------->         i ← i + 1
--------->     end if
---------> end loop
-----> Example
-------> Consider, for example, the group generated by 2 modulo  N=1019 (the order of the group is n=1018, 2 generates the group of units modulo 1019). 
-------> The algorithm is implemented by the following C++ program:
---------> #include <stdio.h>
---------> const int n = 1018, N = n + 1;  /* N = 1019 -- prime     */
---------> const int alpha = 2;            /* generator             */
---------> const int beta = 5;             /* 2^{10} = 1024 = 5 (N) */
---------> void new_xab(int& x, int& a, int& b) {
--------->   switch (x % 3) {
--------->   case 0: x = x * x     % N;  a =  a*2  % n;  b =  b*2  % n;  break;
--------->   case 1: x = x * alpha % N;  a = (a+1) % n;                  break;
--------->   case 2: x = x * beta  % N;                  b = (b+1) % n;  break;
--------->   }
---------> }
---------> int main(void) {
--------->   int x = 1, a = 0, b = 0;
--------->   int X = x, A = a, B = b;
--------->   for (int i = 1; i < n; ++i) {
--------->     new_xab(x, a, b);
--------->     new_xab(X, A, B);
--------->     new_xab(X, A, B);
--------->     printf("%3d  %4d %3d %3d  %4d %3d %3d\n", i, x, a, b, X, A, B);
--------->     if (x == X) break;
--------->   }
--------->   return 0;
---------> }

---> Pohlig–Hellman algorithm
-----> In group theory, the Pohlig–Hellman algorithm, sometimes credited as the Silver–Pohlig–Hellman algorithm, 
-------> is a special-purpose algorithm for computing discrete logarithms in a finite abelian group whose order is a smooth integer.
-----> The algorithm was introduced by Roland Silver, but first published by Stephen Pohlig and Martin Hellman (independent of Silver).
-----> The general algorithm
-------> In this section, we present the general case of the Pohlig–Hellman algorithm. 
---------> The core ingredients are the algorithm from the previous section 
---------> (to compute a logarithm modulo each prime power in the group order) and the Chinese remainder theorem (to combine these to a logarithm in the full group).
-------> (Again, we assume the group to be cyclic, with the understanding that a non-cyclic group must be replaced by the subgroup generated by the logarithm's base element.)
---------> Input. A cyclic group G of order n with generator G, an element h is an element G, and a prime factorization n = ∏ i = 1 r p i e i.
---------> Output. The unique integer x ∈ { 0 , … , n − 1 } such that g x = h.
-----------> For each i ∈ { 1 , … , r } , do:
----------->     Compute g i := g n / p i e i. By Lagrange's theorem, this element has order p i e i.
----------->     Compute h i := h n / p i e i . By construction, h i ∈ ⟨ g i ⟩ .
----------->     Using the algorithm above in the group ⟨ g i ⟩ such that g i x i = h i.
-----------> Solve the simultaneous congruence
-----------> x ≡ x i ( mod p i e i ) ∀ i ∈ { 1 , … , r } . 
-----------> The Chinese remainder theorem guarantees there exists a unique solution x ∈ { 0 , … , n − 1 } .
-----------> Return X.
-------> The correctness of this algorithm can be verified via the classification of finite abelian groups: 
---------> Raising G and h to the power of n / p i e i can be understood as the projection to the factor group of order p i e i. 

---> Euclidean algorithm: 
-----> This computes the greatest common divisor.
-------> In mathematics, the Euclidean algorithm,[note 1] or Euclid's algorithm, is an efficient method for computing the greatest common divisor (GCD) of two integers (numbers), 
-------> the largest number that divides them both without a remainder. It is named after the ancient Greek mathematician Euclid, who first described it in his Elements (c. 300 BC). 
-------> It is an example of an algorithm, a step-by-step procedure for performing a calculation according to well-defined rules, and is one of the oldest algorithms in common use. 
-------> It can be used to reduce fractions to their simplest form, and is a part of many other number-theoretic and cryptographic calculations.
-----> The Euclidean algorithm is based on the principle that the greatest common divisor of two numbers does not change if the larger number is replaced by its difference with the smaller number. 
-------> For example, 21 is the GCD of 252 and 105 (as 252 = 21 × 12 and 105 = 21 × 5), and the same number 21 is also the GCD of 105 and 252 − 105 = 147. 
-------> Since this replacement reduces the larger of the two numbers, repeating this process gives successively smaller pairs of numbers until the two numbers become equal. 
-------> When that occurs, they are the GCD of the original two numbers. 
-------> By reversing the steps or using the extended Euclidean algorithm, the GCD can be expressed as a linear combination of the two original numbers, 
-------> that is the sum of the two numbers, each multiplied by an integer (for example, 21 = 5 × 105 + (−2) × 252).
------->  The fact that the GCD can always be expressed in this way is known as Bézout's identity.
-----> The version of the Euclidean algorithm described above (and by Euclid) can take many subtraction steps to find the GCD when one of the given numbers is much bigger than the other. 
-------> A more efficient version of the algorithm shortcuts these steps, 
-------> instead replacing the larger of the two numbers by its remainder when divided by the smaller of the two (with this version, 
-------> the algorithm stops when reaching a zero remainder). 
-------> With this improvement, the algorithm never requires more steps than five times the number of digits (base 10) of the smaller integer. 
-------> This was proven by Gabriel Lamé in 1844, and marks the beginning of computational complexity theory. 
-------> Additional methods for improving the algorithm's efficiency were developed in the 20th century.
-----> The Euclidean algorithm has many theoretical and practical applications. 
-------> It is used for reducing fractions to their simplest form and for performing division in modular arithmetic. 
-------> Computations using this algorithm form part of the cryptographic protocols that are used to secure internet communications, 
-------> and in methods for breaking these cryptosystems by factoring large composite numbers. 
-------> The Euclidean algorithm may be used to solve Diophantine equations, 
-------> such as finding numbers that satisfy multiple congruences according to the Chinese remainder theorem, to construct continued fractions,
-------> and to find accurate rational approximations to real numbers. 
-------> Finally, it can be used as a basic tool for proving theorems in number theory such as Lagrange's four-square theorem and the uniqueness of prime factorizations. 
-------> The original algorithm was described only for natural numbers and geometric lengths (real numbers), 
-------> but the algorithm was generalized in the 19th century to other types of numbers, such as Gaussian integers and polynomials of one variable. 
-------> This led to modern abstract algebraic notions such as Euclidean domains. 
-----> Implementations
-------> Implementations of the algorithm may be expressed in pseudocode. For example, the division-based version may be programmed as[19]
---------> function gcd(a, b)
--------->     while b ≠ 0
--------->         t := b
--------->         b := a mod b
--------->         a := t
--------->     return a
-------> At the beginning of the kth iteration, the variable b holds the latest remainder rk−1, whereas the variable a holds its predecessor, rk−2. 
---------> The step b := a mod b is equivalent to the above recursion formula rk ≡ rk−2 mod rk−1. 
---------> The temporary variable t holds the value of rk−1 while the next remainder rk is being calculated. 
---------> At the end of the loop iteration, the variable b holds the remainder rk, whereas the variable a holds its predecessor, rk−1.
-------> (If negative inputs are allowed, or if the mod function may return negative values, the last line must be changed into return max(a, −a).)
-------> In the subtraction-based version, which was Euclid's original version, the remainder calculation (b := a mod b) is replaced by repeated subtraction. 
---------> Contrary to the division-based version, which works with arbitrary integers as input, the subtraction-based version supposes that the input consists of positive integers and stops when a = b:
-----------> function gcd(a, b)
----------->     while a ≠ b 
----------->         if a > b
----------->             a := a − b
----------->         else
----------->             b := b − a
----------->     return a
-------> The variables a and b alternate holding the previous remainders rk−1 and rk−2. 
---------> Assume that a is larger than b at the beginning of an iteration; then a equals rk−2, since rk−2 > rk−1. 
---------> During the loop iteration, a is reduced by multiples of the previous remainder b until a is smaller than b. 
---------> Then a is the next remainder rk. 
---------> Then b is reduced by multiples of a until it is again smaller than a, giving the next remainder rk+1, and so on.
-------> The recursive version is based on the equality of the GCDs of successive remainders and the stopping condition gcd(rN−1, 0) = rN−1.
---------> function gcd(a, b)
--------->     if b = 0
--------->         return a
--------->     else
--------->         return gcd(b, a mod b)
---------> (As above, if negative inputs are allowed, or if the mod function may return negative values, the instruction "return a" must be changed into "return max(a, −a)".)
-------> For illustration, the gcd(1071, 462) is calculated from the equivalent gcd(462, 1071 mod 462) = gcd(462, 147). 
---------> The latter GCD is calculated from the gcd(147, 462 mod 147) = gcd(147, 21), which in turn is calculated from the gcd(21, 147 mod 21) = gcd(21, 0) = 21. 

---> Extended Euclidean algorithm: 
-----> This Also solves the equation ax + by = c.
-----> In arithmetic and computer programming, the extended Euclidean algorithm is an extension to the Euclidean algorithm, 
-------> and computes, in addition to the greatest common divisor (gcd) of integers a and b, also the coefficients of Bézout's identity, which are integers x and y such that
---------> a x + b y = gcd(a, b). {\displaystyle ax+by=\gcd(a,b).} ax+by=\gcd(a,b).
-----> This is a certifying algorithm, because the gcd is the only number that can simultaneously satisfy this equation and divide the inputs. 
-------> It allows one to compute also, with almost no extra cost, the quotients of a and b by their greatest common divisor.
-----> Extended Euclidean algorithm also refers to a very similar algorithm for computing the polynomial greatest common divisor and the coefficients of Bézout's identity of two univariate polynomials.
-------> The extended Euclidean algorithm is particularly useful when a and b are coprime. 
-------> With that provision, x is the modular multiplicative inverse of a modulo b, and y is the modular multiplicative inverse of b modulo a. 
-------> Similarly, the polynomial extended Euclidean algorithm allows one to compute the multiplicative inverse in algebraic field extensions and, 
-------> in particular in finite fields of non prime order. It follows that both extended Euclidean algorithms are widely used in cryptography. 
-------> In particular, the computation of the modular multiplicative inverse is an essential step in the derivation of key-pairs in the RSA public-key encryption method. 



---> Integer factorization: 
-----> This breaking an integer into its prime factors
-------> In number theory, integer factorization is the decomposition of a composite number into a product of smaller integers. 
-------> If these factors are further restricted to prime numbers, the process is called prime factorization.
-----> When the numbers are sufficiently large, no efficient non-quantum integer factorization algorithm is known. 
-------> However, it has not been proven that such an algorithm does not exist. 
-------> The presumed difficulty of this problem is important for the algorithms used in cryptography such as RSA public-key encryption and the RSA digital signature.
-------> Many areas of mathematics and computer science have been brought to bear on the problem, including elliptic curves, algebraic number theory, and quantum computing.
-----> In 2019, Fabrice Boudot, Pierrick Gaudry, Aurore Guillevic, Nadia Heninger, Emmanuel Thomé and Paul Zimmermann 
-------> factored a 240-digit (795-bit) number (RSA-240) utilizing approximately 900 core-years of computing power.
-------> The researchers estimated that a 1024-bit RSA modulus would take about 500 times as long.
-----> Not all numbers of a given length are equally hard to factor. 
-------> The hardest instances of these problems (for currently known techniques) are semiprimes, the product of two prime numbers. 
-------> When they are both large, for instance more than two thousand bits long, randomly chosen, and about the same size (but not too close, for example, to avoid efficient factorization by Fermat's factorization method), 
-------> even the fastest prime factorization algorithms on the fastest computers can take enough time to make the search impractical; 
-------> that is, as the number of digits of the primes being factored increases, the number of operations required to perform the factorization on any computer increases drastically.
-----> Many cryptographic protocols are based on the difficulty of factoring large composite integers or a related problem—for example, the RSA problem. 
-------> An algorithm that efficiently factors an arbitrary integer would render RSA-based public-key cryptography insecure. 

-----> Congruence of squares
-------> In number theory, a congruence of squares is a congruence commonly used in integer factorization algorithms. 
-------> Derivation
---------> Given a positive integer n, Fermat's factorization method relies on finding numbers x and y satisfying the equality
-----------> x 2 − y 2 = n 
-------> We can then factor n = x2 − y2 = (x + y)(x − y). 
---------> This algorithm is slow in practice because we need to search many such numbers, and only a few satisfy the equation. 
---------> However, n may also be factored if we can satisfy the weaker congruence of squares condition:
-----------> x 2 ≡ y 2 ( mod n ) 
-----------> x is not equal to ± y ( mod n ) 
-------> From here we easily deduce
-----------> x 2 − y 2 ≡ 0 ( mod n ) 
-----------> ( x + y ) ( x − y ) ≡ 0 ( mod n ) 
-------> This means that n divides the product (x + y)(x − y). 
---------> Thus (x + y) and (x − y) each contain factors of n, but those factors can be trivial. 
---------> In this case we need to find another x and y.
--------->  Computing the greatest common divisors of (x + y, n) and of (x − y, n) will give us these factors; this can be done quickly using the Euclidean algorithm.
-------> Congruences of squares are extremely useful in integer factorization algorithms and are extensively used in,
---------> for example, the quadratic sieve, general number field sieve, continued fraction factorization, and Dixon's factorization. 
---------> Conversely, because finding square roots modulo a composite number turns out to be probabilistic polynomial-time equivalent to factoring that number, 
---------> any integer factorization algorithm can be used efficiently to identify a congruence of squares.
-------> Further generalizations
---------> It is also possible to use factor bases to help find congruences of squares more quickly. 
---------> Instead of looking for x 2 ≡ y 2 ( mod n ) from the outset, we find many x 2 ≡ y ( mod n ) where the y have small prime factors, and try to multiply a few of these together to get a square on the right-hand side. 

-----> Dixon's algorithm
-------> In number theory, Dixon's factorization method (also Dixon's random squares method or Dixon's algorithm) 
---------> is a general-purpose integer factorization algorithm; it is the prototypical factor base method. 
---------> Unlike for other factor base methods, its run-time bound comes with a rigorous proof that does not rely on conjectures about the smoothness properties of the values taken by polynomial.
-------> The algorithm was designed by John D. Dixon, a mathematician at Carleton University, and was published in 1981.
-------> Basic idea
---------> Dixon's method is based on finding a congruence of squares modulo the integer N which is intended to factor. 
-----------> Fermat's factorization method finds such a congruence by selecting random or pseudo-random x values and hoping that the integer x2 mod N is a perfect square (in the integers):
-------------> x 2 ≡ y 2 ( mod  N ) , x ≢ ± y ( mod  N ) . 
---------> For example, if N = 84923, (by starting at 292, the first number greater than √N and counting up) the 5052 mod 84923 is 256, the square of 16. 
-----------> So (505 − 16)(505 + 16) = 0 mod 84923. 
-----------> Computing the greatest common divisor of 505 − 16 and N using Euclid's algorithm gives 163, which is a factor of N.
---------> In practice, selecting random x values will take an impractically long time to find a congruence of squares, since there are only √N squares less than N.
---------> Dixon's method replaces the condition "is the square of an integer" with the much weaker one "has only small prime factors"; 
-----------> for example, there are 292 squares smaller than 84923; 662 numbers smaller than 84923 whose prime factors are only 2,3,5 or 7; 
-----------> and 4767 whose prime factors are all less than 30. (Such numbers are called B-smooth with respect to some bound B.)
---------> If there are many numbers a 1 … a n whose squares can be factorized as a i 2 mod N = ∏ j = 1 m b j e i j  for a fixed set b 1 … b m of small primes, 
-----------> linear algebra modulo 2 on the matrix eij  will give a subset of the ai  whose squares combine to a product of small primes to an even power — that is, 
-----------> a subset of the ai whose squares multiply to the square of a (hopefully different) number mod N.
---------> Method
---------> Suppose the composite number N is being factored. 
-----------> Bound B is chosen, and the factor base is identified (which is called P), the set of all primes less than or equal to B. 
-----------> Next, positive integers z are sought such that z2 mod N is B-smooth. Therefore it can be written, for suitable exponents ai,
-------------> z 2  mod  N = ∏ p i ∈ P p i a i 
---------> When enough of these relations have been generated (it is generally sufficient that the number of relations be a few more than the size of P), 
-----------> the methods of linear algebra can be used (for example, Gaussian elimination) to multiply together these various relations in such a way that the exponents of the primes on the right-hand side are all even:
-------------> z 1 2 z 2 2 ⋯ z k 2 ≡ ∏ p i ∈ P p i a i , 1 + a i , 2 + ⋯ + a i , k   ( mod N ) ( where  a i , 1 + a i , 2 + ⋯ + a i , k ≡ 0 ( mod 2 ) ) 
---------> This yields a congruence of squares of the form a2 ≡ b2 (mod N), which can be turned into a factorization of N, N = gcd(a + b, N) × (N/gcd(a + b, N)). 
-----------> This factorization might turn out to be trivial (i.e. N = N × 1), which can only happen if a ≡ ±b (mod N), 
-----------> in which case another try has to be made with a different combination of relations; but a nontrivial pair of factors of N can be reached, and the algorithm will terminate.
-------> Example
---------> This example will try to factor N = 84923 using bound B = 7. 
---------> The factor base is then P = {2, 3, 5, 7}. A search can be made for integers between ⌈ 84923 ⌉ = 292 and N whose squares mod N are B-smooth. 
---------> Suppose that two of the numbers found are 513 and 537:
-----------> 513 2 mod 84923 = 8400 = 2 4 ⋅ 3 ⋅ 5 2 ⋅ 7 
-----------> 537 2 mod 84923 = 33600 = 2 6 ⋅ 3 ⋅ 5 2 ⋅ 7 
---------> So
-----------> ( 513 ⋅ 537 ) 2 mod 84923 = 2 10 ⋅ 3 2 ⋅ 5 4 ⋅ 7 2 mod 84923 
---------> Then
-----------> ( 513 ⋅ 537 ) 2 mod 84923 = ( 275481 ) 2 mod 84923 = ( 84923 ⋅ 3 + 20712 ) 2 mod 84923 = ( 84923 ⋅ 3 ) 2 + 2 ⋅ ( 84923 ⋅ 3 ⋅ 20712 ) + 20712 2 mod 84923 = 0 + 0 + 20712 2 mod 84923 
---------> That is, 20712 2 mod 84923 = ( 2 5 ⋅ 3 ⋅ 5 2 ⋅ 7 ) 2 mod 84923 = 16800 2 mod 84923. 
---------> The resulting factorization is 84923 = gcd(20712 − 16800, 84923) × gcd(20712 + 16800, 84923) = 163 × 521. 

-----> Fermat's factorization method
-------> Fermat's factorization method, named after Pierre de Fermat, is based on the representation of an odd integer as the difference of two squares:
---------> N = a^2 − b^2 .
-------> That difference is algebraically factorable as (a+b)(a-b); if neither factor equals one, it is a proper factorization of N.
-------> Each odd number has such a representation. Indeed, if N=cd is a factorization of N, then
--------->  N = ( c + d 2 ) 2 − ( c − d 2 ) 2 
-------> Since N is odd, then c and d are also odd, so those halves are integers. (A multiple of four is also a difference of squares: let c and d be even.)
-------> In its simplest form, Fermat's method might be even slower than trial division (worst case). Nonetheless, the combination of trial division and Fermat's is more effective than either. 
-------> Basic method
---------> One tries various values of a, hoping that a 2 − N = b 2 , a square
-----------> FermatFactor(N): // N should be odd
----------->     a ← ceiling(sqrt(N))
----------->     b2 ← a*a - N
----------->     repeat until b2 is a square:
----------->         a ← a + 1
----------->         b2 ← a*a - N 
----------->      // equivalently: 
----------->      // b2 ← b2 + 2*a + 1 
----------->      // a ← a + 1
----------->     return a - sqrt(b2) // or a + sqrt(b2)
-------> For example, to factor N = 5959 {\displaystyle N=5959} N=5959, the first try for a is the square root of 5959 rounded up to the next integer, which is 78. 
---------> Then, b 2 = 78 2 − 5959 = 125 . 
---------> Since 125 is not a square, a second try is made by increasing the value of a by 1.
--------->  The second attempt also fails, because 282 is again not a square.
---------> Try:  1  2  3
---------> a  78  79  80
---------> b2  125  282  441
---------> b  11.18  16.79  21
-------> The third try produces the perfect square of 441. 
---------> So, a=80  b=21, and the factors of 5959 are a − b = 59 a-b=59 and a + b = 101 a+b=101.
-------> Suppose N has more than two prime factors. 
---------> That procedure first finds the factorization with the least values of a and b. 
---------> That is,  a+b is the smallest factor ≥ the square-root of N, and so a-b=N/(a+b) is the largest factor ≤ root-N. 
---------> If the procedure finds N = 1 ⋅ N, that shows that N is prime.
-------> For N = c d {\displaystyle N=cd} N=cd, let c be the largest subroot factor. 
---------> a=(c+d)/2, so the number of steps is approximately ( c + d ) / 2 − N = ( d − c ) 2 / 2 = ( N − c ) 2 / 2 c .
-------> If N is prime (so that c = 1 {\displaystyle c=1} c=1), one needs  O(N) steps. 
---------> This is a bad way to prove primality. But if N has a factor close to its square root, the method works quickly. 
---------> More precisely, if c differs less than ( 4 N ) 1 / 4, the method requires only one step; this is independent of the size of N.

-----> General number field sieve
-------> In number theory, the general number field sieve (GNFS) is the most efficient classical algorithm known for factoring integers larger than 10100. 
---------> Heuristically, its complexity for factoring an integer n (consisting of ⌊log2 n⌋ + 1 bits) is of the form
-----------> exp ⁡ ( ( 64 9 3 + o ( 1 ) ) ( ln ⁡ n ) 1 3 ( ln ⁡ ln ⁡ n ) 2 3 ) = L n [ 1 3 , 64 9 3 ] 
---------> (in L-notation), where ln is the natural logarithm.[1] It is a generalization of the special number field sieve: 
---------> while the latter can only factor numbers of a certain special form, 
---------> the general number field sieve can factor any number apart from prime powers (which are trivial to factor by taking roots).
-------> The principle of the number field sieve (both special and general) can be understood as an improvement to the simpler rational sieve or quadratic sieve. 
---------> When using such algorithms to factor a large number n, it is necessary to search for smooth numbers (i.e. numbers with small prime factors) of order n1/2. 
---------> The size of these values is exponential in the size of n (see below). 
---------> The general number field sieve, on the other hand, manages to search for smooth numbers that are subexponential in the size of n. 
---------> Since these numbers are smaller, they are more likely to be smooth than the numbers inspected in previous algorithms. 
---------> This is the key to the efficiency of the number field sieve. 
---------> In order to achieve this speed-up, the number field sieve has to perform computations and factorizations in number fields. 
---------> This results in many rather complicated aspects of the algorithm, as compared to the simpler rational sieve.
-------> The size of the input to the algorithm is log2 n or the number of bits in the binary representation of n. 
---------> Any element of the order nc for a constant c is exponential in log n. 
---------> The running time of the number field sieve is super-polynomial but sub-exponential in the size of the input. 

-----> Lenstra elliptic curve factorization
-------> The Lenstra elliptic-curve factorization or the elliptic-curve factorization method (ECM) is a fast, 
---------> sub-exponential running time, algorithm for integer factorization, which employs elliptic curves. 
---------> For general-purpose factoring, ECM is the third-fastest known factoring method. 
---------> The second-fastest is the multiple polynomial quadratic sieve, and the fastest is the general number field sieve. 
---------> The Lenstra elliptic-curve factorization is named after Hendrik Lenstra.
-------> Practically speaking, ECM is considered a special-purpose factoring algorithm, as it is most suitable for finding small factors. 
---------> Currently, it is still the best algorithm for divisors not exceeding 50 to 60 digits, 
---------> as its running time is dominated by the size of the smallest factor p rather than by the size of the number n to be factored. 
---------> Frequently, ECM is used to remove small factors from a very large integer with many factors; if the remaining integer is still composite, 
---------> then it has only large factors and is factored using general-purpose techniques. 
---------> The largest factor found using ECM so far has 83 decimal digits and was discovered on 7 September 2013 by R. Propper.
---------> Increasing the number of curves tested improves the chances of finding a factor, but they are not linear with the increase in the number of digits. 

-----> Pollard's p − 1 algorithm
-------> Pollard's p − 1 algorithm is a number theoretic integer factorization algorithm, invented by John Pollard in 1974. 
---------> It is a special-purpose algorithm, meaning that it is only suitable for integers with specific types of factors; 
---------> it is the simplest example of an algebraic-group factorisation algorithm.
-------> The factors it finds are ones for which the number preceding the factor, p − 1, is powersmooth; the essential observation is that, 
---------> by working in the multiplicative group modulo a composite number N, we are also working in the multiplicative groups modulo all of N's factors.
-------> The existence of this algorithm leads to the concept of safe primes, 
---------> being primes for which p − 1 is two times a Sophie Germain prime q and thus minimally smooth. 
---------> These primes are sometimes construed as "safe for cryptographic purposes", 
---------> but they might be unsafe — in current recommendations for cryptographic strong primes (e.g. ANSI X9.31), 
---------> it is necessary but not sufficient that p − 1 has at least one large prime factor. 
---------> Most sufficiently large primes are strong; if a prime used for cryptographic purposes turns out to be non-strong, 
---------> it is much more likely to be through malice than through an accident of random number generation. 
---------> This terminology is considered obsolete by the cryptography industry: 
---------> ECM makes safe primes just as easy to factor as non-safe primes, so size is the important factor.[1] 

-----> Pollard's rho algorithm
-------> Pollard's rho algorithm is an algorithm for integer factorization. 
---------> It was invented by John Pollard in 1975.
---------> It uses only a small amount of space, and its expected running time is proportional to the square root of the size
---------> of the smallest prime factor of the composite number being factorized. 
-------> Core ideas
---------> The algorithm is used to factorize a number n = pq, where p is a non-trivial factor. 
-----------> A polynomial modulo n, called g(x) (e.g., g ( x ) = ( x^2 + 1 ) mod n, is used to generate a pseudorandom sequence: 
-----------> A starting value, say 2, is chosen, and the sequence continues as x 1 = g ( 2 ) , x 2 = g ( g ( 2 ) ) , x 3 = g ( g ( g ( 2 ) ) ), etc. 
-----------> The sequence is related to another sequence { x k mod p }. 
-----------> Since p is not known beforehand, this sequence cannot be explicitly computed in the algorithm. 
-----------> Yet, in it lies the core idea of the algorithm.
---------> Because the number of possible values for these sequences is finite, both the { x k } sequence, which is mod n, and { x k mod p } sequence will eventually repeat, even though these values are unknown. 
-----------> If the sequences were to behave like random numbers, the birthday paradox implies that the number of xk before a repetition occurs would be expected to be O(sqrt(N)), where n is the number of possible values. 
-----------> So the sequence { x k mod p } will likely repeat much earlier than the sequence { x k }. 
-----------> When one has found a k 1 , k 2 such that x k 1 ≠ x k 2  but x k 1 ≡ x k 2 mod p, the number | x k 1 − x k 2 |  is a multiple of p, so p has been found.
---------> Once a sequence has a repeated value, the sequence will cycle, because each value depends only on the one before it. 
-----------> This structure of eventual cycling gives rise to the name "rho algorithm", 
-----------> owing to similarity to the shape of the Greek letter ρ when the values x 1 mod p, x 2 mod p, etc. are represented as nodes in a directed graph.
---------> This is detected by Floyd's cycle-finding algorithm: two nodes i and j (i.e., x i and x j) are kept. 
-----------> In each step, one moves to the next node in the sequence and the other moves forward by two nodes. 
-----------> After that, it is checked whether gcd ( x i − x j , n ) ≠ 1. 
-----------> If it is not 1, then this implies that there is a repetition in the { x k mod p } sequence (i.e. x i mod p = x j mod p ). 
-----------> This works because if the x i {\displaystyle x_{i}} x_{i} is the same as x j, the difference between x i and x j is necessarily a multiple of p. 
-----------> Although this always happens eventually, the resulting greatest common divisor (GCD) is a divisor of n other than 1. 
-----------> This may be n itself, since the two sequences might repeat at the same time. 
-----------> In this (uncommon) case the algorithm fails, and can be repeated with a different parameter.
---------> Algorithm
-----------> The algorithm takes as its inputs n, the integer to be factored; and g(x), a polynomial in x computed modulo n. 
-------------> In the original algorithm, g ( x ) = ( x 2 − 1 ) mod n, but nowadays it is more common to use g ( x ) = ( x 2 + 1 ) mod n. 
-------------> The output is either a non-trivial factor of n, or failure. 
-------------> It performs the following steps:
---------------> x ← 2
---------------> y ← 2
---------------> d ← 1
---------------> while d = 1:
--------------->     x ← g(x)
--------------->     y ← g(g(y))
--------------->     d ← gcd(|x - y|, n)
---------------> if d = n: 
--------------->     return failure
---------------> else:
--------------->     return d
-------------> Here x and y corresponds to x i and x j in the previous section. 
-------------> Note that this algorithm may fail to find a nontrivial factor even when n is composite. 
-------------> In that case, the method can be tried again, using a starting value other than 2 or a different  g(x). 

-----> Prime factorization algorithm
-------> By the fundamental theorem of arithmetic, every positive integer has a unique prime factorization. 
---------> (By convention, 1 is the empty product.) 
---------> Testing whether the integer is prime can be done in polynomial time, for example, by the AKS primality test. 
---------> If composite, however, the polynomial time tests give no insight into how to obtain the factors.
-------> Given a general algorithm for integer factorization, any integer can be factored into its constituent prime factors by repeated application of this algorithm. 
---------> The situation is more complicated with special-purpose factorization algorithms, whose benefits may not be realized as well or even at all with the factors produced during decomposition. 
---------> For example, if n = 171 × p × q where p < q are very large primes, trial division will quickly produce the factors 3 and 19 but will take p divisions to find the next factor. 
---------> As a contrasting example, if n is the product of the primes 13729, 1372933, and 18848997161, where 13729 × 1372933 = 18848997157, 
---------> Fermat's factorization method will begin with ⌈ n ⌉ = 18848997159 which immediately yields b = a 2 − n = 4 = 2 b and hence the factors a − b = 18848997157 and a + b = 18848997161. 
---------> While these are easily recognized as composite and prime respectively, 
---------> Fermat's method will take much longer to factor the composite number because the starting value of ⌈ 18848997157 ⌉ = 137292 for a is nowhere near 1372933. 

-----> Quadratic sieve
-------> The quadratic sieve algorithm (QS) is an integer factorization algorithm and, in practice, the second fastest method known (after the general number field sieve). 
---------> It is still the fastest for integers under 100 decimal digits or so, and is considerably simpler than the number field sieve. 
---------> It is a general-purpose factorization algorithm, meaning that its running time depends solely on the size of the integer to be factored, and not on special structure or properties. 
---------> It was invented by Carl Pomerance in 1981 as an improvement to Schroeppel's linear sieve.[1]
-------> The algorithm
---------> To summarize, the basic quadratic sieve algorithm has these main steps:
-----------> (1) Choose a smoothness bound B. The number π(B), denoting the number of prime numbers less than B, 
-------------> will control both the length of the vectors and the number of vectors needed.
-----------> (2) Use sieving to locate π(B) + 1 numbers ai such that bi = (ai2 mod n) is B-smooth.
-----------> (3) Factor the bi and generate exponent vectors mod 2 for each one.
-----------> (4) Use linear algebra to find a subset of these vectors which add to the zero vector. 
-------------> Multiply the corresponding ai together and give the result mod n the name a; similarly, multiply the bi together which yields a B-smooth square b2.
-----------> (5) We are now left with the equality a2 = b2 mod n from which we get two square roots of (a2 mod n), 
-----------> one by taking the square root in the integers of b2 namely b, and the other the a computed in step 4.
-----------> (6) We now have the desired identity: ( a + b ) ( a − b ) ≡ 0 ( mod n ). 
-------------> Compute the GCD of n with the difference (or sum) of a and b. 
-------------> This produces a factor, although it may be a trivial factor (n or 1). 
-------------> If the factor is trivial, try again with a different linear dependency or different a.

-----> Shor's algorithm
-------> Shor's algorithm is a quantum computer algorithm for finding the prime factors of an integer. It was developed in 1994 by the American mathematician Peter Shor.[1]
-------> On a quantum computer, to factor an integer n, Shor's algorithm runs in polynomial time, meaning the time taken is polynomial in log ⁡ N, the size of the integer given as input. 
---------> Specifically, it takes quantum gates of order O ( ( log ⁡ N ) 2 ( log ⁡ log ⁡ N ) ( log ⁡ log ⁡ log ⁡ N ) ) using fast multiplication,
---------> thus demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is consequently in the complexity class BQP. 
---------> This is almost exponentially faster than the most efficient known classical factoring algorithm, the general number field sieve, 
---------> which works in sub-exponential time: O ( e 1.9 ( log ⁡ N ) 1 / 3 ( log ⁡ log ⁡ N ) 2 / 3 ) . 
---------> The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform, and modular exponentiation by repeated squarings.
-------> If a quantum computer with a sufficient number of qubits could operate without succumbing to quantum noise and other quantum-decoherence phenomena, 
---------> then Shor's algorithm could be used to break public-key cryptography schemes, such as
-----------> The RSA scheme
-----------> The Finite Field Diffie-Hellman key exchange
-----------> The Elliptic Curve Diffie-Hellman key exchange
-------> RSA is based on the assumption that factoring large integers is computationally intractable. 
---------> As far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor integers in polynomial time. 
---------> However, Shor's algorithm shows that factoring integers is efficient on an ideal quantum computer, 
---------> so it may be feasible to defeat RSA by constructing a large quantum computer. 
---------> It was also a powerful motivator for the design and construction of quantum computers, and for the study of new quantum-computer algorithms. 
---------> It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.
-------> In 2001, Shor's algorithm was demonstrated by a group at IBM, who factored 15 into 3 × 5, using an NMR implementation of a quantum computer with 7 qubits.
---------> After IBM's implementation, two independent groups implemented Shor's algorithm using photonic qubits, emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits.
---------> In 2012, the factorization of 15 was performed with solid-state qubits.
---------> Also, in 2012, the factorization of 21 was achieved, 
---------> setting the record for the largest integer factored with Shor's algorithm.
---------> In 2019 an attempt was made to factor the number 35 using Shor's algorithm on an IBM Q System One, but the algorithm failed because of accumulating errors. 
---------> Though larger numbers have been factored by quantum computers using other algorithms, these algorithms are similar to classical brute-force checking of factors, 
---------> so unlike Shor's algorithm, they are not expected to ever perform better than classical factoring algorithms.

-----> Special number field sieve
-------> In number theory, a branch of mathematics, the special number field sieve (SNFS) is a special-purpose integer factorization algorithm.
---------> The general number field sieve (GNFS) was derived from it.
-------> The special number field sieve is efficient for integers of the form re ± s, where r and s are small (for instance Mersenne numbers).
---------> Heuristically, its complexity for factoring an integer n is of the form:
-----------> exp ⁡ ( ( 1 + o ( 1 ) ) ( 32 9 log ⁡ n ) 1 / 3 ( log ⁡ log ⁡ n ) 2 / 3 ) = L n [ 1 / 3 , ( 32 / 9 ) 1 / 3 ] 
---------> in O and L-notations.
-------> The SNFS has been used extensively by NFSNet (a volunteer distributed computing effort), 
---------> NFS@Home and others to factorise numbers of the Cunningham project; 
---------> for some time the records for integer factorization have been numbers factored by SNFS
-------> Overview of method
---------> The SNFS is based on an idea similar to the much simpler rational sieve; in particular, 
-----------> readers may find it helpful to read about the rational sieve first, before tackling the SNFS.
---------> The SNFS works as follows. Let n be the integer we want to factor. 
-----------> As in the rational sieve, the SNFS can be broken into two steps:
-------------> (1) First, find a large number of multiplicative relations among a factor base of elements of Z/nZ, 
---------------> such that the number of multiplicative relations is larger than the number of elements in the factor base.
-------------> (2) Second, multiply together subsets of these relations in such a way that all the exponents are even, 
---------------> resulting in congruences of the form a2≡b2 (mod n). 
---------------> These in turn immediately lead to factorizations of n: n=gcd(a+b,n)×gcd(a-b,n). 
---------------> If done right, it is almost certain that at least one such factorization will be nontrivial.
---------> The second step is identical to the case of the rational sieve, and is a straightforward linear algebra problem. 
---------> The first step, however, is done in a different, more efficient way than the rational sieve, by utilizing number field

-----> Trial division
-------> Trial division is the most laborious but easiest to understand of the integer factorization algorithms. 
---------> The essential idea behind trial division tests to see if an integer n, the integer to be factored, 
---------> can be divided by each number in turn that is less than n. 
---------> For example, for the integer n = 12, the only numbers that divide it are 1, 2, 3, 4, 6, 12. 
---------> Selecting only the largest powers of primes in this list gives that 12 = 3 × 4 = 3 × 22.
-------> Trial division was first described by Fibonacci in his book Liber Abaci (1202).[1] 
-------> Method
---------> Given an integer n (n refers to "the integer to be factored"), the trial division consists of systematically testing whether n is divisible by any smaller number. 
-----------> Clearly, it is only worthwhile to test candidate factors less than n, and in order from two upwards because an arbitrary n is more likely to be divisible by two than by three, and so on. 
-----------> With this ordering, there is no point in testing for divisibility by four if the number has already been determined not divisible by two, and so on for three and any multiple of three, etc. 
-----------> Therefore, the effort can be reduced by selecting only prime numbers as candidate factors. 
-----------> Furthermore, the trial factors need go no further than sqrt {n} because, if n is divisible by some number p, then n = p × q and if q were smaller than p,
-----------> n would have been detected earlier as being divisible by q or by a prime factor of q.
---------> A definite bound on the prime factors is possible. Suppose Pi is the i'th prime, so that P1 = 2, P2 = 3, P3 = 5, etc. 
-----------> Then the last prime number worth testing as a possible factor of n is Pi where P2i + 1 > n; equality here would mean that Pi + 1 is a factor. 
-----------> Thus, testing with 2, 3, and 5 suffices up to n = 48 not just 25 because the square of the next prime is 49, and below n = 25 just 2 and 3 are sufficient. 
-----------> Should the square root of n be integral, then it is a factor and n is a perfect square.
---------> An example of the trial division algorithm, using successive integers as trial factors, is as follows (in Python):
-----------> def trial_division(n: int) -> List[int]:
----------->     """Return a list of the prime factors for a natural number."""
----------->     a = []               # Prepare an empty list.
----------->     f = 2                # The first possible factor.    
----------->     while n > 1:         # While n still has remaining factors...
----------->         if n % f == 0:   # The remainder of n divided by f might be zero.        
----------->             a.append(f)  # If so, it divides n. Add f to the list.
----------->             n //= f       # Divide that factor out of n.
----------->         else:            # But if f is not a factor of n,
----------->             f += 1       # Add one to f and try again.
----------->     return a             # Prime factors may be repeated: 12 factors to 2,2,3.
-----------> Or 2x more efficient:
-----------> def trial_division(n: int) -> List[int]:
----------->     a = []
----------->     while n % 2 == 0:
----------->         a.append(2)
----------->         n //= 2
----------->     f = 3
----------->     while f * f <= n:
----------->         if n % f == 0:
----------->             a.append(f)
----------->             n //= f
----------->         else:
----------->             f += 2
----------->     if n != 1: a.append(n)
----------->     # Only odd number is possible
----------->     return a
---------> These versions of trial division are guaranteed to find a factor of n if there is one since they check all possible factors of n 
-----------> — and if n is a prime number, this means trial factors all the way up to n. 
-----------> Thus, if the algorithm finds one factor only, n, it is proof that n is a prime. 
-----------> If more than one factor is found, then n is a composite integer. 
-----------> A more computationally advantageous way of saying this is, if any prime whose square does not exceed n divides it without a remainder, then n is not prime.
---------> Below is a version in C++ (without squaring f)
-----------> template <class T, class U>
-----------> vector<T> TrialDivision(U n)
-----------> {
----------->     vector<T> v; T f;
----------->     f = 2;
----------->     while (n % 2 == 0) { v.push_back(f); n /= 2; }
----------->     f = 3;
----------->     while (n % 3 == 0) { v.push_back(f); n /= 3; }
----------->     f = 5;
----------->     T ac = 9, temp = 16;
----------->     do {
----------->         ac += temp; // Assume addition does not cause overflow with U type
----------->         if (ac > n) break; 
----------->         if (n % f == 0) {
----------->             v.push_back(f);
----------->             n /= f;
----------->             ac -= temp;
----------->         }
----------->         else { 
----------->             f += 2;
----------->             temp += 8;
----------->         }
----------->     } while (1);
----------->     if (n != 1) v.push_back(n);
----------->     return v;
-----------> }



---> Multiplication algorithms: 
-----> This fast multiplication of two numbers
-----> A multiplication algorithm is an algorithm (or method) to multiply two numbers. 
-------> Depending on the size of the numbers, different algorithms are used. 
-------> Efficient multiplication algorithms have existed since the advent of the decimal system. 

-------> Karatsuba algorithm
---------> The Karatsuba algorithm is a fast multiplication algorithm. 
-----------> It was discovered by Anatoly Karatsuba in 1960 and published in 1962.
-----------> It is a divide-and-conquer algorithm that reduces the multiplication of two n-digit numbers to three multiplications of n/2-digit numbers and, 
-----------> by repeating this reduction, to at most n log 2 ⁡ 3 ≈ n 1.58 single-digit multiplications. 
-----------> It is therefore asymptotically faster than the traditional algorithm, which performs n^2 single-digit products. 
-----------> For example, the Karatsuba algorithm requires 310 = 59,049 single-digit multiplications to multiply two 1024-digit numbers (n = 1024 = 210),
-----------> whereas the traditional algorithm requires (210)2 = 1,048,576 (~17.758 times faster).
---------> The Karatsuba algorithm was the first multiplication algorithm asymptotically faster than the quadratic "grade school" algorithm. 
-----------> The Toom–Cook algorithm (1963) is a faster generalization of Karatsuba's method, and the Schönhage–Strassen algorithm (1971) is even faster, for sufficiently large n. 
-----------> Implementation
-------------> Here is the pseudocode for this algorithm, using numbers represented in base ten. 
---------------> For the binary representation of integers, it suffices to replace everywhere 10 by 2.
-------------> The second argument of the split_at function specifies the number of digits to extract from the right:
---------------> for example, split_at("12345", 3) will extract the 3 final digits, giving: high="12", low="345".
---------------> function karatsuba (num1, num2)
--------------->     if (num1 < 10) or (num2 < 10)
--------------->         return num1 × num2 /* fall back to traditional multiplication */
--------------->     /* Calculates the size of the numbers. */
--------------->     m = min (size_base10(num1), size_base10(num2))
--------------->     m2 = floor (m / 2) 
--------------->     /* m2 = ceil (m / 2) will also work */
--------------->     /* Split the digit sequences in the middle. */
--------------->     high1, low1 = split_at (num1, m2)
--------------->     high2, low2 = split_at (num2, m2)
--------------->     /* 3 recursive calls made to numbers approximately half the size. */
--------------->     z0 = karatsuba (low1, low2)
--------------->     z1 = karatsuba (low1 + high1, low2 + high2)
--------------->     z2 = karatsuba (high1, high2)
--------------->     return (z2 × 10 ^ (m2 × 2)) + ((z1 - z2 - z0) × 10 ^ m2) + z0

-------> Schönhage–Strassen algorithm
---------> The Schönhage–Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. 
-----------> It was developed by Arnold Schönhage and Volker Strassen in 1971.[1] The run-time bit complexity is, in Big O notation, O ( n ⋅ log ⁡ n ⋅ log ⁡ log ⁡ n ) for two n-digit numbers. 
-----------> The algorithm uses recursive fast Fourier transforms in rings with 2n+1 elements, a specific type of number theoretic transform.
---------> The Schönhage–Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, 
-----------> when a new method, Fürer's algorithm, was announced with lower asymptotic complexity;. 
-----------> The current best multiplication algorithm in terms of asymptotic complexity is by David Harvey and Joris van der Hoeven, which gives O(n log n) complexity.
-----------> However, these improvements are too insignificant to be seen in integer sizes seen in practice (see Galactic algorithms).
---------> In practice the Schönhage–Strassen algorithm starts to outperform older methods such as Karatsuba and Toom–Cook multiplication for numbers beyond 2215 to 2217 (10,000 to 40,000 decimal digits).
-----------> The GNU Multi-Precision Library uses it[7] for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture.
-----------> There is a Java implementation of Schönhage–Strassen which uses it above 74,000 decimal digits.[10]
---------> Applications of the Schönhage–Strassen algorithm include mathematical empiricism, such as the Great Internet Mersenne Prime Search and computing approximations of π, 
-----------> as well as practical applications such as Kronecker substitution, in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication;
-----------> this is used in practice by GMP-ECM for Lenstra elliptic curve factorization.

-------> Toom–Cook multiplication
---------> The Schönhage–Strassen algorithm is an asymptotically fast multiplication algorithm for large integers. 
-----------> It was developed by Arnold Schönhage and Volker Strassen in 1971.
-----------> The run-time bit complexity is, in Big O notation, O ( n ⋅ log ⁡ n ⋅ log ⁡ log ⁡ n ) for two n-digit numbers. 
-----------> The algorithm uses recursive fast Fourier transforms in rings with 2n+1 elements, a specific type of number theoretic transform.
---------> The Schönhage–Strassen algorithm was the asymptotically fastest multiplication method known from 1971 until 2007, when a new method, 
-----------> Fürer's algorithm, was announced with lower asymptotic complexity;[2]. 
-----------> The current best multiplication algorithm in terms of asymptotic complexity is by David Harvey and Joris van der Hoeven, which gives O(n log n) complexity. 
-----------> However, these improvements are too insignificant to be seen in integer sizes seen in practice (see Galactic algorithms).
---------> In practice the Schönhage–Strassen algorithm starts to outperform older methods such as Karatsuba and Toom–Cook multiplication for numbers beyond 2215 to 2217 (10,000 to 40,000 decimal digits). 
-----------> The GNU Multi-Precision Library uses it for values of at least 1728 to 7808 64-bit words (33,000 to 150,000 decimal digits), depending on architecture. 
-----------> There is a Java implementation of Schönhage–Strassen which uses it above 74,000 decimal digits.
---------> Applications of the Schönhage–Strassen algorithm include mathematical empiricism, 
-----------> such as the Great Internet Mersenne Prime Search and computing approximations of π, 
-----------> as well as practical applications such as Kronecker substitution, 
-----------> in which multiplication of polynomials with integer coefficients can be efficiently reduced to large integer multiplication; 
-----------> this is used in practice by GMP-ECM for Lenstra elliptic curve factorization.

---> Modular square root (Quadratic residue): 
-----> This computing square roots modulo a prime number.
-------> In number theory, an integer q is called a quadratic residue modulo n if it is congruent to a perfect square modulo n; i.e., if there exists an integer x such that:
---------> x^2 ≡ q ( mod n ).
-----> Otherwise, q is called a quadratic nonresidue modulo n.
-------> Originally an abstract mathematical concept from the branch of number theory known as modular arithmetic, 
-------> quadratic residues are now used in applications ranging from acoustical engineering to cryptography and the factoring of large numbers. 

-----> Tonelli–Shanks algorithm
-------> The Tonelli–Shanks algorithm (referred to by Shanks as the RESSOL algorithm) is used in modular arithmetic to solve for r in a congruence of the form r2 ≡ n (mod p), 
---------> where p is a prime: that is, to find a square root of n modulo p.
-------> Tonelli–Shanks cannot be used for composite moduli: finding square roots modulo composite numbers is a computational problem equivalent to integer factorization.
-------> An equivalent, but slightly more redundant version of this algorithm was developed by Alberto Tonelli[2][3] in 1891. 
---------> The version discussed here was developed independently by Daniel Shanks in 1973, who explained:
-----------> My tardiness in learning of these historical references was because I had lent Volume 1 of Dickson's History to a friend and it was never returned.[4] 
-------> According to Dickson, Tonelli's algorithm can take square roots of x modulo prime powers pλ apart from primes. 

-----> Cipolla's algorithm
-------> In computational number theory, Cipolla's algorithm is a technique for solving a congruence of the form
---------> x^2 ≡ n ( mod p ),
---------> where x,n is an element of Fp, so n is the square of x, and where p is an odd prime. 
-------> Here F p {\displaystyle \mathbf {F} _{p}} \mathbf {F} _{p} denotes the finite field with p elements; { 0 , 1 , … , p − 1 }. 
---------> The algorithm is named after Michele Cipolla, an Italian mathematician who discovered it in 1907.
-------> Apart from prime moduli, Cipolla's algorithm is also able to take square roots modulo prime powers.

-----> Berlekamp's root finding algorithm
-------> In number theory, Berlekamp's root finding algorithm, also called the Berlekamp–Rabin algorithm, 
-------> is the probabilistic method of finding roots of polynomials over a field Zp. 
-------> The method was discovered by Elwyn Berlekamp in 1970 as an auxiliary to the algorithm for polynomial factorization over finite fields. 
-------> The algorithm was later modified by Rabin for arbitrary finite fields in 1979.
-------> The method was also independently discovered before Berlekamp by other researchers.

---> Odlyzko–Schönhage algorithm: 
-----> This calculates nontrivial zeroes of the Riemann zeta function.
-----> In mathematics, the Odlyzko–Schönhage algorithm is a fast algorithm for evaluating the Riemann zeta function at many points, introduced by (Odlyzko & Schönhage 1988). 
-------> The main point is the use of the fast Fourier transform to speed up the evaluation of a finite Dirichlet series of length N at O(N) 
-------> equally spaced values from O(N2) to O(N1+ε) steps (at the cost of storing O(N1+ε) intermediate values). 
-------> The Riemann–Siegel formula used for calculating the Riemann zeta function with imaginary part T uses a finite Dirichlet series with about N = T1/2 terms, 
-------> so when finding about N values of the Riemann zeta function it is sped up by a factor of about T1/2.
-------> This reduces the time to find the zeros of the zeta function with imaginary part at most T from about T3/2+ε steps to about T1+ε steps. 

---> Lenstra–Lenstra–Lovász algorithm (also known as LLL algorithm): 
-----> This find a short, nearly orthogonal lattice basis in polynomial time
-----> The Lenstra–Lenstra–Lovász (LLL) lattice basis reduction algorithm is a polynomial time lattice reduction algorithm invented by Arjen Lenstra, Hendrik Lenstra and László Lovász in 1982. 
-------> Given a basis B = { b 1 , b 2 , … , b d } with n-dimensional integer coordinates, 
-------> for a lattice L (a discrete subgroup of Rn) with d ≤ n,
-------> the LLL algorithm calculates an LLL-reduced (short, nearly orthogonal) lattice basis in time O ( d 5 n log 3 ⁡ B ) 
-------> where B is the largest length of bi under the Euclidean norm, that is, B = max ( ‖ b 1 ‖ 2 , ‖ b 2 ‖ 2 , … , ‖ b d ‖ 2 ).
-----> The original applications were to give polynomial-time algorithms for factorizing polynomials with rational coefficients, 
-------> for finding simultaneous rational approximations to real numbers, and for solving the integer linear programming problem in fixed dimensions. 

---> Primality tests: 
-----> This determining whether a given number is prime.
-------> A primality test is an algorithm for determining whether an input number is prime. 
-------> Among other fields of mathematics, it is used for cryptography. 
-------> Unlike integer factorization, primality tests do not generally give prime factors, only stating whether the input number is prime or not. 
-------> Factorization is thought to be a computationally difficult problem, whereas primality testing is comparatively easy (its running time is polynomial in the size of the input). 
-------> Some primality tests prove that a number is prime, while others like Miller–Rabin prove that a number is composite. 
-------> Therefore, the latter might more accurately be called compositeness tests instead of primality tests. 

-------> AKS primality test
---------> The AKS primality test (also known as Agrawal–Kayal–Saxena primality test and cyclotomic AKS test) 
-----------> is a deterministic primality-proving algorithm created and published by Manindra Agrawal, Neeraj Kayal, and Nitin Saxena, 
-----------> computer scientists at the Indian Institute of Technology Kanpur, on August 6, 2002, in an article titled "PRIMES is in P".
-----------> The algorithm was the first that can provably determine whether any given number is prime or composite in polynomial time, 
-----------> without relying on mathematical conjectures such as the generalized Riemann hypothesis. 
-----------> The proof is also notable for not relying on the field of analysis.
-----------> In 2006 the authors received both the Gödel Prize and Fulkerson Prize for their work. 

-------> Baillie–PSW primality test
---------> The Baillie–PSW primality test is a probabilistic primality testing algorithm that determines whether a number is composite or is a probable prime. 
-----------> It is named after Robert Baillie, Carl Pomerance, John Selfridge, and Samuel Wagstaff.
---------> The Baillie–PSW test is a combination of a strong Fermat probable prime test to base 2 and a strong Lucas probable prime test. 
-----------> The Fermat and Lucas test each have their own list of pseudoprimes, that is, composite numbers that pass the test. 
-----------> For example, the first ten strong pseudoprimes to base 2 are
-------------> 2047, 3277, 4033, 4681, 8321, 15841, 29341, 42799, 49141, and 52633 (sequence A001262 in the OEIS).
-----------> The first ten strong Lucas pseudoprimes (with Lucas parameters (P, Q) defined by Selfridge's Method A) are
-------------> 5459, 5777, 10877, 16109, 18971, 22499, 24569, 25199, 40309, and 58519 (sequence A217255 in the OEIS).

-------> Fermat primality test
---------> The Fermat primality test is a probabilistic test to determine whether a number is a probable prime. 
---------> Algorithm
-----------> The algorithm can be written as follows:
-------------> Inputs: n: a value to test for primality, n>3; k: a parameter that determines the number of times to test for primality
-------------> Output: composite if n is composite, otherwise probably prime
-------------> Repeat k times:
---------------> Pick a randomly in the range [2, n − 2]
---------------> If a^(n−1) ≢ 1 ( mod n ), then return composite
-------------> If composite is never returned: return probably prime
-----------> The a values 1 and n-1 are not used as the equality holds for all n and all odd n respectively, hence testing them adds no value. 

-------> Lucas primality test
---------> In computational number theory, the Lucas test is a primality test for a natural number n; it requires that the prime factors of n−1 be already known.
-----------> It is the basis of the Pratt certificate that gives a concise verification that n is prime. 
---------> Algorithm
-----------> The algorithm can be written in pseudocode as follows:
-----------> algorithm lucas_primality_test is
----------->     input: n > 2, an odd integer to be tested for primality.
----------->            k, a parameter that determines the accuracy of the test.
----------->     output: prime if n is prime, otherwise composite or possibly composite.
----------->     determine the prime factors of n−1.
----------->     LOOP1: repeat k times:
----------->         pick a randomly in the range [2, n − 1]
----------->             if a^(n−1) ≢ 1 ( mod n ) then
----------->                 return composite
----------->             else 
----------->                 LOOP2: for all prime factors q of n−1:
----------->                     if a^(n−1) ≢ 1 ( mod n ) then
----------->                         if we checked this equality for all prime factors of n−1 then
----------->                             return prime
----------->                         else
----------->                             continue LOOP2
----------->                     else # a^((n−1)/q) ≡ 1 ( mod n )
----------->                         continue LOOP1
----------->     return possibly composite.

-------> Miller–Rabin primality test
---------> The Miller–Rabin primality test or Rabin–Miller primality test is a probabilistic primality test: 
-----------> an algorithm which determines whether a given number is likely to be prime, similar to the Fermat primality test and the Solovay–Strassen primality test.
---------> It is of historical significance in the search for a polynomial-time deterministic primality test. 
-----------> Its probabilistic variant remains widely used in practice, as one of the simplest and fastest tests known.
---------> Gary L. Miller discovered the test in 1976; Miller's version of the test is deterministic, 
-----------> but its correctness relies on the unproven extended Riemann hypothesis.
-----------> Michael O. Rabin modified it to obtain an unconditional probabilistic algorithm in 1980.
---------> Miller–Rabin test
-----------> The algorithm can be written in pseudocode as follows. 
-----------> The parameter k determines the accuracy of the test. 
-----------> The greater the number of rounds, the more accurate the result.
-------------> Input #1: n > 3, an odd integer to be tested for primality
-------------> Input #2: k, the number of rounds of testing to perform
-------------> Output: "composite" if n is found to be composite, "probably prime" otherwise
-------------> write n as 2s·d + 1 with d odd (by factoring out powers of 2 from n − 1)
-------------> WitnessLoop: repeat k times:
------------->     pick a random integer a in the range [2, n − 2]
------------->     x ← ad mod n
------------->     if x = 1 or x = n − 1 then
------------->         continue WitnessLoop
------------->     repeat s − 1 times:
------------->         x ← x2 mod n
------------->         if x = n − 1 then
------------->             continue WitnessLoop
------------->     return "composite"
-------------> return "probably prime"


-------> Sieve of Atkin
---------> In mathematics, the sieve of Atkin is a modern algorithm for finding all prime numbers up to a specified integer. 
-----------> Compared with the ancient sieve of Eratosthenes, which marks off multiples of primes, 
-----------> the sieve of Atkin does some preliminary work and then marks off multiples of squares of primes, 
-----------> thus achieving a better theoretical asymptotic complexity. 
-----------> It was created in 2003 by A. O. L. Atkin and Daniel J. Bernstein.
---------> Pseudocode
-----------> The following is pseudocode which combines Atkin's algorithms 3.1, 3.2, and 3.3 by using a combined set "s" 
-------------> of all the numbers modulo 60 excluding those which are multiples of the prime numbers 2, 3, and 5, as per the algorithms, 
-------------> for a straightforward version of the algorithm that supports optional bit packing of the wheel; 
-------------> although not specifically mentioned in the referenced paper, 
-------------> this pseudocode eliminates some obvious combinations of odd/even x's/y's in order to reduce computation where 
-------------> those computations would never pass the modulo tests anyway (i.e. would produce even numbers, or multiples of 3 or 5):
---------------> limit ← 1000000000        // arbitrary search limit
---------------> // set of wheel "hit" positions for a 2/3/5 wheel rolled twice as per the Atkin algorithm
---------------> s ← {1,7,11,13,17,19,23,29,31,37,41,43,47,49,53,59}
---------------> // Initialize the sieve with enough wheels to include limit:
---------------> for n ← 60 × w + x where w ∈ {0,1,...,limit ÷ 60}, x ∈ s:
--------------->     is_prime(n) ← false
---------------> // Put in candidate primes:
---------------> //   integers which have an odd number of
---------------> //   representations by certain quadratic forms.
---------------> // Algorithm step 3.1:
---------------> for n ≤ limit, n ← 4x²+y² where x ∈ {1,2,...} and y ∈ {1,3,...} // all x's odd y's
--------------->     if n mod 60 ∈ {1,13,17,29,37,41,49,53}:
--------------->         is_prime(n) ← ¬is_prime(n)   // toggle state
---------------> // Algorithm step 3.2:
---------------> for n ≤ limit, n ← 3x²+y² where x ∈ {1,3,...} and y ∈ {2,4,...} // only odd x's
--------------->     if n mod 60 ∈ {7,19,31,43}:                                 // and even y's
--------------->         is_prime(n) ← ¬is_prime(n)   // toggle state
---------------> // Algorithm step 3.3:
---------------> for n ≤ limit, n ← 3x²-y² where x ∈ {2,3,...} and y ∈ {x-1,x-3,...,1} //all even/odd
--------------->     if n mod 60 ∈ {11,23,47,59}:                                   // odd/even combos
--------------->         is_prime(n) ← ¬is_prime(n)   // toggle state
---------------> // Eliminate composites by sieving, only for those occurrences on the wheel:
---------------> for n² ≤ limit, n ← 60 × w + x where w ∈ {0,1,...}, x ∈ s, n ≥ 7:
--------------->     if is_prime(n):
--------------->         // n is prime, omit multiples of its square; this is sufficient 
--------------->         // because square-free composites can't get on this list
--------------->         for c ≤ limit, c ← n² × (60 × w + x) where w ∈ {0,1,...}, x ∈ s:
--------------->             is_prime(c) ← false
---------------> // one sweep to produce a sequential list of primes up to limit:
---------------> output 2, 3, 5
---------------> for 7 ≤ n ≤ limit, n ← 60 × w + x where w ∈ {0,1,...}, x ∈ s:
--------------->     if is_prime(n): output n

-------> Sieve of Eratosthenes
---------> In mathematics, the sieve of Eratosthenes is an ancient algorithm for finding all prime numbers up to any given limit.
---------> It does so by iteratively marking as composite (i.e., not prime) the multiples of each prime, starting with the first prime number, 2. 
-----------> The multiples of a given prime are generated as a sequence of numbers starting from that prime, 
-----------> with constant difference between them that is equal to that prime.
-----------> This is the sieve's key distinction from using trial division to sequentially test each candidate number for divisibility by each prime.
-----------> Once all the multiples of each discovered prime have been marked as composites, the remaining unmarked numbers are primes. 
---------> One of a number of prime number sieves, it is one of the most efficient ways to find all of the smaller primes. 
-----------> It may be used to find primes in arithmetic progressions.
---------> Algorithm
-----------> A prime number is a natural number that has exactly two distinct natural number divisors: the number 1 and itself.
-----------> To find all the prime numbers less than or equal to a given integer n by Eratosthenes' method:
-------------> (1) Create a list of consecutive integers from 2 through n: (2, 3, 4, ..., n).
-------------> (2) Initially, let p equal 2, the smallest prime number.
-------------> (3) Enumerate the multiples of p by counting in increments of p from 2p to n, and mark them in the list (these will be 2p, 3p, 4p, ...; the p itself should not be marked).
-------------> (4) Find the smallest number in the list greater than p that is not marked. 
---------------> If there was no such number, stop. 
---------------> Otherwise, let p now equal this new number (which is the next prime), and repeat from step 3.
-------------> (5) When the algorithm terminates, the numbers remaining not marked in the list are all the primes below n.
-----------> The main idea here is that every value given to p will be prime, because if it were composite it would be marked as a multiple of some other, smaller prime. 
-------------> Note that some of the numbers may be marked more than once (e.g., 15 will be marked both for 3 and 5).
-----------> As a refinement, it is sufficient to mark the numbers in step 3 starting from p2, as all the smaller multiples of p will have already been marked at that point. 
-------------> This means that the algorithm is allowed to terminate in step 4 when p2 is greater than n.[1]
-----------> Another refinement is to initially list odd numbers only, (3, 5, ..., n), and count in increments of 2p from p2 in step 3, 
-------------> thus marking only odd multiples of p. This actually appears in the original algorithm.
-------------> This can be generalized with wheel factorization, forming the initial list only from numbers coprime with the first few primes and not just from odds (i.e., numbers coprime with 2), 
-------------> and counting in the correspondingly adjusted increments so that only such multiples of p are generated that are coprime with those small primes, in the first place.[6] 
-----------> The earliest known reference to the sieve (Ancient Greek: κόσκινον Ἐρατοσθένους, kóskinon Eratosthénous) is in Nicomachus of Gerasa's Introduction to Arithmetic,
-------------> an early 2nd cent. CE book, which describes it and attributes it to Eratosthenes of Cyrene, a 3rd cent. BCE Greek mathematician.

-------> Sieve of Sundaram
---------> In mathematics, the sieve of Sundaram is a variant of the sieve of Eratosthenes, 
-----------> a simple deterministic algorithm for finding all the prime numbers up to a specified integer. 
-----------> It was discovered by Indian student S. P. Sundaram in 1934.[1][2]
---------> Algorithm
-----------> Sieve of Sundaram: algorithm steps for primes below 202 (unoptimized).
-----------> Start with a list of the integers from 1 to n. From this list, remove all numbers of the form i + j + 2ij where:
-------------> i,j is an element of N,   1 ≤ i ≤ j 
-------------> i + j + 2*i*j ≤ n
-----------> The remaining numbers are doubled and incremented by one, giving a list of the odd prime numbers (i.e., all primes except 2) below 2n + 2.
-----------> The sieve of Sundaram sieves out the composite numbers just as the sieve of Eratosthenes does, 
-------------> but even numbers are not considered; the work of "crossing out" the multiples of 2 is done by the final double-and-increment step. 
-------------> Whenever Eratosthenes' method would cross out k different multiples of a prime 2i+1, Sundaram's method crosses out i + j ( 2 i + 1 ). 



-> Numerical algorithms
---> Numerical analysis is the study of algorithms that use numerical approximation (as opposed to symbolic manipulations) 
-----> for the problems of mathematical analysis (as distinguished from discrete mathematics). 
-----> It is the study of numerical methods that attempt at finding approximate solutions of problems rather than the exact ones.

---> Differential equation solving
-----> In mathematics, a differential equation is an equation that relates one or more unknown functions and their derivatives.
-----> In applications, the functions generally represent physical quantities, the derivatives represent their rates of change, and the differential equation defines a relationship between the two. 
-----> Such relations are common; therefore, differential equations play a prominent role in many disciplines including engineering, physics, economics, and biology. 

-----> Euler method
-------> In mathematics and computational science, the Euler method (also called forward Euler method) 
---------> is a first-order numerical procedure for solving ordinary differential equations (ODEs) with a given initial value. 
---------> It is the most basic explicit method for numerical integration of ordinary differential equations and is the simplest Runge–Kutta method. 
---------> The Euler method is named after Leonhard Euler, who treated it in his book Institutionum calculi integralis (published 1768–1870).
-------> The Euler method is a first-order method, which means that the local error (error per step) is proportional to the square of the step size, 
---------> and the global error (error at a given time) is proportional to the step size. 
---------> The Euler method often serves as the basis to construct more complex methods, e.g., predictor–corrector method.
 
-----> Backward Euler method
-------> In numerical analysis and scientific computing, the backward Euler method (or implicit Euler method) 
---------> is one of the most basic numerical methods for the solution of ordinary differential equations. 
---------> It is similar to the (standard) Euler method, but differs in that it is an implicit method. 
---------> The backward Euler method has error of order one in time. 

-----> Trapezoidal rule (differential equations)
-------> In numerical analysis and scientific computing, the trapezoidal rule is a numerical method to solve ordinary differential equations derived from the trapezoidal rule for computing integrals. 
---------> The trapezoidal rule is an implicit second-order method, which can be considered as both a Runge–Kutta method and a linear multistep method. 
-------> Method
---------> Suppose that we want to solve the differential equation
-----------> y′ = f(t, y) . 
---------> The trapezoidal rule is given by the formula
-----------> y n + 1 = y n + 1 2 h ( f ( t n , y n ) + f ( t n + 1 , y n + 1 ) ) ,
---------> where h = tn+1 − tn
---------> This is an implicit method: the value yn+1  appears on both sides of the equation, and to actually calculate it, we have to solve an equation which will usually be nonlinear. 
-----------> One possible method for solving this equation is Newton's method. 
-----------> We can use the Euler method to get a fairly good estimate for the solution, which can be used as the initial guess of Newton's method.
-----------> Cutting short, using only the guess from Eulers method is equivalent to performing Heun's method. 

-----> Linear multistep methods
-------> Linear multistep methods are used for the numerical solution of ordinary differential equations. 
---------> Conceptually, a numerical method starts from an initial point and then takes a short step forward in time to find the next solution point. 
---------> The process continues with subsequent steps to map out the solution. 
---------> Single-step methods (such as Euler's method) refer to only one previous point and its derivative to determine the current value. 
---------> Methods such as Runge–Kutta take some intermediate steps (for example, a half-step) to obtain a higher order method, 
---------> but then discard all previous information before taking a second step. 
---------> Multistep methods attempt to gain efficiency by keeping and using the information from previous steps rather than discarding it. 
---------> Consequently, multistep methods refer to several previous points and derivative values. 
---------> In the case of linear multistep methods, a linear combination of the previous points and derivative values is used. 

-----> Runge–Kutta methods
-------> In numerical analysis, the Runge–Kutta methods (English: /ˈrʊŋəˈkʊtɑː/ (listen) RUUNG-ə-KUUT-tah[1]) are a family of implicit and explicit iterative methods, 
---------> which include the Euler method, used in temporal discretization for the approximate solutions of simultaneous nonlinear equations.
-------> These methods were developed around 1900 by the German mathematicians Carl Runge and Wilhelm Kutta. 

-----> Multigrid methods (MG methods), a group of algorithms for solving differential equations using a hierarchy of discretizations
-------> In numerical analysis, a multigrid method (MG method) is an algorithm for solving differential equations using a hierarchy of discretizations. 
---------> They are an example of a class of techniques called multiresolution methods, very useful in problems exhibiting multiple scales of behavior. 
---------> For example, many basic relaxation methods exhibit different rates of convergence for short- and long-wavelength components, 
---------> suggesting these different scales be treated differently, as in a Fourier analysis approach to multigrid.
-------> MG methods can be used as solvers as well as preconditioners. 

-----> Partial differential equation:
-------> In mathematics, a partial differential equation (PDE) is an equation which imposes relations between the various partial derivatives of a multivariable function. 

-------> Finite difference method
---------> In numerical analysis, finite-difference methods (FDM) are a class of numerical techniques for solving differential equations by approximating derivatives with finite differences. 
-----------> Both the spatial domain and time interval (if applicable) are discretized, or broken into a finite number of steps, 
-----------> and the value of the solution at these discrete points is approximated by solving algebraic equations containing finite differences and values from nearby points.
---------> Finite difference methods convert ordinary differential equations (ODE) or partial differential equations (PDE), which may be nonlinear, 
-----------> into a system of linear equations that can be solved by matrix algebra techniques. 
-----------> Modern computers can perform these linear algebra computations efficiently which, 
-----------> along with their relative ease of implementation, has led to the widespread use of FDM in modern numerical analysis. 
-----------> Today, FDM are one of the most common approaches to the numerical solution of PDE, along with finite element methods.[1] 

-------> Crank–Nicolson method for diffusion equations
---------> In numerical analysis, the Crank–Nicolson method is a finite difference method used for numerically solving the heat equation and similar partial differential equations.
-----------> It is a second-order method in time. 
-----------> It is implicit in time, can be written as an implicit Runge–Kutta method, and it is numerically stable. 
-----------> The method was developed by John Crank and Phyllis Nicolson in the mid 20th century.
---------> For diffusion equations (and many other equations), it can be shown the Crank–Nicolson method is unconditionally stable.
-----------> However, the approximate solutions can still contain (decaying) spurious oscillations if the ratio of time step Δ t times the thermal diffusivity to the square of space step, 
-----------> Δ x 2, is large (typically, larger than 1/2 per Von Neumann stability analysis). 
-----------> For this reason, whenever large time steps or high spatial resolution is necessary, 
-----------> the less accurate backward Euler method is often used, which is both stable and immune to oscillations.[citation needed] 

-------> Lax–Wendroff for wave equations
---------> The Lax–Wendroff method, named after Peter Lax and Burton Wendroff, 
-----------> is a numerical method for the solution of hyperbolic partial differential equations, based on finite differences. 
-----------> It is second-order accurate in both space and time.
-----------> This method is an example of explicit time integration where the function that defines the governing equation is evaluated at the current time. 

-----> Verlet integration (French pronunciation: ​[vɛʁˈlɛ]): integrate Newton's equations of motion
-------> Verlet integration (French pronunciation: ​[vɛʁˈlɛ]) is a numerical method used to integrate Newton's equations of motion.
---------> It is frequently used to calculate trajectories of particles in molecular dynamics simulations and computer graphics. 
---------> The algorithm was first used in 1791 by Delambre and has been rediscovered many times since then, most recently by Loup Verlet in the 1960s for use in molecular dynamics. 
---------> It was also used by Cowell and Crommelin in 1909 to compute the orbit of Halley's Comet, 
---------> and by Carl Størmer in 1907 to study the trajectories of electrical particles in a magnetic field (hence it is also called Störmer's method).
---------> The Verlet integrator provides good numerical stability, as well as other properties that are important in physical systems such as time reversibility 
---------> and preservation of the symplectic form on phase space, at no significant additional computational cost over the simple Euler method. 



-> Elementary and special functions

---> Special functions
-----> Special functions are particular mathematical functions that have more or less established names 
-----> and notations due to their importance in mathematical analysis, functional analysis, geometry, physics, or other applications.
-----> The term is defined by consensus, and thus lacks a general formal definition, but the List of mathematical functions contains functions that are commonly accepted as special. 

-----> Computation of π:

-------> Borwein's algorithm: 
---------> This is an algorithm to calculate the value of 1/π
---------> In mathematics, Borwein's algorithm is an algorithm devised by Jonathan and Peter Borwein to calculate the value of 1/π. 
-----------> They devised several other algorithms. 
-----------> They published the book Pi and the AGM – A Study in Analytic Number Theory and Computational Complexity.

-------> Gauss–Legendre algorithm: 
---------> This computes the digits of pi
-----------> The Gauss–Legendre algorithm is an algorithm to compute the digits of π. 
-----------> It is notable for being rapidly convergent, with only 25 iterations producing 45 million correct digits of π. 
-----------> However, it has some drawbacks (for example, it is computer memory-intensive) 
-----------> and therefore all record-breaking calculations for many years have used other methods, almost always the Chudnovsky algorithm. 
-----------> For details, see Chronology of computation of π. 

-------> Chudnovsky algorithm: 
---------> This is a fast method for calculating the digits of π
-----------> The Chudnovsky algorithm is a fast method for calculating the digits of π, based on Ramanujan’s π formulae. It was published by the Chudnovsky brothers in 1988.[1]
-----------> It was used in the world record calculations of 2.7 trillion digits of π in December 2009,
-------------> 10 trillion digits in October 2011,[3][4] 22.4 trillion digits in November 2016,[5] 31.4 trillion digits in September 2018–January 2019,
-------------> 50 trillion digits on January 29, 2020,[7] 62.8 trillion digits on August 14, 2021,[8] and 100 trillion digits on March 21, 2022.[9] 

-------> Bailey–Borwein–Plouffe formula: (
---------> This BBP formula) a spigot algorithm for the computation of the nth binary digit of π
---------> The Bailey–Borwein–Plouffe formula (BBP formula) is a formula for π. 
---------> It was discovered in 1995 by Simon Plouffe and is named after the authors of the article in which it was published, David H. Bailey, Peter Borwein, and Plouffe.
---------> Before that, it had been published by Plouffe on his own site.

-----> Division algorithms: 
-------> This is for computing quotient and/or remainder of two numbers
-------> A division algorithm is an algorithm which, given two integers N and D, 
---------> computes their quotient and/or remainder, the result of Euclidean division. 
---------> Some are applied by hand, while others are employed by digital circuit designs and software.
-------> Division algorithms fall into two main categories: slow division and fast division. 
---------> Slow division algorithms produce one digit of the final quotient per iteration. 
---------> Examples of slow division include restoring, non-performing restoring, non-restoring, and SRT division. 
---------> Fast division methods start with a close approximation to the final quotient and produce twice as many digits of the final quotient on each iteration. 
---------> Newton–Raphson and Goldschmidt algorithms fall into this category.
-------> Variants of these algorithms allow using fast multiplication algorithms. 
---------> It results that, for large integers, the computer time needed for a division is the same, up to a constant factor, 
---------> as the time needed for a multiplication, whichever multiplication algorithm is used.
-------> Discussion will refer to the form N/D=(Q,R), where
-----------> N = numerator (dividend)
-----------> D = denominator (divisor)
---------> is the input, and
-----------> Q = quotient
-----------> R = remainder
---------> is the output. 

-------> Long division
---------> In arithmetic, long division is a standard division algorithm suitable for dividing multi-digit Hindu-Arabic numerals (Positional notation) that is simple enough to perform by hand. 
-----------> It breaks down a division problem into a series of easier steps.
---------> As in all division problems, one number, called the dividend, is divided by another, called the divisor, producing a result called the quotient. 
-----------> It enables computations involving arbitrarily large numbers to be performed by following a series of simple steps.
-----------> The abbreviated form of long division is called short division, which is almost always used instead of long division when the divisor has only one digit. 
-----------> Chunking (also known as the partial quotients method or the hangman method) is a less mechanical form of long division prominent in the U.K which contributes to a more holistic understanding of the division process. 

-------> Restoring division
---------> Restoring division operates on fixed-point fractional numbers and depends on the assumption 0 < D < N.
---------> The quotient digits q are formed from the digit set {0,1}.
---------> The basic algorithm for binary (radix 2) restoring division is:
-----------> R := N
-----------> D := D << n            -- R and D need twice the word width of N and Q
-----------> for i := n − 1 .. 0 do  -- For example 31..0 for 32 bits
----------->   R := 2 * R − D          -- Trial subtraction from shifted value (multiplication by 2 is a shift in binary representation)
----------->   if R >= 0 then
----------->     q(i) := 1          -- Result-bit 1
----------->   else
----------->     q(i) := 0          -- Result-bit 0
----------->     R := R + D         -- New partial remainder is (restored) shifted value
----------->   end
-----------> end
-----------> -- Where: N = numerator, D = denominator, n = #bits, R = partial remainder, q(i) = bit #i of quotient
---------> Non-performing restoring division is similar to restoring division except that the value of 2R is saved, so D does not need to be added back in for the case of R < 0. 

-------> Non-restoring division
---------> Non-restoring division uses the digit set {−1, 1} for the quotient digits instead of {0, 1}. 
---------> The algorithm is more complex, but has the advantage when implemented in hardware that there is only one decision and addition/subtraction per quotient bit; 
---------> there is no restoring step after the subtraction,[2] which potentially cuts down the numbers of operations by up to half and lets it be executed faster.
---------> The basic algorithm for binary (radix 2) non-restoring division of non-negative numbers is:
-----------> R := N
-----------> D := D << n            -- R and D need twice the word width of N and Q
-----------> for i = n − 1 .. 0 do  -- for example 31..0 for 32 bits
----------->   if R >= 0 then
----------->     q[i] := +1
----------->     R := 2 * R − D
----------->   else
----------->     q[i] := −1
----------->     R := 2 * R + D
----------->   end if
-----------> end
-----------> -- Note: N=numerator, D=denominator, n=#bits, R=partial remainder, q(i)=bit #i of quotient.

-------> SRT division
---------> Named for its creators (Sweeney, Robertson, and Tocher), SRT division is a popular method for division in many microprocessor implementations.
-----------> SRT division is similar to non-restoring division, but it uses a lookup table based on the dividend and the divisor to determine each quotient digit.
---------> The most significant difference is that a redundant representation is used for the quotient. 
-----------> For example, when implementing radix-4 SRT division, each quotient digit is chosen from five possibilities: { −2, −1, 0, +1, +2 }. 
-----------> Because of this, the choice of a quotient digit need not be perfect; later quotient digits can correct for slight errors. 
-----------> (For example, the quotient digit pairs (0, +2) and (1, −2) are equivalent, since 0×4+2 = 1×4−2.) 
-----------> This tolerance allows quotient digits to be selected using only a few most-significant bits of the dividend and divisor, rather than requiring a full-width subtraction. 
-----------> This simplification in turn allows a radix higher than 2 to be used.
---------> Like non-restoring division, the final steps are a final full-width subtraction to resolve the last quotient bit, and conversion of the quotient to standard binary form.
---------> The Intel Pentium processor's infamous floating-point division bug was caused by an incorrectly coded lookup table. 
-----------> Five of the 1066 entries had been mistakenly omitted.

-------> Newton–Raphson division: 
---------> This uses Newton's method to find the reciprocal of D, and multiply that reciprocal by N to find the final quotient Q.
---------> Newton–Raphson uses Newton's method to find the reciprocal of d and multiply that reciprocal by N to find the final quotient Q.
---------> The steps of Newton–Raphson division are:
-----------> Calculate an estimate X0 for the reciprocal 1 / D of the divisor D.
-----------> Compute successively more accurate estimates X1, X2, …, XS, of the reciprocal. This is where one employs the Newton–Raphson method as such.
-----------> Compute the quotient by multiplying the dividend by the reciprocal of the divisor: Q = N X S.
 
-------> Goldschmidt division
-----------> Goldschmidt division[8] (after Robert Elliott Goldschmidt[9]) uses an iterative process of repeatedly 
-------------> multiplying both the dividend and divisor by a common factor Fi, chosen such that the divisor converges to 1. 
-------------> This causes the dividend to converge to the sought quotient Q:
---------------> Q = N D F 1 F 1 F 2 F 2 F … F … . 
-----------> The steps for Goldschmidt division are:
-------------> Generate an estimate for the multiplication factor Fi .
-------------> Multiply the dividend and divisor by Fi .
-------------> If the divisor is sufficiently close to 1, return the dividend, otherwise, loop to step 1.


-----> Hyperbolic and Trigonometric Functions:

-------> BKM algorithm: 
---------> This computes elementary functions using a table of logarithms.
---------> The BKM algorithm is a shift-and-add algorithm for computing elementary functions, 
-----------> first published in 1994 by Jean-Claude Bajard, Sylvanus Kla, and Jean-Michel Muller. 
-----------> BKM is based on computing complex logarithms (L-mode) and exponentials (E-mode) 
-----------> using a method similar to the algorithm Henry Briggs used to compute logarithms. 
-----------> By using a precomputed table of logarithms of negative powers of two, 
-----------> the BKM algorithm computes elementary functions using only integer add, shift, and compare operations.
-------> BKM is similar to CORDIC, but uses a table of logarithms rather than a table of arctangents. 
---------> On each iteration, a choice of coefficient is made from a set of nine complex numbers, 
---------> 1, 0, −1, i, −i, 1+i, 1−i, −1+i, −1−i, rather than only −1 or +1 as used by CORDIC. 
---------> BKM provides a simpler method of computing some elementary functions, and unlike CORDIC, BKM needs no result scaling factor. 
---------> The convergence rate of BKM is approximately one bit per iteration, like CORDIC, 
---------> but BKM requires more precomputed table elements for the same precision because the table stores logarithms of complex operands.
-------> As with other algorithms in the shift-and-add class, BKM is particularly well-suited to hardware implementation. 
---------> The relative performance of software BKM implementation in comparison to other methods such as polynomial or rational approximations 
---------> will depend on the availability of fast multi-bit shifts (i.e. a barrel shifter) or hardware floating point arithmetic. 

-------> CORDIC: 
---------> This computes hyperbolic and trigonometric functions using a table of arctangents.
---------> CORDIC (for COordinate Rotation DIgital Computer), also known as Volder's algorithm, 
-----------> or: Digit-by-digit method Circular CORDIC (Jack E. Volder),[1][2] Linear CORDIC, Hyperbolic CORDIC (John Stephen Walther), 
-----------> and Generalized Hyperbolic CORDIC (GH CORDIC) (Yuanyong Luo et al.),[5][6] is a simple and efficient algorithm to calculate trigonometric functions, 
-----------> hyperbolic functions, square roots, multiplications, divisions, and exponentials and logarithms with arbitrary base, 
-----------> typically converging with one digit (or bit) per iteration. CORDIC is therefore also an example of digit-by-digit algorithms. 
-----------> CORDIC and closely related methods known as pseudo-multiplication and pseudo-division or factor 
-----------> combining are commonly used when no hardware multiplier is available (e.g. in simple microcontrollers and FPGAs), 
-----------> as the only operations it requires are additions, subtractions, bitshift and lookup tables. 
-----------> As such, they all belong to the class of shift-and-add algorithms. 
-----------> In computer science, CORDIC is often used to implement floating-point arithmetic when the target platform lacks hardware multiply for cost or space reasons. 

-----> Exponentiation:
-------> Addition-chain exponentiation: 
---------> This exponentiation by positive integer powers that requires a minimal number of multiplications.
-----------> In mathematics and computer science, optimal addition-chain exponentiation 
-----------> is a method of exponentiation by positive integer powers that requires a minimal number of multiplications. 
-----------> This corresponds to the sequence A003313 on the Online Encyclopedia of Integer Sequences. 
-----------> It works by creating the shortest addition chain that generates the desired exponent. 
-----------> Each exponentiation in the chain can be evaluated by multiplying two of the earlier exponentiation results. 
-----------> More generally, addition-chain exponentiation may also refer to exponentiation by non-minimal addition chains 
-----------> constructed by a variety of algorithms (since a shortest addition chain is very difficult to find).
---------> The shortest addition-chain algorithm requires no more multiplications than binary exponentiation and usually less. 
-----------> The first example of where it does better is for a15, where the binary method needs six multiplications but a shortest addition chain requires only five:
-------------> a^15 = a × ( a × [ a × a^2 ]^2 )^2 (binary, 6 multiplications)
-------------> a^15 = a^3 × ( [ a^3 ]^2 )^2 (shortest addition chain, 5 multiplications).
---------> Table demonstrating how to do Exponentiation using Addition Chains
----------------------------------------------------------------------------
| Number of        | Actual         | Specific implementation of
| Multiplications  | Exponentiation | Addition Chains to do Exponentiation  
----------------------------------------------------------------------------
| 0                | a^1            | a
| 1                | a^2            | a × a
| 2                | a^3            | a × a × a
| 2                | a^4            | (a × a→b) × b
| 3                | a^5            | (a × a→b) × b × a
| 3                | a^6            | (a × a→b) × b × b
| 4                | a^7            | (a × a→b) × b × b × a
| 3                | a^8            | ((a × a→b) × b→d) × d
| 4                | a^9            | (a × a × a→c) × c × c
| 4                | a^10           | ((a × a→b) × b→d) × d × b
| 5                | a^11           | ((a × a→b) × b→d) × d × b × a
| 4                | a^12           | ((a × a→b) × b→d) × d × d
| 5                | a^13           | ((a × a→b) × b→d) × d × d × a
| 5                | a^14           | ((a × a→b) × b→d) × d × d × b
| 5                | a^15           | ((a × a→b) × b × a→e) × e × e
| 4                | a^16           | (((a × a→b) × b→d) × d→h) × h 

-------> Exponentiating by squaring: 
---------> This is an algorithm used for the fast computation of large integer powers of a number.
---------> In mathematics and computer programming, exponentiating by squaring is a general method for fast computation of large positive integer powers of a number, 
-----------> or more generally of an element of a semigroup, like a polynomial or a square matrix. 
-----------> Some variants are commonly referred to as square-and-multiply algorithms or binary exponentiation. 
-----------> These can be of quite general use, for example in modular arithmetic or powering of matrices. 
-----------> For semigroups for which additive notation is commonly used, like elliptic curves used in cryptography, this method is also referred to as double-and-add. 
---------> Algorithm
-----------> Recursive version
-----------> The method is based on the observation that, for a positive integer n, one has
----------->     x n = if n is odd {x(x^2)^(n−1)/2, if n is even (x^2)^n/2 , 
-----------> This may be implemented directly as the following recursive algorithm:
----------->   Function exp_by_squaring(x, n)
----------->     if n < 0  then return exp_by_squaring(1 / x, -n);
----------->     else if n = 0  then return  1;
----------->     else if n is even  then return exp_by_squaring(x * x,  n / 2);
----------->     else if n is odd  then return x * exp_by_squaring(x * x, (n - 1) / 2);
-----------> In each recursive call, the least significant digits of the binary representation of n is removed. 
-------------> It follows that the number of recursive calls is ⌈ log 2 ⁡ n ⌉, the number of bits of the binary representation of n. 
-------------> So this algorithm computes this number of squares and a lower number of multiplication, which is equal to the number of 1 in the binary representation of n. 
-------------> This logarithmic number of operations is to be compared with the trivial algorithm which requires n − 1 multiplications.
-----------> This algorithm is not tail-recursive. 
-------------> This implies that it requires an auxiliary memory that is roughly proportional 
-------------> (or higher, if one takes the increasing size of datas into account) to the number of recursive calls. 

-----> Montgomery reduction: 
-------> This is an algorithm that allows modular arithmetic to be performed efficiently when the modulus is large.
---------> In modular arithmetic computation, Montgomery modular multiplication, 
---------> more commonly referred to as Montgomery multiplication, is a method for performing fast modular multiplication. 
---------> It was introduced in 1985 by the American mathematician Peter L. Montgomery.
-------> Montgomery modular multiplication relies on a special representation of numbers called Montgomery form. 
---------> The algorithm uses the Montgomery forms of a and b to efficiently compute the Montgomery form of ab mod N. 
---------> The efficiency comes from avoiding expensive division operations. 
---------> Classical modular multiplication reduces the double-width product ab using division by N and keeping only the remainder. 
---------> This division requires quotient digit estimation and correction. 
---------> The Montgomery form, in contrast, depends on a constant R > N which is coprime to N, 
---------> and the only division necessary in Montgomery multiplication is division by R. 
---------> The constant R can be chosen so that division by R is easy, significantly improving the speed of the algorithm. 
---------> In practice, R is always a power of two, since division by powers of two can be implemented by bit shifting.
-------> The need to convert a and b into Montgomery form and their product out of Montgomery form means that 
---------> computing a single product by Montgomery multiplication is slower than the conventional or Barrett reduction algorithms. 
---------> However, when performing many multiplications in a row, as in modular exponentiation, intermediate results can be left in Montgomery form. 
---------> Then the initial and final conversions become a negligible fraction of the overall computation. 
---------> Many important cryptosystems such as RSA and Diffie–Hellman key exchange are based on arithmetic operations modulo a large odd number, 
---------> and for these cryptosystems, computations using Montgomery multiplication with R a power of two are faster than the available alternatives.

-----> Multiplication algorithms: fast multiplication of two numbers
-------> Booth's multiplication algorithm: 
---------> This is a multiplication algorithm that multiplies two signed binary numbers in two's complement notation
---------> Check discussion above

-------> Fürer's algorithm: 
---------> This is an integer multiplication algorithm for very large numbers possessing a very low asymptotic complexity

-------> Karatsuba algorithm: 
---------> This is an efficient procedure for multiplying large numbers
---------> Check discussion above

-------> Schönhage–Strassen algorithm: 
---------> This is an asymptotically fast multiplication algorithm for large integers
---------> Check discussion above

-------> Toom–Cook multiplication: (Toom3) 
---------> This is a multiplication algorithm for large integers
---------> Check discussion above

-----> Multiplicative inverse Algorithms: 
-------> This is for computing a number's multiplicative inverse (reciprocal).
-------> In mathematics, a multiplicative inverse or reciprocal for a number x, denoted by 1/x or x^−1, 
---------> is a number which when multiplied by x yields the multiplicative identity, 1. 
---------> The multiplicative inverse of a fraction a/b is b/a. For the multiplicative inverse of a real number, divide 1 by the number. 
---------> For example, the reciprocal of 5 is one fifth (1/5 or 0.2), and the reciprocal of 0.25 is 1 divided by 0.25, or 4. 
---------> The reciprocal function, the function f(x) that maps x to 1/x, is one of the simplest examples of a function which is its own inverse (an involution). 

-------> Newton's method
---------> In numerical analysis, Newton's method, also known as the Newton–Raphson method, 
-----------> named after Isaac Newton and Joseph Raphson, 
-----------> is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function. 
-----------> The most basic version starts with a single-variable function f defined for a real variable x, the function's derivative f′, and an initial guess x0 for a root of f.
-----------> If the function satisfies sufficient assumptions and the initial guess is close, then
-------------> x1 = x0 − f(x0)/f′(x0)
-----------> is a better approximation of the root than x0. 
-----------> Geometrically, (x1, 0) is the intersection of the x-axis and the tangent of the graph of f at (x0, f(x0)): 
-----------> that is, the improved guess is the unique root of the linear approximation at the initial point. 
-----------> The process is repeated as
-------------> xn+1 = xn − f(xn)/f′(xn)
-----------> until a sufficiently precise value is reached. 
---------> This algorithm is first in the class of Householder's methods, succeeded by Halley's method. 
---------> The method can also be extended to complex functions and to systems of equations. 

-----> Rounding functions: 
-------> This is the classic ways to round numbers.
---------> Rounding means replacing a number with an approximate value that has a shorter, simpler, or more explicit representation. 
---------> For example, replacing $23.4476 with $23.45, the fraction 312/937 with 1/3, or the expression √2 with 1.414.
-------> Rounding is often done to obtain a value that is easier to report and communicate than the original. 
---------> Rounding can also be important to avoid misleadingly precise reporting of a computed number, measurement, or estimate; for example, 
---------> a quantity that was computed as 123,456 but is known to be accurate only to within a few hundred units is usually better stated as "about 123,500". 
-------> https://en.wikipedia.org/wiki/Rounding
-------> Typical rounding problems include:
-------------------------------------------------------------------------------------------
| Rounding problem                    | Example input | Result   | Rounding criterion
-------------------------------------------------------------------------------------------
| Approximating an irrational number  | π             | 22/7      | 1-digit-denominator
| by a fraction                       |               |           |
-------------------------------------------------------------------------------------------
| Approximating a rational number     | 399 941       | 3/7       | 1-digit-denominator
| by another fraction with smaller    |               |           |
| numerator and denominator           |               |           |
-------------------------------------------------------------------------------------------
| Approximating a fraction, which     | 5/3           | 1.6667    | 4 decimal places
| have periodic decimal expansion,    |               |           | 
| by a finite decimal fraction        |               |           |
-------------------------------------------------------------------------------------------
| Approximating a fractional decimal  | 2.1784        | 2.18      | 2 decimal places
| number by one with fewer digits     |               |           | 
-------------------------------------------------------------------------------------------
| Approximating a decimal integer by  | 23,217        | 23,200    | 3 significant figures
| an integer with more trailing zeros |               |           | 
-------------------------------------------------------------------------------------------
| Approximating a large decimal       | 300,999,999   | 3.01×10^8 | 3 significant figures
| integer using scientific notation   |               |           | 
-------------------------------------------------------------------------------------------
| Approximating a value by a          | 48.2          | 45        | Multiple of 15
| multiple of a specified amount      |               |           | 
-------------------------------------------------------------------------------------------

-----> Spigot algorithm: 
-------> This is way to compute the value of a mathematical constant without knowing preceding digits.
-------> A spigot algorithm is an algorithm for computing the value of a transcendental number (such as π or e)
---------> that generates the digits of the number sequentially from left to right providing increasing precision as the algorithm proceeds. 
---------> Spigot algorithms also aim to minimize the amount of intermediate storage required. 
---------> The name comes from the sense of the word "spigot" for a tap or valve controlling the flow of a liquid. 
---------> Spigot algorithms can be contrasted with algorithms that store 
---------> and process complete numbers to produce successively more accurate approximations to the desired transcendental.
-------> Interest in spigot algorithms was spurred in the early days of computational mathematics by extreme constraints on memory, 
---------> and such an algorithm for calculating the digits of e appeared in a paper by Sale in 1968.
---------> In 1970, Abdali presented a more general algorithm to compute the sums of series in which the ratios of successive terms
---------> can be expressed as quotients of integer functions of term positions. 
---------> This algorithm is applicable to many familiar series for trigonometric functions, logarithms, and transcendental numbers because these series satisfy the above condition.
---------> The name "spigot algorithm" seems to have been coined by Stanley Rabinowitz and Stan Wagon, 
---------> whose algorithm for calculating the digits of π is sometimes referred to as "the spigot algorithm for π".[3]
-------> The spigot algorithm of Rabinowitz and Wagon is bounded, 
---------> in the sense that the number of terms of the infinite series that will be processed must be specified in advance. 
---------> The term "streaming algorithm"[4] indicates an approach without this restriction. 
---------> This allows the calculation to run indefinitely varying the amount of intermediate storage as the calculation progresses.
-------> A variant of the spigot approach uses an algorithm which can be used to 
---------> compute a single arbitrary digit of the transcendental without computing the preceding digits: 
---------> an example is the Bailey–Borwein–Plouffe formula, 
---------> a digit extraction algorithm for π which produces base 16 digits. 
---------> The inevitable truncation of the underlying infinite series of the algorithm 
---------> means that the accuracy of the result may be limited by the number of terms calculated. 

-----> Square and Nth root of a number:

-------> Alpha max plus beta min algorithm: 
---------> This is an approximation of the square-root of the sum of two squares.
---------> The alpha max plus beta min algorithm is a high-speed approximation of the square root of the sum of two squares. 
-----------> The square root of the sum of two squares, also known as Pythagorean addition, is a useful function, 
-----------> because it finds the hypotenuse of a right triangle given the two side lengths, 
-----------> the norm of a 2-D vector, or the magnitude |z| = a^2 + b^2 of a complex number z = a + bi given the real and imaginary parts.
---------> The algorithm avoids performing the square and square-root operations, instead using simple operations such as comparison, multiplication, and addition. 
-----------> Some choices of the α and β parameters of the algorithm allow the multiplication operation to be reduced 
-----------> to a simple shift of binary digits that is particularly well suited to implementation in high-speed digital circuitry.
---------> The approximation is expressed as
-----------> |z| = αMax + βMin,
---------> where Max is the maximum absolute value of a and b, and Min is the minimum absolute value of a and b. 

-------> Methods of computing square roots
---------> Methods of computing square roots are numerical analysis algorithms for approximating the principal, 
---------> or non-negative, square root (usually denoted sqrt(S)) of a real number. 
---------> Arithmetically, it means given S, a procedure for finding a number which when multiplied by itself, yields S; 
---------> algebraically, it means a procedure for finding the non-negative root of the equation x^2 − S = 0; 
---------> geometrically, it means given two line segments, a procedure for constructing their geometric mean. 
---------> https://en.wikipedia.org/wiki/Methods_of_computing_square_roots

---------> nth root algorithm
-----------> Using Newton's method
-------------> The nth root of a number A can be computed with Newton's method, which starts with an initial guess x0 and then iterates using the recurrence relation
---------------> xk+1 = xk − (xk^n−A) / (n*xk^(n−1)) 
-------------> until the desired precision is reached. 
-------------> For computational efficiency, the recurrence relation is commonly rewritten
---------------> xk+1 = (n-1)*xk/n + (A/n) * (1/(xk^(n−1))) 
-------------> This allows to have only one exponentiation, and to compute once for all the first factor of each term.
---------------> For example, to find the fifth root of 34, we plug in n = 5, A = 34 and x0 = 2 (initial guess). 
-------------> The first 5 iterations are, approximately:
---------------> x0 = 2
---------------> x1 = 2.025
---------------> x2 = 2.02439 7...
---------------> x3 = 2.02439 7458...
---------------> x4 = 2.02439 74584 99885 04251 08172...
---------------> x5 = 2.02439 74584 99885 04251 08172 45541 93741 91146 21701 07311 8...

---------> Shifting nth-root algorithm: 
-----------> This is digit by digit root extraction
-----------> The shifting nth root algorithm is an algorithm for extracting the nth root 
-------------> of a positive real number which proceeds iteratively by shifting in n digits of the radicand, 
-------------> starting with the most significant, and produces one digit of the root on each iteration, in a manner similar to long division. 

-----> Summation:

-------> Binary splitting: 
---------> This is a divide and conquer technique which speeds up the numerical evaluation of many types of series with rational terms
-----------> In mathematics, binary splitting is a technique for speeding up numerical evaluation of many types of series with rational terms. 
-----------> In particular, it can be used to evaluate hypergeometric series at rational points. 
---------> Given a series
-----------> S(a, b)= summation of n from a to b (pn/qn)
---------> where pn and qn are integers, the goal of binary splitting is to compute integers P(a, b) and Q(a, b) such that
-----------> S(a, b)= P(a, b)/Q(a, b)
---------> The splitting consists of setting m = [(a + b)/2] and recursively computing P(a, b) and Q(a, b) from P(a, m), P(m, b), Q(a, m), and Q(m, b). 
-----------> When a and b are sufficiently close, P(a, b) and Q(a, b) can be computed directly from pa...pb and qa...qb. 

-------> Kahan summation algorithm: 
---------> This is a more accurate method of summing floating-point numbers.
---------> In numerical analysis, the Kahan summation algorithm, also known as compensated summation,
-----------> significantly reduces the numerical error in the total obtained by adding a sequence of finite-precision floating-point numbers, compared to the obvious approach. 
-----------> This is done by keeping a separate running compensation (a variable to accumulate small errors), 
-------------> in effect extending the precision of the sum by the precision of the compensation variable.
---------> In particular, simply summing n numbers in sequence has a worst-case error that grows proportional to n, 
-----------> and a root mean square error that grows as sqrt(n) for random inputs (the roundoff errors form a random walk).
-----------> With compensated summation, using a compensation variable with sufficiently high precision the worst-case error bound is effectively independent of n, 
-----------> so a large number of values can be summed with an error that only depends on the floating-point precision of the result.[2]
---------> The algorithm is attributed to William Kahan;
-----------> Ivo Babuška seems to have come up with a similar algorithm independently (hence Kahan–Babuška summation). 
-----------> Similar, earlier techniques are, for example, Bresenham's line algorithm, 
-----------> keeping track of the accumulated error in integer operations (although first documented around the same time) and the delta-sigma modulation.

-----> Unrestricted algorithm
-------> An unrestricted algorithm is an algorithm for the computation of a mathematical function that puts 
---------> no restrictions on the range of the argument or on the precision that may be demanded in the result. 
---------> The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.[1][2]
-------> In the problem of developing algorithms for computing, as regards the values of a real-valued function of a real variable (e.g., g[x] in "restricted" algorithms), 
---------> the error that can be tolerated in the result is specified in advance. 
---------> An interval on the real line would also be specified for values when the values of a function are to be evaluated. 
---------> Different algorithms may have to be applied for evaluating functions outside the interval. 
---------> An unrestricted algorithm envisages a situation in which a user may stipulate the value of x and also the precision required in g(x) quite arbitrarily. 
---------> The algorithm should then produce an acceptable result without failure.[1] 

---> Geometric
-----> Filtered back-projection: 
-------> This efficiently computes the inverse 2-dimensional Radon transform.

-----> Level set method (LSM): 
-------> This a numerical technique for tracking interfaces and shapes.
-------> Level-set methods (LSM) are a conceptual framework for using level sets as a tool for numerical analysis of surfaces and shapes. 
---------> The advantage of the level-set model is that one can perform numerical computations involving curves 
---------> and surfaces on a fixed Cartesian grid without having to parameterize these objects (this is called the Eulerian approach).
---------> Also, the level-set method makes it very easy to follow shapes that change topology, 
---------> for example, when a shape splits in two, develops holes, or the reverse of these operations. 
---------> All these make the level-set method a great tool for modeling time-varying objects, like inflation of an airbag, or a drop of oil floating in water. 



-> Interpolation and extrapolation

---> Interpolation
-----> In the mathematical field of numerical analysis, interpolation is a type of estimation, 
-------> a method of constructing (finding) new data points based on the range of a discrete set of known data points.
-----> In engineering and science, one often has a number of data points, obtained by sampling or experimentation, 
-------> which represent the values of a function for a limited number of values of the independent variable. 
-------> It is often required to interpolate; that is, estimate the value of that function for an intermediate value of the independent variable.
-----> A closely related problem is the approximation of a complicated function by a simple function. 
-------> Suppose the formula for some given function is known, but too complicated to evaluate efficiently. 
-------> A few data points from the original function can be interpolated to produce a simpler function which is still fairly close to the original. 
-------> The resulting gain in simplicity may outweigh the loss from interpolation error and give better performance in calculation process. 

---> Extrapolation
-----> In mathematics, extrapolation is a type of estimation, beyond the original observation range, 
-------> of the value of a variable on the basis of its relationship with another variable. 
-------> It is similar to interpolation, which produces estimates between known observations, 
-------> but extrapolation is subject to greater uncertainty and a higher risk of producing meaningless results. 
-------> Extrapolation may also mean extension of a method, assuming similar methods will be applicable. 
-------> Extrapolation may also apply to human experience to project, extend, 
-------> or expand known experience into an area not known or previously experienced so as to arrive at a (usually conjectural) knowledge of the unknown
-------> (e.g. a driver extrapolates road conditions beyond his sight while driving). 
-------> The extrapolation method can be applied in the interior reconstruction problem. 

-----> Birkhoff interpolation: 
-------> This is an extension of polynomial interpolation
-------> In mathematics, Birkhoff interpolation is an extension of polynomial interpolation.
---------> It refers to the problem of finding a polynomial p of degree d such that certain derivatives have specified values at specified points:
-----------> p^(ni)(xi) = yi for  i = 1, …, d, 
---------> where the data points ( x i , y i )  and the nonnegative integers ni are given. 
---------> It differs from Hermite interpolation in that it is possible to specify derivatives of p at some points without specifying the lower derivatives or the polynomial itself. 
---------> The name refers to George David Birkhoff, who first studied the problem.[1] 

-----> Cubic interpolation
-------> In numerical analysis, a cubic Hermite spline or cubic Hermite interpolator is a spline where each piece is a third-degree polynomial specified in Hermite form, 
---------> that is, by its values and first derivatives at the end points of the corresponding domain interval.
-------> Cubic Hermite splines are typically used for interpolation of numeric data specified at given argument values x1, x2, …, xn, to obtain a continuous function. 
---------> The data should consist of the desired function value and derivative at each xk. (If only the values are provided, the derivatives must be estimated from them.) 
---------> The Hermite formula is applied to each interval (xk, xk+1) separately. 
---------> The resulting spline will be continuous and will have continuous first derivative.
-------> Cubic polynomial splines can be specified in other ways, the Bezier cubic being the most common. 
---------> However, these two methods provide the same set of splines, and data can be easily converted between the Bézier and Hermite forms; so the names are often used as if they were synonymous.
-------> Cubic polynomial splines are extensively used in computer graphics and geometric modeling to obtain curves 
---------> or motion trajectories that pass through specified points of the plane or three-dimensional space. 
---------> In these applications, each coordinate of the plane or space is separately interpolated by a cubic spline function of a separate parameter t. 
---------> Cubic polynomial splines are also used extensively in structural analysis applications, such as Euler–Bernoulli beam theory.
-------> Cubic splines can be extended to functions of two or more parameters, in several ways. 
---------> Bicubic splines (Bicubic interpolation) are often used to interpolate data on a regular rectangular grid, 
---------> such as pixel values in a digital image or altitude data on a terrain. 
---------> Bicubic surface patches, defined by three bicubic splines, are an essential tool in computer graphics.
-------> Cubic splines are often called csplines, especially in computer graphics. 
---------> Hermite splines are named after Charles Hermite. 

-----> Hermite interpolation
-------> In numerical analysis, Hermite interpolation, named after Charles Hermite, 
---------> is a method of polynomial interpolation, which generalizes Lagrange interpolation. 
---------> Lagrange interpolation allows computing a polynomial of degree less than n that takes the same value at n given points as a given function. 
---------> Instead, Hermite interpolation computes a polynomial of degree less than mn such that the polynomial and its m − 1 first derivatives 
---------> have the same values at n given points as a given function and its m − 1 first derivatives.
---------> 
-------> Hermite's method of interpolation is closely related to the Newton's interpolation method, 
---------> in that both are derived from the calculation of divided differences. 
---------> However, there other methods for computing a Hermite interpolating polynomial. 
---------> One can use linear algebra, by taking the coefficients of the interpolating polynomial as unknowns, 
---------> and writing as linear equations the constraints that the interpolating polynomial must satisfy. 
---------> For another method, see Chinese remainder theorem § Hermite interpolation. 

-----> Lagrange interpolation: 
-------> This is an interpolation using Lagrange polynomials
---------> In numerical analysis, the Lagrange interpolating polynomial is the unique polynomial of lowest degree that interpolates a given set of data.
-------> Given a data set of coordinate pairs (xj, yj) with 0 ≤ j ≤ k, the xj  are called nodes and the yj are called values. 
---------> The Lagrange polynomial L(x) has degree ≤ k and assumes each value at the corresponding node, L(xj) =yj.
-------> Although named after Joseph-Louis Lagrange, who published it in 1795, the method was first discovered in 1779 by Edward Waring.
---------> It is also an easy consequence of a formula published in 1783 by Leonhard Euler.[2]
-------> Uses of Lagrange polynomials include the Newton–Cotes method of numerical integration and Shamir's secret sharing scheme in cryptography.
-------> For equispaced nodes, Lagrange interpolation is susceptible to Runge's phenomenon of large oscillation. 

-----> Linear interpolation: 
-------> This is a method of curve fitting using linear polynomials
-------> In mathematics, linear interpolation is a method of curve fitting using linear polynomials 
---------> to construct new data points within the range of a discrete set of known data points. 

-----> Monotone cubic interpolation: 
-------> This is an a variant of cubic interpolation that preserves monotonicity of the data set being interpolated.
-------> In the mathematical field of numerical analysis, monotone cubic interpolation is a variant of cubic interpolation 
---------> that preserves monotonicity of the data set being interpolated.
---------> Monotonicity is preserved by linear interpolation but not guaranteed by cubic interpolation. 

-----> Multivariate interpolation
-------> In numerical analysis, multivariate interpolation is interpolation on functions of more than one variable; 
---------> when the variates are spatial coordinates, it is also known as spatial interpolation.
-------> The function to be interpolated is known at given points (xi, yi, zi, …) 
---------> and the interpolation problem consists of yielding values at arbitrary points (x, y, z, …).
-------> Multivariate interpolation is particularly important in geostatistics, 
---------> where it is used to create a digital elevation model from a set of points on the Earth's surface 
---------> (for example, spot heights in a topographic survey or depths in a hydrographic survey). 

-------> Bicubic interpolation, 
---------> This is a generalization of cubic interpolation to two dimensions
---------> In mathematics, bicubic interpolation is an extension of cubic interpolation  for interpolating data points on a two-dimensional regular grid. 
-----------> The interpolated surface is smoother than corresponding surfaces obtained by bilinear interpol
-----------> ation or nearest-neighbor interpolation. 
-----------> Bicubic interpolation can be accomplished using either Lagrange polynomials, cubic splines, or cubic convolution algorithm.
---------> In image processing, bicubic interpolation is often chosen over bilinear or nearest-neighbor interpolation in image resampling, when speed is not an issue. 
-----------> In contrast to bilinear interpolation, which only takes 4 pixels (2×2) into account, bicubic interpolation considers 16 pixels (4×4). 
-----------> Images resampled with bicubic interpolation are smoother and have fewer interpolation artifacts. 
-----------> This is not to be confused with cubic spline interpolation, a method of applying cubic interpolation to a data set

-------> Bilinear interpolation: 
---------> This is an extension of linear interpolation for interpolating functions of two variables on a regular grid.
---------> In mathematics, bilinear interpolation is a method for interpolating functions of two variables (e.g., x and y) using repeated linear interpolation. 
-----------> It is usually applied to functions sampled on a 2D rectilinear grid, 
-----------> though it can be generalized to functions defined on the vertices of (a mesh of) arbitrary convex quadrilaterals.
---------> Bilinear interpolation is performed using linear interpolation first in one direction, and then again in the other direction.
----------->  Although each step is linear in the sampled values and in the position, 
-----------> the interpolation as a whole is not linear but rather quadratic in the sample location.
---------> Bilinear interpolation is one of the basic resampling techniques in computer vision and image processing, 
-----------> where it is also called bilinear filtering or bilinear texture mapping. 

-------> Lanczos resampling ("Lanzosh"): 
---------> This is a multivariate interpolation method used to compute new values for any digitally sampled data
-----------> Lanczos filtering and Lanczos resampling are two applications of a mathematical formula. 
-----------> It can be used as a low-pass filter or used to smoothly interpolate the value of a digital signal between its samples. 
-----------> In the latter case it maps each sample of the given signal to a translated and scaled copy of the Lanczos kernel, 
-----------> which is a sinc function windowed by the central lobe of a second, longer, sinc function. 
-----------> The sum of these translated and scaled kernels is then evaluated at the desired points.
---------> Lanczos resampling is typically used to increase the sampling rate of a digital signal, or to shift it by a fraction of the sampling interval. 
-----------> It is often used also for multivariate interpolation, for example to resize or rotate a digital image.
-----------> It has been considered the "best compromise" among several simple filters for this purpose.[1]
---------> The filter is named after its inventor, Cornelius Lanczos (Hungarian pronunciation: [ˈlaːnt͡soʃ]). 

-------> Nearest-neighbor interpolation
---------> Nearest-neighbor interpolation (also known as proximal interpolation or, in some contexts, point sampling) 
-----------> is a simple method of multivariate interpolation in one or more dimensions.
---------> Interpolation is the problem of approximating the value of a function for a non-given point in some space 
-----------> when given the value of that function in points around (neighboring) that point. 
-----------> The nearest neighbor algorithm selects the value of the nearest point 
-----------> and does not consider the values of neighboring points at all, yielding a piecewise-constant interpolant. 
-----------> The algorithm is very simple to implement and is commonly used (usually along with mipmapping) 
-----------> in real-time 3D rendering to select color values for a textured surface. 

-------> Tricubic interpolation, 
---------> This is a generalization of cubic interpolation to three dimensions
-----------> In the mathematical subfield numerical analysis, tricubic interpolation is a method for obtaining values at arbitrary points in 3D space of a function defined on a regular grid. 
-----------> The approach involves approximating the function locally by an expression of the form 
-------------> f(x, y, z) = summation of i from 0 to 3 (summation of j from 0 to 3 (summation of k from 0 to 3 (aijk * x^i * y^j * z^k) ) ).
---------> This form has 64 coefficients aijk; requiring the function to have a given value or given directional derivative at a point places one linear constraint on the 64 coefficients.
---------> The term tricubic interpolation is used in more than one context; 
-----------> some experiments measure both the value of a function and its spatial derivatives, 
-----------> and it is desirable to interpolate preserving the values and the measured derivatives at the grid points. 
-----------> Those provide 32 constraints on the coefficients, and another 32 constraints can be provided by requiring smoothness of higher derivatives.
---------> In other contexts, we can obtain the 64 coefficients by considering a 3×3×3 grid of small cubes surrounding the cube inside which we evaluate the function, 
-----------> and fitting the function at the 64 points on the corners of this grid. 

-----> Pareto interpolation: 
-------> This is a method of estimating the median and other properties of a population that follows a Pareto distribution.
-------> Pareto interpolation is a method of estimating the median and other properties of a population that follows a Pareto distribution. 
---------> It is used in economics when analysing the distribution of incomes in a population, 
---------> when one must base estimates on a relatively small random sample taken from the population.
-------> The family of Pareto distributions is parameterized by
---------> a positive number κ that is the smallest value that a random variable with a Pareto distribution can take. 
-----------> As applied to distribution of incomes, κ is the lowest income of any person in the population; and
---------> a positive number θ the "Pareto index"; as this increases, the tail of the distribution gets thinner. 
-----------> As applied to distribution of incomes, this means that the larger the value of the Pareto index θ the smaller the proportion of incomes many times as big as the smallest incomes.
---------> Pareto interpolation can be used when the available information includes the proportion of the sample that falls below each of two specified numbers a < b. 
-----------> For example, it may be observed that 45% of individuals in the sample have incomes below a = $35,000 per year, and 55% have incomes below b = $40,000 per year. 

-----> Polynomial interpolation
-------> In numerical analysis, polynomial interpolation is the interpolation of a given data set by the polynomial of lowest possible degree that passes through the points of the dataset.
---------> Given a set of n + 1 data points ( x 0 , y 0 ) , … , ( x n , y n ) , with no two xj the same, 
---------> a polynomial function  p(x) is said to interpolate the data if p(xj) = yj  for each j is an element of { 0 , 1 , … , n } .
-------> Two common explicit formulas for this polynomial are the Lagrange polynomials and Newton polynomials. 

-------> Neville's algorithm
---------> In mathematics, Neville's algorithm is an algorithm used for polynomial interpolation that was derived by the mathematician Eric Harold Neville in 1934. 
-----------> Given n + 1 points, there is a unique polynomial of degree ≤ n which goes through the given points. Neville's algorithm evaluates this polynomial.
---------> Neville's algorithm is based on the Newton form of the interpolating polynomial and the recursion relation for the divided differences. 
-----------> It is similar to Aitken's algorithm (named after Alexander Aitken), which is nowadays not used. 

-----> Spline interpolation: 
-------> This reduces error with Runge's phenomenon.
-------> In the mathematical field of numerical analysis, spline interpolation is a form of interpolation 
---------> where the interpolant is a special type of piecewise polynomial called a spline.
---------> That is, instead of fitting a single, high-degree polynomial to all of the values at once, 
---------> spline interpolation fits low-degree polynomials to small subsets of the values, for example, 
---------> fitting nine cubic polynomials between each of the pairs of ten points, 
---------> instead of fitting a single degree-ten polynomial to all of them. 
---------> Spline interpolation is often preferred over polynomial interpolation because the interpolation error 
---------> can be made small even when using low-degree polynomials for the spline.
---------> Spline interpolation also avoids the problem of Runge's phenomenon, 
---------> in which oscillation can occur between points when interpolating using high-degree polynomials. 

-------> De Boor algorithm: B-splines

---------> In the mathematical subfield of numerical analysis de Boor's algorithm[1] 
-----------> is a polynomial-time and numerically stable algorithm for evaluating spline curves in B-spline form. 
-----------> It is a generalization of de Casteljau's algorithm for Bézier curves.
----------->  The algorithm was devised by Carl R. de Boor. 
-----------> Simplified, potentially faster variants of the de Boor algorithm have been created but they suffer from comparatively lower stability.[2][3]

-------> De Casteljau's algorithm: Bézier curves
---------> In the mathematical field of numerical analysis, 
-----------> De Casteljau's algorithm is a recursive method to evaluate polynomials in Bernstein form or Bézier curves, 
-----------> named after its inventor Paul de Casteljau. 
---------> De Casteljau's algorithm can also be used to split a single Bézier curve into two Bézier curves at an arbitrary parameter value.
---------> Although the algorithm is slower for most architectures when compared with the direct approach, it is more numerically stable. 

-----> Trigonometric interpolation
-------> In mathematics, trigonometric interpolation is interpolation with trigonometric polynomials. 
-------> Interpolation is the process of finding a function which goes through some given data points. 
---------> For trigonometric interpolation, this function has to be a trigonometric polynomial, that is, a sum of sines and cosines of given periods. 
---------> This form is especially suited for interpolation of periodic functions.
-------> An important special case is when the given data points are equally spaced, in which case the solution is given by the discrete Fourier transform. 



-> Linear algebra

---> Eigenvalue algorithms
-----> In numerical analysis, one of the most important problems is designing efficient and stable algorithms for finding the eigenvalues of a matrix. 
-------> These eigenvalue algorithms may also find eigenvectors. 
-----> In linear algebra, an eigenvector or characteristic vector of a linear transformation is a nonzero vector 
-------> that changes at most by a scalar factor when that linear transformation is applied to it. 
-------> The corresponding eigenvalue, often denoted by λ, is the factor by which the eigenvector is scaled.
-----> Geometrically, an eigenvector, corresponding to a real nonzero eigenvalue, 
-------> points in a direction in which it is stretched by the transformation and the eigenvalue is the factor by which it is stretched. 
-------> If the eigenvalue is negative, the direction is reversed.
-------> Loosely speaking, in a multidimensional vector space, the eigenvector is not rotated. 

-----> Arnoldi iteration
-------> In numerical linear algebra, the Arnoldi iteration is an eigenvalue algorithm and an important example of an iterative method. 
---------> Arnoldi finds an approximation to the eigenvalues and eigenvectors of general (possibly non-Hermitian) matrices by constructing an orthonormal basis of the Krylov subspace, 
---------> which makes it particularly useful when dealing with large sparse matrices.
-------> The Arnoldi method belongs to a class of linear algebra algorithms that give a partial result after a small number of iterations, 
---------> in contrast to so-called direct methods which must complete to give any useful results (see for example, Householder transformation). 
---------> The partial result in this case being the first few vectors of the basis the algorithm is building.
-------> When applied to Hermitian matrices it reduces to the Lanczos algorithm. The Arnoldi iteration was invented by W. E. Arnoldi in 1951.

-----> Inverse iteration
-------> In numerical analysis, inverse iteration (also known as the inverse power method) is an iterative eigenvalue algorithm. 
---------> It allows one to find an approximate eigenvector when an approximation to a corresponding eigenvalue is already known. 
---------> The method is conceptually similar to the power method. 
---------> It appears to have originally been developed to compute resonance frequencies in the field of structural mechanics.
-------> The inverse power iteration algorithm starts with an approximation μ for 
---------> the eigenvalue corresponding to the desired eigenvector and a vector b0, 
---------> either a randomly selected vector or an approximation to the eigenvector. 
---------> The method is described by the iteration
-----------> bk+1 = (A−μI)^(−1) * bk / Ck,
---------> where Ck are some constants usually chosen as Ck = ‖ A−μI)^(−1) * bk ‖. 
---------> Since eigenvectors are defined up to multiplication by constant, the choice of Ck can be arbitrary in theory; 
---------> practical aspects of the choice of Ck are discussed below.
-------> At every iteration, the vector bk is multiplied by the matrix (A−μI)^(−1) and normalized. 
---------> It is exactly the same formula as in the power method, except replacing the matrix A by (A−μI)^(−1). 
---------> The closer the approximation μ to the eigenvalue is chosen, the faster the algorithm converges; 
---------> however, incorrect choice of μ can lead to slow convergence or to the convergence to an eigenvector other than the one desired. 
---------> In practice, the method is used when a good approximation for the eigenvalue is known, and hence one needs only few (quite often just one) iterations. 


-----> Jacobi method
-------> In numerical linear algebra, the Jacobi eigenvalue algorithm is an iterative method 
---------> for the calculation of the eigenvalues and eigenvectors of a real symmetric matrix (a process known as diagonalization).
---------> It is named after Carl Gustav Jacob Jacobi, who first proposed the method in 1846, but only became widely used in the 1950s with the advent of computers.


-----> Lanczos iteration
-------> The Lanczos algorithm is an iterative method devised by Cornelius Lanczos that is an adaptation of power methods to find the m "most useful" 
---------> (tending towards extreme highest/lowest) eigenvalues and eigenvectors of an n × n Hermitian matrix, 
---------> where m is often but not necessarily much smaller than n.
---------> Although computationally efficient in principle, the method as initially formulated was not useful, due to its numerical instability.
-------> In 1970, Ojalvo and Newman showed how to make the method numerically stable 
---------> and applied it to the solution of very large engineering structures subjected to dynamic loading.
---------> This was achieved using a method for purifying the Lanczos vectors 
---------> (i.e. by repeatedly reorthogonalizing each newly generated vector with all previously generated ones) to any degree of accuracy, 
---------> which when not performed, produced a series of vectors that were highly contaminated by those associated with the lowest natural frequencies.
-------> In their original work, these authors also suggested how to select a starting vector 
---------> (i.e. use a random-number generator to select each element of the starting vector) 
---------> and suggested an empirically determined method for determining m, the reduced number of vectors 
---------> (i.e. it should be selected to be approximately 1.5 times the number of accurate eigenvalues desired). 
---------> Soon thereafter their work was followed by Paige, who also provided an error analysis.
---------> In 1988, Ojalvo produced a more detailed history of this algorithm and an efficient eigenvalue error test.

-----> Power iteration
-------> In mathematics, power iteration (also known as the power method) is an eigenvalue algorithm: 
---------> given a diagonalizable matrix A, the algorithm will produce a number λ, which is the greatest (in absolute value) eigenvalue of A, and a nonzero vector v, 
---------> which is a corresponding eigenvector of λ , that is, A v = λ v. 
---------> The algorithm is also known as the Von Mises iteration.
-------> Power iteration is a very simple algorithm, but it may converge slowly. 
---------> The most time-consuming operation of the algorithm is the multiplication of matrix A by a vector, 
---------> so it is effective for a very large sparse matrix with appropriate implementation. 

-----> QR algorithm
-------> In numerical linear algebra, the QR algorithm or QR iteration is an eigenvalue algorithm: 
---------> that is, a procedure to calculate the eigenvalues and eigenvectors of a matrix. 
---------> The QR algorithm was developed in the late 1950s by John G. F. Francis and by Vera N. Kublanovskaya, working independently. 
---------> The basic idea is to perform a QR decomposition, writing the matrix as a product of an orthogonal matrix and an upper triangular matrix, 
---------> multiply the factors in the reverse order, and iterate. 

-----> Rayleigh quotient iteration
-------> Rayleigh quotient iteration is an eigenvalue algorithm which extends the idea of the inverse iteration 
---------> by using the Rayleigh quotient to obtain increasingly accurate eigenvalue estimates.
-------> Rayleigh quotient iteration is an iterative method, that is, 
---------> it delivers a sequence of approximate solutions that converges to a true solution in the limit. 
---------> Very rapid convergence is guaranteed and no more than a few iterations are needed in practice to obtain a reasonable approximation. 
---------> The Rayleigh quotient iteration algorithm converges cubically for Hermitian or symmetric matrices, 
---------> given an initial vector that is sufficiently close to an eigenvector of the matrix that is being analyzed. 

---> Gram–Schmidt process: 
-----> This orthogonalizes a set of vectors
-------> In mathematics, particularly linear algebra and numerical analysis, 
---------> the Gram–Schmidt process is a method for orthonormalizing a set of vectors in an inner product space, 
---------> most commonly the Euclidean space Rn equipped with the standard inner product. 
---------> The Gram–Schmidt process takes a finite, linearly independent set of vectors S = {v1, ..., vk} for k ≤ n 
---------> and generates an orthogonal set S′ = {u1, ..., uk} that spans the same k-dimensional subspace of Rn as S.
-------> The method is named after Jørgen Pedersen Gram and Erhard Schmidt, but Pierre-Simon Laplace had been familiar with it before Gram and Schmidt.[1] 
---------> In the theory of Lie group decompositions it is generalized by the Iwasawa decomposition.
-------> The application of the Gram–Schmidt process to the column vectors of a full column rank matrix yields the QR decomposition (it is decomposed into an orthogonal and a triangular matrix). 

---> Matrix multiplication algorithms

-----> Cannon's algorithm: 
-------> This is a distributed algorithm for matrix multiplication especially suitable for computers laid out in an N × N mesh
---------> Because matrix multiplication is such a central operation in many numerical algorithms, much work has been invested in making matrix multiplication algorithms efficient. 
---------> Applications of matrix multiplication in computational problems are found in many fields including scientific computing and pattern recognition 
---------> and in seemingly unrelated problems such as counting the paths through a graph.
---------> Many different algorithms have been designed for multiplying matrices on different types of hardware, 
---------> including parallel and distributed systems, where the computational work is spread over multiple processors (perhaps over a network).
-------> Directly applying the mathematical definition of matrix multiplication gives an algorithm 
---------> that takes time on the order of n3 field operations to multiply two n × n matrices over that field (Θ(n3) in big O notation). 
---------> Better asymptotic bounds on the time required to multiply matrices have been known since the Strassen's algorithm in the 1960s, 
---------> but it is still unknown what the optimal time is (i.e., what the computational complexity of matrix multiplication is). 
---------> As of December 2020, the matrix multiplication algorithm with best asymptotic complexity runs in O(n2.3728596) time, 
---------> given by Josh Alman and Virginia Vassilevska Williams, however this algorithm is a galactic algorithm because of the large constants and cannot be realized practically. 

-----> Coppersmith–Winograd algorithm: 
-------> This is for square matrix multiplication
-------> In computer science, Cannon's algorithm is a distributed algorithm for matrix multiplication for two-dimensional meshes first described in 1969 by Lynn Elliot Cannon.[1][2]
---------> It is especially suitable for computers laid out in an N × N mesh.
---------> While Cannon's algorithm works well in homogeneous 2D grids, extending it to heterogeneous 2D grids has been shown to be difficult.

-----> The main advantage of the algorithm is that its storage requirements remain constant and are independent of the number of processors.
-------> The Scalable Universal Matrix Multiplication Algorithm is a more practical algorithm that requires less workspace 
-------> and overcomes the need for a square 2D grid. It is used by the ScaLAPACK, PLAPACK, and Elemental libraries. 

-----> Freivalds' algorithm: 
-------> This is a randomized algorithm used to verify matrix multiplication
-------> Freivalds' algorithm (named after Rūsiņš Mārtiņš Freivalds) is a probabilistic randomized algorithm used to verify matrix multiplication. 
---------> Given three n × n matrices A, B, and C, a general problem is to verify whether A × B = C. 
---------> A naïve algorithm would compute the product A × B explicitly and compare term by term whether this product equals C. 
---------> However, the best known matrix multiplication algorithm runs in O(n^2.3729) time. 
---------> Freivalds' algorithm utilizes randomization in order to reduce this time bound to O(n^2) with high probability. 
---------> In O(k*n^2) time the algorithm can verify a matrix product with probability of failure less than 2^(−k). 

-----> Strassen algorithm: 
-------> This is for faster matrix multiplication
-------> In linear algebra, the Strassen algorithm, named after Volker Strassen, is an algorithm for matrix multiplication. 
---------> It is faster than the standard matrix multiplication algorithm for large matrices, with a better asymptotic complexity, 
---------> although the naive algorithm is often better for smaller matrices. 
---------> The Strassen algorithm is slower than the fastest known algorithms for extremely large matrices, but such algorithms are not useful in practice, 
---------> as they are much slower for matrices of practical size.
-------> Strassen's algorithm works for any ring, such as plus/multiply, but not all semirings, 
---------> such as min-plus or boolean algebra, where the naive algorithm still works, and so called combinatorial matrix multiplication. 

---> Solving systems of linear equations
-----> In mathematics, a system of linear equations (or linear system) is a collection of one or more linear equations involving the same variables.

-----> Biconjugate gradient method: 
-------> This solves systems of linear equations
-------> In mathematics, more specifically in numerical linear algebra, the biconjugate gradient method is an algorithm to solve systems of linear equations
--------> A x = b
------> Unlike the conjugate gradient method, this algorithm does not require the matrix A to be self-adjoint, 
--------> but instead one needs to perform multiplications by the conjugate transpose A*. 

-----> Conjugate gradient: 
-------> This an algorithm for the numerical solution of particular systems of linear equations
-------> In mathematics, the conjugate gradient method is an algorithm 
---------> for the numerical solution of particular systems of linear equations, namely those whose matrix is positive-definite. 
-------> The conjugate gradient method is often implemented as an iterative algorithm, 
---------> applicable to sparse systems that are too large to be handled by a direct implementation or other direct methods such as the Cholesky decomposition. 
---------> Large sparse systems often arise when numerically solving partial differential equations or optimization problems.
-------> The conjugate gradient method can also be used to solve unconstrained optimization problems such as energy minimization. 
---------> It is commonly attributed to Magnus Hestenes and Eduard Stiefel,[1][2] who programmed it on the Z4,[3] and extensively researched.[4][5]
---------> The biconjugate gradient method provides a generalization to non-symmetric matrices. 
---------> Various nonlinear conjugate gradient methods seek minima of nonlinear optimization problems. 

-----> Gaussian elimination

-----> Gauss–Jordan elimination: 
-------> This solves systems of linear equations.
-------> In mathematics, Gaussian elimination, also known as row reduction, is an algorithm for solving systems of linear equations. 
---------> It consists of a sequence of operations performed on the corresponding matrix of coefficients. 
---------> This method can also be used to compute the rank of a matrix, the determinant of a square matrix, and the inverse of an invertible matrix. 
---------> The method is named after Carl Friedrich Gauss (1777–1855) although some special cases of the method—albeit presented without proof—were known to Chinese mathematicians as early as circa 179 AD.[1]
-------> To perform row reduction on a matrix, one uses a sequence of elementary row operations to modify the matrix until the lower left-hand corner of the matrix is filled with zeros, as much as possible. 
---------> There are three types of elementary row operations:
-----------> Swapping two rows,
-----------> Multiplying a row by a nonzero number,
-----------> Adding a multiple of one row to another row. (subtraction can be achieved by multiplying one row with -1 and adding the result to another row)
-------> Using these operations, a matrix can always be transformed into an upper triangular matrix, and in fact one that is in row echelon form. 
---------> Once all of the leading coefficients (the leftmost nonzero entry in each row) are 1, and every column containing a leading coefficient has zeros elsewhere, 
---------> the matrix is said to be in reduced row echelon form. This final form is unique; in other words, it is independent of the sequence of row operations used. 
---------> For example, in the following sequence of row operations (where two elementary operations on different rows are done at the first and third steps), 
---------> the third and fourth matrices are the ones in row echelon form, and the final matrix is the unique reduced row echelon form. 
-------> Using row operations to convert a matrix into reduced row echelon form is sometimes called Gauss–Jordan elimination. 
---------> In this case, the term Gaussian elimination refers to the process until it has reached its upper triangular, or (unreduced) row echelon form. 
---------> For computational reasons, when solving systems of linear equations, it is sometimes preferable to stop row operations before the matrix is completely reduced. 

-----> Gauss–Seidel method: 
-------> This solves systems of linear equations iteratively
-------> In numerical linear algebra, the Gauss–Seidel method, also known as the Liebmann method or the method of successive displacement, 
---------> is an iterative method used to solve a system of linear equations. 
---------> It is named after the German mathematicians Carl Friedrich Gauss and Philipp Ludwig von Seidel, and is similar to the Jacobi method. 
---------> Though it can be applied to any matrix with non-zero elements on the diagonals, 
---------> convergence is only guaranteed if the matrix is either strictly diagonally dominant, or symmetric and positive definite. 
---------> It was only mentioned in a private letter from Gauss to his student Gerling in 1823.[2] A publication was not delivered before 1874 by Seidel.

-----> Levinson recursion: 
-------> This solves equation involving a Toeplitz matrix
---------> Levinson recursion or Levinson–Durbin recursion is a procedure in linear algebra 
---------> to recursively calculate the solution to an equation involving a Toeplitz matrix.
------->  The algorithm runs in Θ(n2) time, which is a strong improvement over Gauss–Jordan elimination, which runs in Θ(n3).
-------> The Levinson–Durbin algorithm was proposed first by Norman Levinson in 1947, improved by James Durbin in 1960, 
---------> and subsequently improved to 4n2 and then 3n2 multiplications by W. F. Trench and S. Zohar, respectively.
-------> Other methods to process data include Schur decomposition and Cholesky decomposition. 
---------> In comparison to these, Levinson recursion (particularly split Levinson recursion) tends to be faster computationally, 
---------> but more sensitive to computational inaccuracies like round-off errors.
-------> The Bareiss algorithm for Toeplitz matrices (not to be confused with the general Bareiss algorithm) runs about as fast as Levinson recursion, 
---------> but it uses O(n2) space, whereas Levinson recursion uses only O(n) space. 
-------> The Bareiss algorithm, though, is numerically stable,[1][2] whereas Levinson recursion is at best only weakly stable (i.e. it exhibits numerical stability for well-conditioned linear systems).
-------> Newer algorithms, called asymptotically fast or sometimes superfast Toeplitz algorithms, can solve in Θ(n logpn) for various p (e.g. p = 2,[4][5] p = 3 [6]). 
---------> Levinson recursion remains popular for several reasons; for one, it is relatively easy to understand in comparison; for another, it can be faster than a superfast algorithm for small n (usually n < 256).[7] 

-----> Stone's method: 
-------> This also known as the strongly implicit procedure or SIP, is an algorithm for solving a sparse linear system of equations
---------> In numerical analysis, Stone's method, also known as the strongly implicit procedure or SIP, is an algorithm for solving a sparse linear system of equations. 
---------> The method uses an incomplete LU decomposition, which approximates the exact LU decomposition, to get an iterative solution of the problem. 
---------> The method is named after Harold S. Stone, who proposed it in 1968.
-------> The LU decomposition is an excellent general-purpose linear equation solver. 
---------> The biggest disadvantage is that it fails to take advantage of coefficient matrix to be a sparse matrix. reddit
---------> The LU decomposition of a sparse matrix is usually not sparse, thus, for a large system of equations, 
---------> LU decomposition may require a prohibitive amount of memory and number of arithmetical operations.
-------> In the preconditioned iterative methods, if the preconditioner matrix M is a good approximation of coefficient matrix A then the convergence is faster. 
---------> This brings one to idea of using approximate factorization LU of A as the iteration matrix M.
-------> A version of incomplete lower-upper decomposition method was proposed by Stone in 1968. 
---------> This method is designed for equation system arising from discretisation of partial differential equations 
---------> and was firstly used for a pentadiagonal system of equations obtained while solving an elliptic partial differential equation in a two-dimensional space by a finite difference method. 
---------> The LU approximate decomposition was looked[clarification needed] in the same pentadiagonal form as the original matrix (three diagonals for L and three diagonals for U) 
---------> as the best match of the seven possible equations for the five unknowns for each row of the matrix. 

-----> Successive over-relaxation (SOR): 
-------> This method used to speed up convergence of the Gauss–Seidel method
---------> In numerical linear algebra, the method of successive over-relaxation (SOR) is a variant of the Gauss–Seidel method for solving a linear system of equations, resulting in faster convergence. 
---------> A similar method can be used for any slowly converging iterative process.
-------> It was devised simultaneously by David M. Young Jr. and by Stanley P. Frankel in 1950 for the purpose of automatically solving linear systems on digital computers. 
---------> Over-relaxation methods had been used before the work of Young and Frankel. 
---------> An example is the method of Lewis Fry Richardson, and the methods developed by R. V. Southwell. 
---------> However, these methods were designed for computation by human calculators, 
---------> requiring some expertise to ensure convergence to the solution which made them inapplicable for programming on digital computers. 
---------> These aspects are discussed in the thesis of David M. Young Jr.[1] 

-----> Tridiagonal matrix algorithm (Thomas algorithm): 
-------> This solves systems of tridiagonal equations
---------> In numerical linear algebra, the tridiagonal matrix algorithm, also known as the Thomas algorithm (named after Llewellyn Thomas), 
---------> is a simplified form of Gaussian elimination that can be used to solve tridiagonal systems of equations. 
---------> A tridiagonal system for n unknowns may be written as
-----------> aixi−1 + bixi + cixi+1 = di,
-----------> where a1 = 0 and cn = 0. 

---> Sparse matrix algorithms
-----> In numerical analysis and scientific computing, a sparse matrix or sparse array is a matrix in which most of the elements are zero.
-------> There is no strict definition regarding the proportion of zero-value elements for a matrix to qualify as sparse 
-------> but a common criterion is that the number of non-zero elements is roughly equal to the number of rows or columns. 
-------> By contrast, if most of the elements are non-zero, the matrix is considered dense.
-------> The number of zero-valued elements divided by the total number of elements (e.g., m × n for an m × n matrix) is sometimes referred to as the sparsity of the matrix. 

-----> Cuthill–McKee algorithm: 
-------> This reduce the bandwidth of a symmetric sparse matrix
---------> In numerical linear algebra, the Cuthill–McKee algorithm (CM), is an algorithm to permute a sparse matrix that has a symmetric sparsity pattern into a band matrix form with a small bandwidth. 
---------> The reverse Cuthill–McKee algorithm (RCM) due to Alan George and Joseph Liu is the same algorithm but with the resulting index numbers reversed.
---------> In practice this generally results in less fill-in than the CM ordering when Gaussian elimination is applied.
-------> The Cuthill McKee algorithm is a variant of the standard breadth-first search algorithm used in graph algorithms. 
---------> It starts with a peripheral node and then generates levels Ri  i=1, 2,.. until all nodes are exhausted. 
---------> The set  i+1 is created from set Ri by listing all vertices adjacent to all nodes in Ri. 
---------> These nodes are ordered according to predecessors and degree. 
-------> This is named after Elizabeth Cuthill and James McKee

-----> Minimum degree algorithm: 
-------> This permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition
-------> In numerical analysis, the minimum degree algorithm is an algorithm used to
---------> permute the rows and columns of a symmetric sparse matrix before applying the Cholesky decomposition, 
---------> to reduce the number of non-zeros in the Cholesky factor. 
---------> This results in reduced storage requirements and means that the Cholesky factor can be applied with fewer arithmetic operations. 
---------> (Sometimes it may also pertain to an incomplete Cholesky factor used as a preconditioner—for example, in the preconditioned conjugate gradient algorithm.)
-------> Minimum degree algorithms are often used in the finite element method where the reordering of nodes can be carried out depending only on the topology of the mesh, 
---------> rather than on the coefficients in the partial differential equation, resulting in efficiency savings when the same mesh is used for a variety of coefficient values.
-------> Given a linear system
--------->     Ax = b 
---------> where A is an n × n real symmetric sparse square matrix. 
---------> The Cholesky factor L will typically suffer 'fill in', that is have more non-zeros than the upper triangle of A.
---------> We seek a permutation matrix P, so that the matrix P^T * AP, which is also symmetric, has the least possible fill in its Cholesky factor. 
---------> We solve the reordered system
--------->     (P^T * AP) (P^T * x) = P^T * b.

-----> Symbolic Cholesky decomposition: 
-------> This is an efficient way of storing sparse matrix
-------> In the mathematical subfield of numerical analysis the symbolic Cholesky decomposition is an algorithm used to determine 
---------> the non-zero pattern for the L factors of a symmetric sparse matrix when applying the Cholesky decomposition or variants. 



---> Monte Carlo
-----> Monte Carlo methods, or Monte Carlo experiments, are a broad class of computational algorithms that rely on repeated random sampling to obtain numerical results. 
-------> The underlying concept is to use randomness to solve problems that might be deterministic in principle. 
-------> They are often used in physical and mathematical problems and are most useful when it is difficult or impossible to use other approaches. 
-------> Monte Carlo methods are mainly used in three problem classes:[1] optimization, numerical integration, and generating draws from a probability distribution.
-----> In physics-related problems, Monte Carlo methods are useful for simulating systems with many coupled degrees of freedom, 
-------> such as fluids, disordered materials, strongly coupled solids, and cellular structures (see cellular Potts model, interacting particle systems, McKean–Vlasov processes, kinetic models of gases). 
-----> In mathematics, Monte Carlo integration is a technique for numerical integration using random numbers. 
-------> It is a particular Monte Carlo method that numerically computes a definite integral. 
-------> While other algorithms usually evaluate the integrand at a regular grid,Monte Carlo randomly chooses points at which the integrand is evaluated.
-------> This method is particularly useful for higher-dimensional integrals.[3]
-----> There are different methods to perform a Monte Carlo integration, such as uniform sampling, stratified sampling, 
-------> importance sampling, sequential Monte Carlo (also known as a particle filter), and mean-field particle methods

-----> Gibbs sampling: 
-------> This generates a sequence of samples from the joint probability distribution of two or more random variables.
-------> In statistics, Gibbs sampling or a Gibbs sampler is a Markov chain Monte Carlo (MCMC) algorithm for obtaining a sequence of observations 
---------> which are approximated from a specified multivariate probability distribution, when direct sampling is difficult. 
---------> This sequence can be used to approximate the joint distribution (e.g., to generate a histogram of the distribution); 
---------> to approximate the marginal distribution of one of the variables, or some subset of the variables (for example, the unknown parameters or latent variables); 
---------> or to compute an integral (such as the expected value of one of the variables). 
---------> Typically, some of the variables correspond to observations whose values are known, and hence do not need to be sampled.
-------> Gibbs sampling is commonly used as a means of statistical inference, especially Bayesian inference. 
---------> It is a randomized algorithm (i.e. an algorithm that makes use of random numbers),
---------> and is an alternative to deterministic algorithms for statistical inference such as the expectation-maximization algorithm (EM).
-------> As with other MCMC algorithms, Gibbs sampling generates a Markov chain of samples, each of which is correlated with nearby samples. 
---------> As a result, care must be taken if independent samples are desired. 
---------> Generally, samples from the beginning of the chain (the burn-in period) may not accurately represent the desired distribution and are usually discarded. 

-----> Hybrid Monte Carlo: 
-------> This generates a sequence of samples using Hamiltonian weighted Markov chain Monte Carlo, 
---------> from a probability distribution which is difficult to sample directly.
---------> The Hamiltonian Monte Carlo algorithm (originally known as hybrid Monte Carlo) is a Markov chain Monte Carlo method for obtaining a sequence of random samples 
---------> which converge to being distributed according to a target probability distribution for which direct sampling is difficult.
---------> This sequence can be used to estimate integrals with respect to the target distribution (expected values).
-------> Hamiltonian Monte Carlo corresponds to an instance of the Metropolis–Hastings algorithm, 
---------> with a Hamiltonian dynamics evolution simulated using a time-reversible and volume-preserving numerical integrator 
---------> (typically the leapfrog integrator) to propose a move to a new point in the state space. 
---------> Compared to using a Gaussian random walk proposal distribution in the Metropolis–Hastings algorithm, 
---------> Hamiltonian Monte Carlo reduces the correlation between successive sampled states by proposing moves 
---------> to distant states which maintain a high probability of acceptance due to the approximate energy conserving properties 
---------> of the simulated Hamiltonian dynamic when using a symplectic integrator. 
---------> The reduced correlation means fewer Markov chain samples are needed to approximate integrals
---------> with respect to the target probability distribution for a given Monte Carlo error.
-------> The algorithm was originally proposed by Simon Duane, Anthony Kennedy, Brian Pendleton and Duncan Roweth in 1987 for calculations in lattice quantum chromodynamics.
---------> In 1996, Radford M. Neal recognized the usefulness of the method for a broader class of statistical problems, in particular artificial neural networks.
---------> The burden of having to supply gradients of the respective densities delayed the wider adoption of the method in statistics and other quantitative disciplines, 
---------> until in the mid-2010s the developers of Stan implemented HMC in combination with automatic differentiation.

-----> Metropolis–Hastings algorithm: 
-------> This is used to generate a sequence of samples from the probability distribution of one or more variables.
-------> In statistics and statistical physics, the Metropolis–Hastings algorithm is a Markov chain Monte Carlo (MCMC) method 
---------> for obtaining a sequence of random samples from a probability distribution from which direct sampling is difficult. 
---------> This sequence can be used to approximate the distribution (e.g. to generate a histogram) or to compute an integral (e.g. an expected value). 
---------> Metropolis–Hastings and other MCMC algorithms are generally used for sampling from multi-dimensional distributions, 
---------> especially when the number of dimensions is high. For single-dimensional distributions, there are usually other methods 
---------> (e.g. adaptive rejection sampling) that can directly return independent samples from the distribution, 
---------> and these are free from the problem of autocorrelated samples that is inherent in MCMC methods. 

-----> Wang and Landau algorithm: 
-------> This is an extension of Metropolis–Hastings algorithm sampling.
-------> The Wang and Landau algorithm, proposed by Fugao Wang and David P. Landau,
---------> is a Monte Carlo method designed to estimate the density of states of a system. 
---------> The method performs a non-Markovian random walk to build the density of states 
---------> by quickly visiting all the available energy spectrum. 
---------> The Wang and Landau algorithm is an important method to obtain the density of states required to perform a multicanonical simulation.
-------> The Wang–Landau algorithm can be applied to any system which is characterized by a cost (or energy) function. 
---------> For instance, it has been applied to the solution of numerical integrals and the folding of proteins.
---------> The Wang–Landau sampling is related to the metadynamics algorithm.



---> Numerical integration
-----> In analysis, numerical integration comprises a broad family of algorithms for calculating the numerical value of a definite integral, 
-------> and by extension, the term is also sometimes used to describe the numerical solution of differential equations. 
-----> The term numerical quadrature (often abbreviated to quadrature) is more or less a synonym for numerical integration, especially as applied to one-dimensional integrals. 
-------> Some authors refer to numerical integration over more than one dimension as cubature; others take quadrature to include higher-dimensional integration.
-----> The basic problem in numerical integration is to compute an approximate solution to a definite integral:
---------> ∫ f(x) dx (from a to b)
-------> to a given degree of accuracy. 
-------> If f(x) is a smooth function integrated over a small number of dimensions, and the domain of integration is bounded, 
-------> there are many methods for approximating the integral to the desired precision. 

-----> MISER algorithm: Monte Carlo simulation, numerical integration
-------> The MISER algorithm is based on recursive stratified sampling. 
---------> This technique aims to reduce the overall integration error by concentrating integration points in the regions of highest variance.
-------> The idea of stratified sampling begins with the observation that for two disjoint regions a and b with Monte Carlo 
---------> estimates of the integral Ea(f) and Eb(f) and variances σa^2(f) and σb^2(f), the variance Var(f) of the combined estimate
-----------> E(f) = 1/2 * (Ea(f) + Eb(f)) 
---------> is given by,
-----------> Var(f) = (σa^2(f))/(4*Na) + (σb^2(f))/(4*Nb)
-------> It can be shown that this variance is minimized by distributing the points such that,
-----------> Na/(Na+Nb) = σa/(σa+σb)
-------> Hence the smallest error estimate is obtained by allocating sample points in proportion to the standard deviation of the function in each sub-region.
-------> The MISER algorithm proceeds by bisecting the integration region along one coordinate axis to give two sub-regions at each step. 
---------> The direction is chosen by examining all d possible bisections and selecting the one which will minimize the combined variance of the two sub-regions. 
---------> The variance in the sub-regions is estimated by sampling with a fraction of the total number of points available to the current step. 
---------> The same procedure is then repeated recursively for each of the two half-spaces from the best bisection. 
---------> The remaining sample points are allocated to the sub-regions using the formula for Na and Nb. 
---------> This recursive allocation of integration points continues down to a user-specified depth where each sub-region is integrated using a plain Monte Carlo estimate. 
---------> These individual values and their error estimates are then combined upwards to give an overall result and an estimate of its error. 



---> Root finding
-----> In mathematics and computing, a root-finding algorithm is an algorithm for finding zeros, also called "roots", of continuous functions. 
-------> A zero of a function f, from the real numbers to real numbers or from the complex numbers to the complex numbers, is a number x such that f(x) = 0. 
-------> As, generally, the zeros of a function cannot be computed exactly nor expressed in closed form, 
-------> root-finding algorithms provide approximations to zeros, expressed either as floating-point numbers or as small isolating intervals, 
-------> or disks for complex roots (an interval or disk output being equivalent to an approximate output together with an error bound).
------ Solving an equation f(x) = g(x) is the same as finding the roots of the function h(x) = f(x) – g(x). 
-------> Thus root-finding algorithms allow solving any equation defined by continuous functions. 
-------> However, most root-finding algorithms do not guarantee that they will find all the roots; in particular, 
-------> if such an algorithm does not find any root, that does not mean that no root exists.
-----> Most numerical root-finding methods use iteration, producing a sequence of numbers that hopefully converge towards the root as a limit. 
-------> They require one or more initial guesses of the root as starting values, 
-------> then each iteration of the algorithm produces a successively more accurate approximation to the root. 
-------> Since the iteration must be stopped at some point these methods produce an approximation to the root, not an exact solution. 
-------> Many methods compute subsequent values by evaluating an auxiliary function on the preceding values. 
-------> The limit is thus a fixed point of the auxiliary function, 
-------> which is chosen for having the roots of the original equation as fixed points, and for converging rapidly to these fixed points.
-----> The behaviour of general root-finding algorithms is studied in numerical analysis. 
-------> However, for polynomials, root-finding study belongs generally to computer algebra, 
-------> since algebraic properties of polynomials are fundamental for the most efficient algorithms. 
-------> The efficiency of an algorithm may depend dramatically on the characteristics of the given functions. 
-------> For example, many algorithms use the derivative of the input function, while others work on every continuous function. 
-------> In general, numerical algorithms are not guaranteed to find all the roots of a function, 
-------> so failing to find a root does not prove that there is no root. 
-------> However, for polynomials, there are specific algorithms that use algebraic properties for certifying that no root is missed, 
-------> and locating the roots in separate intervals (or disks for complex roots) 
-------> that are small enough to ensure the convergence of numerical methods (typically Newton's method) to the unique root so located. 

-----> Bisection method
-------> In mathematics, the bisection method is a root-finding method that applies to any continuous function for which one knows two values with opposite signs. 
---------> The method consists of repeatedly bisecting the interval defined by these values 
---------> and then selecting the subinterval in which the function changes sign, and therefore must contain a root. 
---------> It is a very simple and robust method, but it is also relatively slow. 
---------> Because of this, it is often used to obtain a rough approximation to a solution which is then used as a starting point for more rapidly converging methods. 
---------> The method is also called the interval halving method, the binary search method, or the dichotomy method.
---------> For polynomials, more elaborate methods exist for testing the existence of a root in an interval (Descartes' rule of signs, Sturm's theorem, Budan's theorem). 
---------> They allow extending the bisection method into efficient algorithms for finding all real roots of a polynomial; see Real-root isolation. 

-----> False position method: 
-------> This approximates roots of a function
-------> In mathematics, the regula falsi, method of false position, or false position method 
---------> is a very old method for solving an equation with one unknown; this method, in modified form, is still in use. 
---------> In simple terms, the method is the trial and error technique of using test ("false") values for the variable 
---------> and then adjusting the test value according to the outcome. 
---------> This is sometimes also referred to as "guess and check". 
---------> Versions of the method predate the advent of algebra and the use of equations.
-------> As an example, consider problem 26 in the Rhind papyrus, which asks for a solution of (written in modern notation) the equation x + x/4 = 15. 
---------> This is solved by false position.[1] First, guess that x = 4 to obtain, on the left, 4 + 4/4 = 5. 
---------> This guess is a good choice since it produces an integer value. However, 4 is not the solution of the original equation, 
---------> as it gives a value which is three times too small. 
---------> To compensate, multiply x (currently set to 4) by 3 and substitute again to get 12 + 12/4 = 15, verifying that the solution is x = 12.
-------> Modern versions of the technique employ systematic ways of choosing new test values 
---------> and are concerned with the questions of whether or not an approximation to a solution can be obtained, and if it can, how fast can the approximation be found. 

-----> ITP method: 
-------> This minmax optimal and superlinar convergence simultaneously.
-------> In numerical analysis, the ITP method, short for Interpolate Truncate and Project, 
---------> is the first root-finding algorithm that achieves the superlinear convergence of the secant method 
---------> while retaining the optimal worst-case performance of the bisection method. 
---------> It is also the first method with guaranteed average performance strictly better than the bisection method under any continuous distribution. 
---------> In practice it performs better than traditional interpolation and hybrid based strategies (Brent's Method, Ridders, Illinois), 
---------> since it not only converges super-linearly over well behaved functions but also guarantees fast performance under ill-behaved functions where interpolations fail.
-------> The ITP method follows the same structure of standard bracketing strategies that keeps track of upper and lower bounds for the location of the root; 
---------> but it also keeps track of the region where worst-case performance is kept upper-bounded. 
---------> As a bracketing strategy, in each iteration the ITP queries the value of the function on one point 
---------> and discards the part of the interval between two points where the function value shares the same sign. 
---------> The queried point is calculated with three steps: it interpolates finding the regula falsi estimate, 
---------> then it perturbes/truncates the estimate (similar to Regula falsi § Improvements in regula falsi) 
---------> and then projects the perturbed estimate onto an interval in the neighbourhood of the bisection midpoint. 
---------> The neighbourhood around the bisection point is calculated in each iteration in order to guarantee minmax optimality (Theorem 2.1 of [3]). 
---------> The method depends on three hyper-parameters κ 1 ∈ ( 0 , ∞ ) , κ 2 ∈ [ 1 , 1 + ϕ ) and n 0 ∈ [ 0 , ∞ ) where ϕ is the golden ratio 1/2 * (1 + sqrt(5)): 
---------> the first two control the size of the truncation and the third is a slack variable that controls the size of the interval for the projection step.

-----> Newton's method: 
-------> This finds zeros of functions with calculus
-------> In numerical analysis, Newton's method, also known as the Newton–Raphson method, named after Isaac Newton and Joseph Raphson, 
---------> is a root-finding algorithm which produces successively better approximations to the roots (or zeroes) of a real-valued function. 
---------> The most basic version starts with a single-variable function f defined for a real variable x, the function's derivative f′, and an initial guess x0 for a root of f. 
---------> If the function satisfies sufficient assumptions and the initial guess is close, then
-----------> x1 = x0−f(x0)/f′(x0) 
---------> is a better approximation of the root than x0. 
---------> Geometrically, (x1, 0) is the intersection of the x-axis and the tangent of the graph of f at (x0, f(x0)): 
---------> that is, the improved guess is the unique root of the linear approximation at the initial point. 
---------> The process is repeated as
-----------> xn+1 = xn − f(xn)/f′(xn) 
---------> until a sufficiently precise value is reached. 
---------> This algorithm is first in the class of Householder's methods, succeeded by Halley's method. 
---------> The method can also be extended to complex functions and to systems of equations. 

-----> Halley's method: 
-------> This uses first and second derivatives
-------> In numerical analysis, Halley's method is a root-finding algorithm used for 
---------> functions of one real variable with a continuous second derivative.
---------> It is named after its inventor Edmond Halley.
-------> The algorithm is second in the class of Householder's methods, after Newton's method. 
---------> Like the latter, it iteratively produces a sequence of approximations to the root; their rate of convergence to the root is cubic. 
---------> Multidimensional versions of this method exist.
-------> Halley's method exactly finds the roots of a linear-over-linear Padé approximation to the function, 
---------> in contrast to Newton's method or the Secant method which approximate the function linearly, or Muller's method which approximates the function quadratically.

-----> Secant method: 2-point, 1-sided
-------> In numerical analysis, the secant method is a root-finding algorithm 
---------> that uses a succession of roots of secant lines to better approximate a root of a function f. 
---------> The secant method can be thought of as a finite-difference approximation of Newton's method. 
---------> However, the secant method predates Newton's method by over 3000 years.

-----> False position method and Illinois method: 2-point, bracketing
-------> The Illinois algorithm halves the y-value of the retained end point in the next estimate computation 
---------> when the new y-value (that is, f(ck)) has the same sign as the previous one (f(ck − 1)), meaning that the end point of the previous step will be retained. 
---------> Hence:
-----------> ck = (1/2*f(bk)*ak − f(ak)*bk) / (1/2*f(bk) − f(ak)) 
---------> or
-----------> ck = (f(bk)*ak − 1/2*f(ak)*bk) / (f(bk) − 1/2*f(ak)) 
---------> down-weighting one of the endpoint values to force the next ck to occur on that side of the function.
-------> The factor ½ used above looks arbitrary, but it guarantees superlinear convergence 
---------> (asymptotically, the algorithm will perform two regular steps after any modified step, and has order of convergence 1.442). 
---------> There are other ways to pick the rescaling which give even better superlinear convergence rates.
-------> The above adjustment to regula falsi is called the Illinois algorithm by some scholars. 
---------> Ford (1995) summarizes and analyzes this and other similar superlinear variants of the method of false position

-----> Ridder's method: 3-point, exponential scaling
-------> In numerical analysis, Ridders' method is a root-finding algorithm based on the false position method
---------> and the use of an exponential function to successively approximate a root of a continuous function f(x). 
---------> The method is due to C. Ridders.
-------> Ridders' method is simpler than Muller's method or Brent's method but with similar performance.
---------> The formula below converges quadratically when the function is well-behaved, 
---------> which implies that the number of additional significant digits found at each step approximately doubles; 
---------> but the function has to be evaluated twice for each step, so the overall order of convergence of the method is sqrt(2). 
---------> If the function is not well-behaved, the root remains bracketed 
---------> and the length of the bracketing interval at least halves on each iteration, so convergence is guaranteed. 

-----> Muller's method: 3-point, quadratic interpolation
-------> Muller's method is a root-finding algorithm, a numerical method for solving equations of the form f(x) = 0. 
---------> It was first presented by David E. Muller in 1956.
-------> Muller's method is based on the secant method, which constructs at every iteration a line through two points on the graph of f. 
---------> Instead, Muller's method uses three points, constructs the parabola through these three points, 
---------> and takes the intersection of the x-axis with the parabola to be the next approximation. 

-----> Brent's method:
-------> In numerical analysis, Brent's method is a hybrid root-finding algorithm combining the bisection method, 
---------> the secant method and inverse quadratic interpolation. 
---------> It has the reliability of bisection but it can be as quick as some of the less-reliable methods. 
---------> The algorithm tries to use the potentially fast-converging secant method or inverse quadratic interpolation if possible, 
---------> but it falls back to the more robust bisection method if necessary. 
---------> Brent's method is due to Richard Brent and builds on an earlier algorithm by Theodorus Dekker.
---------> Consequently, the method is also known as the Brent–Dekker method.
-------> Modern improvements on Brent's method include Chandrupatla's method, 
---------> which is simpler and faster for functions that are flat around their roots;
---------> Ridders' method, which performs exponential interpolations instead of quadratic providing a simpler closed formula for the iterations; 
---------> and the ITP method which is a hybrid between regula-falsi and bisection that achieves optimal worst-case and asymptotic guarantees. 



---> Optimization algorithms
-----> Mathematical optimization (alternatively spelled optimisation) or mathematical programming is the selection of a best element, 
-------> with regard to some criterion, from some set of available alternatives. 
-------> Optimization problems of sorts arise in all quantitative disciplines from computer science and engineering to operations research and economics, 
-------> and the development of solution methods has been of interest in mathematics for centuries.
-----> In the simplest case, an optimization problem consists of maximizing or minimizing a real function by systematically 
-------> choosing input values from within an allowed set and computing the value of the function. 
-------> The generalization of optimization theory and techniques to other formulations constitutes a large area of applied mathematics. 
-------> More generally, optimization includes finding "best available" values of some objective function given a defined domain (or input), 
-------> including a variety of different types of objective functions and different types of domains. 

-----> Alpha–beta pruning: 
-------> This search to reduce number of nodes in minimax algorithm.
-------> Alpha–beta pruning is a search algorithm that seeks to decrease the number of nodes that are evaluated by the minimax algorithm in its search tree. 
---------> It is an adversarial search algorithm used commonly for machine playing of two-player games (Tic-tac-toe, Chess, Connect 4, etc.). 
---------> It stops evaluating a move when at least one possibility has been found that proves the move to be worse than a previously examined move. 
---------> Such moves need not be evaluated further. 
---------> When applied to a standard minimax tree, it returns the same move as minimax would, but prunes away branches that cannot possibly influence the final decision.
-------> Pseudocode
---------> The pseudo-code for depth limited minimax with alpha–beta pruning is as follows:[13]
---------> Implementations of alpha–beta pruning can often be delineated by whether they are "fail-soft," or "fail-hard". 
-----------> With fail-soft alpha–beta, the alphabeta function may return values (v) that exceed (v < α or v > β) the α and β bounds set by its function call arguments. 
-----------> In comparison, fail-hard alpha–beta limits its function return value into the inclusive range of α and β. 
-----------> The main difference between fail-soft and fail-hard implementations is whether α and β are updated before or after the cutoff check. 
-----------> If they are updated before the check, then they can exceed initial bounds and the algorithm is fail-soft.
---------> The following pseudo-code illustrates the fail-hard variation.[1]
-----------> function alphabeta(node, depth, α, β, maximizingPlayer) is
----------->     if depth = 0 or node is a terminal node then
----------->         return the heuristic value of node
----------->     if maximizingPlayer then
----------->         value := −∞
----------->         for each child of node do
----------->             value := max(value, alphabeta(child, depth − 1, α, β, FALSE))
----------->             if value ≥ β then
----------->                 break (* β cutoff *)
----------->             α := max(α, value)
----------->         return value
----------->     else
----------->         value := +∞
----------->         for each child of node do
----------->             value := min(value, alphabeta(child, depth − 1, α, β, TRUE))
----------->             if value ≤ α then
----------->                 break (* α cutoff *)
----------->             β := min(β, value)
----------->         return value
-----------> (* Initial call *)
-----------> alphabeta(origin, depth, −∞, +∞, TRUE)
-----------> function alphabeta(node, depth, α, β, maximizingPlayer) is
----------->     if depth = 0 or node is a terminal node then
----------->         return the heuristic value of node
----------->     if maximizingPlayer then
----------->         value := −∞
----------->         for each child of node do
----------->             value := max(value, alphabeta(child, depth − 1, α, β, FALSE))
----------->             α := max(α, value)
----------->             if value ≥ β then
----------->                 break (* β cutoff *)
----------->         return value
----------->     else
----------->         value := +∞
----------->         for each child of node do
----------->             value := min(value, alphabeta(child, depth − 1, α, β, TRUE))
----------->             β := min(β, value)
----------->             if value ≤ α then
----------->                 break (* α cutoff *)
----------->         return value
-----------> (* Initial call *)
-----------> alphabeta(origin, depth, −∞, +∞, TRUE)

-----> Branch and bound
-------> Branch and bound (BB, B&B, or BnB) is an algorithm design paradigm for discrete and combinatorial optimization problems, as well as mathematical optimization. 
-------> A branch-and-bound algorithm consists of a systematic enumeration of candidate solutions by means of state space search: 
---------> the set of candidate solutions is thought of as forming a rooted tree with the full set at the root. 
---------> The algorithm explores branches of this tree, which represent subsets of the solution set. 
---------> Before enumerating the candidate solutions of a branch, the branch is checked against upper and lower estimated bounds on the optimal solution, 
---------> and is discarded if it cannot produce a better solution than the best one found so far by the algorithm.
-------> The algorithm depends on efficient estimation of the lower and upper bounds of regions/branches of the search space. 
---------> If no bounds are available, the algorithm degenerates to an exhaustive search.
-------> The name "branch and bound" first occurred in the work of Little et al. on the traveling salesman problem.
---------> Branch and bound methods do not go deep like Depth-first search; the first direction is lateral movement in the tree similar to Breadth-first search (BFS). 
-------> Generic version
---------> The following is the skeleton of a generic branch and bound algorithm for minimizing an arbitrary objective function f.
-----------> To obtain an actual algorithm from this, one requires a bounding function bound, 
-----------> that computes lower bounds of f on nodes of the search tree, as well as a problem-specific branching rule. 
-----------> As such, the generic algorithm presented here is a higher-order function.
-----------> (1) Using a heuristic, find a solution xh to the optimization problem. Store its value, B = f(xh). 
-------------> (If no heuristic is available, set B to infinity.) B will denote the best solution found so far, and will be used as an upper bound on candidate solutions.
-----------> (2) Initialize a queue to hold a partial solution with none of the variables of the problem assigned.
-----------> (3) Loop until the queue is empty:
-------------> (1) Take a node N off the queue.
-------------> (2) If N represents a single candidate solution x and f(x) < B, then x is the best solution so far. Record it and set B ← f(x).
-------------> (3) Else, branch on N to produce new nodes Ni. For each of these:
---------------> (1) If bound(Ni) > B, do nothing; since the lower bound on this node is greater than the upper bound of the problem, it will never lead to the optimal solution, and can be discarded.
---------------> (2) Else, store Ni on the queue.
---------> Several different queue data structures can be used. 
-----------> This FIFO queue-based implementation yields a breadth-first search. 
-----------> A stack (LIFO queue) will yield a depth-first algorithm. 
-----------> A best-first branch and bound algorithm can be obtained by using a priority queue that sorts nodes on their lower bound.
-----------> Examples of best-first search algorithms with this premise are Dijkstra's algorithm and its descendant A* search. 
-----------> The depth-first variant is recommended when no good heuristic is available for producing an initial solution, because it quickly produces full solutions, and therefore upper bounds.[7]
---------> Pseudocode
-----------> // C++-like implementation of branch and bound, 
-----------> // assuming the objective function f is to be minimized
-----------> CombinatorialSolution branch_and_bound_solve(
----------->     CombinatorialProblem problem, 
----------->     ObjectiveFunction objective_function /*f*/,
----------->     BoundingFunction lower_bound_function /*bound*/) 
-----------> {
----------->     // Step 1 above
----------->     double problem_upper_bound = std::numeric_limits<double>::infinity; // = B
----------->     CombinatorialSolution heuristic_solution = heuristic_solve(problem); // x_h
----------->     problem_upper_bound = objective_function(heuristic_solution); // B = f(x_h)
----------->     CombinatorialSolution current_optimum = heuristic_solution;
----------->     // Step 2 above
----------->     queue<CandidateSolutionTree> candidate_queue;
----------->     // problem-specific queue initialization
----------->     candidate_queue = populate_candidates(problem);
----------->     while (!candidate_queue.empty()) { // Step 3 above
----------->         // Step 3.1
----------->         CandidateSolutionTree node = candidate_queue.pop();
----------->         // "node" represents N above
----------->         if (node.represents_single_candidate()) { // Step 3.2
----------->             if (objective_function(node.candidate()) < problem_upper_bound) {
----------->                 current_optimum = node.candidate();
----------->                 problem_upper_bound = objective_function(current_optimum);
----------->             }
----------->             // else, node is a single candidate which is not optimum
----------->         }
----------->         else { // Step 3.3: node represents a branch of candidate solutions
----------->             // "child_branch" represents N_i above
----------->             for (auto&& child_branch : node.candidate_nodes) {
----------->                 if (lower_bound_function(child_branch) <= problem_upper_bound) {
----------->                     candidate_queue.enqueue(child_branch); // Step 3.3.2
----------->                 }
----------->                 // otherwise, bound(N_i) > B so we prune the branch; step 3.3.1
----------->             }
----------->         }
----------->     }
----------->     return current_optimum;
-----------> }
---------> In the above pseudocode, the functions heuristic_solve and populate_candidates called as subroutines must be provided as applicable to the problem. 
-----------> The functions f (objective_function) and bound (lower_bound_function) are treated as function objects as written, 
-----------> and could correspond to lambda expressions, function pointers and other types of callable objects in the C++ programming language. 
-------> Note: The method was first proposed by Ailsa Land and Alison Doig whilst carrying out research 
---------> at the London School of Economics sponsored by British Petroleum in 1960 for discrete programming,
---------> and has become the most commonly used tool for solving NP-hard optimization problems.

-----> Bruss algorithm: 
-------> This is also known as odds algorithm.
-------> The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. 
---------> Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below.
-------> The odds-algorithm applies to a class of problems called last-success-problems. 
---------> Formally, the objective in these problems is to maximize the probability of identifying in a sequence of 
---------> sequentially observed independent events the last event satisfying a specific criterion (a "specific event"). 
---------> This identification must be done at the time of observation. No revisiting of preceding observations is permitted. 
---------> Usually, a specific event is defined by the decision maker as an event that is of true interest in the view of "stopping" to take a well-defined action. 
---------> Such problems are encountered in several situations.
-------> Algorithmic procedure
---------> The odds-algorithm sums up the odds in reverse order
-----------> rn + rn−1 + rn−2 + ⋯, 
---------> until this sum reaches or exceeds the value 1 for the first time. 
---------> If this happens at index s, it saves s and the corresponding sum
-----------> Rs = rn + rn−1 + rn−2 + ⋯ + rs . 
---------> If the sum of the odds does not reach 1, it sets s = 1. At the same time it computes
-----------> Q s = qn * qn−1 ⋯ qs . 
---------> The output is
-----------> (1) s, the stopping threshold
-----------> (2) w = Qs * Rs, the win probability.

-----> Chain matrix multiplication
-------> Matrix chain multiplication (or the matrix chain ordering problem) is an optimization problem 
---------> concerning the most efficient way to multiply a given sequence of matrices. 
---------> The problem is not actually to perform the multiplications, 
---------> but merely to decide the sequence of the matrix multiplications involved. 
---------> The problem may be solved using dynamic programming.
-------> There are many options because matrix multiplication is associative. 
---------> In other words, no matter how the product is parenthesized, the result obtained will remain the same. 
---------> For example, for four matrices A, B, C, and D, there are five possible options:
-----------> ((AB)C)D = (A(BC))D = (AB)(CD) = A((BC)D) = A(B(CD)).
-------> Although it does not affect the product, the order in which the terms are parenthesized affects 
---------> the number of simple arithmetic operations needed to compute the product, that is, the computational complexity. 
---------> The straightforward multiplication of a matrix that is X × Y by a matrix that is Y × Z requires XYZ ordinary multiplications and X(Y − 1)Z ordinary additions. 
---------> In this context, it is typical to use the number of ordinary multiplications as a measure of the runtime complexity.
-------> If A is a 10 × 30 matrix, B is a 30 × 5 matrix, and C is a 5 × 60 matrix, then
---------> computing (AB)C needs (10×30×5) + (10×5×60) = 1500 + 3000 = 4500 operations, while
---------> computing A(BC) needs (30×5×60) + (10×30×60) = 9000 + 18000 = 27000 operations.
-------> Clearly the first method is more efficient. With this information, 
---------> the problem statement can be refined as "how to determine the optimal parenthesization of a product of n matrices?" 
---------> Checking each possible parenthesization (brute force) would require a run-time that is exponential in the number of matrices, 
---------> which is very slow and impractical for large n. 
---------> A quicker solution to this problem can be achieved by breaking up the problem into a set of related subproblems. 
-------> A dynamic programming algorithm
---------> To begin, let us assume that all we really want to know is the minimum cost, 
-----------> or minimum number of arithmetic operations needed to multiply out the matrices. 
-----------> If we are only multiplying two matrices, there is only one way to multiply them, 
-----------> so the minimum cost is the cost of doing this. 
-----------> In general, we can find the minimum cost using the following recursive algorithm:
-------------> Take the sequence of matrices and separate it into two subsequences.
-------------> Find the minimum cost of multiplying out each subsequence.
-------------> Add these costs together, and add in the cost of multiplying the two result matrices.
-------------> Do this for each possible position at which the sequence of matrices can be split, and take the minimum over all of them.
---------> For example, if we have four matrices ABCD, we compute the cost required to find each of (A)(BCD), (AB)(CD), and (ABC)(D),
-----------> making recursive calls to find the minimum cost to compute ABC, AB, CD, and BCD. We then choose the best one. 
-----------> Better still, this yields not only the minimum cost, but also demonstrates the best way of doing the multiplication:
-----------> group it the way that yields the lowest total cost, and do the same for each factor.
---------> However, this algorithm has exponential runtime complexity making it as inefficient as the naive approach of trying all permutations. 
-----------> The reason is that the algorithm does a lot of redundant work. 
-----------> For example, above we made a recursive call to find the best cost for computing both ABC and AB. 
-----------> But finding the best cost for computing ABC also requires finding the best cost for AB. 
-----------> As the recursion grows deeper, more and more of this type of unnecessary repetition occurs.
---------> One simple solution is called memoization: each time we compute the minimum cost needed to multiply out a specific subsequence, we save it. 
-----------> If we are ever asked to compute it again, we simply give the saved answer, and do not recompute it. 
-----------> Since there are about n2/2 different subsequences, where n is the number of matrices, the space required to do this is reasonable. 
-----------> It can be shown that this simple trick brings the runtime down to O(n3) from O(2n), which is more than efficient enough for real applications. 
-----------> This is top-down dynamic programming.
---------> The following bottom-up approach [1] computes, for each 2 ≤ k ≤ n, the minimum costs of all subsequences of length k using the costs of smaller subsequences already computed. 
-----------> It has the same asymptotic runtime and requires no recursion.
-----------> Pseudocode:
-------------> // Matrix A[i] has dimension dims[i-1] x dims[i] for i = 1..n
-------------> MatrixChainOrder(int dims[])
-------------> {
------------->     // length[dims] = n + 1
------------->     n = dims.length - 1;
------------->     // m[i,j] = Minimum number of scalar multiplications (i.e., cost)
------------->     // needed to compute the matrix A[i]A[i+1]...A[j] = A[i..j]
------------->     // The cost is zero when multiplying one matrix
------------->     for (i = 1; i <= n; i++)
------------->         m[i, i] = 0;
------------->     for (len = 2; len <= n; len++) { // Subsequence lengths
------------->         for (i = 1; i <= n - len + 1; i++) {
------------->             j = i + len - 1;
------------->             m[i, j] = MAXINT;
------------->             for (k = i; k <= j - 1; k++) {
------------->                 cost = m[i, k] + m[k+1, j] + dims[i-1]*dims[k]*dims[j];
------------->                 if (cost < m[i, j]) {
------------->                     m[i, j] = cost;
------------->                     s[i, j] = k; // Index of the subsequence split that achieved minimal cost
------------->                 }
------------->             }
------------->         }
------------->     }
-------------> }
-----------> Note : The first index for dims is 0 and the first index for m and s is 1.

-----> Combinatorial optimization: 
-------> This are optimization problems where the set of feasible solutions is discrete.
-------> Combinatorial optimization is a subfield of mathematical optimization that consists of finding an optimal object from a finite set of objects, 
---------> where the set of feasible solutions is discrete or can be reduced to a discrete set. 
---------> Typical combinatorial optimization problems are the travelling salesman problem ("TSP"), the minimum spanning tree problem ("MST"), and the knapsack problem. 
---------> In many such problems, such as the ones previously mentioned, exhaustive search is not tractable, 
---------> and so specialized algorithms that quickly rule out large parts of the search space or approximation algorithms must be resorted to instead.
-------> Combinatorial optimization is related to operations research, algorithm theory, and computational complexity theory. 
---------> It has important applications in several fields, including artificial intelligence, 
---------> machine learning, auction theory, software engineering, applied mathematics and theoretical computer science.
-------> Some research literature considers discrete optimization to consist of integer programming together with combinatorial optimization 
---------> (which in turn is composed of optimization problems dealing with graph structures), 
---------> although all of these topics have closely intertwined research literature. 
---------> It often involves determining the way to efficiently allocate resources used to find solutions to mathematical problems.

-------> Greedy randomized adaptive search procedure (GRASP): 
---------> This are successive constructions of a greedy randomized solution and subsequent iterative improvements of it through a local search.
---------> The greedy randomized adaptive search procedure (also known as GRASP) is a metaheuristic algorithm commonly applied to combinatorial optimization problems. 
-----------> GRASP typically consists of iterations made up from successive constructions of a greedy randomized solution 
-----------> and subsequent iterative improvements of it through a local search.
-----------> The greedy randomized solutions are generated by adding elements to the problem's solution set 
-----------> from a list of elements ranked by a greedy function according to the quality of the solution they will achieve. 
-----------> To obtain variability in the candidate set of greedy solutions, 
-----------> well-ranked candidate elements are often placed in a restricted candidate list, and chosen at random when building up the solution. 
-----------> This kind of greedy randomized construction method is also known as a semi-greedy heuristic, first described in Hart and Shogan (1987).
---------> GRASP was first introduced in Feo and Resende (1989).[3] Survey papers on GRASP include Feo and Resende (1995),[1] and Resende and Ribeiro (2003).[4]
---------> There are variations of the classical algorithm, such as the Reactive GRASP. 
-----------> In this variation, the basic parameter that defines the restrictiveness of the RCL during the construction phase 
-----------> is self-adjusted according to the quality of the solutions previously found.
-----------> There are also techniques for search speed-up, such as cost perturbations, bias functions, 
-----------> memorization and learning, and local search on partially constructed solutions

-------> Hungarian method: 
---------> This is a combinatorial optimization algorithm which solves the assignment problem in polynomial time.
---------> The Hungarian method is a combinatorial optimization algorithm that solves the assignment problem 
-----------> in polynomial time and which anticipated later primal–dual methods. 
---------> James Munkres reviewed the algorithm in 1957 and observed that it is (strongly) polynomial.
-----------> Since then the algorithm has been known also as the Kuhn–Munkres algorithm or Munkres assignment algorithm. 
-----------> The time complexity of the original algorithm was O(n^4), however Edmonds and Karp, 
-----------> and independently Tomizawa noticed that it can be modified to achieve an O(n^3) running time. 
-----------> One of the most popular O(n^3) variants is the Jonker–Volgenant algorithm.
-----------> Ford and Fulkerson extended the method to general maximum flow problems in form of the Ford–Fulkerson algorithm. 
-----------> In 2006, it was discovered that Carl Gustav Jacobi had solved the assignment problem in the 19th century, and the solution had been published posthumously in 1890 in Latin.
---------> Note: It was developed and published in 1955 by Harold Kuhn, who gave the name "Hungarian method" 
-----------> because the algorithm was largely based on the earlier works of two Hungarian mathematicians: Dénes Kőnig and Jenő Egerváry.[1][2]

-----> Constraint satisfaction
-------> In artificial intelligence and operations research, constraint satisfaction is the process of finding a solution 
---------> through a set of constraints that impose conditions that the variables must satisfy. 
---------> A solution is therefore a set of values for the variables that satisfies all constraints—that is, a point in the feasible region.
-------> The techniques used in constraint satisfaction depend on the kind of constraints being considered. 
---------> Often used are constraints on a finite domain, to the point that constraint satisfaction problems 
---------> are typically identified with problems based on constraints on a finite domain. 
---------> Such problems are usually solved via search, in particular a form of backtracking or local search. 
---------> Constraint propagation are other methods used on such problems; most of them are incomplete in general, 
---------> that is, they may solve the problem or prove it unsatisfiable, but not always. 
---------> Constraint propagation methods are also used in conjunction with search to make a given problem simpler to solve. 
---------> Other considered kinds of constraints are on real or rational numbers; 
---------> solving problems on these constraints is done via variable elimination or the simplex algorithm.
-------> Constraint satisfaction as a general problem originated in the field of artificial intelligence in the 1970s (see for example (Laurière 1978)). 
---------> However, when the constraints are expressed as multivariate linear equations defining (in)equalities, the field goes back to Joseph Fourier in the 19th century: 
---------> George Dantzig's invention of the Simplex Algorithm for Linear Programming (a special case of mathematical optimization) 
---------> in 1946 has allowed determining feasible solutions to problems containing hundreds of variables.
-------> During the 1980s and 1990s, embedding of constraints into a programming language were developed. 
---------> The first languages devised expressly with intrinsic support for constraint programming was Prolog. 
---------> Since then, constraint-programming libraries have become available in other languages, such as C++ or Java (e.g., Choco for Java[2]). 

-------> General algorithms for the constraint satisfaction

---------> AC-3 algorithm
-----------> In constraint satisfaction, the AC-3 algorithm (short for Arc Consistency Algorithm #3) 
-------------> is one of a series of algorithms used for the solution of constraint satisfaction problems (or CSP's). 
-------------> It was developed by Alan Mackworth in 1977. The earlier AC algorithms are often considered too inefficient, 
-------------> and many of the later ones are difficult to implement, and so AC-3 is the one most often taught and used in very simple constraint solvers. 
-----------> Algorithm
-------------> AC-3 operates on constraints, variables, and the variables' domains (scopes). 
---------------> A variable can take any of several discrete values; the set of values for a particular variable is known as its domain. 
---------------> A constraint is a relation that limits or constrains the values a variable may have. 
---------------> The constraint may involve the values of other variables.
-------------> The current status of the CSP during the algorithm can be viewed as a directed graph, 
---------------> where the nodes are the variables of the problem, 
---------------> with edges or arcs between variables that are related by symmetric constraints, w
---------------> here each arc in the worklist represents a constraint that needs to be checked for consistency. 
---------------> AC-3 proceeds by examining the arcs between pairs of variables (x, y). 
---------------> It removes those values from the domain of x which aren't consistent with the constraints between x and y. 
---------------> The algorithm keeps a collection of arcs that are yet to be checked; 
---------------> when the domain of a variable has any values removed, 
---------------> all the arcs of constraints pointing to that pruned variable (except the arc of the current constraint) are added to the collection. 
---------------> Since the domains of the variables are finite and either one arc or at least one value are removed at each step, 
---------------> this algorithm is guaranteed to terminate.
-------------> For illustration, here is an example of a very simple constraint problem: X (a variable) 
---------------> has the possible values {0, 1, 2, 3, 4, 5} -- the set of these values are the domain of X, or D(X). 
---------------> The variable Y has the domain D(Y) = {0, 1, 2, 3, 4, 5, 6, 7, 8, 9}. 
---------------> Together with the constraints C1 = "X must be even" and C2 = "X + Y must equal 4" we have a CSP which AC-3 can solve. 
---------------> Notice that the actual constraint graph representing this problem must contain two edges 
---------------> between X and Y since C2 is undirected but the graph representation being used by AC-3 is directed.
-------------> It does so by first removing the non-even values out of the domain of X as required by C1, leaving D(X) = { 0, 2, 4 }. 
---------------> It then examines the arcs between X and Y implied by C2. 
---------------> Only the pairs (X=0, Y=4), (X=2, Y=2), and (X=4, Y=0) match the constraint C2. 
---------------> AC-3 then terminates, with D(X) = {0, 2, 4} and D(Y) = {0, 2, 4}.
---------------> AC-3 is expressed in pseudocode as follows:
---------------> Input:
-----------------> A set of variables X
-----------------> A set of domains D(x) for each variable x in X. D(x) contains vx0, vx1... vxn, the possible values of x
-----------------> A set of unary constraints R1(x) on variable x that must be satisfied
-----------------> A set of binary constraints R2(x, y) on variables x and y that must be satisfied
---------------> Output:
-----------------> Arc consistent domains for each variable.
-------------------> function ac3 (X, D, R1, R2)
------------------->     // Initial domains are made consistent with unary constraints.
------------------->     for each x in X
------------------->         D(x) := { vx in D(x) | vx satisfies R1(x) }   
------------------->     // 'worklist' contains all arcs we wish to prove consistent or not.
------------------->     worklist := { (x, y) | there exists a relation R2(x, y) or a relation R2(y, x) }
------------------->     do
------------------->         select any arc (x, y) from worklist
------------------->         worklist := worklist - (x, y)
------------------->         if arc-reduce (x, y) 
------------------->             if D(x) is empty
------------------->                 return failure
------------------->             else
------------------->                 worklist := worklist + { (z, x) | z != y and there exists a relation R2(x, z) or a relation R2(z, x) }
------------------->     while worklist not empty
-------------------> function arc-reduce (x, y)
------------------->     bool change = false
------------------->     for each vx in D(x)
------------------->         find a value vy in D(y) such that vx and vy satisfy the constraint R2(x, y)
------------------->         if there is no such vy {
------------------->             D(x) := D(x) - vx
------------------->             change := true
------------------->         }
------------------->     return change
-------------> The algorithm has a worst-case time complexity of O(ed3) and space complexity of O(e), where e is the number of arcs and d is the size of the largest domain. 

---------> Difference map algorithm
-----------> The difference-map algorithm is a search algorithm for general constraint satisfaction problems. 
-------------> It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. 
-------------> From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. 
-------------> Solutions are encoded as fixed points of the mapping.
-----------> Although originally conceived as a general method for solving the phase problem, 
-------------> the difference-map algorithm has been used for the boolean satisfiability problem, protein structure prediction, 
-------------> Ramsey numbers, diophantine equations, and Sudoku,[1] as well as sphere- and disk-packing problems.
-------------> Since these applications include NP-complete problems, the scope of the difference map is that of an incomplete algorithm. 
-------------> Whereas incomplete algorithms can efficiently verify solutions (once a candidate is found), they cannot prove that a solution does not exist.
-----------> The difference-map algorithm is a generalization of two iterative methods: 
-------------> Fienup's Hybrid input output (HIO) algorithm for phase retrieval and the Douglas-Rachford algorithm for convex optimization. 
-------------> Iterative methods, in general, have a long history in phase retrieval and convex optimization. 
-------------> The use of this style of algorithm for hard, non-convex problems is a more recent development. 

---------> Min conflicts algorithm
-----------> In computer science, the min-conflicts algorithm is a search algorithm or heuristic method to solve constraint satisfaction problems.
-----------> Given an initial assignment of values to all the variables of a constraint satisfaction problem, 
-------------> the algorithm randomly selects a variable from the set of variables with conflicts violating one or more its constraints. 
-------------> Then it assigns to this variable the value that minimizes the number of conflicts. 
-------------> If there is more than one value with a minimum number of conflicts, it chooses one randomly. 
-------------> This process of random variable selection and min-conflict value assignment is iterated until 
-------------> a solution is found or a pre-selected maximum number of iterations is reached.
-----------> Because a constraint satisfaction problem can be interpreted as a local search problem when all the variables have an assigned value (called a complete state), 
-------------> the min conflicts algorithm can be seen as a repair heuristic that chooses the state with the minimum number of conflicts. 

-------> Chaff algorithm: 
---------> This is an algorithm for solving instances of the boolean satisfiability problem.
---------> Chaff is an algorithm for solving instances of the Boolean satisfiability problem in programming. 
-----------> It was designed by researchers at Princeton University, United States. 
-----------> The algorithm is an instance of the DPLL algorithm with a number of enhancements for efficient implementation. 

-------> Davis–Putnam algorithm: 
---------> This is check the validity of a first-order logic formula
---------> The Davis–Putnam algorithm was developed by Martin Davis and Hilary Putnam for checking the validity 
-----------> of a first-order logic formula using a resolution-based decision procedure for propositional logic. 
-----------> Since the set of valid first-order formulas is recursively enumerable but not recursive, 
-----------> there exists no general algorithm to solve this problem. 
-----------> Therefore, the Davis–Putnam algorithm only terminates on valid formulas. 
-----------> Today, the term "Davis–Putnam algorithm" is often used synonymously with the resolution-based propositional decision procedure 
-----------> (Davis–Putnam procedure) that is actually only one of the steps of the original algorithm. 

-------> Davis–Putnam–Logemann–Loveland algorithm (DPLL): 
---------> This is an algorithm for deciding the satisfiability of propositional logic formula in conjunctive normal form, i.e. for solving the CNF-SAT problem
---------> In logic and computer science, the Davis–Putnam–Logemann–Loveland (DPLL) algorithm is a complete, 
-----------> backtracking-based search algorithm for deciding the satisfiability of propositional logic formulae in conjunctive normal form, i.e. for solving the CNF-SAT problem.
---------> It was introduced in 1961 by Martin Davis, George Logemann and Donald W. Loveland and is a refinement of the earlier Davis–Putnam algorithm, 
-----------> which is a resolution-based procedure developed by Davis and Hilary Putnam in 1960. 
-----------> Especially in older publications, the Davis–Logemann–Loveland algorithm is often referred to as the "Davis–Putnam method" or the "DP algorithm". 
-----------> Other common names that maintain the distinction are DLL and DPLL. 

-------> Exact cover problem
---------> In the mathematical field of topology, given a collection S of subsets of a set X, 
-----------> an exact cover is a subcollection S* of S such that each element in X is contained in exactly one subset in S*. 
-----------> One says that each element in X is covered by exactly one subset in S*. 
-----------> An exact cover is a kind of cover.
---------> In computer science, the exact cover problem is a decision problem to determine if an exact cover exists. 
-----------> The exact cover problem is NP-complete[1] and is one of Karp's 21 NP-complete problems.
-----------> It is NP-complete even when each subset in S contains exactly three elements; this restricted problem is known as exact cover by 3-sets, often abbreviated X3C.
-----------> The exact cover problem is a kind of constraint satisfaction problem.
---------> An exact cover problem can be represented by an incidence matrix or a bipartite graph.
---------> Knuth's Algorithm X is an algorithm that finds all solutions to an exact cover problem. 
-----------> DLX is the name given to Algorithm X when it is implemented efficiently using Donald Knuth's Dancing Links technique on a computer.
---------> The standard exact cover problem can be generalized slightly to involve not only "exactly one" constraints but also "at-most-one" constraints.
---------> Finding Pentomino tilings and solving Sudoku are noteworthy examples of exact cover problems. The n queens problem is a slightly generalized exact cover problem. 

---------> Algorithm X: 
-----------> This a nondeterministic algorithm for solving the exact cover problem. 
-------------> It is a straightforward recursive, nondeterministic, depth-first, 
-------------> backtracking algorithm used by Donald Knuth to demonstrate an efficient implementation called DLX, which uses the dancing links technique.
-----------> The exact cover problem is represented in Algorithm X by a matrix A consisting of 0s and 1s.
------------->  The goal is to select a subset of the rows such that the digit 1 appears in each column exactly once.
-----------> Algorithm X functions as follows:
-------------> # If the matrix A has no columns, the current partial solution is a valid solution; terminate successfully.
---------------> Otherwise choose a column c (deterministically).
---------------> Choose a row r such that Ar, c = 1 (nondeterministically).
---------------> Include row r in the partial solution.
---------------> For each column j such that Ar, j = 1,
--------------->     for each row i such that Ai, j = 1,
--------------->         delete row i from matrix A.
--------------->     delete column j from matrix A.
---------------> Repeat this algorithm recursively on the reduced matrix A.
-------------> The nondeterministic choice of r means that the algorithm recurses over independent subalgorithms; 
---------------> each subalgorithm inherits the current matrix A, but reduces it with respect to a different row r. 
---------------> If column c is entirely zero, there are no subalgorithms and the process terminates unsuccessfully.
-------------> The subalgorithms form a search tree in a natural way, 
---------------> with the original problem at the root and with level k containing each subalgorithm that corresponds to k chosen rows. 
---------------> Backtracking is the process of traversing the tree in preorder, depth first.
-------------> Any systematic rule for choosing column c in this procedure will find all solutions, but some rules work much better than others. 
---------------> To reduce the number of iterations, Knuth suggests that the column-choosing algorithm select a column with the smallest number of 1s in it. 

---------> Dancing Links: 
-----------> This an efficient implementation of Algorithm X
-----------> In computer science, dancing links (DLX) is a technique for adding and deleting a node from a circular doubly linked list. 
-------------> It is particularly useful for efficiently implementing backtracking algorithms, such as Donald Knuth's Algorithm X for the exact cover problem.
-------------> Algorithm X is a recursive, nondeterministic, depth-first, backtracking algorithm that finds all solutions to the exact cover problem. 
-------------> Some of the better-known exact cover problems include tiling, the n queens problem, and Sudoku.
-----------> The name dancing links, which was suggested by Donald Knuth, stems from the way the algorithm works, 
-------------> as iterations of the algorithm cause the links to "dance" with partner links so as to resemble an "exquisitely choreographed dance." 
-------------> Knuth credits Hiroshi Hitotsumatsu and Kōhei Noshita with having invented the idea in 1979, but it is his paper which has popularized it. 

-----> Cross-entropy method: 
-------> This a general Monte Carlo approach to combinatorial and continuous multi-extremal optimization and importance sampling.
-------> The cross-entropy (CE) method is a Monte Carlo method for importance sampling and optimization. 
---------> It is applicable to both combinatorial and continuous problems, with either a static or noisy objective.
---------> The method approximates the optimal importance sampling estimator by repeating two phases:
-----------> Draw a sample from a probability distribution.
-----------> Minimize the cross-entropy between this distribution and a target distribution to produce a better sample in the next iteration.
---------> Reuven Rubinstein developed the method in the context of rare event simulation, where tiny probabilities must be estimated, 
-----------> for example in network reliability analysis, queueing models, or performance analysis of telecommunication systems. 
-----------> The method has also been applied to the traveling salesman, quadratic assignment, DNA sequence alignment, max-cut and buffer allocation problems. 

-----> Differential evolution
-------> In evolutionary computation, differential evolution (DE) is a method that optimizes a problem 
---------> by iteratively trying to improve a candidate solution with regard to a given measure of quality. 
---------> Such methods are commonly known as metaheuristics as they make few or no assumptions 
---------> about the problem being optimized and can search very large spaces of candidate solutions.
---------> However, metaheuristics such as DE do not guarantee an optimal solution is ever found.
-------> DE is used for multidimensional real-valued functions but does not use the gradient of the problem being optimized, 
---------> which means DE does not require the optimization problem to be differentiable, 
---------> as is required by classic optimization methods such as gradient descent and quasi-newton methods. 
---------> DE can therefore also be used on optimization problems that are not even continuous, are noisy, change over time, etc.
-------> DE optimizes a problem by maintaining a population of candidate solutions 
---------> and creating new candidate solutions by combining existing ones according to its simple formulae, 
---------> and then keeping whichever candidate solution has the best score or fitness on the optimization problem at hand.
---------> In this way, the optimization problem is treated as a black box that merely provides 
---------> a measure of quality given a candidate solution and the gradient is therefore not needed.
-------> DE was introduced by Storn and Price in the 1990s.
---------> Books have been published on theoretical and practical aspects of using DE in parallel computing, 
---------> multiobjective optimization, constrained optimization, and the books also contain surveys of application areas.
-------> Surveys on the multi-faceted research aspects of DE can be found in journal articles.
 
-----> Dynamic Programming: 
-------> This solves problems exhibiting the properties of overlapping subproblems and optimal substructure
---------> Dynamic programming is both a mathematical optimization method and a computer programming method. 
-------> In both contexts it refers to simplifying a complicated problem by breaking it down into simpler sub-problems in a recursive manner. 
---------> While some decision problems cannot be taken apart this way, decisions that span several points in time do often break apart recursively. 
---------> Likewise, in computer science, if a problem can be solved optimally by breaking it into sub-problems 
---------> and then recursively finding the optimal solutions to the sub-problems, then it is said to have optimal substructure.
-------> If sub-problems can be nested recursively inside larger problems, so that dynamic programming methods are applicable, 
---------> then there is a relation between the value of the larger problem and the values of the sub-problems.
---------> In the optimization literature this relationship is called the Bellman equation. 
-------> Note: The method was developed by Richard Bellman in the 1950s and has found applications in numerous fields, from aerospace engineering to economics.

-----> Ellipsoid method: 
-------> This is an algorithm for solving convex optimization problems
---------> In mathematical optimization, the ellipsoid method is an iterative method for minimizing convex functions. 
---------> When specialized to solving feasible linear optimization problems with rational data, 
---------> the ellipsoid method is an algorithm which finds an optimal solution in a number of steps that is polynomial in the input size.
-------> The ellipsoid method generates a sequence of ellipsoids whose volume uniformly decreases at every step, thus enclosing a minimizer of a convex function. 

-----> Evolutionary computation: 
-------> This is an optimization inspired by biological mechanisms of evolution
-------> In computer science, evolutionary computation is a family of algorithms for global optimization inspired by biological evolution, 
---------> and the subfield of artificial intelligence and soft computing studying these algorithms. 
---------> In technical terms, they are a family of population-based trial and error problem solvers with a metaheuristic or stochastic optimization character.
-------> In evolutionary computation, an initial set of candidate solutions is generated and iteratively updated. 
---------> Each new generation is produced by stochastically removing less desired solutions, and introducing small random changes. 
---------> In biological terminology, a population of solutions is subjected to natural selection (or artificial selection) and mutation. 
---------> As a result, the population will gradually evolve to increase in fitness, in this case the chosen fitness function of the algorithm.
-------> Evolutionary computation techniques can produce highly optimized solutions in a wide range of problem settings, making them popular in computer science. 
---------> Many variants and extensions exist, suited to more specific families of problems and data structures. 
---------> Evolutionary computation is also sometimes used in evolutionary biology as an in silico experimental procedure to study common aspects of general evolutionary processes. 

-------> Evolution strategy
---------> In computer science, an evolution strategy (ES) is an optimization technique based on ideas of evolution. 
-----------> It belongs to the general class of evolutionary computation or artificial evolution methodologies. 
---------> Methods
-----------> Evolution strategies use natural problem-dependent representations, and primarily mutation and selection, as search operators. 
-------------> In common with evolutionary algorithms, the operators are applied in a loop. An iteration of the loop is called a generation. 
-------------> The sequence of generations is continued until a termination criterion is met.
-----------> For real-valued search spaces, mutation is performed by adding a normally distributed random vector. 
-------------> The step size or mutation strength (i.e. the standard deviation of the normal distribution) is often governed by self-adaptation (see evolution window). 
-------------> Individual step sizes for each coordinate, or correlations between coordinates, which are essentially defined by an underlying covariance matrix, 
-------------> are controlled in practice either by self-adaptation or by covariance matrix adaptation (CMA-ES). 
-------------> When the mutation step is drawn from a multivariate normal distribution using an evolving covariance matrix, 
-------------> it has been hypothesized that this adapted matrix approximates the inverse Hessian of the search landscape. 
-------------> This hypothesis has been proven for a static model relying on a quadratic approximation.[1]
-----------> The (environmental) selection in evolution strategies is deterministic and only based on the fitness rankings, not on the actual fitness values. 
-------------> The resulting algorithm is therefore invariant with respect to monotonic transformations of the objective function. 
-------------> The simplest evolution strategy operates on a population of size two: the current point (parent) and the result of its mutation. 
-------------> Only if the mutant's fitness is at least as good as the parent one, it becomes the parent of the next generation. 
-------------> Otherwise the mutant is disregarded. This is a (1 + 1)-ES. 
-------------> More generally, λ mutants can be generated and compete with the parent, called (1 + λ)-ES. In (1 , λ)-ES 
-------------> the best mutant becomes the parent of the next generation while the current parent is always disregarded. 
-------------> For some of these variants, proofs of linear convergence (in a stochastic sense) have been derived on unimodal objective functions.
-----------> Contemporary derivatives of evolution strategy often use a population of μ parents and recombination as an additional operator, called (μ/ρ+, λ)-ES. 
-------------> This makes them less prone to settle in local optima.

-------> Gene expression programming
---------> In computer programming, gene expression programming (GEP) is an evolutionary algorithm that creates computer programs or models. 
-----------> These computer programs are complex tree structures that learn and adapt by changing their sizes, shapes, and composition, much like a living organism. 
-----------> And like living organisms, the computer programs of GEP are also encoded in simple linear chromosomes of fixed length. 
-----------> Thus, GEP is a genotype–phenotype system, benefiting from a simple genome to keep and transmit the genetic information 
-----------> and a complex phenotype to explore the environment and adapt to it. 

-------> Genetic algorithms
---------> In computer science and operations research, a genetic algorithm (GA) is a metaheuristic inspired by 
-----------> the process of natural selection that belongs to the larger class of evolutionary algorithms (EA). 
-----------> Genetic algorithms are commonly used to generate high-quality solutions to optimization 
-----------> and search problems by relying on biologically inspired operators such as mutation, crossover and selection.
-----------> Some examples of GA applications include optimizing decision trees for better performance, solving sudoku puzzles,[2] hyperparameter optimization, etc. 

---------> Fitness proportionate selection (also known as roulette-wheel selection)
-----------> Fitness proportionate selection, also known as roulette wheel selection, 
-------------> is a genetic operator used in genetic algorithms for selecting potentially useful solutions for recombination.
-----------> In fitness proportionate selection, as in all selection methods, the fitness function assigns a fitness to possible solutions or chromosomes. 
-------------> This fitness level is used to associate a probability of selection with each individual chromosome. 
-------------> If fi is the fitness of individual i in the population, its probability of being selected is
---------------> pi = fi / Summation of j from 1 to N (fj),
-------------> where N is the number of individuals in the population.
-----------> This could be imagined similar to a Roulette wheel in a casino. 
-------------> Usually a proportion of the wheel is assigned to each of the possible selections based on their fitness value. 
-------------> This could be achieved by dividing the fitness of a selection by the total fitness of all the selections, thereby normalizing them to 1. 
-------------> Then a random selection is made similar to how the roulette wheel is rotated. 

---------> Stochastic universal sampling
-----------> Stochastic universal sampling (SUS) is a technique used in genetic algorithms for selecting potentially useful solutions for recombination. 
-----------> SUS is a development of fitness proportionate selection (FPS) which exhibits no bias and minimal spread. 
-------------> Where FPS chooses several solutions from the population by repeated random sampling, 
-------------> SUS uses a single random value to sample all of the solutions by choosing them at evenly spaced intervals. 
-------------> This gives weaker members of the population (according to their fitness) a chance to be chosen.
-----------> FPS can have bad performance when a member of the population has a really large fitness in comparison with other members. 
-------------> Using a comb-like ruler, SUS starts from a small random number, and chooses the next candidates from the rest of population remaining, 
-------------> not allowing the fittest members to saturate the candidate space.
-----------> Described as an algorithm, pseudocode for SUS looks like:
-------------> SUS(Population, N)
------------->     F := total fitness of Population
------------->     N := number of offspring to keep
------------->     P := distance between the pointers (F/N)
------------->     Start := random number between 0 and P
------------->     Pointers := [Start + i*P | i in [0..(N-1)]]
------------->     return RWS(Population,Pointers)
-------------> RWS(Population, Points)
------------->     Keep = []
------------->     for P in Points
------------->         I := 0
------------->         while fitness sum of Population[0..I] < P
------------->             I++
------------->         add Population[I] to Keep
------------->     return Keep
-----------> Where Population[0..I] is the set of individuals with array-index 0 to (and including) I.
-----------> Here RWS() describes the bulk of fitness proportionate selection (also known as "roulette wheel selection") –
-------------> in true fitness proportional selection the parameter Points is always a (sorted) list of random numbers from 0 to F. 
-------------> The algorithm above is intended to be illustrative rather than canonical. 
-----------> It was introduced by James Baker.

---------> Truncation selection
-----------> In animal and plant breeding, truncation selection is a standard method in 
-------------> selective breeding in selecting animals to be bred for the next generation. 
-------------> Animals are ranked by their phenotypic value on some trait such as milk production, and the top percentage is reproduced. 
-------------> The effects of truncation selection for a continuous trait can be modeled 
-------------> by the standard breeder's equation by using heritability and truncated normal distributions. 
-------------> On a binary trait, it can be modeled easily using the liability threshold model. 
-------------> It is considered an easy and efficient method of breeding.

---------> Tournament selection
-----------> Tournament selection is a method of selecting an individual from a population of individuals in a genetic algorithm.
-------------> Tournament selection involves running several "tournaments" among a few individuals (or "chromosomes") chosen at random from the population. 
-------------> The winner of each tournament (the one with the best fitness) is selected for crossover. 
-------------> Selection pressure, a probabilistic measure of a chromosome's likelihood of participation in the tournament based on the participant selection pool size, 
-------------> is easily adjusted by changing the tournament size, the reason is that if the tournament size is larger, 
-------------> weak individuals have a smaller chance to be selected, because, if a weak individual is selected to be in a tournament, 
-------------> there is a higher probability that a stronger individual is also in that tournament.
-----------> The tournament selection method may be described in pseudo code:
-------------> choose k (the tournament size) individuals from the population at random
-------------> choose the best individual from the tournament with probability p
-------------> choose the second best individual with probability p*(1-p)
-------------> choose the third best individual with probability p*((1-p)^2)
-------------> and so on
-----------> Deterministic tournament selection selects the best individual (when p = 1) in any tournament. 
-------------> A 1-way tournament (k = 1) selection is equivalent to random selection. 
-------------> There are two variants of the selection: with and without replacement. 
-------------> The variant without replacement guarantees that when selecting N individuals from a population of N elements, 
-------------> each individual participates in exactly k tournaments. 
-------------> An algorithm is proposed in.
-------------> Note that depending on the number of elements selected, 
-------------> selection without replacement does not guarantee that no individual is selected more than once. 
-------------> It just guarantees that each individual has an equal chance of participating in the same number of tournaments.
-----------> In comparison with the (stochastic) fitness proportionate selection method, 
-------------> tournament selection is often implemented in practice due to its lack of stochastic noise.
-----------> Tournament selection has several benefits over alternative selection methods for genetic algorithms 
-------------> (for example, fitness proportionate selection and reward-based selection): 
-------------> it is efficient to code, works on parallel architectures and allows the selection pressure to be easily adjusted.
-------------> Tournament selection has also been shown to be independent of the scaling of the genetic algorithm 
-------------> fitness function (or 'objective function') in some classifier systems.

-------> Memetic algorithm
---------> A memetic algorithm (MA) in computer science and operations research, is an extension of the traditional genetic algorithm. 
-----------> It may provide a sufficiently good solution to an optimization problem. 
-----------> It uses a local search technique to reduce the likelihood of premature convergence.
---------> Memetic algorithms represent one of the recent growing areas of research in evolutionary computation. 
-----------> The term MA is now widely used as a synergy of evolutionary or any population-based approach 
-----------> with separate individual learning or local improvement procedures for problem search. 
-----------> Quite often, MAs are also referred to in the literature as Baldwinian evolutionary algorithms (EAs), Lamarckian EAs, cultural algorithms, or genetic local search. 

-------> Swarm intelligence
---------> Swarm intelligence (SI) is the collective behavior of decentralized, self-organized systems, natural or artificial. 
-----------> The concept is employed in work on artificial intelligence. 
-----------> The expression was introduced by Gerardo Beni and Jing Wang in 1989, in the context of cellular robotic systems.
---------> SI systems consist typically of a population of simple agents or boids interacting locally with one another and with their environment.
-----------> The inspiration often comes from nature, especially biological systems. 
-----------> The agents follow very simple rules, and although there is no centralized control structure dictating how individual agents should behave, local, 
-----------> and to a certain degree random, interactions between such agents lead to the emergence of "intelligent" global behavior, unknown to the individual agents.
-----------> Examples of swarm intelligence in natural systems include ant colonies, bee colonies, bird flocking, hawks hunting, animal herding, bacterial growth, fish schooling and microbial intelligence.
---------> The application of swarm principles to robots is called swarm robotics while swarm intelligence refers to the more general set of algorithms. 
-----------> Swarm prediction has been used in the context of forecasting problems. 
-----------> Similar approaches to those proposed for swarm robotics are considered for genetically modified organisms in synthetic collective intelligence.

---------> Ant colony optimization
-----------> The ant colony optimization algorithm (ACO) is a probabilistic technique for solving computational problems 
-------------> which can be reduced to finding good paths through graphs. 
-------------> Artificial ants stand for multi-agent methods inspired by the behavior of real ants. 
-------------> The pheromone-based communication of biological ants is often the predominant paradigm used. 
-------------> Combinations of artificial ants and local search algorithms have become a method of choice 
-------------> for numerous optimization tasks involving some sort of graph, e.g., vehicle routing and internet routing. 

---------> Bees algorithm: 
-----------> This a search algorithm which mimics the food foraging behavior of swarms of honey bees
-----------> The bees algorithm is a population-based search algorithm which was developed by Pham, Ghanbarzadeh et al. in 2005. 
-------------> It mimics the food foraging behaviour of honey bee colonies. 
-------------> In its basic version the algorithm performs a kind of neighbourhood search combined with global search, 
-------------> and can be used for both combinatorial optimization and continuous optimization. 
-------------> only condition for the application of the bees algorithm is that some measure of distance between the solutions is defined. 
-------------> The effectiveness and specific abilities of the bees algorithm have been proven in a number of studies.

---------> Particle swarm
-----------> In computational science, particle swarm optimization is a computational method that optimizes a problem 
-------------> by iteratively trying to improve a candidate solution with regard to a given measure of quality. 
-------------> It solves a problem by having a population of candidate solutions, here dubbed particles, 
-------------> and moving these particles around in the search-space according to simple mathematical formula over the particle's position and velocity. 
-------------> Each particle's movement is influenced by its local best known position, 
-------------> but is also guided toward the best known positions in the search-space, which are updated as better positions are found by other particles. 
-------------> This is expected to move the swarm toward the best solutions. 

-----> Frank-Wolfe algorithm: 
---------> This is an iterative first-order optimization algorithm for constrained convex optimization.
---------> The Frank–Wolfe algorithm is an iterative first-order optimization algorithm for constrained convex optimization. 
-----------> Also known as the conditional gradient method,[1] reduced gradient algorithm and the convex combination algorith
-----------> m, the method was originally proposed by Marguerite Frank and Philip Wolfe in 1956.[2] 
-----------> In each iteration, the Frank–Wolfe algorithm considers a linear approximation of the objective function, 
-----------> and moves towards a minimizer of this linear function (taken over the same domain). 

-----> Golden-section search: 
-------> This is an algorithm for finding the maximum of a real function
-------> The golden-section search is a technique for finding an extremum (minimum or maximum) of a function inside a specified interval. 
---------> For a strictly unimodal function with an extremum inside the interval, it will find that extremum, 
---------> while for an interval containing multiple extrema (possibly including the interval boundaries), it will converge to one of them. 
---------> If the only extremum on the interval is on a boundary of the interval, it will converge to that boundary point. 
---------> The method operates by successively narrowing the range of values on the specified interval, which makes it relatively slow, but very robust. 
---------> The technique derives its name from the fact that the algorithm maintains 
---------> the function values for four points whose three interval widths are in the ratio φ:1:φ where φ is the golden ratio. 
---------> These ratios are maintained for each iteration and are maximally efficient. 
---------> Excepting boundary points, when searching for a minimum, the central point is always less than or equal to the outer points, 
---------> assuring that a minimum is contained between the outer points. The converse is true when searching for a maximum. 
---------> The algorithm is the limit of Fibonacci search (also described below) for many function evaluations. 
---------> Fibonacci search and golden-section search were discovered by Kiefer (1953) (see also Avriel and Wilde (1966)). 

-----> Gradient descent
-------> In mathematics, gradient descent (also often called steepest descent) is a first-order iterative optimization algorithm 
---------> for finding a local minimum of a differentiable function. 
---------> The idea is to take repeated steps in the opposite direction of the gradient (or approximate gradient) 
---------> of the function at the current point, because this is the direction of steepest descent. 
---------> Conversely, stepping in the direction of the gradient will lead to a local maximum of that function; the procedure is then known as gradient ascent. 

-----> Grid Search
-------> The traditional way of performing hyperparameter optimization has been grid search, or a parameter sweep, 
---------> which is simply an exhaustive searching through a manually specified subset of the hyperparameter space of a learning algorithm. 
---------> A grid search algorithm must be guided by some performance metric, 
---------> typically measured by cross-validation on the training set[4] or evaluation on a hold-out validation set.
-------> Since the parameter space of a machine learner may include real-valued or unbounded value spaces for certain parameters, 
---------> manually set bounds and discretization may be necessary before applying grid search.
-------> For example, a typical soft-margin SVM classifier equipped with an RBF kernel has at least two hyperparameters 
---------> that need to be tuned for good performance on unseen data: a regularization constant C and a kernel hyperparameter γ. 
---------> Both parameters are continuous, so to perform grid search, one selects a finite set of "reasonable" values for each, say
-----------> C is an element { 10 , 100 , 1000 }
-----------> γ is an element { 0.1 , 0.2 , 0.5 , 1.0 }
-------> Grid search then trains an SVM with each pair (C, γ) in the Cartesian product of these two sets and evaluates their performance on a held-out validation set 
---------> (or by internal cross-validation on the training set, in which case multiple SVMs are trained per pair). 
---------> Finally, the grid search algorithm outputs the settings that achieved the highest score in the validation procedure.
-------> Grid search suffers from the curse of dimensionality, 
---------> but is often embarrassingly parallel because the hyperparameter settings it evaluates are typically independent of each other.

-----> Harmony search (HS): 
-------> This is a metaheuristic algorithm mimicking the improvisation process of musicians
-------> In the HS algorithm, a set of possible solutions is randomly generated (called Harmony memory). 
---------> A new solution is generated by using all the solutions in the Harmony memory (rather than just two as used in GA) 
---------> and if this new solution is better than the worst solution in Harmony memory, the worst solution gets replaced by this new solution. 
---------> The effectiveness and advantages of HS have been demonstrated in various applications like design of municipal water distribution networks, 
---------> structural design, load dispatch problem in electrical engineering, multi-objective optimization, rostering problems, clustering,
---------> and classification and feature selection.
---------> A detailed survey on applications of HS can be found i and applications of HS in data mining can be found in.
-------> Dennis (2015) claimed that harmony search is a special case of the evolution strategies algorithm.
---------> However, Saka et al. (2016) argues that the structure of evolution strategies is different from that of harmony search.
-------> Note: Harmony search is a phenomenon-mimicking metaheuristic introduced in 2001 by Zong Woo Geem, Joong Hoon Kim, and G. V. Loganathan
---------> and is inspired by the improvization process of jazz musicians. 

-----> Interior point method
-------> Interior-point methods (also referred to as barrier methods or IPMs) are a certain class of algorithms that solve linear and nonlinear convex optimization problems.
-------> It enabled solutions of linear programming problems that were beyond the capabilities of the simplex method. 
---------> Contrary to the simplex method, it reaches a best solution by traversing the interior of the feasible region. 
---------> The method can be generalized to convex programming based on a self-concordant barrier function used to encode the convex set. 
-------> An interior point method was discovered by Soviet mathematician I. I. Dikin in 1967 and reinvented in the U.S. in the mid-1980s. 
---------> In 1984, Narendra Karmarkar developed a method for linear programming called Karmarkar's algorithm, 
---------> which runs in provably polynomial time and is also very efficient in practice.

-----> Linear programming
-------> Linear programming (LP), also called linear optimization, is a method to achieve the best outcome (such as maximum profit or lowest cost) 
---------> in a mathematical model whose requirements are represented by linear relationships. 
---------> Linear programming is a special case of mathematical programming (also known as mathematical optimization).
-------> More formally, linear programming is a technique for the optimization of a linear objective function, 
---------> subject to linear equality and linear inequality constraints. 
---------> Its feasible region is a convex polytope, which is a set defined as the intersection of finitely many half spaces, 
---------> each of which is defined by a linear inequality. 
---------> Its objective function is a real-valued affine (linear) function defined on this polyhedron. 
---------> A linear programming algorithm finds a point in the polytope where this function has the smallest (or largest) value if such a point exists.
-------> Linear programs are problems that can be expressed in canonical form as
---------> Find a vector x that maximizes (c^T)*x subject to A*x ≤ b and x ≥ 0
 
-------> Benson's algorithm: 
---------> This is an algorithm for solving linear vector optimization problems
---------> Benson's algorithm, named after Harold Benson, is a method for solving multi-objective linear programming problems and vector linear programs. 
-----------> This works by finding the "efficient extreme points in the outcome set".
-----------> The primary concept in Benson's algorithm is to evaluate the upper image of the vector optimization problem by cutting planes.

-------> Dantzig–Wolfe decomposition: 
---------> This is an algorithm for solving linear programming problems with special structure
---------> Dantzig–Wolfe decomposition is an algorithm for solving linear programming problems with special structure. 
-----------> It was originally developed by George Dantzig and Philip Wolfe and initially published in 1960.
-----------> Many texts on linear programming have sections dedicated to discussing this decomposition algorithm.
---------> Dantzig–Wolfe decomposition relies on delayed column generation for improving the tractability of large-scale linear programs. 
-----------> For most linear programs solved via the revised simplex algorithm, at each step, most columns (variables) are not in the basis. 
-----------> In such a scheme, a master problem containing at least the currently active columns (the basis) uses a subproblem or subproblems 
-----------> to generate columns for entry into the basis such that their inclusion improves the objective function. 

-------> Delayed column generation
---------> Column generation or delayed column generation is an efficient algorithm for solving large linear programs.
---------> The overarching idea is that many linear programs are too large to consider all the variables explicitly. 
-----------> The idea is thus to start by solving the considered program with only a subset of its variables. 
-----------> Then iteratively, variables that have the potential to improve the objective function are added to the program. 
-----------> Once it is possible to demonstrate that adding new variables would no longer improve the value of the objective function, the procedure stops. 
-----------> The hope when applying a column generation algorithm is that only a very small fraction of the variables will be generated. 
-----------> This hope is supported by the fact that in the optimal solution, most variables will be non-basic
-----------> and assume a value of zero, so the optimal solution can be found without them.
---------> In many cases, this method allows to solve large linear programs that would otherwise be intractable. 
-----------> The classical example of a problem where it is successfully used is the cutting stock problem. 
-----------> One particular technique in linear programming which uses this kind of approach is the Dantzig–Wolfe decomposition algorithm. 
-----------> Additionally, column generation has been applied to many problems such as crew scheduling, vehicle routing, and the capacitated p-median problem.
---------> Algorithm
-----------> The algorithm considers two problems: the master problem and the subproblem. 
-----------> The master problem is the original problem with only a subset of variables being considered. 
-----------> The subproblem is a new problem created to identify an improving variable (i.e. which can improve the objective function of the master problem).
-----------> The algorithm then proceeds as follow:
-------------> (1) Initialise the master problem and the subproblem
-------------> (2) Solve the master problem
-------------> (3) Search for an improving variable with the subproblem
-------------> (4) If an improving variable is found: add it to the master problem then go to step 2
-------------> (5) Else: The solution of the master problem is optimal. Stop.

-------> Integer linear programming: 
---------> This solves linear programming problems where some or all the unknowns are restricted to integer values.
---------> An integer programming problem is a mathematical optimization 
-----------> or feasibility program in which some or all of the variables are restricted to be integers. 
-----------> In many settings the term refers to integer linear programming (ILP), 
-----------> in which the objective function and the constraints (other than the integer constraints) are linear.
---------> Integer programming is NP-complete. In particular, the special case of 0-1 integer linear programming,
-----------> in which unknowns are binary, and only the restrictions must be satisfied, is one of Karp's 21 NP-complete problems.
---------> If some decision variables are not discrete, the problem is known as a mixed-integer programming problem.

---------> Branch and cut
-----------> Branch and cut[1] is a method of combinatorial optimization for solving integer linear programs (ILPs), 
-------------> that is, linear programming (LP) problems where some or all the unknowns are restricted to integer values.
-------------> Branch and cut involves running a branch and bound algorithm and using cutting planes to tighten the linear programming relaxations. 
-------------> Note that if cuts are only used to tighten the initial LP relaxation, the algorithm is called cut and branch. 

---------> Cutting-plane method
-----------> In mathematical optimization, the cutting-plane method is any of a variety of optimization methods that iteratively 
-------------> refine a feasible set or objective function by means of linear inequalities, termed cuts. 
-------------> Such procedures are commonly used to find integer solutions to mixed integer linear programming (MILP) problems, 
-------------> as well as to solve general, not necessarily differentiable convex optimization problems.
-------------> The use of cutting planes to solve MILP was introduced by Ralph E. Gomory.
-----------> Cutting plane methods for MILP work by solving a non-integer linear program, 
-------------> the linear relaxation of the given integer program. 
-------------> The theory of Linear Programming dictates that under mild assumptions 
-------------> (if the linear program has an optimal solution, and if the feasible region does not contain a line), 
-------------> one can always find an extreme point or a corner point that is optimal. 
-------------> The obtained optimum is tested for being an integer solution. 
-------------> If it is not, there is guaranteed to exist a linear inequality that
------------->  separates the optimum from the convex hull of the true feasible set. 
-------------> Finding such an inequality is the separation problem, and such an inequality is a cut. 
-------------> A cut can be added to the relaxed linear program. 
-------------> Then, the current non-integer solution is no longer feasible to the relaxation.
-------------> This process is repeated until an optimal integer solution is found. 

-------> Karmarkar's algorithm: The first reasonably efficient algorithm that solves the linear programming problem in polynomial time.
---------> Karmarkar's algorithm is an algorithm introduced by Narendra Karmarkar in 1984 for solving linear programming problems. 
---------> It was the first reasonably efficient algorithm that solves these problems in polynomial time. 
-----------> The ellipsoid method is also polynomial time but proved to be inefficient in practice.
---------> Denoting n as the number of variables and L as the number of bits of input to the algorithm, 
-----------> Karmarkar's algorithm requires O(n^(3.5)*L) operations on O(L) digit numbers, as compared to O(n^(6)*L) such operations for the ellipsoid algorithm. 
-----------> The runtime of Karmarkar's algorithm is thus
----------->     O(n^(3.5) * L^2 * log ⁡ L * log ⁡ log ⁡ L) 
-----------> using FFT-based multiplication (see Big O notation).
---------> Karmarkar's algorithm falls within the class of interior point methods: 
-----------> the current guess for the solution does not follow the boundary of the feasible set as in the simplex method, 
-----------> but it moves through the interior of the feasible region, improving the approximation of the optimal solution by a definite fraction with every iteration, 
-----------> and converging to an optimal solution with rational data.

-------> Simplex algorithm: An algorithm for solving linear programming problems
---------> In mathematical optimization, Dantzig's simplex algorithm (or simplex method) is a popular algorithm for linear programming.
---------> The name of the algorithm is derived from the concept of a simplex and was suggested by T. S. Motzkin.
-----------> Simplices are not actually used in the method, but one interpretation of it is that it operates on simplicial cones, 
-----------> and these become proper simplices with an additional constraint. 
-----------> The simplicial cones in question are the corners (i.e., the neighborhoods of the vertices) of a geometric object called a polytope. 
-----------> The shape of this polytope is defined by the constraints applied to the objective function. 

-----> Line search
-------> In optimization, the line search strategy is one of two basic iterative approaches to find a local minimum x∗ of an objective function f : R^n → R. 
---------> The other approach is trust region.
-------> The line search approach first finds a descent direction along which the objective function f will be reduced 
---------> and then computes a step size that determines how far x should move along that direction. 
---------> The descent direction can be computed by various methods, such as gradient descent or quasi-Newton method. 
---------> The step size can be determined either exactly or inexactly. 

-----> Local search: 
-------> This a metaheuristic for solving computationally hard optimization problems
-------> In computer science, local search is a heuristic method for solving computationally hard optimization problems. 
---------> Local search can be used on problems that can be formulated as finding a solution maximizing a criterion among a number of candidate solutions. 
---------> Local search algorithms move from solution to solution in the space of candidate solutions (the search space) by applying local changes,
---------> until a solution deemed optimal is found or a time bound is elapsed.
-------> Local search algorithms are widely applied to numerous hard computational problems, 
---------> including problems from computer science (particularly artificial intelligence), mathematics, operations research, engineering, and bioinformatics. 
---------> Examples of local search algorithms are WalkSAT, the 2-opt algorithm for the Traveling Salesman Problem and the Metropolis–Hastings algorithm.[1] 

-------> Random-restart hill climbing
---------> In numerical analysis, hill climbing is a mathematical optimization technique which belongs to the family of local search. 
-----------> It is an iterative algorithm that starts with an arbitrary solution to a problem, 
-----------> then attempts to find a better solution by making an incremental change to the solution. 
-----------> If the change produces a better solution, another incremental change is made to the new solution, and so on until no further improvements can be found.
---------> For example, hill climbing can be applied to the travelling salesman problem. 
-----------> It is easy to find an initial solution that visits all the cities but will likely be very poor compared to the optimal solution. 
-----------> The algorithm starts with such a solution and makes small improvements to it, 
-----------> such as switching the order in which two cities are visited. 
-----------> Eventually, a much shorter route is likely to be obtained. 
---------> Hill climbing finds optimal solutions for convex problems – for other problems it will find only local optima 
-----------> (solutions that cannot be improved upon by any neighboring configurations), 
-----------> which are not necessarily the best possible solution (the global optimum) out of all possible solutions (the search space). 
-----------> Examples of algorithms that solve convex problems by hill-climbing include the simplex algorithm for linear programming and binary search.
-----------> To attempt to avoid getting stuck in local optima, one could use restarts (i.e. repeated local search), 
-----------> or more complex schemes based on iterations (like iterated local search), 
-----------> or on memory (like reactive search optimization and tabu search), 
-----------> or on memory-less stochastic modifications (like simulated annealing).
---------> The relative simplicity of the algorithm makes it a popular first choice amongst optimizing algorithms. 
-----------> It is used widely in artificial intelligence, for reaching a goal state from a starting node. 
-----------> Different choices for next nodes and starting nodes are used in related algorithms. 
-----------> Although more advanced algorithms such as simulated annealing or tabu search may give better results, in some situations hill climbing works just as well. 
-----------> Hill climbing can often produce a better result than other algorithms when the amount of time available to perform a search is limited, 
-----------> such as with real-time systems, so long as a small number of increments typically converges on a good solution (the optimal solution or a close approximation). 
-----------> At the other extreme, bubble sort can be viewed as a hill climbing algorithm (every adjacent element exchange decreases the number of disordered element pairs), 
-----------> yet this approach is far from efficient for even modest N, as the number of exchanges required grows quadratically.
---------> Hill climbing is an anytime algorithm: it can return a valid solution even if it's interrupted at any time before it ends. 

-------> Tabu search
---------> Tabu search is a metaheuristic search method employing local search methods used for mathematical optimization. 
---------> Local (neighborhood) searches take a potential solution to a problem and check its immediate neighbors 
-----------> (that is, solutions that are similar except for very few minor details) in the hope of finding an improved solution. 
-----------> Local search methods have a tendency to become stuck in suboptimal regions or on plateaus where many solutions are equally fit.
---------> Tabu search enhances the performance of local search by relaxing its basic rule. 
-----------> First, at each step worsening moves can be accepted if no improving move is available (like when the search is stuck at a strict local minimum). 
-----------> In addition, prohibitions (henceforth the term tabu) are introduced to discourage the search from coming back to previously-visited solutions.
---------> The implementation of tabu search uses memory structures that describe the visited solutions or user-provided sets of rules. 
-----------> If a potential solution has been previously visited within a certain short-term period or if it has violated a rule, 
-----------> it is marked as "tabu" (forbidden) so that the algorithm does not consider that possibility repeatedly. 
---------> Note: It was created by Fred W. Glover in 1986[1] and formalized in 1989.

-----> Minimax used in game programming
-------> A minimax algorithm is a recursive algorithm for choosing the next move in an n-player game, usually a two-player game. 
---------> A value is associated with each position or state of the game. 
---------> This value is computed by means of a position evaluation function and it indicates how good it would be for a player to reach that position. 
---------> The player then makes the move that maximizes the minimum value of the position resulting from the opponent's possible following moves. 
---------> If it is A's turn to move, A gives a value to each of their legal moves.
-------> A possible allocation method consists in assigning a certain win for A as +1 and for B as −1. 
---------> This leads to combinatorial game theory as developed by J.H. Conway. 
---------> An alternative is using a rule that if the result of a move is an immediate win for A it is assigned positive infinity and if it is an immediate win for B, negative infinity. 
---------> The value to A of any other move is the maximum of the values resulting from each of B's possible replies. 
---------> For this reason, A is called the maximizing player and B is called the minimizing player, hence the name minimax algorithm. 
---------> The above algorithm will assign a value of positive or negative infinity to any position 
---------> since the value of every position will be the value of some final winning or losing position. 
---------> Often this is generally only possible at the very end of complicated games such as chess or go, 
---------> since it is not computationally feasible to look ahead as far as the completion of the game, except towards the end, 
---------> and instead, positions are given finite values as estimates of the degree of belief that they will lead to a win for one player or another.
-------> This can be extended if we can supply a heuristic evaluation function which gives values t
---------> o non-final game states without considering all possible following complete sequences. 
---------> We can then limit the minimax algorithm to look only at a certain number of moves ahead. 
---------> This number is called the "look-ahead", measured in "plies". For example, the chess computer Deep Blue 
---------> (the first one to beat a reigning world champion, Garry Kasparov at that time) 
---------> looked ahead at least 12 plies, then applied a heuristic evaluation function.
-------> The algorithm can be thought of as exploring the nodes of a game tree. 
---------> The effective branching factor of the tree is the average number of children of each node 
---------> (i.e., the average number of legal moves in a position). 
---------> The number of nodes to be explored usually increases exponentially with the number of plies 
---------> (it is less than exponential if evaluating forced moves or repeated positions). 
---------> The number of nodes to be explored for the analysis of a game is therefore 
---------> approximately the branching factor raised to the power of the number of plies.
---------> It is therefore impractical to completely analyze games such as chess using the minimax algorithm.
-------> The performance of the naïve minimax algorithm may be improved dramatically, 
---------> without affecting the result, by the use of alpha–beta pruning. 
---------> Other heuristic pruning methods can also be used, but not all of them are guaranteed to give the same result as the unpruned search.
-------> A naïve minimax algorithm may be trivially modified to additionally return an entire Principal Variation along with a minimax score.
-------> Pseudocode
---------> The pseudocode for the depth-limited minimax algorithm is given below.
---------> function  minimax( node, depth, maximizingPlayer ) is
--------->     if depth = 0 or node is a terminal node then
--------->         return the heuristic value of node
--------->     if maximizingPlayer then
--------->         value := −∞
--------->         for each child of node do
--------->             value := max(value, minimax(child, depth − 1, FALSE))
--------->         return value
--------->     else (* minimizing player *)
--------->         value := +∞
--------->         for each child of node do
--------->             value := min( value, minimax( child, depth − 1, TRUE ) )
--------->         return value
---------> (* Initial call *)
---------> minimax( origin, depth, TRUE )
-------> The minimax function returns a heuristic value for leaf nodes (terminal nodes and nodes at the maximum search depth). 
---------> Non-leaf nodes inherit their value from a descendant leaf node. 
---------> The heuristic value is a score measuring the favorability of the node for the maximizing player. 
---------> Hence nodes resulting in a favorable outcome, such as a win, 
---------> for the maximizing player have higher scores than nodes more favorable for the minimizing player. 
---------> The heuristic value for terminal (game ending) leaf nodes are scores corresponding to win, loss, or draw, for the maximizing player. 
---------> For non terminal leaf nodes at the maximum search depth, an evaluation function estimates a heuristic value for the node. 
---------> The quality of this estimate and the search depth determine the quality and accuracy of the final minimax result.
-------> Minimax treats the two players (the maximizing player and the minimizing player) separately in its code. 
---------> Based on the observation that max(a, b) = −min(−a, −b), minimax may often be simplified into the negamax algorithm. 

-----> Nearest neighbor search (NNS): 
-------> This find closest points in a metric space
---------> Nearest neighbor search (NNS), as a form of proximity search, is the optimization problem of finding the point in a given set that is closest (or most similar) to a given point. 
---------> Closeness is typically expressed in terms of a dissimilarity function: the less similar the objects, the larger the function values.
-------> Formally, the nearest-neighbor (NN) search problem is defined as follows: given a set S of points in a space M and a query point q ∈ M, find the closest point in S to q. 
---------> Donald Knuth in vol. 3 of The Art of Computer Programming (1973) called it the post-office problem, referring to an application of assigning to a residence the nearest post office. 
---------> A direct generalization of this problem is a k-NN search, where we need to find the k closest points.
-------> Most commonly M is a metric space and dissimilarity is expressed as a distance metric, which is symmetric and satisfies the triangle inequality. 
---------> Even more common, M is taken to be the d-dimensional vector space where dissimilarity is measured using the Euclidean distance, 
---------> Manhattan distance or other distance metric. However, the dissimilarity function can be arbitrary. 
---------> One example is asymmetric Bregman divergence, for which the triangle inequality does not hold.

-------> Best Bin First: 
---------> This find an approximate solution to the nearest neighbor search problem in very-high-dimensional spaces
---------> Best bin first is a search algorithm that is designed to efficiently find an approximate solution
-----------> to the nearest neighbor search problem in very-high-dimensional spaces. 
-----------> The algorithm is based on a variant of the kd-tree search algorithm which makes indexing higher-dimensional spaces possible.
-----------> Best bin first is an approximate algorithm which returns the nearest neighbor for a large fraction of queries and a very close neighbor otherwise.
 
-----> Newton's method in optimization
-------> In calculus, Newton's method is an iterative method for finding the roots of a differentiable function F, 
---------> which are solutions to the equation F (x) = 0. 
---------> As such, Newton's method can be applied to the derivative f ′ of a twice-differentiable function f 
---------> to find the roots of the derivative (solutions to f ′(x) = 0), also known as the critical points of f. 
---------> These solutions may be minima, maxima, or saddle points; see section "Several variables" in Critical point (mathematics) 
---------> and also section "Geometric interpretation" in this article.
---------> This is relevant in optimization, which aims to find (global) minima of the function f. 

-----> Nonlinear optimization
-------> In mathematics, nonlinear programming (NLP) is the process of solving an optimization problem where some of the constraints or the objective function are nonlinear. 
---------> An optimization problem is one of calculation of the extrema (maxima, minima or stationary points) of an objective function 
---------> over a set of unknown real variables and conditional to the satisfaction of a system of equalities and inequalities, collectively termed constraints. 
---------> It is the sub-field of mathematical optimization that deals with problems that are not linear.

-------> BFGS method: 
---------> A nonlinear optimization algorithm
---------> In numerical optimization, the Broyden–Fletcher–Goldfarb–Shanno (BFGS) algorithm is an iterative method for solving unconstrained nonlinear optimization problems. 
-----------> Like the related Davidon–Fletcher–Powell method, BFGS determines the descent direction by preconditioning the gradient with curvature information. 
-----------> It does so by gradually improving an approximation to the Hessian matrix of the loss function, 
-----------> obtained only from gradient evaluations (or approximate gradient evaluations) via a generalized secant method.
---------> Since the updates of the BFGS curvature matrix do not require matrix inversion, 
-----------> its computational complexity is only O(n^2), compared to O(n^3) in Newton's method. 
-----------> Also in common use is L-BFGS, which is a limited-memory version of BFGS that is particularly suited to problems with very large numbers of variables (e.g., >1000). 
-----------> The BFGS-B variant handles simple box constraints.[3]
---------> The algorithm is named after Charles George Broyden, Roger Fletcher, Donald Goldfarb and David Shanno.

-------> Gauss–Newton algorithm: 
---------> An algorithm for solving nonlinear least squares problems.
-------> Levenberg–Marquardt algorithm: 
---------> An algorithm for solving nonlinear least squares problems.
-------> Nelder–Mead method (downhill simplex method): 
---------> A nonlinear optimization algorithm

-----> Odds algorithm (Bruss algorithm): 
-------> This finds the optimal strategy to predict a last specific event in a random sequence event
-------> The Gauss–Newton algorithm is used to solve non-linear least squares problems, 
---------> which is equivalent to minimizing a sum of squared function values. 
---------> It is an extension of Newton's method for finding a minimum of a non-linear function. Since a sum of squares must be nonnegative, 
---------> the algorithm can be viewed as using Newton's method to iteratively approximate zeroes of the sum, and thus minimizing the sum. 
---------> It has the advantage that second derivatives, which can be challenging to compute, are not required.
-------> Non-linear least squares problems arise, for instance, in non-linear regression, 
---------> where parameters in a model are sought such that the model is in good agreement with available observations.
-------> The method is named after the mathematicians Carl Friedrich Gauss and Isaac Newton, 
---------> and first appeared in Gauss' 1809 work Theoria motus corporum coelestium in sectionibus conicis solem ambientum. 

-----> Random Search
-------> Random Search replaces the exhaustive enumeration of all combinations by selecting them randomly. 
---------> This can be simply applied to the discrete setting described above, but also generalizes to continuous and mixed spaces. 
---------> It can outperform Grid search, especially when only a small number of hyperparameters affects the final performance of the machine learning algorithm.
---------> In this case, the optimization problem is said to have a low intrinsic dimensionality.
---------> Random Search is also embarrassingly parallel, and additionally allows the inclusion of prior knowledge by specifying the distribution from which to sample. 
---------> Despite its simplicity, random search remains one of the important base-lines against which to compare the performance of new hyperparameter optimization methods. 

-----> Simulated annealing
-------> Simulated annealing (SA) is a probabilistic technique for approximating the global optimum of a given function. 
---------> Specifically, it is a metaheuristic to approximate global optimization in a large search space for an optimization problem. 
---------> It is often used when the search space is discrete (for example the traveling salesman problem, 
---------> the boolean satisfiability problem, protein structure prediction, and job-shop scheduling). 
---------> For problems where finding an approximate global optimum is more important than finding a precise local optimum in a fixed amount of time, 
---------> simulated annealing may be preferable to exact algorithms such as gradient descent or branch and bound.
-------> The name of the algorithm comes from annealing in metallurgy, 
---------> a technique involving heating and controlled cooling of a material to alter its physical properties. 
---------> Both are attributes of the material that depend on their thermodynamic free energy. 
---------> Heating and cooling the material affects both the temperature and the thermodynamic free energy or Gibbs energy. 
---------> Simulated annealing can be used for very hard computational optimization problems where exact algorithms fail; 
---------> even though it usually achieves an approximate solution to the global minimum, it could be enough for many practical problems.
-------> The problems solved by SA are currently formulated by an objective function of many variables, subject to several constraints. 
---------> In practice, the constraint can be penalized as part of the objective function. 

-----> Stochastic tunneling
-------> In numerical analysis, stochastic tunneling (STUN) is an approach to global optimization based on the Monte Carlo method
---------> sampling of the function to be objective minimized in which the function is nonlinearly transformed 
---------> to allow for easier tunneling among regions containing function minima. 
---------> Easier tunneling allows for faster exploration of sample space and faster convergence to a good solution. 

-----> Subset sum algorithm
-------> The subset sum problem (SSP) is a decision problem in computer science. 
---------> In its most general formulation, there is a multiset S of integers and a target-sum T, 
---------> and the question is to decide whether any subset of the integers sum to precisely T. 
---------> The problem is known to be NP. Moreover, some restricted variants of it are NP-complete too, for example:
-----------> The variant in which all inputs are positive.
-----------> The variant in which inputs may be positive or negative, and T=0. 
-------------> For example, given the set {−7, −3, −2, 9000, 5, 8}, the answer is yes because the subset {−3, −2, 5} sums to zero.
-----------> The variant in which all inputs are positive, and the target sum is exactly half the sum of all inputs, i.e., T = 1/2 * (a1 + ... + an). 
---------> This special case of SSP is known as the partition problem.
---------> SSP can also be regarded as an optimization problem: find a subset whose sum is at most T, and subject to that, as close as possible to T. 
-----------> It is NP-hard, but there are several algorithms that can solve it reasonably quickly in practice.
---------> SSP is a special case of the knapsack problem and of the multiple subset sum problem. 



-> Computational science

---> Astronomy

-----> Doomsday algorithm: 
-------> This algorithm is for day of the week
-------> The Doomsday rule, Doomsday algorithm or Doomsday method is an algorithm of determination of the day of the week for a given date. 
---------> It provides a perpetual calendar because the Gregorian calendar moves in cycles of 400 years. 
---------> The algorithm for mental calculation was devised by John Conway in 1973, 
---------> drawing inspiration from Lewis Carroll's perpetual calendar algorithm.
---------> It takes advantage of each year having a certain day of the week upon which certain easy-to-remember dates, called the doomsdays, fall; 
---------> for example, the last day of February, 4/4, 6/6, 8/8, 10/10, and 12/12 all occur on the same day of the week in any year. 
---------> Applying the Doomsday algorithm involves three steps: Determination of the anchor day for the century, 
---------> calculation of the anchor day for the year from the one for the century, 
---------> and selection of the closest date out of those that always fall on the doomsday, e.g., 4/4 and 6/6, 
---------> and count of the number of days (modulo 7) between that date and the date in question to arrive at the day of the week. 
---------> The technique applies to both the Gregorian calendar and the Julian calendar, although their doomsdays are usually different days of the week.
-------> The algorithm is simple enough that it can be computed mentally. 
---------> Conway could usually give the correct answer in under two seconds. 
---------> To improve his speed, he practiced his calendrical calculations on his computer, 
---------> which was programmed to quiz him with random dates every time he logged on.

-----> Zeller's congruence is an algorithm to calculate the day of the week for any Julian or Gregorian calendar date
-------> Zeller's congruence is an algorithm devised by Christian Zeller in the 19th century 
---------> to calculate the day of the week for any Julian or Gregorian calendar date. 
---------> It can be considered to be based on the conversion between Julian day and the calendar date.
 
-----> various Easter algorithms are used to calculate the day of Easter

---> Bioinformatics

-----> Basic Local Alignment Search Tool also known as BLAST: 
-------> This an algorithm for comparing primary biological sequence information
---------> The Kabsch algorithm, named after Wolfgang Kabsch, 
---------> is a method for calculating the optimal rotation matrix that minimizes the RMSD (root mean squared deviation) between two paired sets of points. 
---------> It is useful in graphics, cheminformatics to compare molecular structures, 
---------> and also bioinformatics for comparing protein structures (in particular, see root-mean-square deviation (bioinformatics)).
-------> The algorithm only computes the rotation matrix, but it also requires the computation of a translation vector. 
---------> When both the translation and rotation are actually performed, 
---------> the algorithm is sometimes called partial Procrustes superimposition (see also orthogonal Procrustes problem).

-----> Kabsch algorithm: 
-------> This calculate the optimal alignment of two sets of points in order to compute the root mean squared deviation between two protein structures.
---------> Velvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. 
---------> This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions.
---------> Velvet has also been implemented in commercial packages, such as Sequencher, Geneious, MacVector and BioNumerics.

-----> Velvet: 
-------> This a set of algorithms manipulating de Bruijn graphs for genomic sequence assembly.
---------> Velvet is an algorithm package that has been designed to deal with de novo genome assembly and short read sequencing alignments. 
---------> This is achieved through the manipulation of de Bruijn graphs for genomic sequence assembly via the removal of errors and the simplification of repeated regions.
---------> Velvet has also been implemented in commercial packages, such as Sequencher, Geneious, MacVector and BioNumerics.

-----> Sorting by signed reversals: 
-------> This an algorithm for understanding genomic evolution.

-----> Maximum parsimony (phylogenetics): 
-------> This an algorithm for finding the simplest phylogenetic tree to explain a given character matrix.
---------> In phylogenetics, maximum parsimony is an optimality criterion under which the phylogenetic tree that minimizes the total number of character-state changes is to be preferred. 
---------> Under the maximum-parsimony criterion, the optimal tree will minimize the amount of homoplasy (i.e., convergent evolution, parallel evolution, and evolutionary reversals). 
---------> In other words, under this criterion, the shortest possible tree that explains the data is considered best. 
---------> Some of the basic ideas behind maximum parsimony were presented by James S. Farris in 1970 and Walter M. Fitch in 1971.

-----> UPGMA: 
-------> This a distance-based phylogenetic tree construction algorithm.
---------> UPGMA (unweighted pair group method with arithmetic mean) is a simple agglomerative (bottom-up) hierarchical clustering method. 
---------> The method is generally attributed to Sokal and Michener.[1]
-------> The UPGMA method is similar to its weighted variant, the WPGMA method.
-------> Note that the unweighted term indicates that all distances contribute equally to each average that is computed and does not refer to the math by which it is achieved. 
---------> Thus the simple averaging in WPGMA produces a weighted result and the proportional averaging in UPGMA produces an unweighted result (see the working example).

---> Geoscience

-----> Vincenty's formulae: a fast algorithm to calculate the distance between two latitude/longitude points on an ellipsoid
-------> Vincenty's formulae are two related iterative methods used in geodesy to calculate 
---------> the distance between two points on the surface of a spheroid, developed by Thaddeus Vincenty (1975a). 
---------> They are based on the assumption that the figure of the Earth is an oblate spheroid, 
---------> and hence are more accurate than methods that assume a spherical Earth, such as great-circle distance.
-------> The first (direct) method computes the location of a point that is a given distance and azimuth (direction) from another point. 
---------> The second (inverse) method computes the geographical distance and azimuth between two given points. 
---------> They have been widely used in geodesy because they are accurate to within 0.5 mm (0.020 in) on the Earth ellipsoid. 

-----> Geohash: a public domain algorithm that encodes a decimal latitude/longitude pair as a hash string
-------> Geohash is a public domain geocode system invented in 2008 by Gustavo Niemeyer 
---------> which encodes a geographic location into a short string of letters and digits. 
---------> Similar ideas were introduced by G.M. Morton in 1966.
---------> It is a hierarchical spatial data structure which subdivides space into buckets of grid shape, 
---------> which is one of the many applications of what is known as a Z-order curve, and generally space-filling curves.
-------> Geohashes offer properties like arbitrary precision and the possibility of gradually removing characters 
---------> from the end of the code to reduce its size (and gradually lose precision). 
---------> Geohashing guarantees that the longer a shared prefix between two geohashes is, the spatially closer they are together. 
---------> The reverse of this is not guaranteed, as two points can be very close but have a short or no shared prefix. 

---> Linguistics

-----> Lesk algorithm: 
-------> This is for word sense disambiguation
-------> The Lesk algorithm is based on the assumption that words in a given "neighborhood" (section of text) will tend to share a common topic. 
---------> A simplified version of the Lesk algorithm is to compare the dictionary definition of an ambiguous word with the terms contained in its neighborhood. 
---------> Versions have been adapted to use WordNet.[2] An implementation might look like this:
-----------> for every sense of the word being disambiguated one should count the number of words that are in both the neighborhood of that word and in the dictionary definition of that sense
-----------> the sense that is to be chosen is the sense that has the largest number of this count.
-------> A frequently used example illustrating this algorithm is for the context "pine cone". The following dictionary definitions are used:
---------> PINE 
-----------> 1. kinds of evergreen tree with needle-shaped leaves
-----------> 2. waste away through sorrow or illness
---------> CONE 
-----------> 1. solid body which narrows to a point
-----------> 2. something of this shape whether solid or hollow
-----------> 3. fruit of certain evergreen trees
-------> As can be seen, the best intersection is Pine #1 ⋂ Cone #3 = 2. 
-------> Note: The Lesk algorithm is a classical algorithm for word sense disambiguation introduced by Michael E. Lesk in 1986.

-----> Stemming algorithm: 
-------> This is a method of reducing words to their stem, base, or root form
---------> In linguistic morphology and information retrieval, stemming is the process of reducing inflected 
-----------> (or sometimes derived) words to their word stem, base or root form—generally a written word form. 
-----------> The stem need not be identical to the morphological root of the word; it is usually sufficient that related words map to the same stem, 
-----------> even if this stem is not in itself a valid root. 
-----------> Algorithms for stemming have been studied in computer science since the 1960s. 
-----------> Many search engines treat words with the same stem as synonyms as a kind of query expansion, a process called conflation.
---------> A computer program or subroutine that stems word may be called a stemming program, stemming algorithm, or stemmer.

-----> Sukhotin's algorithm:
-------> This is a statistical classification algorithm for classifying characters in a text as vowels or consonants
-------> Computational linguistics is an interdisciplinary field concerned with the computational modelling of natural language, 
---------> as well as the study of appropriate computational approaches to linguistic questions. 
---------> In general, computational linguistics draws upon linguistics, computer science, artificial intelligence, 
---------> mathematics, logic, philosophy, cognitive science, cognitive psychology, psycholinguistics, anthropology and neuroscience, among others. 

---> Medicine

-----> ESC algorithm for the diagnosis of heart failure
-------> The ESC algorithm weights these parameters in establishing the diagnosis of heart failure:
----------------------------------------------------------------------------------------------
| Diagnostic assessments supporting the presence of heart failure 
----------------------------------------------------------------------------------------------
|                            | Diagnosis of heart failure                        |
| Assessment                 | Supports if present | Opposes if normal or absent |
----------------------------------------------------------------------------------------------
| Compatible symptoms        | ++                  | ++                          |
----------------------------------------------------------------------------------------------
| Compatible signs           | ++                  | +                           |
----------------------------------------------------------------------------------------------
| Cardiac dysfunction        | +++                 | +++                         |
| on echocardiography        |                     |                             |
----------------------------------------------------------------------------------------------
| Response of symptoms       | +++                 | ++                          |
| or signs to therapy        |                     |                             |                
----------------------------------------------------------------------------------------------
| ECG
----------------------------------------------------------------------------------------------
| Normal                     | ++                  |                             |
| Abnormal                   | ++                  | +                           |  
| Dysrhythmia                | +++                 | +                           |
----------------------------------------------------------------------------------------------
| Laboratory                 
----------------------------------------------------------------------------------------------
| Elevated BNP/NT-proBNP     | +++                 | +                           |
----------------------------------------------------------------------------------------------
| Low/normal BNP/NT-proBNP   | +                   | +++                         |
----------------------------------------------------------------------------------------------
| Low blood sodium           | +                   | +                           |
----------------------------------------------------------------------------------------------
| Kidney dysfunction         | +                   | +                           |
----------------------------------------------------------------------------------------------
| Mild elevations            | +                   | +                           |
| of troponin                |                     |                             | 
----------------------------------------------------------------------------------------------
| Chest X-ray
----------------------------------------------------------------------------------------------
| Pulmonary congestion      | +++                 | +                           |
----------------------------------------------------------------------------------------------
| Reduced exercise capacity | +++                 | ++                          |
----------------------------------------------------------------------------------------------
| Abnormal pulmonary        | +                   | +                           |
| function tests            |                     |                             | 
----------------------------------------------------------------------------------------------
| Abnormal hemodynamics     | +++                 | ++                          |
| at rest                   |                     |                             |  
----------------------------------------------------------------------------------------------
+ = some importance; ++ = intermediate importance; +++ = great importance. 
----------------------------------------------------------------------------------------------

-----> Manning Criteria for irritable bowel syndrome
-------> The Manning criteria are a diagnostic algorithm used in the diagnosis of irritable bowel syndrome (IBS). 
---------> The criteria consist of a list of questions the physician can ask the patient.
---------> The answers are used in a process to produce a diagnostic decision regarding whether the patient can be considered to have IBS.
-------> The Manning criteria have been compared with other diagnostic algorithms for IBS, 
---------> such as the Rome I criteria, the Rome II process, and the Kruis criteria.
---------> A 2013 validation study found the Manning criteria to have less sensitivity but more specificity than the Rome criteria.
-------> The threshold for a positive diagnosis varies from two to four of the Manning criteria below.
---------> Onset of pain linked to more frequent bowel movements
---------> Looser stools associated with onset of pain
---------> Pain relieved by passage of stool
---------> Noticeable abdominal bloating
---------> Sensation of incomplete evacuation more than 25% of the time
---------> Diarrhea with mucus more than 25% of the time

-----> Pulmonary embolism diagnostic algorithms
-------> Recommendations for a diagnostic algorithm were published by the PIOPED investigators; 
---------> however, these recommendations do not reflect research using 64 slice MDCT.[33] These investigators recommended:
-----------> Low clinical probability. 
-------------> If negative D-dimer, PE is excluded. 
-------------> If positive D-dimer, obtain MDCT and base treatment on results.
-----------> Moderate clinical probability. 
-------------> If negative D-dimer, PE is excluded. 
-------------> However, the authors were not concerned that a negative MDCT with negative D-dimer in this setting has a 5% probability of being false. 
-------------> Presumably, the 5% error rate will fall as 64 slice MDCT is more commonly used. If positive D-dimer, obtain MDCT and base treatment on results.
-----------> High clinical probability. 
-------------> Proceed to MDCT. If positive, treat, if negative, more tests are needed to exclude PE. 
-------------> A D-dimer of less than 750 ug/L does not rule out PE in those who are at high risk.

-----> Texas Medication Algorithm Project
-------> The Texas Medication Algorithm Project (TMAP) is a decision-tree medical algorithm, the design of which was based on the expert opinions of mental health specialists. 
---------> It has provided and rolled out a set of psychiatric management guidelines for doctors treating certain mental disorders
---------> within Texas' publicly funded mental health care system, along with manuals relating to each of them. 
---------> The algorithms commence after diagnosis and cover pharmacological treatment (hence "Medication Algorithm"). 



---> Physics

-----> Constraint algorithm: 
-------> This is a class of algorithms for satisfying constraints for bodies that obey Newton's equations of motion
-------> In computational chemistry, a constraint algorithm is a method for satisfying the Newtonian motion of a rigid body which consists of mass points. 
---------> A restraint algorithm is used to ensure that the distance between mass points is maintained. 
---------> The general steps involved are: 
---------> (i) choose novel unconstrained coordinates (internal coordinates), 
---------> (ii) introduce explicit constraint forces, 
---------> (iii) minimize constraint forces implicitly by the technique of Lagrange multipliers or projection methods.
-------> Constraint algorithms are often applied to molecular dynamics simulations. 
---------> Although such simulations are sometimes performed using internal coordinates that automatically satisfy the bond-length, bond-angle and torsion-angle constraints, 
---------> simulations may also be performed using explicit or implicit constraint forces for these three constraints. 
---------> However, explicit constraint forces give rise to inefficiency; more computational power is required to get a trajectory of a given length. 
---------> Therefore, internal coordinates and implicit-force constraint solvers are generally preferred.
-------> Constraint algorithms achieve computational efficiency by neglecting motion along some degrees of freedom. 
---------> For instance, in atomistic molecular dynamics, typically the length of covalent bonds to hydrogen are constrained; 
---------> however, constraint algorithms should not be used if vibrations along these degrees of freedom are important for the phenomenon being studied. 

-----> Demon algorithm: 
-------> This is a Monte Carlo method for efficiently sampling members of a microcanonical ensemble with a given energy
-------> The demon algorithm is a Monte Carlo method for efficiently sampling members of a microcanonical ensemble with a given energy. 
---------> An additional degree of freedom, called 'the demon', is added to the system and is able to store and provide energy. 
---------> If a drawn microscopic state has lower energy than the original state, the excess energy is transferred to the demon. 
---------> For a sampled state that has higher energy than desired, the demon provides the missing energy if it is available. 
---------> The demon can not have negative energy and it does not interact with the particles beyond exchanging energy. 
---------> Note that the additional degree of freedom of the demon does not alter a system with many particles significantly on a macroscopic level.
-------> In thermodynamical systems, equal macroscopic properties (e. g. temperature) can result from different microscopic properties (e. g. velocities of individual particles). 
---------> Computer simulations of the full equations of motion for every individual particle to simulate microscopic properties is computationally very expensive. 
---------> Monte Carlo methods can overcome this problem by sampling microscopic states according to stochastic rules instead of modeling the complete microphysics.
-------> The microcanonical ensemble is a collection of microscopic states which have fixed energy, volume and number of particles. 
---------> In an enclosed system with a certain number of particles, energy is the only macroscopic variable affected by the microphysics. 
---------> The Monte Carlo simulation of a microcanonical ensemble thus requires sampling different microscopic states with the same energy. 
---------> When the number of possible microscopic states of thermodynamical systems is very large, 
---------> it is inefficient to randomly draw a state from all possible states and accept it for the simulation if it has the right energy, 
---------> since many drawn states would be rejected. 

-----> Featherstone's algorithm: 
-------> This computes the effects of forces applied to a structure of joints and links
-------> Featherstone's algorithm is a technique used for computing the effects of forces applied to a structure of joints 
---------> and links (an "open kinematic chain") such as a skeleton used in ragdoll physics.
-------> The Featherstone's algorithm uses a reduced coordinate representation. 
---------> This is in contrast to the more popular Lagrange multiplier method, which uses maximal coordinates. 
---------> Brian Mirtich's PhD Thesis has a very clear and detailed description of the algorithm. 
---------> Baraff's paper "Linear-time dynamics using Lagrange multipliers" has a discussion and comparison of both algorithms. 

-----> Ground state approximation
-------> The ground state of a quantum-mechanical system is its stationary state of lowest energy; 
---------> the energy of the ground state is known as the zero-point energy of the system. 
---------> An excited state is any state with energy greater than the ground state. 
---------> In quantum field theory, the ground state is usually called the vacuum state or the vacuum.
-------> If more than one ground state exists, they are said to be degenerate. 
---------> Many systems have degenerate ground states. 
---------> Degeneracy occurs whenever there exists a unitary operator that acts non-trivially on a ground state and commutes with the Hamiltonian of the system.
-------> According to the third law of thermodynamics, a system at absolute zero temperature exists in its ground state; 
---------> thus, its entropy is determined by the degeneracy of the ground state. 
---------> Many systems, such as a perfect crystal lattice, have a unique ground state and therefore have zero entropy at absolute zero. 
---------> It is also possible for the highest excited state to have absolute zero temperature for systems that exhibit negative temperature. 

-------> Variational method
---------> The calculus of variations is a field of mathematical analysis that uses variations, 
-----------> which are small changes in functions and functionals, 
-----------> to find maxima and minima of functionals: mappings from a set of functions to the real numbers.
-----------> Functionals are often expressed as definite integrals involving functions and their derivatives. 
-----------> Functions that maximize or minimize functionals may be found using the Euler–Lagrange equation of the calculus of variations.
---------> A simple example of such a problem is to find the curve of shortest length connecting two points. 
-----------> If there are no constraints, the solution is a straight line between the points. 
-----------> However, if the curve is constrained to lie on a surface in space, then the solution is less obvious, and possibly many solutions may exist. 
-----------> Such solutions are known as geodesics. A related problem is posed by Fermat's principle: 
-----------> light follows the path of shortest optical length connecting two points, which depends upon the material of the medium. 
-----------> One corresponding concept in mechanics is the principle of least/stationary action.
---------> Many important problems involve functions of several variables. 
-----------> Solutions of boundary value problems for the Laplace equation satisfy the Dirichlet's principle. 
-----------> Plateau's problem requires finding a surface of minimal area that spans a given contour in space: 
-----------> a solution can often be found by dipping a frame in soapy water. 
-----------> Although such experiments are relatively easy to perform, their mathematical interpretation is far from simple: 
-----------> there may be more than one locally minimizing surface, and they may have non-trivial topology. 

---------> Ritz method
-----------> The Ritz method is a direct method to find an approximate solution for boundary value problems. 
-------------> The method is named after Walther Ritz, although also commonly called the Rayleigh–Ritz method and the Ritz-Galerkin method.
-----------> In quantum mechanics, a system of particles can be described in terms of an "energy functional" or Hamiltonian, 
-------------> which will measure the energy of any proposed configuration of said particles. 
-------------> It turns out that certain privileged configurations are more likely than other configurations, 
-------------> and this has to do with the eigenanalysis ("analysis of characteristics") of this Hamiltonian system. 
-------------> Because it is often impossible to analyze all of the infinite configurations of particles to find the one with the least amount of energy, 
-------------> it becomes essential to be able to approximate this Hamiltonian in some way for the purpose of numerical computations.
-----------> The Ritz method can be used to achieve this goal. In the language of mathematics, 
-------------> it is exactly the finite element method used to compute the eigenvectors and eigenvalues of a Hamiltonian system. 

-----> n-body problems
-------> In physics, the n-body problem is the problem of predicting the individual motions of a group of celestial objects interacting with each other gravitationally.
---------> Solving this problem has been motivated by the desire to understand the motions of the Sun, Moon, planets, and visible stars. 
---------> In the 20th century, understanding the dynamics of globular cluster star systems became an important n-body problem.
---------> The n-body problem in general relativity is considerably more difficult to solve due to additional factors like time and space distortions.
-------> The classical physical problem can be informally stated as the following:
---------> Given the quasi-steady orbital properties (instantaneous position, velocity and time) of a group of celestial bodies, 
--------->  predict their interactive forces; and consequently, predict their true orbital motions for all future times.
---------> 
---------> The two-body problem has been completely solved and is discussed below, as well as the famous restricted three-body problem.

-------> Barnes–Hut simulation: 
---------> This solves the n-body problem in an approximate way that has the order O(n log n) instead of O(n2) as in a direct-sum simulation.
---------> The Barnes–Hut simulation (named after Josh Barnes and Piet Hut) is an approximation algorithm for performing an n-body simulation. 
-----------> It is notable for having order O(n log n) compared to a direct-sum algorithm which would be O(n2).
---------> The simulation volume is usually divided up into cubic cells via an octree (in a three-dimensional space), 
-----------> so that only particles from nearby cells need to be treated individually, 
-----------> and particles in distant cells can be treated as a single large particle centered at the cell's center of mass (or as a low-order multipole expansion). 
-----------> This can dramatically reduce the number of particle pair interactions that must be computed.
---------> Some of the most demanding high-performance computing projects do computational astrophysics using the Barnes–Hut treecode algorithm, such as DEGIMA.

-------> Fast multipole method (FMM): 
---------> This speeds up the calculation of long-ranged forces
-----------> The fast multipole method (FMM) is a numerical technique that was developed 
-------------> to speed up the calculation of long-ranged forces in the n-body problem. 
-------------> It does this by expanding the system Green's function using a multipole expansion, 
-------------> which allows one to group sources that lie close together and treat them as if they are a single source.
-----------> The FMM has also been applied in accelerating the iterative solver in the method of moments (MOM) 
-------------> as applied to computational electromagnetics problems.
-------------> The FMM was first introduced in this manner by Leslie Greengard and Vladimir Rokhlin Jr. 
-------------> and is based on the multipole expansion of the vector Helmholtz equation. 
-------------> By treating the interactions between far-away basis functions using the FMM, 
-------------> the corresponding matrix elements do not need to be explicitly stored, 
-------------> resulting in a significant reduction in required memory. 
-------------> If the FMM is then applied in a hierarchical manner, 
-------------> it can improve the complexity of matrix-vector products in an iterative solver from O(N^2) to O (N) in finite arithmetic, 
-------------> i.e., given a tolerance ε, the matrix-vector product is guaranteed to be within a tolerance ε. 
-------------> The dependence of the complexity on the tolerance ε, i.e., the complexity of FMM is O(N*log⁡(1/ε)). 
-------------> This has expanded the area of applicability of the MOM to far greater problems than were previously possible.
-----------> The FMM, introduced by Rokhlin Jr. and Greengard has been said to be one of the top ten algorithms of the 20th century. 
-------------> The FMM algorithm reduces the complexity of matrix-vector multiplication 
-------------> involving a certain type of dense matrix which can arise out of many physical systems.
-----------> The FMM has also been applied for efficiently treating the Coulomb interaction 
-------------> in the Hartree–Fock method and density functional theory calculations in quantum chemistry. 

-----> Rainflow-counting algorithm: 
-------> This reduces a complex stress history to a count of elementary stress-reversals for use in fatigue analysis
-------> The rainflow-counting algorithm is used in calculating the fatigue life of a component in order to convert a loading sequence 
---------> of varying stress into an equivalent set of constant amplitude stress reversals. 
---------> The method successively extracts the smaller interruption cycles from a sequence, 
---------> which models the material memory effect seen with stress-strain hysteresis cycles. 
---------> This simplification allows the number of cycles until failure of a component to be determined 
---------> for each rainflow cycle using either Miner's rule to calculate the fatigue damage, 
---------> or in a crack growth equation to calculate the crack increments.
---------> Both methods give an estimate of the fatigue life of a component. 
---------> The algorithm was developed by Tatsuo Endo and M. Matsuishi in 1968.
-------> The rainflow method is compatible with the cycles obtained from examination of the stress-strain hysteresis cycles. 
---------> When a material is cyclically strained, a plot of stress against strain shows loops forming from the smaller interruption cycles. 
---------> At the end of the smaller cycle, the material resumes the stress-strain path of the original cycle, as if the interruption had not occurred. 
---------> The closed loops represent the energy dissipated by the material.

-----> Sweep and prune: 
-------> This is a broad phase algorithm used during collision detection to limit the number of pairs of solids that need to be checked for collision.
-------> In physical simulations, sweep and prune is a broad phase algorithm used during collision detection 
---------> to limit the number of pairs of solids that need to be checked for collision, i.e. intersection. 
---------> This is achieved by sorting the starts (lower bound) and ends (upper bound) 
---------> of the bounding volume of each solid along a number of arbitrary axes. 
---------> As the solids move, their starts and ends may overlap. When the bounding volumes of two solids overlap 
---------> in all axes they are flagged to be tested by more precise and time-consuming algorithms.
-------> Sweep and prune exploits temporal coherence as it is likely that 
---------> solids do not move significantly between two simulation steps. 
---------> Because of that, at each step, the sorted lists of bounding volume starts 
---------> and ends can be updated with relatively few computational operations. 
---------> Sorting algorithms which are fast at sorting almost-sorted lists, 
---------> such as insertion sort, are particularly good for this purpose.
-------> According with the type of bounding volume used, it is necessary to update the bounding volume dimensions every time a solid is reoriented. 
---------> To circumvent this, temporal coherence can be used to compute the changes in bounding volume geometry with fewer operations. 
---------> Another approach is to use bounding spheres or other orientation independent bounding volumes.
-------> Sweep and prune is also known as sort and sweep, referred to this way in David Baraff's Ph.D. thesis in 1992.
---------> Later works like the 1995 paper about I-COLLIDE by Jonathan D. Cohen et al. refer to the algorithm as sweep and prune. 

-----> VEGAS algorithm: 
-------> This is a method for reducing error in Monte Carlo simulations.
-------> The VEGAS algorithm, due to G. Peter Lepage, is a method for reducing error 
---------> in Monte Carlo simulations by using a known or approximate probability distribution function 
---------> to concentrate the search in those areas of the integrand that make the greatest contribution to the final integral.
-------> The VEGAS algorithm is based on importance sampling. 
---------> It samples points from the probability distribution described by the function |f| 
---------> so that the points are concentrated in the regions that make the largest contribution to the integral. 
---------> The GNU Scientific Library (GSL) provides a VEGAS routine. 

-----> Glauber dynamics: 
-------> This a method for simulating the Ising Model on a computer.
-------> In statistical physics, Glauber dynamics is a way to simulate the Ising model (a model of magnetism) on a computer.
---------> It is a type of Markov Chain Monte Carlo algorithm. [3]



---> Statistics
-----> Computational statistics, or statistical computing, is the bond between statistics and computer science. 
-------> It means statistical methods that are enabled by using computational methods.
-------> It is the area of computational science (or scientific computing) specific to the mathematical science of statistics. 
-------> This area is also developing rapidly, 
-------> leading to calls that a broader concept of computing should be taught as part of general statistical education.
-----> As in traditional statistics the goal is to transform raw data into knowledge,
-------> but the focus lies on computer intensive statistical methods, 
-------> such as cases with very large sample size and non-homogeneous data sets.
-----> The terms 'computational statistics' and 'statistical computing' are often used interchangeably, 
-------> although Carlo Lauro (a former president of the International Association for Statistical Computing) proposed making a distinction, 
-------> defining 'statistical computing' as "the application of computer science to statistics", and 'computational statistics' 
-------> as "aiming at the design of algorithm for implementing statistical methods on computers, 
-------> including the ones unthinkable before the computer age (e.g. bootstrap, simulation), 
-------> as well as to cope with analytically intractable problems" [sic].
-----> The term 'Computational statistics' may also be used to refer to computationally intensive statistical methods including resampling methods, 
-------> Markov chain Monte Carlo methods, local regression, kernel density estimation, artificial neural networks and generalized additive models. 

-----> Algorithms for calculating variance: 
-------> This is avoiding instability and numerical overflow
-------> Algorithms for calculating variance play a major role in computational statistics. 
---------> A key difficulty in the design of good algorithms for this problem is that formulas for the variance may involve sums of squares, 
---------> which can lead to numerical instability as well as to arithmetic overflow when dealing with large values. 

-----> Approximate counting algorithm: 
-------> This is allows counting large number of events in a small register
-------> The approximate counting algorithm allows the counting of a large number of events using a small amount of memory. 
---------> It uses probabilistic techniques to increment the counter. 
---------> When focused on high quality of approximation and low probability of failure, 
---------> Nelson and Yu showed that a very slight modification to the Morris Counter is asymptotically optimal amongst all algorithms for the problem.
---------> The algorithm is considered one of the precursors of streaming algorithms, 
---------> and the more general problem of determining the frequency moments of a data stream has been central to the field. 
-------> Note: This was invented in 1977 by Robert Morris (cryptographer) of Bell Labs.
-------> Note: It was fully analyzed in the early 1980s by Philippe Flajolet of INRIA Rocquencourt, 
---------> who coined the name approximate counting, and strongly contributed to its recognition among the research community. 

-----> Bayesian statistics
-------> Bayesian statistics is a theory in the field of statistics based on the Bayesian interpretation of probability
---------> where probability expresses a degree of belief in an event. 
---------> The degree of belief may be based on prior knowledge about the event, 
---------> such as the results of previous experiments, or on personal beliefs about the event. 
---------> This differs from a number of other interpretations of probability, 
---------> such as the frequentist interpretation that views probability as the limit of the relative frequency of an event after many trials.
-------> Bayesian statistical methods use Bayes' theorem to compute and update probabilities after obtaining new data. 
---------> Bayes' theorem describes the conditional probability of an event based on data as well 
---------> as prior information or beliefs about the event or conditions related to the event.
---------> For example, in Bayesian inference, Bayes' theorem can be used to estimate the parameters of a probability distribution or statistical model. 
---------> Since Bayesian statistics treats probability as a degree of belief, Bayes' 
---------> theorem can directly assign a probability distribution that quantifies the belief to the parameter or set of parameters.

-------> Nested sampling algorithm: 
---------> This a computational approach to the problem of comparing models in Bayesian statistics
---------> The nested sampling algorithm is a computational approach to the Bayesian statistics problems 
-----------> of comparing models and generating samples from posterior distributions. 
-----------> It was developed in 2004 by physicist John Skilling.

-----> Clustering Algorithms
-------> Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster)
---------> are more similar (in some sense) to each other than to those in other groups (clusters). 
---------> It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, 
---------> including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning.
-------> Cluster analysis itself is not one specific algorithm, but the general task to be solved. 
---------> It can be achieved by various algorithms that differ significantly in their understanding of what constitutes a cluster and how to efficiently find them. 
---------> Popular notions of clusters include groups with small distances between cluster members, dense areas of the data space, 
---------> intervals or particular statistical distributions. Clustering can therefore be formulated as a multi-objective optimization problem. 
---------> The appropriate clustering algorithm and parameter settings (including parameters such as the distance function to use, 
---------> a density threshold or the number of expected clusters) depend on the individual data set and intended use of the results. 
---------> Cluster analysis as such is not an automatic task, but an iterative process of knowledge discovery
--------->  or interactive multi-objective optimization that involves trial and failure. 
--------->  It is often necessary to modify data preprocessing and model parameters until the result achieves the desired properties.
-------> Besides the term clustering, there are a number of terms with similar meanings, including automatic classification, 
---------> numerical taxonomy, botryology (from Greek βότρυς "grape"), typological analysis, and community detection. 
---------> The subtle differences are often in the use of the results: while in data mining, the resulting groups are the matter of interest,
---------> in automatic classification the resulting discriminative power is of interest.
-------> Cluster analysis was originated in anthropology by Driver and Kroeber in 1932 
---------> and introduced to psychology by Joseph Zubin in 1938 and Robert Tryon in 1939[and famously used by Cattell beginning in 1943[4] 
---------> for trait theory classification in personality psychology. 

-------> Average-linkage clustering: 
---------> This is a simple agglomerative clustering algorithm
---------> UPGMA (unweighted pair group method with arithmetic mean) is a simple agglomerative (bottom-up) hierarchical clustering method. 
---------> The UPGMA method is similar to its weighted variant, the WPGMA method.
---------> Note that the unweighted term indicates that all distances contribute equally to each average that is computed and does not refer to the math by which it is achieved. 
-----------> Thus the simple averaging in WPGMA produces a weighted result 
-----------> and the proportional averaging in UPGMA produces an unweighted result (see the working example).
---------> The method is generally attributed to Sokal and Michener.[1]

-------> Canopy clustering algorithm: 
---------> This is an unsupervised pre-clustering algorithm related to the K-means algorithm
-----------> It is often used as preprocessing step for the K-means algorithm or the Hierarchical clustering algorithm. 
-----------> It is intended to speed up clustering operations on large data sets, where using another algorithm directly may be impractical due to the size of the data set. 
---------> The canopy clustering algorithm is an unsupervised pre-clustering algorithm introduced by Andrew McCallum, Kamal Nigam and Lyle Ungar in 2000.

-------> Complete-linkage clustering: 
---------> This is a simple agglomerative clustering algorithm
---------> Complete-linkage clustering is one of several methods of agglomerative hierarchical clustering. 
-----------> At the beginning of the process, each element is in a cluster of its own. 
-----------> The clusters are then sequentially combined into larger clusters until all elements end up being in the same cluster. 
-----------> The method is also known as farthest neighbour clustering. 
-----------> The result of the clustering can be visualized as a dendrogram, 
-----------> which shows the sequence of cluster fusion and the distance at which each fusion took place.

-------> DBSCAN: 
---------> This is a density based clustering algorithm
---------> It is a density-based clustering non-parametric algorithm: 
-----------> given a set of points in some space, it groups together points that are closely packed together (points with many nearby neighbors), 
-----------> marking as outliers points that lie alone in low-density regions (whose nearest neighbors are too far away). 
-----------> DBSCAN is one of the most common clustering algorithms and also most cited in scientific literature.[2] 
-----------> Density-based spatial clustering of applications with noise (DBSCAN) is a data clustering algorithm proposed by Martin Ester, Hans-Peter Kriegel, Jörg Sander and Xiaowei Xu in 1996.

-------> Expectation-maximization algorithm
---------> In statistics, an expectation–maximization (EM) algorithm is an iterative method 
-----------> to find (local) maximum likelihood or maximum a posteriori (MAP) estimates of parameters in statistical models, 
-----------> where the model depends on unobserved latent variables. 
-----------> The EM iteration alternates between performing an expectation (E) step, 
-----------> which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, 
-----------> and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. 
-----------> These parameter-estimates are then used to determine the distribution of the latent variables in the next E step. 

-------> Fuzzy clustering: 
---------> This is a class of clustering algorithms where each point has a degree of belonging to clusters
---------> Fuzzy clustering (also referred to as soft clustering or soft k-means) 
-----------> is a form of clustering in which each data point can belong to more than one cluster.
---------> Clustering or cluster analysis involves assigning data points to clusters such that items in the same cluster are as similar as possible, 
-----------> while items belonging to different clusters are as dissimilar as possible. 
-----------> Clusters are identified via similarity measures. 
-----------> These similarity measures include distance, connectivity, and intensity. 
-----------> Different similarity measures may be chosen based on the data or the application.

---------> Fuzzy c-means
-----------> Fuzzy C-means clustering
-------------> One of the most widely used fuzzy clustering algorithms is the Fuzzy C-means clustering (FCM) algorithm.
-------------> Fuzzy c-means (FCM) clustering was developed by J.C. Dunn in 1973,[2] and improved by J.C. Bezdek in 1981.
-------------> The fuzzy c-means algorithm is very similar to the k-means algorithm:
---------------> Choose a number of clusters.
---------------> Assign coefficients randomly to each data point for being in the clusters.
---------------> Repeat until the algorithm has converged (that is, the coefficients' change between two iterations is no more than ε, the given sensitivity threshold) :
-----------------> Compute the centroid for each cluster (shown below).
-----------------> For each data point, compute its coefficients of being in the clusters.

---------> FLAME clustering (Fuzzy clustering by Local Approximation of MEmberships): 
-----------> This is define clusters in the dense parts of a dataset and perform cluster assignment solely based on the neighborhood relationships among objects
-----------> Fuzzy clustering by Local Approximation of MEmberships (FLAME) is a data clustering algorithm that defines clusters in the dense parts of a dataset 
-------------> and performs cluster assignment solely based on the neighborhood relationships among objects. 
-------------> The key feature of this algorithm is that the neighborhood relationships among neighboring objects in the feature space 
-------------> are used to constrain the memberships of neighboring objects in the fuzzy membership space. 
-----------> Description of the FLAME algorithm
-------------> The FLAME algorithm is mainly divided into three steps:
---------------> (1) Extraction of the structure information from the dataset:
-----------------> (1) Construct a neighborhood graph to connect each object to its K-Nearest Neighbors (KNN);
-----------------> (2) Estimate a density for each object based on its proximities to its KNN;
-----------------> (3) Objects are classified into 3 types:
-------------------> (1) Cluster Supporting Object (CSO): object with density higher than all its neighbors;
-------------------> (2) Cluster Outliers: object with density lower than all its neighbors, and lower than a predefined threshold;
-------------------> (3) the rest.
---------------> (2) Local/Neighborhood approximation of fuzzy memberships:
-----------------> (1) Initialization of fuzzy membership:
-------------------> (1) Each CSO is assigned with fixed and full membership to itself to represent one cluster;
-------------------> (2) All outliers are assigned with fixed and full membership to the outlier group;
-------------------> (3) The rest are assigned with equal memberships to all clusters and the outlier group;
-----------------> (2) Then the fuzzy memberships of all type 3 objects are updated by a converging iterative procedure 
-------------------> called Local/Neighborhood Approximation of Fuzzy Memberships, 
-------------------> in which the fuzzy membership of each object is updated by a linear combination
-------------------> of the fuzzy memberships of its nearest neighbors.
---------------> (3) Cluster construction from fuzzy memberships in two possible ways:
-----------------> (1) One-to-one object-cluster assignment, to assign each object to the cluster in which it has the highest membership;
-----------------> (2) One-to-multiple object-clusters assignment, to assign each object to the cluster in which it has a membership higher than a threshold.

-------> KHOPCA clustering algorithm: 
---------> This a local clustering algorithm, which produces hierarchical multi-hop clusters in static and mobile environments.
---------> KHOPCA is an adaptive clustering algorithm originally developed for dynamic networks. 
-----------> KHOPCA (k-hop clustering algorithm) provides a fully distributed and localized approach 
-----------> to group elements such as nodes in a network according to their distance from each other. 
-----------> KHOPCA operates proactively through a simple set of rules that defines clusters, which are optimal with respect to the applied distance function.
---------> KHOPCA's clustering process explicitly supports joining and leaving of nodes, 
-----------> which makes KHOPCA suitable for highly dynamic networks. 
-----------> However, it has been demonstrated that KHOPCA also performs in static networks.
---------> Besides applications in ad hoc and wireless sensor networks, KHOPCA can be used in localization and navigation problems, 
---------> KHOPCA (k-hop clustering algorithm) operates proactively through a simple set of rules that defines clusters with variable k-hops. 
-----------> A set of local rules describes the state transition between nodes. 
-----------> A node's weight is determined only depending on the current state of its neighbors in communication range. 
-----------> Each node of the network is continuously involved in this process. 
-----------> As result, k-hop clusters are formed and maintained in static as well as dynamic networks.
---------> KHOPCA does not require any predetermined initial configuration. 
-----------> Therefore, a node can potentially choose any weight (between M N and MAX). 
-----------> However, the choice of the initial configuration does influence the convergence time. 


-------> k-means clustering: 
---------> This cluster objects based on attributes into partitions
---------> k-means clustering is a method of vector quantization, originally from signal processing, 
-----------> that aims to partition n observations into k clusters in which each observation belongs to the cluster
-----------> with the nearest mean (cluster centers or cluster centroid), serving as a prototype of the cluster.
-----------> This results in a partitioning of the data space into Voronoi cells. 
-----------> k-means clustering minimizes within-cluster variances (squared Euclidean distances), 
-----------> but not regular Euclidean distances, which would be the more difficult Weber problem: 
-----------> the mean optimizes squared errors, whereas only the geometric median minimizes Euclidean distances. 
-----------> For instance, better Euclidean solutions can be found using k-medians and k-medoids.
---------> The problem is computationally difficult (NP-hard); however, efficient heuristic algorithms converge quickly to a local optimum. 
-----------> These are usually similar to the expectation-maximization algorithm for mixtures of Gaussian distributions 
-----------> via an iterative refinement approach employed by both k-means and Gaussian mixture modeling. 
-----------> They both use cluster centers to model the data; 
-----------> however, k-means clustering tends to find clusters of comparable spatial extent, 
-----------> while the Gaussian mixture model allows clusters to have different shapes.
---------> The unsupervised k-means algorithm has a loose relationship to the k-nearest neighbor classifier, 
-----------> a popular supervised machine learning technique for classification that is often confused with k-means due to the name. 
-----------> Applying the 1-nearest neighbor classifier to the cluster centers obtained by k-means classifies new data into the existing clusters. 
-----------> This is known as nearest centroid classifier or Rocchio algorithm. 


-------> k-means++: 
---------> This is a variation of this, using modified random seeds
---------> In data mining, k-means++ is an algorithm for choosing the initial values (or "seeds") for the k-means clustering algorithm. 
-----------> It was proposed in 2007 by David Arthur and Sergei Vassilvitskii, 
-----------> as an approximation algorithm for the NP-hard k-means problem
-----------> a way of avoiding the sometimes poor clusterings found by the standard k-means algorithm. 
-----------> It is similar to the first of three seeding methods proposed,
-----------> in independent work, in 2006 by Rafail Ostrovsky, Yuval Rabani, Leonard Schulman and Chaitanya Swamy. 
-----------> (The distribution of the first seed is different.) 


-------> k-medoids: 
---------> This is similar to k-means, but chooses datapoints or medoids as centers
---------> The k-medoids problem is a clustering problem similar to k-means. 
-----------> The name was coined by Leonard Kaufman and Peter J. Rousseeuw with their PAM algorithm.
-----------> Both the k-means and k-medoids algorithms are partitional (breaking the dataset up into groups) 
-----------> and attempt to minimize the distance between points labeled to be in a cluster and a point designated as the center of that cluster. 
-----------> In contrast to the k-means algorithm, k-medoids chooses actual data points as centers (medoids or exemplars), 
-----------> and thereby allows for greater interpretability of the cluster centers than in k-means, 
-----------> where the center of a cluster is not necessarily one of the input data points (it is the average between the points in the cluster).
-----------> Furthermore, k-medoids can be used with arbitrary dissimilarity measures, whereas k-means generally requires Euclidean distance for efficient solutions. 
-----------> Because k-medoids minimizes a sum of pairwise dissimilarities instead of a sum of squared Euclidean distances, it is more robust to noise and outliers than k-means.
---------> k-medoids is a classical partitioning technique of clustering that splits the data set of n objects into k clusters, 
-----------> where the number k of clusters assumed known a priori (which implies that the programmer must specify k before the execution of a k-medoids algorithm). 
-----------> The "goodness" of the given value of k can be assessed with methods such as the silhouette method.
---------> The medoid of a cluster is defined as the object in the cluster whose average dissimilarity 
-----------> to all the objects in the cluster is minimal, that is, it is a most centrally located point in the cluster. 

-------> Linde–Buzo–Gray algorithm: 
---------> This a vector quantization algorithm to derive a good codebook
---------> The Linde–Buzo–Gray algorithm (introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980) 
-----------> is a vector quantization algorithm to derive a good codebook.
---------> It is similar to the k-means method in data clustering. 
-------> Lloyd's algorithm (Voronoi iteration or relaxation): 
---------> This group data points into a given number of categories, a popular algorithm for k-means clustering
---------> Lloyd's algorithm, also known as Voronoi iteration or relaxation, 
-----------> is used for finding evenly spaced sets of points in subsets of Euclidean spaces and partitions of these subsets into well-shaped and uniformly sized convex cells.
-----------> Like the closely related k-means clustering algorithm, it repeatedly finds the centroid of each set in the partition 
-----------> and then re-partitions the input according to which of these centroids is closest. 
-----------> In this setting, the mean operation is an integral over a region of space, and the nearest centroid operation results in Voronoi diagrams.
---------> Although the algorithm may be applied most directly to the Euclidean plane, 
-----------> similar algorithms may also be applied to higher-dimensional spaces or to spaces with other non-Euclidean metrics. 
-----------> Lloyd's algorithm can be used to construct close approximations to centroidal Voronoi tessellations of the input,
-----------> which can be used for quantization, dithering, and stippling. 
-----------> Other applications of Lloyd's algorithm include smoothing of triangle meshes in the finite element method. 
---------> Note: This is an algorithm named after Stuart P. Lloyd 

-------> OPTICS: 
---------> This a density based clustering algorithm with a visual evaluation method
---------> Ordering points to identify the clustering structure (OPTICS) is an algorithm for finding density-based clusters in spatial data. 
-----------> It was presented by Mihael Ankerst, Markus M. Breunig, Hans-Peter Kriegel and Jörg Sander. 
-----------> Its basic idea is similar to DBSCAN, but it addresses one of DBSCAN's major weaknesses: 
-----------> the problem of detecting meaningful clusters in data of varying density. 
-----------> To do so, the points of the database are (linearly) ordered such that spatially closest points become neighbors in the ordering. 
-----------> Additionally, a special distance is stored for each point that represents the density that must be accepted for a cluster so that both points belong to the same cluster. 
-----------> This is represented as a dendrogram. 

-------> Single-linkage clustering: 
---------> This a simple agglomerative clustering algorithm
---------> In statistics, single-linkage clustering is one of several methods of hierarchical clustering. 
-----------> It is based on grouping clusters in bottom-up fashion (agglomerative clustering), 
-----------> at each step combining two clusters that contain the closest pair of elements not yet belonging to the same cluster as each other.
---------> A drawback of this method is that it tends to produce long thin clusters in which nearby elements of the same cluster have small distances, 
-----------> but elements at opposite ends of a cluster may be much farther from each other than two elements of other clusters. 
-----------> This may lead to difficulties in defining classes that could usefully subdivide the data.

-------> SUBCLU: 
---------> This a subspace clustering algorithm.
---------> SUBCLU is an algorithm for clustering high-dimensional data by Karin Kailing, Hans-Peter Kriegel and Peer Kröger.
-----------> It is a subspace clustering algorithm that builds on the density-based clustering algorithm DBSCAN. 
-----------> SUBCLU can find clusters in axis-parallel subspaces, and uses a bottom-up, greedy strategy to remain efficient. 

-------> Ward's method: 
---------> This an agglomerative clustering algorithm, extended to more general Lance–Williams algorithms
---------> In statistics, Ward's method is a criterion applied in hierarchical cluster analysis. 
-----------> Ward's minimum variance method is a special case of the objective function approach originally presented by Joe H. Ward, Jr. 
-----------> Ward suggested a general agglomerative hierarchical clustering procedure, 
-----------> where the criterion for choosing the pair of clusters to merge at each step is based on the optimal value of an objective function. 
-----------> This objective function could be "any function that reflects the investigator's purpose." 
-----------> Many of the standard clustering procedures are contained in this very general class. 
-----------> To illustrate the procedure, Ward used the example where the objective function is the error sum of squares, 
-----------> and this example is known as Ward's method or more precisely Ward's minimum variance method.
---------> The nearest-neighbor chain algorithm can be used to find the same clustering defined by Ward's method, 
-----------> in time proportional to the size of the input distance matrix and space linear in the number of points being clustered. 

-------> WACA clustering algorithm: 
---------> This a local clustering algorithm with potentially multi-hop structures; for dynamic networks
---------> WACA is a clustering algorithm for dynamic networks.
-----------> WACA (Weighted Application-aware Clustering Algorithm) uses a heuristic weight function for self-organized cluster creation. 
-----------> The election of clusterheads is based on local network information only. 

-----> Estimation Theory
-------> Estimation theory is a branch of statistics that deals with estimating the values of parameters based on measured empirical data that has a random component. 
---------> The parameters describe an underlying physical setting in such a way that their value affects the distribution of the measured data. 
---------> An estimator attempts to approximate the unknown parameters using the measurements. 
---------> In estimation theory, two approaches are generally considered:
-----------> (1) The probabilistic approach (described in this article) assumes that the measured data is random with probability distribution dependent on the parameters of interest.
-----------> (2) The set-membership approach assumes that the measured data vector belongs to a set which depends on the parameter vector.
 
-------> Expectation-maximization algorithm 
---------> This is a class of related algorithms for finding maximum likelihood estimates of parameters in probabilistic models
---------> In statistics, an expectation–maximization (EM) algorithm is an iterative method to find (local) maximum likelihood 
-----------> or maximum a posteriori (MAP) estimates of parameters in statistical models, 
-----------> where the model depends on unobserved latent variables. 
-----------> The EM iteration alternates between performing an expectation (E) step, 
-----------> which creates a function for the expectation of the log-likelihood evaluated using the current estimate for the parameters, 
-----------> and a maximization (M) step, which computes parameters maximizing the expected log-likelihood found on the E step. 
-----------> These parameter-estimates are then used to determine the distribution of the latent variables in the next E step. 

---------> Ordered subset expectation maximization (OSEM): 
-----------> This used in medical imaging for positron emission tomography, single-photon emission computed tomography and X-ray computed tomography.
-------------> In mathematical optimization, the ordered subset expectation maximization (OSEM) method is an iterative method that is used in computed tomography.
-----------> In applications in medical imaging, the OSEM method is used for positron emission tomography, 
-------------> for single photon emission computed tomography, and for X-ray computed tomography.
-----------> The OSEM method is related to the expectation maximization (EM) method of statistics. 
-------------> The OSEM method is also related to methods of filtered back projection. 

-------> Odds algorithm (Bruss algorithm) 
---------> This Optimal online search for distinguished value in sequential random input
---------> The odds-algorithm is a mathematical method for computing optimal strategies for a class of problems that belong to the domain of optimal stopping problems. 
-----------> Their solution follows from the odds-strategy, and the importance of the odds-strategy lies in its optimality, as explained below.
---------> The odds-algorithm applies to a class of problems called last-success-problems. 
-----------> Formally, the objective in these problems is to maximize the probability of identifying in a sequence of sequentially observed independent events 
-----------> the last event satisfying a specific criterion (a "specific event"). 
-----------> This identification must be done at the time of observation. 
-----------> No revisiting of preceding observations is permitted. Usually, a specific event is defined by the decision maker as an event 
-----------> that is of true interest in the view of "stopping" to take a well-defined action. 
-----------> Such problems are encountered in several situations. 

-------> Kalman filter: 
---------> This estimates the state of a linear dynamic system from a series of noisy measurements
---------> For statistics and control theory, Kalman filtering, also known as linear quadratic estimation (LQE), 
-----------> is an algorithm that uses a series of measurements observed over time, 
-----------> including statistical noise and other inaccuracies, 
-----------> and produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone, 
-----------> by estimating a joint probability distribution over the variables for each timeframe. The filter is named after Rudolf E. Kálmán, 
-----------> who was one of the primary developers of its theory.
---------> This digital filter is sometimes termed the Stratonovich–Kalman–Bucy filter because it is a special case of a more general, 
-----------> nonlinear filter developed somewhat earlier by the Soviet mathematician Ruslan Stratonovich. 
-----------> In fact, some of the special case linear filter's equations appeared in papers by Stratonovich that were published before summer 1960, 
-----------> when Kalman met with Stratonovich during a conference in Moscow.[5]
---------> Kalman filtering has numerous technological applications. A common application is for guidance, 
-----------> navigation, and control of vehicles, particularly aircraft, spacecraft and ships positioned dynamically. 
-----------> Furthermore, Kalman filtering is a concept much applied in time series analysis used for topics such as signal processing and econometrics. 
-----------> Kalman filtering is also one of the main topics of robotic motion planning and control and can be used for trajectory optimization.
-----------> Kalman filtering also works for modeling the central nervous system's control of movement. 
-----------> Due to the time delay between issuing motor commands and receiving sensory feedback, 
-----------> the use of Kalman filters provides a realistic model for making estimates of the current state of a motor system and issuing updated commands.
---------> The algorithm works by a two-phase process. 
-----------> For the prediction phase, the Kalman filter produces estimates of the current state variables, along with their uncertainties. 
-----------> Once the outcome of the next measurement (necessarily corrupted with some error, including random noise) is observed, 
-----------> these estimates are updated using a weighted average, with more weight being given to estimates with greater certainty. 
-----------> The algorithm is recursive. 
-----------> It can operate in real time, using only the present input measurements 
-----------> and the state calculated previously and its uncertainty matrix; no additional past information is required.
---------> Optimality of Kalman filtering assumes that errors have a normal (Gaussian) distribution. 
-----------> In the words of Rudolf E. Kálmán: "In summary, the following assumptions are made about random processes: 
-----------> Physical random phenomena may be thought of as due to primary random sources exciting dynamic systems. 
-----------> The primary sources are assumed to be independent gaussian random processes with zero mean; the dynamic systems will be linear."
-----------> Though regardless of Gaussianity, if the process and measurement covariances are known, 
-----------> the Kalman filter is the best possible linear estimator in the minimum mean-square-error sense.
---------> Extensions and generalizations of the method have also been developed, 
-----------> such as the extended Kalman filter and the unscented Kalman filter which work on nonlinear systems. 
-----------> The basis is a hidden Markov model such that the state space of the latent variables is continuous 
-----------> and all latent and observed variables have Gaussian distributions. 
-----------> Also, Kalman filtering has been used successfully in multi-sensor fusion,
-----------> and distributed sensor networks to develop distributed or consensus Kalman filtering.[12] 

-----> False nearest neighbor algorithm (FNN)
-------> This estimates fractal dimension
-------> Within abstract algebra, the false nearest neighbor algorithm is an algorithm for estimating the embedding dimension. 
---------> The main idea is to examine how the number of neighbors of a point along a signal trajectory change with increasing embedding dimension. 
---------> In too low an embedding dimension, many of the neighbors will be false, but in an appropriate embedding dimension or higher, the neighbors are real. 
---------> With increasing dimension, the false neighbors will no longer be neighbors. 
---------> Therefore, by examining how the number of neighbors change as a function of dimension, an appropriate embedding can be determined.
-------> The concept was proposed by Kennel et al.

-----> Hidden Markov model
-------> A hidden Markov model (HMM) is a statistical Markov model in which the system being modeled 
---------> is assumed to be a Markov process — call it X — with unobservable ("hidden") states. 
---------> As part of the definition, HMM requires that there be an observable process Y 
---------> whose outcomes are "influenced" by the outcomes of X in a known way. 
---------> Since X cannot be observed directly, the goal is to learn about X by observing Y. 
---------> HMM has an additional requirement that the outcome of Y at time t=t0 must be "influenced" exclusively by the outcome of X at t=t0 
---------> and that the outcomes of X and Y at t<t0 must not affect the outcome of Y at t = t0. 
-------> Hidden Markov models are known for their applications to thermodynamics, statistical mechanics, 
---------> physics, chemistry, economics, finance, signal processing, information theory, pattern recognition
---------> such as speech, handwriting, gesture recognition part-of-speech tagging, musical score following, partial discharges[3] and bioinformatics.


-------> Baum–Welch algorithm: 
---------> This computes maximum likelihood estimates and posterior mode estimates for the parameters of a hidden Markov model
---------> In electrical engineering, statistical computing and bioinformatics, 
-----------> the Baum–Welch algorithm is a special case of the expectation–maximization algorithm 
-----------> used to find the unknown parameters of a hidden Markov model (HMM). 
-----------> It makes use of the forward-backward algorithm to compute the statistics for the expectation step. 

-------> Forward-backward algorithm: 
---------> This is a dynamic programming algorithm for computing the probability of a particular observation sequence
---------> The forward–backward algorithm is an inference algorithm for hidden Markov models which computes the posterior marginals of all hidden state variables 
-----------> given a sequence of observations/emissions o 1 : T := o1 , … , o T, i.e. it computes, for all hidden state variables X t ∈ {X1, …, XT} , the distribution P(Xt | o1:T). 
-----------> This inference task is usually called smoothing. 
-----------> The algorithm makes use of the principle of dynamic programming to efficiently compute the values that are required to obtain the posterior marginal distributions in two passes. 
-----------> The first pass goes forward in time while the second goes backward in time; hence the name forward–backward algorithm.
---------> The term forward–backward algorithm is also used to refer to any algorithm belonging to the general class of algorithms that operate on sequence models in a forward–backward manner. 
-----------> In this sense, the descriptions in the remainder of this article refer but to one specific instance of this class. 

-------> Viterbi algorithm: 
---------> This finds the most likely sequence of hidden states in a hidden Markov model
---------> The Viterbi algorithm is a dynamic programming algorithm for obtaining the maximum a posteriori probability estimate 
-----------> of the most likely sequence of hidden states—called the Viterbi path—that results in a sequence of observed events, 
-----------> especially in the context of Markov information sources and hidden Markov models (HMM).
---------> The algorithm has found universal application in decoding the convolutional codes used in both CDMA and GSM digital cellular, 
-----------> dial-up modems, satellite, deep-space communications, and 802.11 wireless LANs. 
-----------> It is now also commonly used in speech recognition, speech synthesis, diarization, 
-----------> keyword spotting, computational linguistics, and bioinformatics. 
-----------> For example, in speech-to-text (speech recognition), the acoustic signal is treated as the observed sequence of events, 
-----------> and a string of text is considered to be the "hidden cause" of the acoustic signal. 
-----------> The Viterbi algorithm finds the most likely string of text given the acoustic signal. 

-----> Partial least squares regression: 
-------> This finds a linear model describing some predicted variables in terms of other observable variables
-------> Partial least squares regression (PLS regression) is a statistical method that bears some relation to principal components regression; 
---------> instead of finding hyperplanes of maximum variance between the response and independent variables, 
---------> it finds a linear regression model by projecting the predicted variables and the observable variables to a new space. 
---------> Because both the X and Y data are projected to new spaces, the PLS family of methods are known as bilinear factor models. 
---------> Partial least squares discriminant analysis (PLS-DA) is a variant used when the Y is categorical.
-------> PLS is used to find the fundamental relations between two matrices (X and Y), i.e. a latent variable approach to modeling the covariance structures in these two spaces. 
---------> A PLS model will try to find the multidimensional direction in the X space that explains the maximum multidimensional variance direction in the Y space. 
---------> PLS regression is particularly suited when the matrix of predictors has more variables than observations, and when there is multicollinearity among X values. 
---------> By contrast, standard regression will fail in these cases (unless it is regularized).
-------> Partial least squares was introduced by the Swedish statistician Herman O. A. Wold, who then developed it with his son, Svante Wold. 
---------> An alternative term for PLS is projection to latent structures,[1][2] but the term partial least squares is still dominant in many areas. 
---------> Although the original applications were in the social sciences, 
---------> PLS regression is today most widely used in chemometrics and related areas. It is also used in bioinformatics, sensometrics, neuroscience, and anthropology. 

-----> Queuing theory
-------> Queueing theory is the mathematical study of waiting lines, or queues.
---------> A queueing model is constructed so that queue lengths and waiting time can be predicted.
---------> Queueing theory is generally considered a branch of operations research because the results 
---------> are often used when making business decisions about the resources needed to provide a service.
-------> Queueing theory has its origins in research by Agner Krarup Erlang when he created models to describe the system of Copenhagen Telephone Exchange company, a Danish company. 
---------> The ideas have since seen applications including telecommunication, traffic engineering, computing 
---------> and, particularly in industrial engineering, in the design of factories, shops, offices and hospitals, as well as in project management.

-------> Buzen's algorithm: 
---------> This is an algorithm for calculating the normalization constant G(K) in the Gordon–Newell theorem
---------> In queueing theory, a discipline within the mathematical theory of probability, 
-----------> Buzen's algorithm (or convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon–Newell theorem. 
-----------> This method was first proposed by Jeffrey P. Buzen in 1973.
-----------> Computing G(N) is required to compute the stationary probability distribution of a closed queueing network.
---------> Performing a naïve computation of the normalising constant requires enumeration of all states. 
-----------> For a system with N jobs and M states there are (N+M−1, M−1) combinations. 
-----------> Buzen's algorithm "computes G(1), G(2), ..., G(N) using a total of NM multiplications and NM additions." 
-----------> This is a significant improvement and allows for computations to be performed with much larger networks.

-----> RANSAC (an abbreviation for "RANdom SAmple Consensus"): 
-------> This is an iterative method to estimate parameters of a mathematical model from a set of observed data which contains outliers
-------> In queueing theory, a discipline within the mathematical theory of probability, 
---------> Buzen's algorithm (or convolution algorithm) is an algorithm for calculating the normalization constant G(N) in the Gordon–Newell theorem. 
---------> This method was first proposed by Jeffrey P. Buzen in 1973.
---------> Computing G(N) is required to compute the stationary probability distribution of a closed queueing network.
-------> Performing a naïve computation of the normalising constant requires enumeration of all states. 
---------> For a system with N jobs and M states there are (N+M−1, M−1) combinations. 
---------> Buzen's algorithm "computes G(1), G(2), ..., G(N) using a total of NM multiplications and NM additions." 
---------> This is a significant improvement and allows for computations to be performed with much larger networks.[1] 

-----> Scoring algorithm: 
-------> This is a form of Newton's method used to solve maximum likelihood equations numerically
-------> Scoring algorithm, also known as Fisher's scoring, is a form of Newton's method used in statistics to solve maximum likelihood equations numerically, named after Ronald Fisher. 

-----> Yamartino method: 
-------> This calculate an approximation to the standard deviation σθ of wind direction θ during a single pass through the incoming data
-------> The Yamartino method is an algorithm for calculating an approximation of the standard deviation of wind direction during a single pass through the incoming data.

-----> Ziggurat algorithm: 
-------> This generates random numbers from a non-uniform distribution
-------> The ziggurat algorithm is an algorithm for pseudo-random number sampling. 
---------> Belonging to the class of rejection sampling algorithms, it relies on an underlying source of uniformly-distributed random numbers, 
---------> typically from a pseudo-random number generator, as well as precomputed tables. 
---------> The algorithm is used to generate values from a monotonically decreasing probability distribution. 
---------> It can also be applied to symmetric unimodal distributions, such as the normal distribution, 
---------> by choosing a value from one half of the distribution and then randomly choosing which half the value is considered to have been drawn from. 
---------> It was developed by George Marsaglia and others in the 1960s.
-------> A typical value produced by the algorithm only requires the generation of one random floating-point value and one random table index, 
---------> followed by one table lookup, one multiply operation and one comparison. 
---------> Sometimes (2.5% of the time, in the case of a normal or exponential distribution when using typical table sizes) more computations are required. 
---------> Nevertheless, the algorithm is computationally much faster[citation needed] than the two most commonly used methods of generating normally distributed random numbers, 
---------> the Marsaglia polar method and the Box–Muller transform, which require at least one logarithm and one square root calculation for each pair of generated values. 
---------> However, since the ziggurat algorithm is more complex to implement it is best used when large quantities of random numbers are required.
-------> The term ziggurat algorithm dates from Marsaglia's paper with Wai Wan Tsang in 2000; 
---------> it is so named because it is conceptually based on covering the probability distribution with rectangular segments stacked in decreasing order of size, 
---------> resulting in a figure that resembles a ziggurat. 



-> Computer science

---> Computer architecture
-----> In computer engineering, computer architecture is a set of rules and methods 
-------> that describe the functionality, organization, and implementation of computer systems. 
-------> The architecture of a system refers to its structure in terms 
-------> of separately specified components of that system and their interrelationships.
-----> Some definitions of architecture define it as describing the capabilities 
-------> and programming model of a computer but not a particular implementation. 
-------> In other definitions computer architecture involves instruction set architecture design,
-------> microarchitecture design, logic design, and implementation.[3] 

-----> Tomasulo algorithm: 
-------> This allows sequential instructions that would normally be stalled due to certain dependencies to execute non-sequentially
---------> Tomasulo's algorithm is a computer architecture hardware algorithm for dynamic scheduling of instructions 
---------> that allows out-of-order execution and enables more efficient use of multiple execution units. 
-------> The major innovations of Tomasulo’s algorithm include register renaming in hardware, 
---------> reservation stations for all execution units, and a common data bus (CDB) on which computed values broadcast 
---------> to all reservation stations that may need them. 
---------> These developments allow for improved parallel execution of instructions 
---------> that would otherwise stall under the use of scoreboarding or other earlier algorithms.
-------> Note: Robert Tomasulo received the Eckert–Mauchly Award in 1997 for his work on the algorithm.
-------> Note: It was developed by Robert Tomasulo at IBM in 1967 
---------> and was first implemented in the IBM System/360 Model 91’s floating point unit.

---> Computer graphics
-----> Computer graphics deals with generating images with the aid of computers. 
-------> Today, computer graphics is a core technology in digital photography, 
-------> film, video games, cell phone and computer displays, and many specialized applications. 
-------> A great deal of specialized hardware and software has been developed, 
-------> with the displays of most devices being driven by computer graphics hardware. 
-------> It is a vast and recently developed area of computer science. 
-------> The phrase was coined in 1960 by computer graphics researchers Verne Hudson and William Fetter of Boeing. 
-------> It is often abbreviated as CG, or typically in the context of film as computer generated imagery (CGI). 
-------> The non-artistic aspects of computer graphics are the subject of computer science research.[1]
-----> Some topics in computer graphics include user interface design, sprite graphics, rendering, ray tracing, geometry processing, 
-------> computer animation, vector graphics, 3D modeling, shaders, GPU design, implicit surfaces, visualization, scientific computing, 
-------> image processing, computational photography, scientific visualization, computational geometry and computer vision, among others. 
-------> The overall methodology depends heavily on the underlying sciences of geometry, optics, physics, and perception.
-----> Computer graphics is responsible for displaying art and image data effectively and meaningfully to the consumer. 
-------> It is also used for processing image data received from the physical world, such as photo and video content. 
-------> Computer graphics development has had a significant impact on many types of media and has revolutionized animation, movies, advertising, video games, in general. 

-----> Clipping
-------> Clipping, in the context of computer graphics, is a method to selectively enable or disable rendering operations within a defined region of interest. 
---------> Mathematically, clipping can be described using the terminology of constructive geometry. 
---------> A rendering algorithm only draws pixels in the intersection between the clip region and the scene model. 
---------> Lines and surfaces outside the view volume (aka. frustum) are removed.
-------> Clip regions are commonly specified to improve render performance. 
---------> A well-chosen clip allows the renderer to save time and energy by skipping calculations related to pixels that the user cannot see. 
---------> Pixels that will be drawn are said to be within the clip region. 
---------> Pixels that will not be drawn are outside the clip region. 
---------> More informally, pixels that will not be drawn are said to be "clipped." 

-------> Line clipping
---------> In computer graphics, line clipping is the process of removing (clipping) lines or portions of lines outside an area of interest (a viewport or view volume). 
-----------> Typically, any part of a line which is outside of the viewing area is removed.
---------> There are two common algorithms for line clipping: Cohen–Sutherland and Liang–Barsky.
---------> A line-clipping method consists of various parts. Tests are conducted on a given line segment to find out whether it lies outside the view area or volume. 
-----------> Then, intersection calculations are carried out with one or more clipping boundaries.
-----------> Determining which portion of the line is inside or outside of the clipping volume is done by processing the endpoints of the line with regards to the intersection. 

---------> Cohen–Sutherland
-----------> In computer graphics, the Cohen–Sutherland algorithm is an algorithm used for line clipping. 
-------------> The algorithm divides a two-dimensional space into 9 regions and then efficiently determines the lines 
-------------> and portions of lines that are visible in the central region of interest (the viewport).
-----------> Algorithm
-------------> The algorithm includes, excludes or partially includes the line based on whether:
---------------> Both endpoints are in the viewport region (bitwise OR of endpoints = 0000): trivial accept.
---------------> Both endpoints share at least one non-visible region, which implies that the line does not cross the visible region. 
-----------------> (bitwise AND of endpoints ≠ 0000): trivial reject.
---------------> Both endpoints are in different regions: in case of this nontrivial situation the algorithm finds one of the two points 
-----------------> that is outside the viewport region (there will be at least one point outside). 
-----------------> The intersection of the outpoint and extended viewport border is then calculated 
-----------------> (i.e. with the parametric equation for the line), and this new point replaces the outpoint. 
-----------------> The algorithm repeats until a trivial accept or reject occurs.
-------------> The numbers in the figure below are called outcodes. 
---------------> An outcode is computed for each of the two points in the line. 
---------------> The outcode will have 4 bits for two-dimensional clipping, or 6 bits in the three-dimensional case. 
---------------> The first bit is set to 1 if the point is above the viewport. 
---------------> The bits in the 2D outcode represent: top, bottom, right, left. 
---------------> For example, the outcode 1010 represents a point that is top-right of the viewport.
-----------------------------------
          | left | central | right
-----------------------------------
| top     | 1001 | 1000    | 1010
| central | 0001 | 0000    | 0010
| bottom  | 0101 | 0100    | 0110 
-----------> Note that the outcodes for endpoints must be recalculated on each iteration after the clipping occurs.
-------------> The Cohen–Sutherland algorithm can be used only on a rectangular clip window. 
-----------> Note: The algorithm was developed in 1967 during flight simulator work by Danny Cohen and Ivan Sutherland.

---------> Cyrus–Beck
-----------> In computer graphics, the Cyrus–Beck algorithm is a generalized algorithm for line clipping. 
-------------> It was designed to be more efficient than the Cohen–Sutherland algorithm, which uses repetitive clipping. 
-------------> Cyrus–Beck is a general algorithm and can be used with a convex polygon clipping window, 
-------------> unlike Cohen-Sutherland, which can be used only on a rectangular clipping area.
-----------> Here the parametric equation of a line in the view plane is
-------------> p(t) = t*p1 + (1−t) p0 
-------------> where 0 ≤ t ≤ 1.
-----------> Now to find the intersection point with the clipping window, we calculate the value of the dot product.
-------------> Let pE be a point on the clipping plane E.
-----------> Calculate n * (p(t)− pE):
-------------> if < 0, vector pointed towards interior;
-------------> if = 0, vector pointed parallel to plane containing p;
-------------> if > 0, vector pointed away from interior.
-----------> Here n stands for normal of the current clipping plane (pointed away from interior).
-----------> By this we select the point of intersection of line and clipping window where (dot product is 0) and hence clip the line. 

---------> Fast-clipping
-----------> This algorithm has similarities with Cohen–Sutherland. 
-------------> The start and end positions are classified by which portion of the 9-area grid they occupy. 
-------------> A large switch statement jumps to a specialized handler for that case. 
-------------> In contrast, Cohen–Sutherland may have to iterate several times to handle the same case.[3]

---------> Liang–Barsky
-----------> In computer graphics, the Liang–Barsky algorithm (named after You-Dong Liang and Brian A. Barsky) is a line clipping algorithm. 
-------------> The Liang–Barsky algorithm uses the parametric equation of a line and inequalities describing 
-------------> the range of the clipping window to determine the intersections between the line and the clip window. 
-------------> With these intersections it knows which portion of the line should be drawn. 
-------------> This algorithm is significantly more efficient than Cohen–Sutherland. 
-------------> The idea of the Liang–Barsky clipping algorithm is to do as much testing as possible before computing line intersections.
-----------> Consider first the usual parametric form of a straight line:
-------------> x = x0 + t*(x1 − x0) = x0 + t*Δx ,
-------------> y = y0 + t*(y1 − y0) = y0 + t*Δy .
-----------> A point is in the clip window, if
-------------> xmin ≤ x0 + t*Δx ≤ xmax
-----------> and
-------------> ymin ≤ y0 + t*Δy ≤ ymax,
-----------> which can be expressed as the 4 inequalities
-------------> t*pi ≤ qi , i = 1,2,3,4,
-----------> where
-------------> p1 = −Δx , q1 = x0 − xmin, (left) 
-------------> p2 = Δx , q2 = xmax − x0, (right) 
-------------> p3 = −Δy , q3 = y0 − ymin, (bottom) 
-------------> p4 = Δy , q4 = ymax − y0. (top) 
-----------> To compute the final line segment:
-------------> (1) A line parallel to a clipping window edge has pi=0 for that boundary.
-------------> (2) If for that i, qi < 0, then the line is completely outside and can be eliminated.
-------------> (3) When pi < 0, the line proceeds outside to inside the clip window, and when pi > 0, the line proceeds inside to outside.
-------------> (4) For nonzero pi, u = q i / p i gives t for the intersection point of the line and the window edge (possibly projected).
-------------> (5) The two actual intersections of the line with the window edges, if they exist, are described by u1 and u2, calculated as follows. 
---------------> For u1, look at boundaries for which pi < 0 (i.e. outside to inside). Take u1 to be the largest among {0 , qi / pi}. 
---------------> For u2, look at boundaries for which pi > 0 (i.e. inside to outside). Take u2 to be the minimum of {1 , qi / pi} .
-------------> (6) If u1 > u 2 , the line is entirely outside the clip window. 
---------------> If u1 < 0 < 1 < u 2  it is entirely inside it.
 
---------> Nicholl–Lee–Nicholl
-----------> In computer graphics, the Nicholl–Lee–Nicholl algorithm is a fast algorithm for line clipping 
-------------> that reduces the chances of clipping a single line segment multiple times, as may happen in the Cohen–Sutherland algorithm. 

-------> Polygon clipping

---------> Sutherland–Hodgman
-----------> The Sutherland–Hodgman algorithm is an algorithm used for clipping polygons. 
-----------> It works by extending each line of the convex clip polygon in turn and selecting only vertices from the subject polygon that are on the visible side. 

---------> Vatti
-----------> The Vatti clipping algorithm is used in computer graphics. 
-------------> It allows clipping of any number of arbitrarily shaped subject polygons by any number of arbitrarily shaped clip polygons. 
-------------> Unlike the Sutherland–Hodgman and Weiler–Atherton polygon clipping algorithms, 
-------------> the Vatti algorithm does not restrict the types of polygons that can be used as subjects or clips. 
-------------> Even complex (self-intersecting) polygons, and polygons with holes can be processed. 
-------------> The algorithm is generally applicable only in 2D space. 

---------> Weiler–Atherton
-----------> The Weiler–Atherton is a polygon-clipping algorithm. 
-------------> It is used in areas like computer graphics and games development where clipping of polygons is needed. 
-------------> It allows clipping of a subject or candidate polygon by an arbitrarily shaped clipping polygon/area/region.
-----------> It is generally applicable only in 2D. 
-------------> However, it can be used in 3D through visible surface determination and with improved efficiency through Z-ordering.

-----> Contour lines and Isosurfaces
-------> A contour line (also isoline, isopleth, or isarithm) of a function of two variables is a curve along which the function has a constant value, 
---------> so that the curve joins points of equal value.
---------> It is a plane section of the three-dimensional graph of the function f(x,y) parallel to the (x,y) plane. 
---------> More generally, a contour line for a function of two variables is a curve connecting points where the function has the same particular value.
-------> An isosurface is a three-dimensional analog of an isoline. 
---------> It is a surface that represents points of a constant value (e.g. pressure, temperature, velocity, density) within a volume of space; 
---------> in other words, it is a level set of a continuous function whose domain is 3-space.
-------> The term isoline is also sometimes used for domains of more than 3 dimensions.[1] 

-------> Marching cubes: 
---------> This extracts a polygonal mesh of an isosurface from a three-dimensional scalar field (sometimes called voxels)
---------> Marching cubes is a computer graphics algorithm, published in the 1987 SIGGRAPH proceedings by Lorensen and Cline, 
-----------> for extracting a polygonal mesh of an isosurface from a three-dimensional discrete scalar field (the elements of which are sometimes called voxels). 
-----------> The applications of this algorithm are mainly concerned with medical visualizations such as CT and MRI scan data images, 
-----------> and special effects or 3-D modelling with what is usually called metaballs or other metasurfaces. 
-----------> The marching cubes algorithm is meant to be used for 3-D; the 2-D version of this algorithm is called the marching squares algorithm. 

-------> Marching squares: 
---------> This generates contour lines for a two-dimensional scalar field
---------> In computer graphics, marching squares is an algorithm that generates contours 
-----------> for a two-dimensional scalar field (rectangular array of individual numerical values). 
-----------> A similar method can be used to contour 2D triangle meshes.
---------> The contours can be of two kinds:
-----------> Isolines – lines following a single data level, or isovalue.
-----------> Isobands – filled areas between isolines.
---------> Typical applications include the contour lines on topographic maps or the generation of isobars for weather maps.
---------> Marching squares takes a similar approach to the 3D marching cubes algorithm:
-----------> Process each cell in the grid independently.
-----------> Calculate a cell index using comparisons of the contour level(s) with the data values at the cell corners.
-----------> Use a pre-built lookup table, keyed on the cell index, to describe the output geometry for the cell.
-----------> Apply linear interpolation along the boundaries of the cell to calculate the exact contour position.
 
-------> Marching tetrahedrons: 
---------> This is an alternative to Marching cubes
---------> Marching tetrahedra is an algorithm in the field of computer graphics to render implicit surfaces. 
-----------> It clarifies a minor ambiguity problem of the marching cubes algorithm with some cube configurations. 
-----------> It was originally introduced in 1991.
---------> While the original marching cubes algorithm was protected by a software patent, 
-----------> marching tetrahedrons offered an alternative algorithm that did not require a patent license. M
-----------> ore than 20 years have passed from the patent filing date (June 5, 1985), 
-----------> and the marching cubes algorithm can now be used freely. 
-----------> Optionally, the minor improvements of marching tetrahedrons may be used to correct the aforementioned ambiguity in some configurations.
---------> In marching tetrahedra, each cube is split into six irregular tetrahedra by cutting the cube in half three times, 
-----------> cutting diagonally through each of the three pairs of opposing faces. 
-----------> In this way, the tetrahedra all share one of the main diagonals of the cube. 
-----------> Instead of the twelve edges of the cube, we now have nineteen edges: the original twelve,
-----------> six face diagonals, and the main diagonal. 
-----------> Just like in marching cubes, the intersections of these edges with the isosurface are approximated by linearly interpolating the values at the grid points.
---------> Adjacent cubes share all edges in the connecting face, including the same diagonal. 
-----------> This is an important property to prevent cracks in the rendered surface, 
-----------> because interpolation of the two distinct diagonals of a face usually gives slightly different intersection points. 
-----------> An added benefit is that up to five computed intersection points can be reused when handling the neighbor cube. 
-----------> This includes the computed surface normals and other graphics attributes at the intersection points.
---------> Each tetrahedron has sixteen possible configurations, falling into three classes: no intersection, 
-----------> intersection in one triangle and intersection in two (adjacent) triangles. 
-----------> It is straightforward to enumerate all sixteen configurations and map them to vertex index lists defining the appropriate triangle strips. 

-----> Discrete Green's Theorem: 
-------> This is an algorithm for computing double integral over a generalized rectangular domain in constant time. 
-------> It is a natural extension to the summed area table algorithm

-----> Flood fill: 
-------> This fills a connected region of a multi-dimensional array with a specified symbol
-------> Flood fill, also called seed fill, is a flooding algorithm that determines 
---------> and alters the area connected to a given node in a multi-dimensional array with some matching attribute. 
---------> It is used in the "bucket" fill tool of paint programs to fill connected, 
---------> similarly-colored areas with a different color, and in games such as Go and Minesweeper for determining which pieces are cleared. 
---------> A variant called boundary fill uses the same algorithms but is defined as the area connected to a given node that does not have a particular attribute.
-------> Note that flood filling is not suitable for drawing filled polygons, as it will miss some pixels in more acute corners. 
---------> Instead, see Even-odd rule and Nonzero-rule. 

-----> Global illumination algorithms: 
-------> This considers direct illumination and reflection from other objects.
-------> Global illumination[1] (GI), or indirect illumination, 
---------> is a group of algorithms used in 3D computer graphics that are meant to add more realistic lighting to 3D scenes. 
---------> Such algorithms take into account not only the light that comes directly from a light source (direct illumination), 
---------> but also subsequent cases in which light rays from the same source are reflected by other surfaces in the scene, 
---------> whether reflective or not (indirect illumination).
-------> Theoretically, reflections, refractions, and shadows are all examples of global illumination, 
---------> because when simulating them, one object affects the rendering of another (as opposed to an object being affected only by a direct source of light). 
---------> In practice, however, only the simulation of diffuse inter-reflection or caustics is called global illumination. 

-------> Ambient occlusion
---------> In 3D computer graphics, modeling, and animation, ambient occlusion is a shading 
-----------> and rendering technique used to calculate how exposed each point in a scene is to ambient lighting.
-----------> For example, the interior of a tube is typically more occluded (and hence darker) than the exposed outer surfaces, 
-----------> and becomes darker the deeper inside the tube one goes.
---------> Ambient occlusion can be seen as an accessibility value that is calculated for each surface point.
-----------> In scenes with open sky, this is done by estimating the amount of visible sky for each point, while in indoor environments, 
-----------> only objects within a certain radius are taken into account and the walls are assumed to be the origin of the ambient light. 
-----------> The result is a diffuse, non-directional shading effect that casts no clear shadows but that darkens enclosed 
-----------> and sheltered areas and can affect the rendered image's overall tone. It is often used as a post-processing effect.
---------> Unlike local methods such as Phong shading, ambient occlusion is a global method, 
-----------> meaning that the illumination at each point is a function of other geometry in the scene. 
-----------> However, it is a very crude approximation to full global illumination. 
-----------> The appearance achieved by ambient occlusion alone is similar to the way an object might appear on an overcast day.
---------> The first method that allowed simulating ambient occlusion in real time was developed by the research and development department of Crytek (CryEngine 2).
-----------> With the release of hardware capable of real time ray tracing (GeForce 20 series) by Nvidia in 2018, 
-----------> ray traced ambient occlusion (RTAO) became possible in games and other real time applications.
-----------> This feature was added to the Unreal Engine with version 4.22.

-------> Beam tracing
---------> Beam tracing is an algorithm to simulate wave propagation. 
-----------> It was developed in the context of computer graphics to render 3D scenes, 
-----------> but it has been also used in other similar areas such as acoustics and electromagnetism simulations.
---------> Beam tracing is a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with beams. 
-----------> Beams are shaped like unbounded pyramids, with (possibly complex) polygonal cross sections. 
-----------> Beam tracing was first proposed by Paul Heckbert and Pat Hanrahan.

-------> Cone tracing
---------> Cone tracing and beam tracing are a derivative of the ray tracing algorithm that replaces rays, which have no thickness, with thick rays. 

-------> Image-based lighting
---------> Image-based lighting (IBL) is a 3D rendering technique which involves capturing an omnidirectional representation
-----------> of real-world light information as an image, typically using a 360° camera. 
-----------> This image is then projected onto a dome or sphere analogously to environment mapping, 
-----------> and this is used to simulate the lighting for the objects in the scene. 
-----------> This allows highly detailed real-world lighting to be used to light a scene, 
-----------> instead of trying to accurately model illumination using an existing rendering technique.
-----------> Image-based lighting often uses high-dynamic-range imaging for greater realism, though this is not universal.
--------> According to Fxguide, "Almost all modern rendering software offers some type of image-based lighting, though the exact terminology used in the system may vary."
---------> Motion picture production makes use of image-based lighting, and it can be seen in movies like Monsters University, The Great Gatsby, and Iron Man 2.

-------> Metropolis light transport
---------> Metropolis light transport (MLT) is a global illumination application of a variant of the Monte Carlo method 
-----------> called the Metropolis–Hastings algorithm to the rendering equation for generating images from detailed physical descriptions of three-dimensional scenes.
---------> The procedure constructs paths from the eye to a light source using bidirectional path tracing, then constructs slight modifications to the path. 
-----------> Some careful statistical calculation (the Metropolis algorithm) is used to compute the appropriate distribution of brightness over the image. 
-----------> This procedure has the advantage, relative to bidirectional path tracing, that once a path has been found from light to eye, 
-----------> the algorithm can then explore nearby paths; thus difficult-to-find light paths can be explored more thoroughly with the same number of simulated photons. 
-----------> In short, the algorithm generates a path and stores the path's 'nodes' in a list. It can then modify the path by adding extra nodes and creating a new light path. 
-----------> While creating this new path, the algorithm decides how many new 'nodes' to add and whether or not these new nodes will actually create a new path.
---------> Metropolis light transport is an unbiased method that, in some cases (but not always), 
-----------> converges to a solution of the rendering equation faster than other unbiased algorithms such as path tracing or bidirectional path tracing.[citation needed]
---------> Energy Redistribution Path Tracing (ERPT) uses Metropolis sampling-like mutation strategies instead of an intermediate probability distribution step.

-------> Path tracing
---------> Path tracing is a computer graphics Monte Carlo method of rendering images of three-dimensional scenes such that the global illumination is faithful to reality. 
-----------> Fundamentally, the algorithm is integrating over all the illuminance arriving to a single point on the surface of an object. 
-----------> This illuminance is then reduced by a surface reflectance function (BRDF) to determine how much of it will go towards the viewpoint camera. 
-----------> This integration procedure is repeated for every pixel in the output image. 
-----------> When combined with physically accurate models of surfaces, accurate models of real light sources (light bulbs), 
-----------> and optically correct cameras, path tracing can produce still images that are indistinguishable from photographs. 

-------> Photon mapping
---------> In computer graphics, photon mapping is a two-pass global illumination rendering algorithm developed by Henrik Wann Jensen between 1995 
-----------> and 2001 that approximately solves the rendering equation for integrating light radiance at a given point in space. 
-----------> Rays from the light source (like photons) and rays from the camera are traced independently until some termination criterion is met, 
-----------> then they are connected in a second step to produce a radiance value. 
-----------> The algorithm is used to realistically simulate the interaction of light with different types of objects (similar to other photorealistic rendering techniques). 
-----------> Specifically, it is capable of simulating the refraction of light through a transparent substance such as glass or water (including caustics), 
-----------> diffuse interreflection between illuminated objects, the subsurface scattering of light in translucent materials, 
-----------> and some of the effects caused by particulate matter such as smoke or water vapor.
-----------> Photon mapping can also be extended to more accurate simulations of light, such as spectral rendering. 
-----------> Progressive photon mapping (PPM) starts with ray tracing and then adds more and more photon mapping passes to provide a progressively more accurate render.
---------> Unlike path tracing, bidirectional path tracing, volumetric path tracing, and Metropolis light transport, .
-----------> photon mapping is a "biased" rendering algorithm, which means that averaging infinitely many renders of the same scene 
-----------> using this method does not converge to a correct solution to the rendering equation. 
-----------> However, it is a consistent method, and the accuracy of a render can be increased by increasing the number of photons. 
-----------> As the number of photons approaches infinity, a render will get closer and closer to the solution of the rendering equation. 

-------> Radiosity
---------> In 3D computer graphics, radiosity is an application of the finite element method to solving the rendering equation for scenes with surfaces that reflect light diffusely. 
-----------> Unlike rendering methods that use Monte Carlo algorithms (such as path tracing), 
-----------> which handle all types of light paths, typical radiosity only account for paths (represented by the code "LD*E") 
-----------> which leave a light source and are reflected diffusely some number of times (possibly zero) before hitting the eye. 
-----------> Radiosity is a global illumination algorithm in the sense that the illumination arriving 
-----------> on a surface comes not just directly from the light sources, but also from other surfaces reflecting light. 
-----------> Radiosity is viewpoint independent, which increases the calculations involved, but makes them useful for all viewpoints.
-----------> 
-----------> Radiosity methods were first developed in about 1950 in the engineering field of heat transfer. 
-----------> They were later refined specifically for the problem of rendering computer graphics in 1984 by researchers at Cornell University[2] and Hiroshima University.[3]
-----------> 
-----------> Notable commercial radiosity engines are Enlighten by Geomerics (used for games including Battlefield 3 and Need for Speed: The Run);
-----------> 3ds Max; form•Z; LightWave 3D and the Electric Image Animation System. 

-------> Ray tracing
---------> In 3D computer graphics, ray tracing is a technique for modeling light transport for use in a wide variety of rendering algorithms for generating digital images.
---------> On a spectrum of computational cost and visual fidelity, ray tracing-based rendering techniques, 
-----------> such as ray casting, recursive ray tracing, distribution ray tracing, photon mapping and path tracing, 
-----------> are generally slower and higher fidelity than scanline rendering methods.
-----------> Thus, ray tracing was first deployed in applications where taking a relatively long time to render could be tolerated, 
-----------> such as in still computer-generated images, and film and television visual effects (VFX), 
-----------> but was less suited to real-time applications such as video games, where speed is critical in rendering each frame.
---------> Since 2018, however, hardware acceleration for real-time ray tracing has become standard on new commercial graphics cards, 
-----------> and graphics APIs have followed suit, allowing developers to use hybrid ray tracing and rasterization-based rendering in games 
-----------> and other real-time applications with a lesser hit to frame render times.
---------> Ray tracing is capable of simulating a variety of optical effects, such as reflection, refraction, soft shadows, scattering, depth of field, 
-----------> motion blur, caustics, ambient occlusion and dispersion phenomena (such as chromatic aberration). 
-----------> It can also be used to trace the path of sound waves in a similar fashion to light waves, 
-----------> making it a viable option for more immersive sound design in video games by rendering realistic reverberation and echoes. 
-----------> In fact, any physical wave or particle phenomenon with approximately linear motion can be simulated with ray tracing.
---------> Ray tracing-based rendering techniques that involve sampling light over a domain generate image noise artifacts 
-----------> that can be addressed by tracing a very large number of rays or using denoising techniques. 


-----> Hidden-surface removal or Visual surface determination
-------> In 3D computer graphics, hidden-surface determination 
---------> (also known as shown-surface determination, hidden-surface removal (HSR), occlusion culling (OC) or visible-surface determination (VSD)) 
---------> is the process of identifying what surfaces and parts of surfaces can be seen from a particular viewing angle. 
---------> A hidden-surface determination algorithm is a solution to the visibility problem, 
---------> which was one of the first major problems in the field of 3D computer graphics. 
---------> The process of hidden-surface determination is sometimes called hiding, and such an algorithm is sometimes called a hider. 
---------> When referring to line rendering it is known as hidden-line removal. 
---------> Hidden-surface determination is necessary to render a scene correctly,
---------> so that one may not view features hidden behind the model itself, allowing only the naturally viewable portion of the graphic to be visible. 

-------> Newell's algorithm: 
---------> This eliminates polygon cycles in the depth sorting required in hidden-surface removal
---------> Newell's Algorithm is a 3D computer graphics procedure for elimination of polygon cycles in the depth sorting required in hidden surface removal. 
-----------> It was proposed in 1972 by brothers Martin Newell and Dick Newell, and Tom Sancha, while all three were working at CADCentre.
---------> In the depth sorting phase of hidden surface removal, 
-----------> if two polygons have no overlapping extents or extreme minimum and maximum values in the x, y, and z directions, then they can be easily sorted. 
-----------> If two polygons, Q and P, do have overlapping extents in the Z direction, then it is possible that cutting is necessary.
---------> In that case Newell's algorithm tests the following:
-----------> Test for Z overlap; implied in the selection of the face Q from the sort list
-----------> The extreme coordinate values in X of the two faces do not overlap (minimax test in X)
-----------> The extreme coordinate values in Y of the two faces do not overlap (minimax test in Y)
-----------> All vertices of P lie deeper than the plane of Q
-----------> All vertices of Q lie closer to the viewpoint than the plane of P
-----------> The rasterisation of P and Q do not overlap
---------> The tests are given in order of increasing computational difficulty. 
-----------> The polygons must be planar. 
-----------> If the tests are all false, then switch the order of P and Q in the sort, record having done so, and try again. 
-----------> If there is an attempt to switch the order of a polygon a second time, there is a visibility cycle, and the polygons must be split. 
-----------> Splitting is accomplished by selecting one polygon and cutting it along the line of intersection with the other polygon.
-----------> The above tests are again performed, and the algorithm continues until all polygons pass the above tests. 

-------> Painter's algorithm: 
---------> This detects visible parts of a 3-dimensional scenery
---------> The painter’s algorithm (also depth-sort algorithm and priority fill) 
-----------> is an algorithm for visible surface determination in 3D computer graphics that works on a polygon-by-polygon basis rather than a pixel-by-pixel, 
-----------> row by row, or area by area basis of other Hidden Surface Removal algorithms.
-----------> The painter’s algorithm creates images by sorting the polygons within the image by their depth 
-----------> and placing each polygon in order from the farthest to the closest object.

-------> Scanline rendering: 
---------> This constructs an image by moving an imaginary line over the image.
---------> Scanline rendering (also scan line rendering and scan-line rendering) is an algorithm for visible surface determination, 
-----------> in 3D computer graphics, that works on a row-by-row basis rather than a polygon-by-polygon or pixel-by-pixel basis. 
-----------> All of the polygons to be rendered are first sorted by the top y coordinate at which they first appear, 
-----------> then each row or scan line of the image is computed using the intersection of a scanline with the polygons on the front of the sorted list, 
-----------> while the sorted list is updated to discard no-longer-visible polygons as the active scan line is advanced down the picture.
---------> The main advantage of this method is that sorting vertices along the normal of the scanning plane reduces the number of comparisons between edges. 
-----------> Another advantage is that it is not necessary to translate the coordinates of all vertices from the main memory 
-----------> into the working memory—only vertices defining edges that intersect the current scan line need to be in active memory, and each vertex is read in only once. 
-----------> The main memory is often very slow compared to the link between the central processing unit and cache memory, 
-----------> and thus avoiding re-accessing vertices in main memory can provide a substantial speedup.
---------> This kind of algorithm can be easily integrated with many other graphics techniques, such as the Phong reflection model or the Z-buffer algorithm. 

-------> Warnock algorithm
---------> The Warnock algorithm is a hidden surface algorithm invented by John Warnock that is typically used in the field of computer graphics.
-----------> It solves the problem of rendering a complicated image by recursive subdivision of a scene until areas are obtained that are trivial to compute. 
-----------> In other words, if the scene is simple enough to compute efficiently then it is rendered; 
-----------> otherwise it is divided into smaller parts which are likewise tested for simplicity.
---------> This is a divide and conquer algorithm with run-time of  O(np)[dubious – discuss], 
-----------> where n is the number of polygons and p is the number of pixels in the viewport.
---------> The inputs are a list of polygons and a viewport. The best case is that if the list of polygons is simple, then draw the polygons in the viewport. 
-----------> Simple is defined as one polygon (then the polygon or its part is drawn in appropriate part of a viewport) 
-----------> or a viewport that is one pixel in size (then that pixel gets a color of the polygon closest to the observer). 
-----------> The continuous step is to split the viewport into 4 equally sized quadrants and to recursively call the algorithm for each quadrant, 
-----------> with a polygon list modified such that it only contains polygons that are visible in that quadrant.
---------> Warnock expressed his algorithm in words and pictures, rather than software code, as the core of his PhD thesis,
-----------> which also described protocols for shading oblique surfaces and other features that are now the core of 3-dimensional computer graphics. 
-----------> The entire thesis was only 26 pages from Introduction to Bibliography. 

-----> Line Drawing: 
-------> This is graphical algorithm for approximating a line segment on discrete graphical media.
-------> In computer graphics, a line drawing algorithm is an algorithm for approximating a line segment 
---------> on discrete graphical media, such as pixel-based displays and printers. 
---------> On such media, line drawing requires an approximation (in nontrivial cases). 
---------> Basic algorithms rasterize lines in one color. 
---------> A better representation with multiple color gradations requires an advanced process, spatial anti-aliasing.
-------> On continuous media, by contrast, no algorithm is necessary to draw a line. 
---------> For example, cathode-ray oscilloscopes use analog phenomena to draw lines and curves. 

-------> Bresenham's line algorithm: 
---------> This plots points of a 2-dimensional array to form a straight line between 2 specified points (uses decision variables)
---------> Bresenham's line algorithm is a line drawing algorithm that determines the points of an n-dimensional raster 
-----------> that should be selected in order to form a close approximation to a straight line between two points. 
-----------> It is commonly used to draw line primitives in a bitmap image (e.g. on a computer screen), as it uses only integer addition, 
-----------> subtraction and bit shifting, all of which are very cheap operations in commonly used computer instruction sets such as x86_64. 
-----------> It is an incremental error algorithm. 
-----------> It is one of the earliest algorithms developed in the field of computer graphics. 
-----------> An extension to the original algorithm may be used for drawing circles.
---------> While algorithms such as Wu's algorithm are also frequently used in modern computer graphics because they can support antialiasing, 
-----------> the speed and simplicity of Bresenham's line algorithm means that it is still important. 
-----------> The algorithm is used in hardware such as plotters and in the graphics chips of modern graphics cards. 
-----------> It can also be found in many software graphics libraries. 
-----------> Because the algorithm is very simple, it is often implemented in either the firmware or the graphics hardware of modern graphics cards.
---------> The label "Bresenham" is used today for a family of algorithms extending or modifying Bresenham's original algorithm. 

-------> DDA line algorithm: 
---------> This plots points of a 2-dimensional array to form a straight line between 2 specified points (uses floating-point math)
---------> In computer graphics, a digital differential analyzer (DDA) is hardware or software used for interpolation of variables over an interval between start and end point. 
-----------> DDAs are used for rasterization of lines, triangles and polygons. 
-----------> They can be extended to non linear functions, such as perspective correct texture mapping, quadratic curves, and traversing voxels.
---------> In its simplest implementation for linear cases such as lines, 
-----------> the DDA algorithm interpolates values in interval by computing for each xi the equations xi = xi−1 + 1, yi = yi−1 + m, 
-----------> where m is the slope of the line. This slope can be expressed in DDA as follows:
-------------> m = (yend − ystart) / (xend − xstart) 
---------> In fact any two consecutive points (x,y) lying on this line segment should satisfy the equation. 

-------> Xiaolin Wu's line algorithm: 
---------> This algorithm for line antialiasing.
---------> Xiaolin Wu's line algorithm is an algorithm for line antialiasing.
---------> In digital signal processing, spatial anti-aliasing is a technique 
-----------> for minimizing the distortion artifacts (aliasing) when representing a high-resolution image at a lower resolution. 
-----------> Anti-aliasing is used in digital photography, computer graphics, digital audio, and many other applications.
---------> Anti-aliasing means removing signal components that have a higher frequency than is able to be properly resolved by the recording (or sampling) device. 
-----------> This removal is done before (re)sampling at a lower resolution. 
-----------> When sampling is performed without removing this part of the signal, it causes undesirable artifacts such as black-and-white noise. 

 
-----> Midpoint circle algorithm: 
---------> This an algorithm used to determine the points needed for drawing a circle
---------> In computer graphics, the midpoint circle algorithm is an algorithm used to determine the points needed for rasterizing a circle. 
-----------> Bresenham's circle algorithm is derived from the midpoint circle algorithm.[citation needed] The algorithm can be generalized to conic sections.[1]
---------> The algorithm is related to work by Pitteway and Van Aken.

-----> Ramer–Douglas–Peucker algorithm: 
---------> Given a 'curve' composed of line segments to find a curve not too dissimilar but that has fewer points
---------> The Ramer–Douglas–Peucker algorithm, also known as the Douglas–Peucker algorithm and iterative end-point fit algorithm, 
-----------> is an algorithm that decimates a curve composed of line segments to a similar curve with fewer points. 
-----------> It was one of the earliest successful algorithms developed for cartographic generalization. 

-----> Shading
-------> Shading refers to the depiction of depth perception in 3D models (within the field of 3D computer graphics) 
---------> or illustrations (in visual art) by varying the level of darkness.
---------> Shading tries to approximate local behavior of light on the object's surface and is not to be confused with techniques of adding shadows, 
---------> such as shadow mapping or shadow volumes, which fall under global behavior of light

-------> Gouraud shading: 
---------> This is an algorithm to simulate the differing effects of light and colour across the surface of an object in 3D computer graphics
---------> Gouraud shading, named after Henri Gouraud, is an interpolation method used in computer graphics to produce continuous shading of surfaces represented by polygon meshes. 
-----------> In practice, Gouraud shading is most often used to achieve continuous lighting on triangle meshes 
-----------> by computing the lighting at the corners of each triangle and linearly interpolating the resulting colours for each pixel covered by the triangle.
-----------> Gouraud first published the technique in 1971.

-------> Phong shading: 
---------> This is an algorithm to interpolate surface normal-vectors for surface shading in 3D computer graphics
---------> In 3D computer graphics, Phong shading, Phong interpolation, or normal-vector interpolation shading is an interpolation technique 
-----------> for surface shading invented by computer graphics pioneer Bui Tuong Phong. 
-----------> Phong shading interpolates surface normals across rasterized polygons and computes pixel colors based on the interpolated normals and a reflection model. 
-----------> Phong shading may also refer to the specific combination of Phong interpolation and the Phong reflection model. 

-----> Slerp (spherical linear interpolation): 
---------> This quaternion interpolation for the purpose of animating 3D rotation
---------> In computer graphics, Slerp is shorthand for spherical linear interpolation, 
-----------> introduced by Ken Shoemake in the context of quaternion interpolation for the purpose of animating 3D rotation. 
-----------> It refers to constant-speed motion along a unit-radius great circle arc, given the ends and an interpolation parameter between 0 and 1. 

-----> Summed area table (also known as an integral image): 
---------> This is an algorithm for computing the sum of values in a rectangular subset of a grid in constant time
---------> A summed-area table is a data structure and algorithm for quickly and efficiently generating the sum of values in a rectangular subset of a grid. 
-----------> In the image processing domain, it is also known as an integral image. It was introduced to computer graphics in 1984 by Frank Crow for use with mipmaps. 
-----------> In computer vision it was popularized by Lewis[1] and then given the name "integral image" and prominently used within the Viola–Jones object detection framework in 2001. 
-----------> Historically, this principle is very well known in the study of multi-dimensional probability distribution functions, 
-----------> namely in computing 2D (or ND) probabilities (area under the probability distribution) from the respective cumulative distribution functions.



---> Cryptography
-----> Cryptography, or cryptology, is the practice and study of techniques for secure communication in the presence of adversarial behavior.
-------> More generally, cryptography is about constructing and analyzing protocols that prevent third parties or the public from reading private messages;
-------> various aspects of information security such as data confidentiality, data integrity, authentication, and non-repudiation are central to modern cryptography.
-------> Modern cryptography exists at the intersection of the disciplines of mathematics, computer science, electrical engineering, communication science, and physics. 
-------> Applications of cryptography include electronic commerce, chip-based payment cards, digital currencies, computer passwords, and military communications. 

-----> Asymmetric (public key) encryption:
-------> Public-key cryptography, or asymmetric cryptography, is a cryptographic system that uses pairs of keys. 
---------> Each pair consists of a public key (which may be known to others) and a private key (which may not be known by anyone except the owner).
---------> The generation of such key pairs depends on cryptographic algorithms which are based on mathematical problems termed one-way functions. 
---------> Effective security requires keeping the private key private; the public key can be openly distributed without compromising security. 

-------> ElGamal
---------> In cryptography, the ElGamal encryption system is an asymmetric key encryption algorithm for public-key cryptography which is based on the Diffie–Hellman key exchange. 
-----------> It was described by Taher Elgamal in 1985.[1] ElGamal encryption is used in the free GNU Privacy Guard software, recent versions of PGP, and other cryptosystems. 
-----------> The Digital Signature Algorithm (DSA) is a variant of the ElGamal signature scheme, which should not be confused with ElGamal encryption.
---------> ElGamal encryption can be defined over any cyclic group G, like multiplicative group of integers modulo n. 
-----------> Its security depends upon the difficulty of a certain problem in G related to computing discrete logarithms. 

-----> Elliptic curve cryptography
-------> Elliptic-curve cryptography (ECC) is an approach to public-key cryptography based on the algebraic structure of elliptic curves over finite fields. 
---------> ECC allows smaller keys compared to non-EC cryptography (based on plain Galois fields) to provide equivalent security.[1]
-------> Elliptic curves are applicable for key agreement, digital signatures, pseudo-random generators and other tasks. 
---------> Indirectly, they can be used for encryption by combining the key agreement with a symmetric encryption scheme. 
---------> Elliptic curves are also used in several integer factorization algorithms based on elliptic curves 
---------> that have applications in cryptography, such as Lenstra elliptic-curve factorization. 

-------> MAE1
---------> Matei Array Encreption

-------> NTRUEncrypt

---------> The NTRUEncrypt public key cryptosystem, also known as the NTRU encryption algorithm, 
-----------> is an NTRU lattice-based alternative to RSA and elliptic curve cryptography (ECC) and is based 
-----------> on the shortest vector problem in a lattice (which is not known to be breakable using quantum computers). 
---------> It relies on the presumed difficulty of factoring certain polynomials in a truncated polynomial ring 
-----------> into a quotient of two polynomials having very small coefficients. 
-----------> Breaking the cryptosystem is strongly related, though not equivalent, 
-----------> to the algorithmic problem of lattice reduction in certain lattices. 
-----------> Careful choice of parameters is necessary to thwart some published attacks.
---------> Since both encryption and decryption use only simple polynomial multiplication, 
-----------> these operations are very fast compared to other asymmetric encryption schemes, 
-----------> such as RSA, ElGamal and elliptic curve cryptography. However, 
-----------> NTRUEncrypt has not yet undergone a comparable amount of cryptographic analysis in deployed form.
---------> A related algorithm is the NTRUSign digital signature algorithm.
---------> Specifically, NTRU operations are based on objects in a truncated polynomial ring   R = Z [ X ] / ( X N − 1 ) with convolution multiplication 
------------->     a = a 0 + a 1 X + a 2 X 2 + ⋯ + a N − 2 X N − 2 + a N − 1 X N − 1 
---------> NTRU is actually a parameterised family of cryptosystems; 
-----------> each system is specified by three integer parameters (N, p, q) which represent the maximal degree N-1 for all polynomials in the truncated ring R, 
-----------> a small modulus and a large modulus, respectively, where it is assumed that N is prime, 
-----------> q is always larger than p, and p and q are coprime; and four sets of polynomials   L f , L g , L m  and  L r 
-----------> (a polynomial part of the private key, a polynomial for generation of the public key, 
-----------> the message and a blinding value, respectively), all of degree at most \ N-1 . 

-------> RSA
---------> RSA (Rivest–Shamir–Adleman) is a public-key cryptosystem that is widely used for secure data transmission. 
-----------> It is also one of the oldest. The acronym "RSA" comes from the surnames of Ron Rivest, Adi Shamir and Leonard Adleman, who publicly described the algorithm in 1977. 
-----------> An equivalent system was developed secretly in 1973 at GCHQ (the British signals intelligence agency) by the English mathematician Clifford Cocks. 
-----------> That system was declassified in 1997.[1]
---------> In a public-key cryptosystem, the encryption key is public and distinct from the decryption key, which is kept secret (private). 
-----------> An RSA user creates and publishes a public key based on two large prime numbers, along with an auxiliary value. 
-----------> The prime numbers are kept secret. 
-----------> Messages can be encrypted by anyone, via the public key, but can only be decoded by someone who knows the prime numbers.
---------> The security of RSA relies on the practical difficulty of factoring the product of two large prime numbers, the "factoring problem". 
-----------> Breaking RSA encryption is known as the RSA problem. Whether it is as difficult as the factoring problem is an open question. 
-----------> There are no published methods to defeat the system if a large enough key is used.
---------> RSA is a relatively slow algorithm. Because of this, it is not commonly used to directly encrypt user data. 
-----------> More often, RSA is used to transmit shared keys for symmetric-key cryptography, which are then used for bulk encryption–decryption. 



-----> Digital signatures (asymmetric authentication):
-------> A digital signature is a mathematical scheme for verifying the authenticity of digital messages or documents. 
---------> A valid digital signature, where the prerequisites are satisfied, 
---------> gives a recipient very high confidence that the message was created by a known sender (authenticity), 
---------> and that the message was not altered in transit (integrity).[1]
-------> Digital signatures are a standard element of most cryptographic protocol suites, 
---------> and are commonly used for software distribution, financial transactions, contract management software, 
---------> and in other cases where it is important to detect forgery or tampering. 

-------> DSA, and its variants:
---------> The Digital Signature Algorithm (DSA) is a Federal Information Processing Standard for digital signatures, 
-----------> based on the mathematical concept of modular exponentiation and the discrete logarithm problem. 
-----------> DSA is a variant of the Schnorr and ElGamal signature schemes.
---------> The National Institute of Standards and Technology (NIST) proposed DSA for use in their Digital Signature Standard (DSS) in 1991, 
-----------> and adopted it as FIPS 186 in 1994.[2] Four revisions to the initial specification have been released. 
-----------> The newest specification is: FIPS 186-4 from July 2013.
-----------> DSA is patented but NIST has made this patent available worldwide royalty-free. 
-----------> A draft version of the specification FIPS 186-5 indicates DSA will no longer be approved for digital signature generation, 
-----------> but may be used to verify signatures generated prior to the implementation date of that standard. 

---------> ECDSA and Deterministic ECDSA
-----------> In cryptography, the Elliptic Curve Digital Signature Algorithm (ECDSA) offers a variant of the Digital Signature Algorithm (DSA) which uses elliptic-curve cryptography. 

---------> EdDSA (Ed25519)
-----------> In public-key cryptography, Edwards-curve Digital Signature Algorithm (EdDSA) is a digital signature scheme using a variant of Schnorr signature based on twisted Edwards curves.
-------------> It is designed to be faster than existing digital signature schemes without sacrificing security. 
-------------> It was developed by a team including Daniel J. Bernstein, Niels Duif, Tanja Lange, Peter Schwabe, and Bo-Yin Yang.
-------------> The reference implementation is public domain software.[3]


-------> RSA
---------> RSA (Rivest–Shamir–Adleman) is a public-key cryptosystem that is widely used for secure data transmission. 
-----------> It is also one of the oldest. The acronym "RSA" comes from the surnames of Ron Rivest, Adi Shamir and Leonard Adleman, who publicly described the algorithm in 1977.
-----------> An equivalent system was developed secretly in 1973 at GCHQ (the British signals intelligence agency) by the English mathematician Clifford Cocks. 
-----------> That system was declassified in 1997.
---------> In a public-key cryptosystem, the encryption key is public and distinct from the decryption key, which is kept secret (private). 
-----------> An RSA user creates and publishes a public key based on two large prime numbers, along with an auxiliary value. 
---------> The prime numbers are kept secret. Messages can be encrypted by anyone, via the public key, but can only be decoded by someone who knows the prime numbers.[2]
---------> The security of RSA relies on the practical difficulty of factoring the product of two large prime numbers, the "factoring problem". 
-----------> Breaking RSA encryption is known as the RSA problem.
-----------> Whether it is as difficult as the factoring problem is an open question.
-----------> There are no published methods to defeat the system if a large enough key is used.
---------> RSA is a relatively slow algorithm. Because of this, it is not commonly used to directly encrypt user data. 
-----------> More often, RSA is used to transmit shared keys for symmetric-key cryptography, which are then used for bulk encryption–decryption. 

-----> Cryptographic hash functions (see also the section on message authentication codes):

-------> A cryptographic hash function (CHF) is a mathematical algorithm that maps data of an arbitrary size (often called the "message") 
---------> to a bit array of a fixed size (the "hash value", "hash", or "message digest"). 
---------> It is a one-way function, that is, a function for which it is practically infeasible to invert or reverse the computation.
---------> Ideally, the only way to find a message that produces a given hash is to attempt a brute-force search 
---------> of possible inputs to see if they produce a match, or use a rainbow table of matched hashes. 
---------> Cryptographic hash functions are a basic tool of modern cryptography.
-------> A cryptographic hash function must be deterministic, meaning that the same message always results in the same hash. 
---------> Ideally it should also have the following properties:
-----------> it is quick to compute the hash value for any given message
-----------> it is infeasible to generate a message that yields a given hash value (i.e. to reverse the process that generated the given hash value)
-----------> it is infeasible to find two different messages with the same hash value
-----------> a small change to a message should change the hash value so extensively that a new hash value appears uncorrelated with the old hash value (avalanche effect)[2]
-------> Cryptographic hash functions have many information-security applications, 
---------> notably in digital signatures, message authentication codes (MACs), and other forms of authentication. 
---------> They can also be used as ordinary hash functions, to index data in hash tables, for fingerprinting, 
---------> to detect duplicate data or uniquely identify files, and as checksums to detect accidental data corruption. 
---------> Indeed, in information-security contexts, cryptographic hash values are sometimes called (digital) fingerprints, checksums, or just hash values, 
---------> even though all these terms stand for more general functions with rather different properties and purposes.

-------> BLAKE
---------> BLAKE is a cryptographic hash function based on Daniel J. Bernstein's ChaCha stream cipher, 
-----------> but a permuted copy of the input block, XORed with round constants, is added before each ChaCha round. 
-----------> Like SHA-2, there are two variants differing in the word size. ChaCha operates on a 4×4 array of words. 
-----------> BLAKE repeatedly combines an 8-word hash value with 16 message words, truncating the ChaCha result to obtain the next hash value. 
-----------> BLAKE-256 and BLAKE-224 use 32-bit words and produce digest sizes of 256 bits and 224 bits, respectively, 
-----------> while BLAKE-512 and BLAKE-384 use 64-bit words and produce digest sizes of 512 bits and 384 bits, respectively.
---------> The BLAKE2 hash function, based on BLAKE, was announced in 2012. 
-----------> The BLAKE3 hash function, based on BLAKE2, was announced in 2020. 

-------> MD5
---------> Notes that there is now a method of generating collisions for MD5
---------> The MD5 message-digest algorithm is a cryptographically broken but still widely used hash function producing a 128-bit hash value. 
-----------> Although MD5 was initially designed to be used as a cryptographic hash function, it has been found to suffer from extensive vulnerabilities. 
-----------> It can still be used as a checksum to verify data integrity, but only against unintentional corruption. 
-----------> It remains suitable for other non-cryptographic purposes, for example for determining the partition for a particular key in a partitioned database, 
-----------> and may be preferred due to lower computational requirements than more recent Secure Hash Algorithms.
---------> MD5 was designed by Ronald Rivest in 1991 to replace an earlier hash function MD4, and was specified in 1992 as RFC 1321.
---------> One basic requirement of any cryptographic hash function is that it should be computationally infeasible to find two distinct messages that hash to the same value. 
-----------> MD5 fails this requirement catastrophically; such collisions can be found in seconds on an ordinary home computer.
---------> On 31 December 2008, the CMU Software Engineering Institute concluded that MD5 was essentially "cryptographically broken and unsuitable for further use". 
-----------> The weaknesses of MD5 have been exploited in the field, most infamously by the Flame malware in 2012. 
-----------> As of 2019, MD5 continues to be widely used, despite its well-documented weaknesses and deprecation by security experts.

-------> RIPEMD-160
---------> RIPEMD (RIPE Message Digest) is a family of cryptographic hash functions developed in 1992 (the original RIPEMD) and 1996 (other variants). 
-----------> There are five functions in the family: RIPEMD, RIPEMD-128, RIPEMD-160, RIPEMD-256, and RIPEMD-320, of which RIPEMD-160 is the most common.
---------> The original RIPEMD, as well as RIPEMD-128, is not considered secure because 128-bit result is too small and also (for the original RIPEMD) because of design weaknesses. 
-----------> The 256- and 320-bit versions of RIPEMD provide the same level of security as RIPEMD-128 and RIPEMD-160, respectively; 
-----------> they are designed for applications where the security level is sufficient but longer hash result is necessary.
---------> While RIPEMD functions are less popular than SHA-1 and SHA-2, they are used, among others, in Bitcoin and other cryptocurrencies based on Bitcoin. 

-------> SHA-1
---------> Note that there is now a method of generating collisions for SHA-1
---------> In cryptography, SHA-1 (Secure Hash Algorithm 1) is a cryptographically broken but still widely used hash function 
-----------> which takes an input and produces a 160-bit (20-byte) hash value known as a message digest 
-----------> typically rendered as 40 hexadecimal digits. 
-----------> It was designed by the United States National Security Agency, and is a U.S. Federal Information Processing Standard.[10]
---------> Since 2005, SHA-1 has not been considered secure against well-funded opponents;
-----------> as of 2010 many organizations have recommended its replacement.
-----------> NIST formally deprecated use of SHA-1 in 2011 and disallowed its use for digital signatures in 2013. 
-----------> As of 2020, chosen-prefix attacks against SHA-1 are practical.
-----------> As such, it is recommended to remove SHA-1 from products as soon as possible and instead use SHA-2 or SHA-3. 
-----------> Replacing SHA-1 is urgent where it is used for digital signatures.
---------> All major web browser vendors ceased acceptance of SHA-1 SSL certificates in 2017.
-----------> In February 2017, CWI Amsterdam and Google announced they had performed a collision attack against SHA-1, 
-----------> publishing two dissimilar PDF files which produced the same SHA-1 hash.
-----------> However, SHA-1 is still secure for HMAC.
---------> Microsoft has discontinued SHA-1 code signing support for Windows Update on August 7, 2020. 

-------> SHA-2 (SHA-224, SHA-256, SHA-384, SHA-512)
---------> SHA-2 (Secure Hash Algorithm 2) is a set of cryptographic hash functions designed by the United States National Security Agency (NSA) and first published in 2001.
-----------> They are built using the Merkle–Damgård construction, from a one-way compression function itself built using the Davies–Meyer structure from a specialized block cipher.
---------> SHA-2 includes significant changes from its predecessor, SHA-1. 
-----------> The SHA-2 family consists of six hash functions with digests (hash values) that are 224, 256, 384 or 512 bits:[5] SHA-224, SHA-256, SHA-384, SHA-512, SHA-512/224, SHA-512/256. 
-----------> SHA-256 and SHA-512 are novel hash functions computed with eight 32-bit and 64-bit words, respectively. 
-----------> They use different shift amounts and additive constants, 
-----------> but their structures are otherwise virtually identical, 
-----------> differing only in the number of rounds. 
-----------> SHA-224 and SHA-384 are truncated versions of SHA-256 and SHA-512 respectively, computed with different initial values. 
-----------> SHA-512/224 and SHA-512/256 are also truncated versions of SHA-512, 
-----------> but the initial values are generated using the method described in Federal Information Processing Standards (FIPS) PUB 180-4.
---------> SHA-2 was first published by the National Institute of Standards and Technology (NIST) as a U.S. federal standard (FIPS). 
-----------> The SHA-2 family of algorithms are patented in US.[6] The United States has released the patent under a royalty-free license.
---------> As of 2011, the best public attacks break preimage resistance for 52 out of 64 rounds of SHA-256 or 57 out of 80 rounds of SHA-512, 
-----------> and collision resistance for 46 out of 64 rounds of SHA-256.

-------> SHA-3 (SHA3-224, SHA3-256, SHA3-384, SHA3-512, SHAKE128, SHAKE256)
---------> SHA-3 (Secure Hash Algorithm 3) is the latest member of the Secure Hash Algorithm family of standards, 
-----------> released by NIST on August 5, 2015.
-----------> Although part of the same series of standards, SHA-3 is internally different from the MD5-like structure of SHA-1 and SHA-2.
---------> SHA-3 is a subset of the broader cryptographic primitive family Keccak designed by Guido Bertoni, Joan Daemen, Michaël Peeters, and Gilles Van Assche, building upon RadioGatún. 
-----------> Keccak's authors have proposed additional uses for the function, not (yet) standardized by NIST, including a stream cipher, 
-----------> an authenticated encryption system, a "tree" hashing scheme for faster hashing on certain architectures, and AEAD ciphers Keyak and Ketje.[11][12]
---------> Keccak is based on a novel approach called sponge construction.
-----------> Sponge construction is based on a wide random function or random permutation, and allows inputting ("absorbing" in sponge terminology) any amount of data, 
-----------> and outputting ("squeezing") any amount of data, while acting as a pseudorandom function with regard to all previous inputs. 
-----------> This leads to great flexibility.
---------> NIST does not currently plan to withdraw SHA-2 or remove it from the revised Secure Hash Standard.
-----------> The purpose of SHA-3 is that it can be directly substituted for SHA-2 in current applications if necessary, 
-----------> and to significantly improve the robustness of NIST's overall hash algorithm toolkit.
---------> The creators of the Keccak algorithms and the SHA-3 functions suggest using the faster function KangarooTwelve with adjusted parameters 
-----------> and a new tree hashing mode without extra overhead for small message sizes. 

-------> Tiger (TTH), 
---------> This usually used in Tiger tree hashes
---------> In cryptography, Tiger is a cryptographic hash function designed by Ross Anderson and Eli Biham in 1995 for efficiency on 64-bit platforms. 
-----------> The size of a Tiger hash value is 192 bits. 
-----------> Truncated versions (known as Tiger/128 and Tiger/160) can be used for compatibility with protocols assuming a particular hash size. 
-----------> Unlike the SHA-2 family, no distinguishing initialization values are defined; they are simply prefixes of the full Tiger/192 hash value.
---------> Tiger2[2] is a variant where the message is padded by first appending a byte with the hexadecimal value of 0x80 as in MD4, MD5 and SHA, 
-----------> rather than with the hexadecimal value of 0x01 as in the case of Tiger. 
-----------> The two variants are otherwise identical. 

-------> WHIRLPOOL
---------> In computer science and cryptography, Whirlpool (sometimes styled WHIRLPOOL) is a cryptographic hash function. 
-----------> It was designed by Vincent Rijmen (co-creator of the Advanced Encryption Standard) and Paulo S. L. M. Barreto, who first described it in 2000.
---------> The hash has been recommended by the NESSIE project. 
-----------> It has also been adopted by the International Organization for Standardization (ISO) 
-----------> and the International Electrotechnical Commission (IEC) as part of the joint ISO/IEC 10118-3 international standard. 



-----> Cryptographically secure pseudo-random number generators
---------> A cryptographically secure pseudorandom number generator (CSPRNG) or cryptographic pseudorandom number generator (CPRNG)
-----------> is a pseudorandom number generator (PRNG) with properties that make it suitable for use in cryptography. 
-----------> It is also loosely known as a cryptographic random number generator (CRNG) (see Random number generation § "True" vs. pseudo-random numbers).
---------> Most cryptographic applications require random numbers, for example:
-----------> key generation
-----------> nonces
-----------> salts in certain signature schemes, including ECDSA, RSASSA-PSS

-------> Blum Blum Shub – based on the hardness of factorization
---------> Blum Blum Shub (B.B.S.) is a pseudorandom number generator proposed in 1986 by Lenore Blum, Manuel Blum +
-----------> and Michael Shub[1] that is derived from Michael O. Rabin's one-way function.
---------> Blum Blum Shub takes the form
-----------> xn+1 = xn^2 mod M,
-----------> where M = pq is the product of two large primes p and q. 
---------> At each step of the algorithm, some output is derived from xn+1; 
-----------> the output is commonly either the bit parity of xn+1 or one or more of the least significant bits of xn+1.
---------> The seed x0 should be an integer that is co-prime to M (i.e. p and q are not factors of x0) and not 1 or 0.
---------> The two primes, p and q, should both be congruent to 3 (mod 4) (this guarantees that each quadratic residue has one square root which is also a quadratic residue), 
---------> and should be safe primes with a small gcd((p-3)/2, (q-3)/2) (this makes the cycle length large).
---------> An interesting characteristic of the Blum Blum Shub generator is the possibility to calculate any xi value directly (via Euler's theorem):
-----------> xi = ( x0^(2^i mod λ(M))) mod M,
-----------> where λ is the Carmichael function. (Here we have λ(M) = λ(p*q) = lcm⁡(p−1, q−1). 

-------> Fortuna, 
---------> This is intended as an improvement on Yarrow algorithm
---------> Fortuna is a cryptographically secure pseudorandom number generator (PRNG) devised by Bruce Schneier and Niels Ferguson and published in 2003. 
-----------> It is named after Fortuna, the Roman goddess of chance. 
-----------> FreeBSD uses Fortuna for /dev/random and /dev/urandom is symbolically linked to it since FreeBSD 11.
-----------> Apple OSes have switched to Fortuna since 2020 Q1.

-------> Linear-feedback shift register 
---------> Note: many LFSR-based algorithms are weak or have been broken
---------> In computing, a linear-feedback shift register (LFSR) is a shift register whose input bit is a linear function of its previous state.
---------> The most commonly used linear function of single bits is exclusive-or (XOR). 
-----------> Thus, an LFSR is most often a shift register whose input bit is driven by the XOR of some bits of the overall shift register value.
---------> The initial value of the LFSR is called the seed, and because the operation of the register is deterministic, 
-----------> the stream of values produced by the register is completely determined by its current (or previous) state. 
-----------> Likewise, because the register has a finite number of possible states, it must eventually enter a repeating cycle. 
-----------> However, an LFSR with a well-chosen feedback function can produce a sequence of bits that appears random and has a very long cycle.
---------> Applications of LFSRs include generating pseudo-random numbers, pseudo-noise sequences, fast digital counters, and whitening sequences. +
-----------> Both hardware and software implementations of LFSRs are common.
---------> The mathematics of a cyclic redundancy check, used to provide a quick check against transmission errors, are closely related to those of an LFSR.
-----------> In general, the arithmetics behind LFSRs makes them very elegant as an object to study and implement. 
-----------> One can produce relatively complex logics with simple building blocks. 
-----------> However, other methods, that are less elegant but perform better, should be considered as well. 

-------> Yarrow algorithm
---------> The Yarrow algorithm is a family of cryptographic pseudorandom number generators (CPRNG) devised by John Kelsey, Bruce Schneier, and Niels Ferguson and published in 1999. 
---------> The Yarrow algorithm is explicitly unpatented, royalty-free, and open source; no license is required to use it. 
---------> An improved design from Ferguson and Schneier, Fortuna, is described in their book, Practical Cryptography
-------> Yarrow was used in FreeBSD, but is now superseded by Fortuna.
---------> Yarrow was also incorporated in iOS and macOS for their /dev/random devices, but Apple has switched to Fortuna since 2020 Q1.

-----> Key exchange
-------> Key exchange (also key establishment) is a method in cryptography by which cryptographic keys are exchanged between two parties, allowing use of a cryptographic algorithm.
---------> In the Diffie–Hellman key exchange scheme, each party generates a public/private key pair and distributes the public key. 
---------> After obtaining an authentic copy of each other's public keys, Alice and Bob can compute a shared secret offline. 
---------> The shared secret can be used, for instance, as the key for a symmetric cipher.
-------> If the sender and receiver wish to exchange encrypted messages, each must be equipped to encrypt messages to be sent and decrypt messages received. 
---------> The nature of the equipping they require depends on the encryption technique they might use. 
---------> If they use a code, both will require a copy of the same codebook. 
---------> If they use a cipher, they will need appropriate keys. If the cipher is a symmetric key cipher, both will need a copy of the same key. 
---------> If it is an asymmetric key cipher with the public/private key property, both will need the other's public key. 

-------> Diffie–Hellman key exchange
---------> Diffie–Hellman key exchange is a method of securely exchanging cryptographic keys over a public channel
-----------> and was one of the first public-key protocols as conceived by Ralph Merkle and named after Whitfield Diffie and Martin Hellman. 
-----------> DH is one of the earliest practical examples of public key exchange implemented within the field of cryptography.
-----------> Published in 1976 by Diffie and Hellman, this is the earliest publicly known work that proposed the idea of a private key and a corresponding public key.
---------> Traditionally, secure encrypted communication between two parties required that they first exchange keys by some secure physical means,
-----------> such as paper key lists transported by a trusted courier. 
-----------> The Diffie–Hellman key exchange method allows two parties that have no prior knowledge of each other to jointly establish a shared secret key over an insecure channel. 
-----------> This key can then be used to encrypt subsequent communications using a symmetric-key cipher.
---------> Diffie–Hellman is used to secure a variety of Internet services. 
-----------> However, research published in October 2015 suggests that the parameters in use for many DH Internet applications at that time
-----------> are not strong enough to prevent compromise by very well-funded attackers, such as the security services of some countries.
---------> The scheme was published by Whitfield Diffie and Martin Hellman in 1976, 
-----------> but in 1997 it was revealed that James H. Ellis, Clifford Cocks, and Malcolm J. Williamson of GCHQ, 
-----------> the British signals intelligence agency, had previously shown in 1969[5] how public-key cryptography could be achieved.
---------> Although Diffie–Hellman key agreement itself is a non-authenticated key-agreement protocol, 
-----------> it provides the basis for a variety of authenticated protocols, 
-----------> and is used to provide forward secrecy in Transport Layer Security's ephemeral modes (referred to as EDH or DHE depending on the cipher suite).
---------> The method was followed shortly afterwards by RSA, an implementation of public-key cryptography using asymmetric algorithms.
---------> Expired U.S. Patent 4,200,770 from 1977 describes the now public-domain algorithm. 
-----------> It credits Hellman, Diffie, and Merkle as inventors. 

-------> Elliptic-curve Diffie–Hellman (ECDH)
---------> Elliptic-curve Diffie–Hellman (ECDH) is a key agreement protocol that allows two parties, 
-----------> each having an elliptic-curve public–private key pair, to establish a shared secret over an insecure channel.
-----------> This shared secret may be directly used as a key, or to derive another key.
-----------> The key, or the derived key, can then be used to encrypt subsequent communications using a symmetric-key cipher. 
-----------> It is a variant of the Diffie–Hellman protocol using elliptic-curve cryptography. 

-----> Key derivation functions
-------> This is often used for password hashing and key stretching
-------> In cryptography, a key derivation function (KDF) is a cryptographic algorithm that derives one or more secret keys from a secret value such as a main key, 
---------> a password, or a passphrase using a pseudorandom function (which typically uses a cryptographic hash function or block cipher).
---------> KDFs can be used to stretch keys into longer keys or to obtain keys of a required format, 
---------> such as converting a group element that is the result of a Diffie–Hellman key exchange into a symmetric key for use with AES. 
---------> Keyed cryptographic hash functions are popular examples of pseudorandom functions used for key derivation.

-------> bcrypt
---------> bcrypt is a password-hashing function designed by Niels Provos and David Mazières, 
-----------> based on the Blowfish cipher and presented at USENIX in 1999.
-----------> Besides incorporating a salt to protect against rainbow table attacks, 
-----------> bcrypt is an adaptive function: over time,
-----------> the iteration count can be increased to make it slower, 
-----------> so it remains resistant to brute-force search attacks even with increasing computation power.
---------> The bcrypt function is the default password hash algorithm for OpenBSD and was the default for some Linux distributions such as SUSE Linux.[3]
---------> There are implementations of bcrypt in C, C++, C#, Embarcadero Delphi, Elixir, Go, Java, JavaScript,[8] Perl, PHP, Python, Ruby, and other languages. 

-------> PBKDF2
---------> In cryptography, PBKDF1 and PBKDF2 (Password-Based Key Derivation Function 1 and 2) 
-----------> are key derivation functions with a sliding computational cost, used to reduce vulnerabilities of brute-force attacks.
---------> PBKDF2 is part of RSA Laboratories' Public-Key Cryptography Standards (PKCS) series, specifically PKCS #5 v2.0, 
-----------> also published as Internet Engineering Task Force's RFC 2898. It supersedes PBKDF1, 
-----------> which could only produce derived keys up to 160 bits long.[2] RFC 8018 (PKCS #5 v2.1), published in 2017, recommends PBKDF2 for password hashing.

-------> scrypt
---------> In cryptography, scrypt (pronounced "ess crypt"[1]) is a password-based key derivation function created 
-----------> by Colin Percival in March 2009, originally for the Tarsnap online backup service. 
-----------> The algorithm was specifically designed to make it costly to perform large-scale custom 
-----------> hardware attacks by requiring large amounts of memory. 
-----------> In 2016, the scrypt algorithm was published by IETF as RFC 7914.
-----------> A simplified version of scrypt is used as a proof-of-work scheme by a number of cryptocurrencies, 
-----------> first implemented by an anonymous programmer called ArtForz in Tenebrix and followed by Fairbrix and Litecoin soon after.

-------> Argon2
---------> Argon2 is a key derivation function that was selected as the winner of the 2015 Password Hashing Competition.
-----------> It was designed by Alex Biryukov, Daniel Dinu, and Dmitry Khovratovich from the University of Luxembourg.
-----------> The reference implementation of Argon2 is released under a Creative Commons CC0 license (i.e. public domain) 
-----------> or the Apache License 2.0, and provides three related versions:
-------------> Argon2d maximizes resistance to GPU cracking attacks. 
---------------> It accesses the memory array in a password dependent order, which reduces the possibility of time–memory trade-off (TMTO) attacks, but introduces possible side-channel attacks.
-------------> Argon2i is optimized to resist side-channel attacks. 
---------------> It accesses the memory array in a password independent order.
-------------> Argon2id is a hybrid version. 
---------------> It follows the Argon2i approach for the first half pass over memory and the Argon2d approach for subsequent passes. 
---------------> The RFC[4] recommends using Argon2id if you do not know the difference between the types or you consider side-channel attacks to be a viable threat.
---------> All three modes allow specification by three parameters that control:
-----------> execution time
-----------> memory required
-----------> degree of parallelism

-----> Message authentication codes (symmetric authentication algorithms, which take a key as a parameter):
-------> In cryptography, a message authentication code (MAC), sometimes known as a tag, is a short piece of information used for authenticating a message. 
---------> In other words, to confirm that the message came from the stated sender (its authenticity) and has not been changed. 
---------> The MAC value protects a message's data integrity, as well as its authenticity, by allowing verifiers (who also possess the secret key) to detect any changes to the message content. 

-------> HMAC: keyed-hash message authentication
---------> In cryptography, an HMAC (sometimes expanded as either keyed-hash message authentication code or hash-based message authentication code) 
-----------> is a specific type of message authentication code (MAC) involving a cryptographic hash function and a secret cryptographic key. 
-----------> As with any MAC, it may be used to simultaneously verify both the data integrity and authenticity of a message.
---------> HMAC can provide authentication using a shared secret instead of using digital signatures with asymmetric cryptography. 
-----------> It trades off the need for a complex public key infrastructure by delegating the key exchange to the communicating parties, 
-----------> who are responsible for establishing and using a trusted channel to agree on the key prior to communication. 

-------> Poly1305
---------> Poly1305 is a cryptographic message authentication code (MAC) created by Daniel J. Bernstein. 
-----------> It can be used to verify the data integrity and the authenticity of a message. 
-----------> A variant of Bernstein's Poly1305 that does not require AES has been standardized by the Internet Engineering Task Force in RFC 8439. 

-------> SipHash
---------> SipHash is an add–rotate–xor (ARX) based family of pseudorandom functions created by Jean-Philippe Aumasson 
-----------> and Daniel J. Bernstein in 2012, in response to a spate of "hash flooding" denial-of-service attacks (HashDoS) in late 2011.
---------> Although designed for use as a hash function to ensure security, 
-----------> SipHash is fundamentally different from cryptographic hash functions like SHA in that it is only suitable as a message authentication code: 
-----------> a keyed hash function like HMAC. 
-----------> That is, SHA is designed so that it is difficult for an attacker to
----------->  find two messages X and Y such that SHA(X) = SHA(Y), even though anyone may compute SHA(X). 
-----------> SipHash instead guarantees that, having seen Xi and SipHash(Xi, k), 
-----------> an attacker who does not know the key k cannot find (any information about) k 
-----------> or SipHash(Y, k) for any message Y ∉ {Xi} which they have not seen before. 

-----> Secret sharing, Secret Splitting, Key Splitting, M of N algorithms
-------> Secret sharing (also called secret splitting) refers to methods for distributing a secret among a group, 
---------> in such a way that no individual holds any intelligible information about the secret, 
---------> but when a sufficient number of individuals combine their 'shares', the secret may be reconstructed. 
---------> Whereas insecure secret sharing allows an attacker to gain more information with each share, 
---------> secure secret sharing is 'all or nothing' (where 'all' means the necessary number of shares).
-------> In one type of secret sharing scheme there is one dealer and n players. 
---------> The dealer gives a share of the secret to the players, 
---------> but only when specific conditions are fulfilled will the players be able to reconstruct the secret from their shares. 
---------> The dealer accomplishes this by giving each player a share in such a way that any group of t (for threshold) 
---------> or more players can together reconstruct the secret but no group of fewer than t players can. 
---------> Such a system is called a (t, n)-threshold scheme (sometimes it is written as an (n, t)-threshold scheme).
-------> Secret sharing was invented independently by Adi Shamir[1] and George Blakley[2] in 1979. 

-------> Blakey's Scheme

-------> Shamir's Scheme
---------> Shamir's Secret Sharing, formulated by Adi Shamir, is one of the first secret sharing schemes in cryptography. 
-----------> It is based on polynomial interpolation over finite fields.[1] 

-----> Symmetric (secret key) encryption:
-------> Symmetric-key algorithms[a] are algorithms for cryptography that use the same cryptographic keys 
---------> for both the encryption of plaintext and the decryption of ciphertext. 
---------> The keys may be identical, or there may be a simple transformation to go between the two keys.
---------> The keys, in practice, represent a shared secret between two or more parties that can be used to maintain a private information link.
---------> The requirement that both parties have access to the secret key is one of the main drawbacks of symmetric-key encryption, 
---------> in comparison to public-key encryption (also known as asymmetric-key encryption).
---------> However, symmetric-key encryption algorithms are usually better for bulk encryption. 
---------> They have a smaller key size, which means less storage space and faster transmission. 
---------> Due to this, asymmetric-key encryption is often used to exchange the secret key for symmetric-key encryption.

-------> Advanced Encryption Standard (AES), winner of NIST competition, also known as Rijndael
---------> The Advanced Encryption Standard (AES), also known by its original name Rijndael (Dutch pronunciation: [ˈrɛindaːl]),
-----------> is a specification for the encryption of electronic data established by the U.S. National Institute of Standards and Technology (NIST) in 2001.
---------> AES is a variant of the Rijndael block cipher[3] developed by two Belgian cryptographers, 
-----------> Joan Daemen and Vincent Rijmen, who submitted a proposal[5] to NIST during the AES selection process.
-----------> Rijndael is a family of ciphers with different key and block sizes. For AES, NIST selected three members of the Rijndael family,
----------->  each with a block size of 128 bits, but three different key lengths: 128, 192 and 256 bits.
---------> AES has been adopted by the U.S. government. It supersedes the Data Encryption Standard (DES), which was published in 1977. 
-----------> The algorithm described by AES is a symmetric-key algorithm, meaning the same key is used for both encrypting and decrypting the data.
---------> In the United States, AES was announced by the NIST as U.S. FIPS PUB 197 (FIPS 197) on November 26, 2001.
-----------> This announcement followed a five-year standardization process in which fifteen competing designs were presented and evaluated, 
---------> AES is included in the ISO/IEC 18033-3 standard. 
-----------> AES became effective as a U.S. federal government standard on May 26, 2002, after approval by the U.S. Secretary of Commerce. 
-----------> AES is available in many different encryption packages, and is the first (and only) publicly accessible cipher approved 
-----------> by the U.S. National Security Agency (NSA) for top secret information when used in an NSA approved cryptographic module.

-------> Blowfish
---------> Blowfish is a symmetric-key block cipher, designed in 1993 
-----------> by Bruce Schneier and included in many cipher suites and encryption products. 
-----------> Blowfish provides a good encryption rate in software, 
-----------> and no effective cryptanalysis of it has been found to date. 
-----------> However, the Advanced Encryption Standard (AES) now receives more attention, 
-----------> and Schneier recommends Twofish for modern applications.
---------> Schneier designed Blowfish as a general-purpose algorithm, 
-----------> intended as an alternative to the aging DES and free of the problems 
-----------> and constraints associated with other algorithms. 
-----------> At the time Blowfish was released, many other designs were proprietary,
-----------> encumbered by patents or were commercial or government secrets. 
-----------> Schneier has stated that "Blowfish is unpatented, and will remain so in all countries. 
-----------> The algorithm is hereby placed in the public domain, and can be freely used by anyone."[3]
---------> Notable features of the design include key-dependent S-boxes and a highly complex key schedule. 

-------> Twofish
---------> In cryptography, Twofish is a symmetric key block cipher with a block size of 128 bits and key sizes up to 256 bits. 
-----------> It was one of the five finalists of the Advanced Encryption Standard contest, but it was not selected for standardization. 
-----------> Twofish is related to the earlier block cipher Blowfish.
---------> Twofish's distinctive features are the use of pre-computed key-dependent S-boxes, and a relatively complex key schedule. 
-----------> One half of an n-bit key is used as the actual encryption key and the other half of the n-bit key 
-----------> is used to modify the encryption algorithm (key-dependent S-boxes). 
-----------> Twofish borrows some elements from other designs; 
-----------> for example, the pseudo-Hadamard transform[3] (PHT) from the SAFER family of ciphers. 
-----------> Twofish has a Feistel structure like DES. Twofish also employs a Maximum Distance Separable matrix.
---------> When it was introduced in 1998, Twofish was slightly slower than Rijndael (the chosen algorithm for Advanced Encryption Standard) 
-----------> for 128-bit keys, but somewhat faster for 256-bit keys. 
-----------> Since 2008, virtually all AMD and Intel processors have included hardware acceleration of the Rijndael algorithm via the AES instruction set; 
-----------> Rijndael implementations that use the instruction set are now orders of magnitude faster than (software) Twofish implementations.
---------> Twofish was designed by Bruce Schneier, John Kelsey, Doug Whiting, David Wagner, Chris Hall, and Niels Ferguson: the "extended Twofish team" who met to perform further cryptanalysis of Twofish. 
-----------> Other AES contest entrants included Stefan Lucks, Tadayoshi Kohno, and Mike Stay.
---------> The Twofish cipher has not been patented, and the reference implementation has been placed in the public domain. 
-----------> As a result, the Twofish algorithm is free for anyone to use without any restrictions whatsoever. 
-----------> It is one of a few ciphers included in the OpenPGP standard (RFC 4880). 
-----------> However, Twofish has seen less widespread usage than Blowfish, which has been available longer. 

-------> Threefish
---------> Threefish is a symmetric-key tweakable block cipher designed as part of the Skein hash function, 
-----------> an entry in the NIST hash function competition. 
-----------> Threefish uses no S-boxes or other table lookups in order to avoid cache timing attacks;
-----------> its nonlinearity comes from alternating additions with exclusive ORs. 
-----------> In that respect, it is similar to Salsa20, TEA, and the SHA-3 candidates CubeHash and BLAKE.
---------> Threefish and the Skein hash function were designed by Bruce Schneier, 
-----------> Niels Ferguson, Stefan Lucks, Doug Whiting, Mihir Bellare, Tadayoshi Kohno, Jon Callas, and Jesse Walker. 

-------> Data Encryption Standard (DES)
---------> This is sometimes called the DE Algorithm, winner of NBS selection competition, replaced by AES for most purposes
---------> The Data Encryption Standard (DES /ˌdiːˌiːˈɛs, dɛz/) is a symmetric-key algorithm for the encryption of digital data. 
-----------> Although its short key length of 56 bits makes it too insecure for modern applications, it has been highly influential in the advancement of cryptography.
---------> Developed in the early 1970s at IBM and based on an earlier design by Horst Feistel, 
-----------> the algorithm was submitted to the National Bureau of Standards (NBS) following the agency's invitation 
-----------> to propose a candidate for the protection of sensitive, unclassified electronic government data. 
-----------> In 1976, after consultation with the National Security Agency (NSA), 
-----------> the NBS selected a slightly modified version (strengthened against differential cryptanalysis, 
-----------> but weakened against brute-force attacks), 
-----------> which was published as an official Federal Information Processing Standard (FIPS) for the United States in 1977.
---------> The publication of an NSA-approved encryption standard led to its quick international adoption and widespread academic scrutiny. 
-----------> Controversies arose from classified design elements, a relatively short key length of the symmetric-key block cipher design, 
-----------> and the involvement of the NSA, raising suspicions about a backdoor. 
-----------> The S-boxes that had prompted those suspicions were designed by the NSA to remove a backdoor they secretly knew (differential cryptanalysis). 
-----------> However, the NSA also ensured that the key size was drastically reduced so that they could break the cipher by brute force attack. 
-----------> The intense academic scrutiny the algorithm received over time led to the modern understanding of block ciphers and their cryptanalysis.
---------> DES is insecure due to the relatively short 56-bit key size. 
-----------> In January 1999, distributed.net and the Electronic Frontier Foundation collaborated to publicly break a DES key in 22 hours and 15 minutes (see chronology). 
-----------> There are also some analytical results which demonstrate theoretical weaknesses in the cipher, although they are infeasible in practice. 
-----------> The algorithm is believed to be practically secure in the form of Triple DES, although there are theoretical attacks. 
-----------> This cipher has been superseded by the Advanced Encryption Standard (AES). DES has been withdrawn as a standard by the National Institute of Standards and Technology.[3]
---------> Some documents distinguish between the DES standard and its algorithm, referring to the algorithm as the DEA (Data Encryption Algorithm). 

-------> IDEA
---------> In cryptography, the International Data Encryption Algorithm (IDEA), 
-----------> originally called Improved Proposed Encryption Standard (IPES), 
-----------> is a symmetric-key block cipher designed by James Massey of ETH Zurich and Xuejia Lai and was first described in 1991. 
-----------> The algorithm was intended as a replacement for the Data Encryption Standard (DES). 
-----------> IDEA is a minor revision of an earlier cipher Proposed Encryption Standard (PES).
---------> The cipher was designed under a research contract with the Hasler Foundation, which became part of Ascom-Tech AG. 
-----------> The cipher was patented in a number of countries but was freely available for non-commercial use. 
-----------> The name "IDEA" is also a trademark. 
-----------> The last patents expired in 2012, and IDEA is now patent-free and thus completely free for all uses.
---------> IDEA was used in Pretty Good Privacy (PGP) v2.0 and was incorporated after the original cipher used in v1.0, BassOmatic, was found to be insecure.
-----------> IDEA is an optional algorithm in the OpenPGP standard. 

-------> RC4 (cipher)
---------> In cryptography, RC4 (Rivest Cipher 4, also known as ARC4 or ARCFOUR, meaning Alleged RC4, see below) is a stream cipher. 
-----------> While it is remarkable for its simplicity and speed in software, 
-----------> multiple vulnerabilities have been discovered in RC4, rendering it insecure.
-----------> It is especially vulnerable when the beginning of the output keystream is not discarded, 
-----------> or when nonrandom or related keys are used. 
-----------> Particularly problematic uses of RC4 have led to very insecure protocols such as WEP.[5]
---------> As of 2015, there is speculation that some state cryptologic agencies may possess the capability to break RC4 when used in the TLS protocol.
-----------> IETF has published RFC 7465 to prohibit the use of RC4 in TLS;[3] Mozilla and Microsoft have issued similar recommendations.
---------> A number of attempts have been made to strengthen RC4, notably Spritz, RC4A, VMPC, and RC4+. 

-------> Tiny Encryption Algorithm (TEA)
---------> In cryptography, the Tiny Encryption Algorithm (TEA) is a block cipher notable 
-----------> for its simplicity of description and implementation, typically a few lines of code. 
-----------> It was designed by David Wheeler and Roger Needham of the Cambridge Computer Laboratory; 
-----------> it was first presented at the Fast Software Encryption workshop in Leuven in 1994, and first published in the proceedings of that workshop.[4]
---------> The cipher is not subject to any patents. 

-------> Salsa20, and its updated variant ChaCha20
---------> Salsa20 and the closely related ChaCha are stream ciphers developed by Daniel J. Bernstein. 
-----------> Salsa20, the original cipher, was designed in 2005, 
-----------> then later submitted to the eSTREAM European Union cryptographic validation process by Bernstein. 
-----------> ChaCha is a modification of Salsa20 published in 2008. 
-----------> It uses a new round function that increases diffusion and increases performance on some architectures.[4]
---------> Both ciphers are built on a pseudorandom function based on add-rotate-XOR (ARX) operations — 32-bit addition, bitwise addition (XOR) and rotation operations. 
-----------> The core function maps a 256-bit key, a 64-bit nonce, and a 64-bit counter to a 512-bit block of the key stream (a Salsa version with a 128-bit key also exists). 
-----------> This gives Salsa20 and ChaCha the unusual advantage that the user can efficiently seek to any position in the key stream in constant time. 
-----------> Salsa20 offers speeds of around 4–14 cycles per byte in software on modern x86 processors,[5] and reasonable hardware performance. 
-----------> It is not patented, and Bernstein has written several public domain implementations optimized for common architectures.[6] 

-----> Post-quantum cryptography
-------> In cryptography, post-quantum cryptography (sometimes referred to as quantum-proof, quantum-safe or quantum-resistant) 
---------> refers to cryptographic algorithms (usually public-key algorithms) that are thought to be secure against a cryptanalytic attack by a quantum computer. 
---------> The problem with currently popular algorithms is that their security relies on one of three hard mathematical problems: 
---------> the integer factorization problem, the discrete logarithm problem or the elliptic-curve discrete logarithm problem. 
---------> All of these problems could be easily solved on a sufficiently powerful quantum computer running Shor's algorithm.
-------> Even though current, publicly known, experimental quantum computers lack processing power to break any real cryptographic algorithm, 
---------> many cryptographers are designing new algorithms to prepare for a time when quantum computing becomes a threat. 
---------> This work has gained greater attention from academics and industry through the PQCrypto conference series since 2006 
---------> and more recently by several workshops on Quantum Safe Cryptography hosted by the European Telecommunications Standards Institute (ETSI) and the Institute for Quantum Computing.
-------> In contrast to the threat quantum computing poses to current public-key algorithms, 
---------> most current symmetric cryptographic algorithms and hash functions are considered to be relatively secure against attacks by quantum computers.
---------> While the quantum Grover's algorithm does speed up attacks against symmetric ciphers, doubling the key size can effectively block these attacks.
---------> Thus post-quantum symmetric cryptography does not need to differ significantly from current symmetric cryptography. 

-----> Proof-of-work algorithms
-------> Proof of work (PoW) is a form of cryptographic proof in which one party (the prover) proves to others (the verifiers)
---------> that a certain amount of a specific computational effort has been expended. 
---------> Verifiers can subsequently confirm this expenditure with minimal effort on their part. 
---------> The concept was invented by Moni Naor and Cynthia Dwork in 1993 as a way to deter denial-of-service attacks 
---------> and other service abuses such as spam on a network by requiring some work from a service requester, 
---------> usually meaning processing time by a computer. The term "proof of work" was first coined and formalized in a 1999 paper by Markus Jakobsson and Ari Juels.
---------> Proof of work was later popularized by Bitcoin as a foundation for consensus in permissionless decentralized network, 
---------> in which miners compete to append blocks and mint new currency, each miner experiencing a success probability proportional to the computational effort expended. 
---------> PoW and PoS (proof of stake) are the two best known Sybil deterrence mechanisms. In the context of cryptocurrencies they are the most common mechanisms.
-------> A key feature of proof-of-work schemes is their asymmetry: the work – the computation – must be moderately hard (yet feasible) on the prover 
---------> or requester side but easy to check for the verifier or service provider. 
---------> This idea is also known as a CPU cost function, client puzzle, computational puzzle, or CPU pricing function. 
---------> Another common feature is built-in incentive-structures that reward allocating computational capacity to the network with value in the form of money.[citation needed][4]
-------> The purpose of proof-of-work algorithms is not proving that certain work was carried out or that a computational puzzle was "solved", 
---------> but deterring manipulation of data by establishing large energy and hardware-control requirements to be able to do so.
---------> Proof-of-work systems have been criticized by environmentalists for their energy consumption.



---> Digital logic

-----> Boolean minimization

-------> Quine–McCluskey algorithm: 
---------> This is also called as Q-M algorithm, programmable method for simplifying the boolean equations.
---------> The Quine–McCluskey algorithm (QMC), also known as the method of prime implicants, 
-----------> is a method used for minimization of Boolean functions that was developed by Willard V. Quine in 1952 and extended by Edward J. McCluskey in 1956.
-----------> As a general principle this approach had already been demonstrated by the logician Hugh McColl in 1878,
-----------> was proved by Archie Blake in 1937, and was rediscovered by Edward W. Samson and Burton E. Mills in 1954 and by Raymond J. Nelson in 1955.
-----------> Also in 1955, Paul W. Abrahams and John G. Nordahl[12] as well as Albert A. Mullin and Wayne G. Kellner proposed a decimal variant of the method.
---------> The Quine–McCluskey algorithm is functionally identical to Karnaugh mapping, 
-----------> but the tabular form makes it more efficient for use in computer algorithms, 
-----------> and it also gives a deterministic way to check that the minimal form of a Boolean function has been reached. 
-----------> It is sometimes referred to as the tabulation method.
---------> The method involves two steps:
-----------> (1) Finding all prime implicants of the function.
-----------> (2) Use those prime implicants in a prime implicant chart to find the essential prime implicants of the function, 
-------------> as well as other prime implicants that are necessary to cover the function.

-------> Petrick's method: 
---------> This is another algorithm for boolean simplification.
---------> In Boolean algebra, Petrick's method (also known as Petrick function or branch-and-bound method)
-----------> is a technique for determining all minimum sum-of-products solutions from a prime implicant chart.
-----------> Petrick's method is very tedious for large charts, but it is easy to implement on a computer.
---------> Note: This is described by Stanley R. Petrick (1931–2006) in 1956
---------> Note: The method was improved by Insley B. Pyne and Edward Joseph McCluskey in 1962.[8][9]

-------> Espresso heuristic logic minimizer: 
---------> This is fast algorithm for boolean function minimization.
---------> The ESPRESSO logic minimizer is a computer program using heuristic and specific algorithms for efficiently reducing the complexity of digital logic gate circuits. 
---------> ESPRESSO-I was originally developed at IBM by Robert K. Brayton et al. in 1982 and improved as ESPRESSO-II in 1984. 
-----------> Richard L. Rudell later published the variant ESPRESSO-MV in 1986[6] and ESPRESSO-EXACT in 1987.
---------> Espresso has inspired many derivatives. 



---> Machine learning and statistical classification
-----> Machine learning 
-------> Machine learning (ML) is a field of inquiry devoted to understanding and building methods that 'learn', that is, 
---------> methods that leverage data to improve performance on some set of tasks.[1] It is seen as a part of artificial intelligence. 
---------> Machine learning algorithms build a model based on sample data, known as training data, 
---------> in order to make predictions or decisions without being explicitly programmed to do so.
---------> Machine learning algorithms are used in a wide variety of applications, such as in medicine, 
---------> email filtering, speech recognition, and computer vision, 
---------> where it is difficult or unfeasible to develop conventional algorithms to perform the needed tasks.
-------> A subset of machine learning is closely related to computational statistics, 
---------> which focuses on making predictions using computers, but not all machine learning is statistical learning. 
---------> The study of mathematical optimization delivers methods, theory and application domains to the field of machine learning. 
---------> Data mining is a related field of study, focusing on exploratory data analysis through unsupervised learning.
---------> Some implementations of machine learning use data and neural networks in a way that mimics the working of a biological brain. 
---------> In its application across business problems, machine learning is also referred to as predictive analytics. 
-----> Statistical classification
-------> In statistics, classification is the problem of identifying which of a set of categories 
---------> (sub-populations) an observation (or observations) belongs to. 
---------> Examples are assigning a given email to the "spam" or "non-spam" class, 
---------> and assigning a diagnosis to a given patient based on observed characteristics of the patient 
---------> (sex, blood pressure, presence or absence of certain symptoms, etc.).
-------> Often, the individual observations are analyzed into a set of quantifiable properties, 
---------> known variously as explanatory variables or features. 
---------> These properties may variously be categorical (e.g. "A", "B", "AB" or "O", for blood type), ordinal (e.g. "large", "medium" or "small"),
---------> integer-valued (e.g. the number of occurrences of a particular word in an email) or real-valued (e.g. a measurement of blood pressure). 
---------> Other classifiers work by comparing observations to previous observations by means of a similarity or distance function.
-------> An algorithm that implements classification, especially in a concrete implementation, is known as a classifier. 
---------> The term "classifier" sometimes also refers to the mathematical function, implemented by a classification algorithm, that maps input data to a category.
-------> Terminology across fields is quite varied. In statistics, where classification is often done with logistic regression or a similar procedure, 
---------> the properties of observations are termed explanatory variables (or independent variables, regressors, etc.), 
---------> and the categories to be predicted are known as outcomes, which are considered to be possible values of the dependent variable. 
---------> In machine learning, the observations are often known as instances, the explanatory variables are termed features (grouped into a feature vector), 
---------> and the possible categories to be predicted are classes.
---------> Other fields may use different terminology: e.g. in community ecology, the term "classification" normally refers to cluster analysis. 


-----> ALOPEX: 
-------> This a correlation-based machine-learning algorithm
-------> ALOPEX (an acronym from "ALgorithms Of Pattern EXtraction") is a correlation based machine learning algorithm first proposed by Tzanakou and Harth in 1974. 
-------> Principle
-------> In machine learning, the goal is to train a system to minimize a cost function or (referring to ALOPEX) a response function. 
---------> Many training algorithms, such as backpropagation, have an inherent susceptibility to getting "stuck" in local minima or maxima of the response function. 
---------> ALOPEX uses a cross-correlation of differences and a stochastic process to overcome this in an attempt to reach the absolute minimum (or maximum) of the response function. 

-----> Association rule learning: 
-------> This discover interesting relations between variables, used in data mining
-------> Association rule learning is a rule-based machine learning method for discovering interesting relations between variables in large databases. 
---------> It is intended to identify strong rules discovered in databases using some measures of interestingness.
---------> In any given transaction with a variety of items, association rules are meant to discover the rules that determine how or why certain items are connected.
-------> Based on the concept of strong rules, Rakesh Agrawal, Tomasz Imieliński and Arun Swami
---------> introduced association rules for discovering regularities between products in large-scale transaction data recorded by point-of-sale (POS) systems in supermarkets. 
---------> For example, the rule {onions, potatoes} ⇒ {burger} found in the sales data of a supermarket 
---------> would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. 
---------> Such information can be used as the basis for decisions about marketing activities such as, e.g., promotional pricing or product placements.
-------> In addition to the above example from market basket analysis association rules are employed today 
---------> in many application areas including Web usage mining, intrusion detection, continuous production, and bioinformatics. 
---------> In contrast with sequence mining, association rule learning typically does not consider the order of items either within a transaction or across transactions.
-------> The association rule algorithm itself consists of various parameters that can make it difficult 
---------> for those without some expertise in data mining to execute, with many rules that are arduous to understand.

-------> Apriori algorithm
---------> Apriori is an algorithm for frequent item set mining and association rule learning over relational databases. 
-----------> It proceeds by identifying the frequent individual items in the database and extending them to larger 
-----------> and larger item sets as long as those item sets appear sufficiently often in the database. 
-----------> The frequent item sets determined by Apriori can be used to determine association rules which highlight general trends in the database: 
-----------> this has applications in domains such as market basket analysis. 

-------> Eclat algorithm
---------> Eclat[11] (alt. ECLAT, stands for Equivalence Class Transformation) is a backtracking algorithm, 
-----------> which traverses the frequent itemset lattice graph in a depth-first search (DFS) fashion. 
-----------> Whereas the breadth-first search (BFS) traversal used in the Apriori algorithm will end up checking every subset of an itemset before checking it, 
-----------> DFS traversal checks larger itemsets and can save on checking the support of some of its subsets by virtue of the downward-closer property. 
-----------> Furthermore it will almost certainly use less memory as DFS has a lower space complexity than BFS.
---------> To illustrate this, let there be a frequent itemset {a, b, c}. 
-----------> a DFS may check the nodes in the frequent itemset lattice in the following order: {a} → {a, b} → {a, b, c}, 
-----------> at which point it is known that {b}, {c}, {a, c}, {b, c} all satisfy the support constraint by the downward-closure property. 
-----------> BFS would explore each subset of {a, b, c} before finally checking it. 
-----------> As the size of an itemset increases, the number of its subsets undergoes combinatorial explosion.
---------> It is suitable for both sequential as well as parallel execution with locality-enhancing properties.

-------> FP-growth algorithm
---------> FP stands for frequent pattern.[30]
---------> In the first pass, the algorithm counts the occurrences of items (attribute-value pairs) 
-----------> in the dataset of transactions, and stores these counts in a 'header table'. 
-----------> In the second pass, it builds the FP-tree structure by inserting transactions into a trie.
---------> Items in each transaction have to be sorted by descending order of their frequency in the dataset 
-----------> before being inserted so that the tree can be processed quickly. 
-----------> Items in each transaction that do not meet the minimum support requirement are discarded. 
-----------> If many transactions share most frequent items, the FP-tree provides high compression close to tree root.
---------> Recursive processing of this compressed version of the main dataset grows frequent item sets directly, 
-----------> instead of generating candidate items and testing them against the entire database (as in the apriori algorithm).
---------> Growth begins from the bottom of the header table i.e. the item with the smallest support 
-----------> by finding all sorted transactions that end in that item. 
-----------> Call this item I.
---------> A new conditional tree is created which is the original FP-tree projected onto I. 
-----------> The supports of all nodes in the projected tree are re-counted with each node getting the sum of its children counts. 
-----------> Nodes (and hence subtrees) that do not meet the minimum support are pruned. 
-----------> Recursive growth ends when no individual items conditional on I meet the minimum support threshold. 
-----------> The resulting paths from root to I will be frequent itemsets. 
-----------> After this step, processing continues with the next least-supported header item of the original FP-tree.
---------> Once the recursive process has completed, all frequent item sets will have been found, and association rule creation begins.

-------> One-attribute rule

-------> Zero-attribute rule

-----> Boosting (meta-algorithm): 
-------> This uses many weak learners to boost effectiveness
-------> In machine learning, boosting is an ensemble meta-algorithm for primarily reducing bias, and also variance[1] in supervised learning, 
---------> and a family of machine learning algorithms that convert weak learners to strong ones.
---------> Boosting is based on the question posed by Kearns and Valiant (1988, 1989):[3][4] "Can a set of weak learners create a single strong learner?" 
---------> A weak learner is defined to be a classifier that is only slightly correlated with the true classification (it can label examples better than random guessing). 
---------> In contrast, a strong learner is a classifier that is arbitrarily well-correlated with the true classification.
-------> Robert Schapire's affirmative answer in a 1990 paper[5] to the question of Kearns and Valiant 
---------> has had significant ramifications in machine learning and statistics, most notably leading to the development of boosting.
-------> When first introduced, the hypothesis boosting problem simply referred to the process of turning a weak learner into a strong learner.
---------> "Informally, [the hypothesis boosting] problem asks whether an efficient learning algorithm […] 
---------> that outputs a hypothesis whose performance is only slightly better than random guessing [i.e. a weak learner] 
---------> implies the existence of an efficient algorithm that outputs a hypothesis of arbitrary accuracy [i.e. a strong learner]."
---------> Algorithms that achieve hypothesis boosting quickly became simply known as "boosting". 
---------> Freund and Schapire's arcing (Adapt[at]ive Resampling and Combining), as a general technique, is more or less synonymous with boosting.

-------> AdaBoost: 
---------> This is adaptive boosting
---------> AdaBoost, short for Adaptive Boosting, is a statistical classification meta-algorithm formulated by Yoav Freund and Robert Schapire in 1995, 
-----------> who won the 2003 Gödel Prize for their work. It can be used in conjunction with many other types of learning algorithms to improve performance. 
-----------> The output of the other learning algorithms ('weak learners') is combined into a weighted sum that represents the final output of the boosted classifier. 
-----------> Usually, AdaBoost is presented for binary classification, although it can be generalized to multiple classes or bounded intervals on the real line.
---------> AdaBoost is adaptive in the sense that subsequent weak learners are tweaked in favor of those instances misclassified by previous classifiers. 
-----------> In some problems it can be less susceptible to the overfitting problem than other learning algorithms. 
-----------> The individual learners can be weak, but as long as the performance of each one is slightly better than random guessing, the final model can be proven to converge to a strong learner.
---------> Although AdaBoost is typically used to combine weak base learners (such as decision stumps), 
-----------> it has been shown that it can also effectively combine strong base learners (such as deep decision trees), producing an even more accurate model.[3]
---------> Every learning algorithm tends to suit some problem types better than others, 
-----------> and typically has many different parameters and configurations to adjust before it achieves optimal performance on a dataset. 
-----------> AdaBoost (with decision trees as the weak learners) is often referred to as the best out-of-the-box classifier.
-----------> When used with decision tree learning, information gathered at each stage of the AdaBoost algorithm about the relative 'hardness' 
-----------> of each training sample is fed into the tree growing algorithm such that later trees tend to focus on harder-to-classify examples. 

-------> BrownBoost: 
---------> This is a boosting algorithm that may be robust to noisy datasets
---------> BrownBoost is a boosting algorithm that may be robust to noisy datasets. 
-----------> BrownBoost is an adaptive version of the boost by majority algorithm. 
-----------> As is true for all boosting algorithms, BrownBoost is used in conjunction with other machine learning methods. 
-----------> BrownBoost was introduced by Yoav Freund in 2001.[1] 

-------> LogitBoost: 
---------> This is logistic regression boosting.
---------> LogitBoost is a boosting algorithm formulated by Jerome Friedman, Trevor Hastie, and Robert Tibshirani. 
-----------> The original paper casts the AdaBoost algorithm into a statistical framework.
-----------> Specifically, if one considers AdaBoost as a generalized additive model 
-----------> and then applies the cost function of logistic regression, one can derive the LogitBoost algorithm. 

-------> LPBoost: 
---------> This is linear programming boosting
---------> Linear Programming Boosting (LPBoost) is a supervised classifier from the boosting family of classifiers. 
-----------> LPBoost maximizes a margin between training samples of different classes 
-----------> and hence also belongs to the class of margin-maximizing supervised classification algorithms. 
-----------> Consider a classification function
-------------> f : X → {−1, 1},
-----------> which classifies samples from a space X into one of two classes, labelled 1 and -1, respectively. 
---------> LPBoost is an algorithm to learn such a classification function given a set of training examples with known class labels. 
-----------> LPBoost is a machine learning technique and especially suited for applications of joint classification and feature selection in structured domains. 

-----> Bootstrap aggregating (bagging): 
-------> This technique is to improve stability and classification accuracy
-------> Bootstrap aggregating, also called bagging (from bootstrap aggregating), 
---------> is a machine learning ensemble meta-algorithm designed to improve the stability 
---------> and accuracy of machine learning algorithms used in statistical classification and regression. 
---------> It also reduces variance and helps to avoid overfitting. Although it is usually applied to decision tree methods, 
---------> it can be used with any type of method. Bagging is a special case of the model averaging approach. 

-----> Computer Vision
-------> Computer vision is an interdisciplinary scientific field that deals with how computers can gain high-level understanding from digital images or videos. 
---------> From the perspective of engineering, it seeks to understand and automate tasks that the human visual system can do.
-------> Computer vision tasks include methods for acquiring, processing, analyzing and understanding digital images, 
---------> and extraction of high-dimensional data from the real world in order to produce numerical or symbolic information, e.g. in the forms of decisions.
---------> Understanding in this context means the transformation of visual images (the input of the retina) 
---------> into descriptions of the world that make sense to thought processes and can elicit appropriate action. 
---------> This image understanding can be seen as the disentangling of symbolic information from image data 
---------> using models constructed with the aid of geometry, physics, statistics, and learning theory.
-------> The scientific discipline of computer vision is concerned with the theory behind artificial systems that extract information from images. 
---------> The image data can take many forms, such as video sequences, views from multiple cameras, 
---------> multi-dimensional data from a 3D scanner, or medical scanning device.
--------->  The technological discipline of computer vision seeks to apply its theories and models to the construction of computer vision systems.
-------> Sub-domains of computer vision include scene reconstruction, object detection, event detection, video tracking, object recognition, 
---------> 3D pose estimation, learning, indexing, motion estimation, visual servoing, 3D scene modeling, and image restoration. 

-------> Grabcut based on Graph cuts
---------> GrabCut is an image segmentation method based on graph cuts.
---------> Starting with a user-specified bounding box around the object to be segmented, 
-----------> the algorithm estimates the color distribution of the target object and that of the background using a Gaussian mixture model. 
-----------> This is used to construct a Markov random field over the pixel labels, 
-----------> with an energy function that prefers connected regions having the same label, 
-----------> and running a graph cut based optimization to infer their values. 
-----------> As this estimate is likely to be more accurate than the original, 
-----------> taken from the bounding box, this two-step procedure is repeated until convergence.
---------> Estimates can be further corrected by the user by pointing out misclassified regions and rerunning the optimization. 
-----------> The method also corrects the results to preserve edges.[citation needed]
---------> There are several open source implementations available including OpenCV (as of version 2.1).

-----> Decision Trees
-------> Decision Tree Learning is a supervised learning approach used in statistics, data mining and machine learning. 
---------> In this formalism, a classification or regression decision tree is used as a predictive model to draw conclusions about a set of observations.
-------> Tree models where the target variable can take a discrete set of values are called classification trees; 
---------> in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. 
---------> Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees.
-------> Decision trees are among the most popular machine learning algorithms given their intelligibility and simplicity.
-------> In decision analysis, a decision tree can be used to visually and explicitly represent decisions and decision making. 
---------> In data mining, a decision tree describes data (but the resulting classification tree can be an input for decision making). 

-------> C4.5 algorithm: 
---------> This an extension to ID3
---------> C4.5 is an algorithm used to generate a decision tree developed by Ross Quinlan.[1] C4.5 is an extension of Quinlan's earlier ID3 algorithm. 
-----------> The decision trees generated by C4.5 can be used for classification, and for this reason, C4.5 is often referred to as a statistical classifier. 
-----------> In 2011, authors of the Weka machine learning software described the C4.5 algorithm as 
-----------> "a landmark decision tree program that is probably the machine learning workhorse most widely used in practice to date".
---------> It became quite popular after ranking #1 in the Top 10 Algorithms in Data Mining pre-eminent paper published by Springer LNCS in 2008.

-------> ID3 algorithm (Iterative Dichotomiser 3): 
---------> This uses a heuristic to generate small decision trees
-----------> In decision tree learning, ID3 (Iterative Dichotomiser 3) is an algorithm invented by Ross Quinlan[1] used to generate a decision tree from a dataset. 
-----------> ID3 is the precursor to the C4.5 algorithm, and is typically used in the machine learning and natural language processing domains. 

-----> Clustering: 
-------> This is a class of unsupervised learning algorithms for grouping and bucketing related input vector.
-------> Cluster analysis or clustering is the task of grouping a set of objects in such a way that objects in the same group (called a cluster) 
---------> are more similar (in some sense) to each other than to those in other groups (clusters). 
---------> It is a main task of exploratory data analysis, and a common technique for statistical data analysis, used in many fields, 
---------> including pattern recognition, image analysis, information retrieval, bioinformatics, data compression, computer graphics and machine learning. 

-------> k-nearest neighbors (k-NN): 
---------> This is a method for classifying objects based on closest training examples in the feature space
---------> In statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first 
-----------> It is used for classification and regression. In both cases, the input consists of the k closest training examples in a data set. 
-----------> The output depends on whether k-NN is used for classification or regression:
-------------> In k-NN classification, the output is a class membership. 
---------------> An object is classified by a plurality vote of its neighbors, with the object being assigned to the class most common 
---------------> among its k nearest neighbors (k is a positive integer, typically small). 
---------------> If k = 1, then the object is simply assigned to the class of that single nearest neighbor.
-------------> In k-NN regression, the output is the property value for the object. This value is the average of the values of k nearest neighbors.
---------> k-NN is a type of classification where the function is only approximated locally and all computation is deferred until function evaluation. 
-----------> Since this algorithm relies on distance for classification, if the features represent different physical units 
-----------> or come in vastly different scales then normalizing the training data can improve its accuracy dramatically.
---------> Both for classification and regression, a useful technique can be to assign weights to the contributions of the neighbors, 
-----------> so that the nearer neighbors contribute more to the average than the more distant ones. 
-----------> For example, a common weighting scheme consists in giving each neighbor a weight of 1/d, where d is the distance to the neighbor.
---------> The neighbors are taken from a set of objects for which the class (for k-NN classification) or the object property value (for k-NN regression) is known. 
-----------> This can be thought of as the training set for the algorithm, though no explicit training step is required.
---------> A peculiarity of the k-NN algorithm is that it is sensitive to the local structure of the data. 
---------> Note: This is developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover.

-----> Linde–Buzo–Gray algorithm: 
-------> This is a vector quantization algorithm used to derive a good codebook
-------> The Linde–Buzo–Gray algorithm is a vector quantization algorithm to derive a good codebook.
---------> It is similar to the k-means method in data clustering.
-------> Algorithm
---------> At each iteration, each vector is split into two new vectors.
-----------> A initial state: centroid of the training sequence;
-----------> B initial estimation #1: code book of size 2;
-----------> C final estimation after LGA: Optimal code book with 2 vectors;
-----------> D initial estimation #2: code book of size 4;
-----------> E final estimation after LGA: Optimal code book with 4 vectors;
-------> Note: This was introduced by Yoseph Linde, Andrés Buzo and Robert M. Gray in 1980

-----> Locality-sensitive hashing (LSH): 
-------> This is a method of performing probabilistic dimension reduction of high-dimensional data
-------> Locality-sensitive hashing (LSH) is an algorithmic technique that hashes similar input items into the same "buckets" with high probability. 
---------> (The number of buckets is much smaller than the universe of possible input items.)
---------> Since similar items end up in the same buckets, this technique can be used for data clustering and nearest neighbor search. 
---------> It differs from conventional hashing techniques in that hash collisions are maximized, not minimized. 
---------> Alternatively, the technique can be seen as a way to reduce the dimensionality of high-dimensional data; 
---------> high-dimensional input items can be reduced to low-dimensional versions while preserving relative distances between items.
-------> Hashing-based approximate nearest neighbor search algorithms generally use one of two main categories of hashing methods: 
---------> either data-independent methods, such as locality-sensitive hashing (LSH); or data-dependent methods, such as locality-preserving hashing (LPH).

-----> Neural Network
-------> Artificial neural networks (ANNs), usually simply called neural networks (NNs) or neural nets,
---------> are computing systems inspired by the biological neural networks that constitute animal brains.
-------> An ANN is based on a collection of connected units or nodes called artificial neurons, which loosely model the neurons in a biological brain. 
---------> Each connection, like the synapses in a biological brain, can transmit a signal to other neurons. 
---------> An artificial neuron receives signals then processes them and can signal neurons connected to it. 
---------> The "signal" at a connection is a real number, and the output of each neuron is computed by some non-linear function of the sum of its inputs. 
---------> The connections are called edges. 
---------> Neurons and edges typically have a weight that adjusts as learning proceeds. 
---------> The weight increases or decreases the strength of the signal at a connection. 
---------> Neurons may have a threshold such that a signal is sent only if the aggregate signal crosses that threshold. 
---------> Typically, neurons are aggregated into layers. Different layers may perform different transformations on their inputs. 
---------> Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.



-------> Backpropagation: 
---------> This is a supervised learning method which requires a teacher that knows, or can calculate, the desired output for any given input
---------> Backpropagation (backprop,[1] BP) is a widely used algorithm for training feedforward neural networks. 
-----------> Generalizations of backpropagation exist for other artificial neural networks (ANNs), and for functions generally. 
-----------> These classes of algorithms are all referred to generically as "backpropagation".[2] 
-----------> In fitting a neural network, backpropagation computes the gradient of the loss function with respect to the weights of the network for a single input–output example, 
-----------> and does so efficiently, unlike a naive direct computation of the gradient with respect to each weight individually. 
-----------> This efficiency makes it feasible to use gradient methods for training multilayer networks, 
-----------> updating weights to minimize loss; gradient descent, or variants such as stochastic gradient descent, are commonly used. 
-----------> The backpropagation algorithm works by computing the gradient of the loss function with respect to each weight by the chain rule, 
-----------> computing the gradient one layer at a time, iterating backward from the last layer to avoid redundant calculations of intermediate terms in the chain rule; this is an example of dynamic programming.[3]
---------> The term backpropagation strictly refers only to the algorithm for computing the gradient, not how the gradient is used; 
-----------> however, the term is often used loosely to refer to the entire learning algorithm, including how the gradient is used, such as by stochastic gradient descent.
-----------> Backpropagation generalizes the gradient computation in the delta rule, which is the single-layer version of backpropagation, 
-----------> and is in turn generalized by automatic differentiation, where backpropagation is a special case of reverse accumulation (or "reverse mode").
-----------> The term backpropagation and its general use in neural networks was announced in Rumelhart, Hinton & Williams (1986a), 
-----------> then elaborated and popularized in Rumelhart, Hinton & Williams (1986b), 
-----------> but the technique was independently rediscovered many times, and had many predecessors dating to the 1960s; see § History.
-----------> A modern overview is given in the deep learning textbook by Goodfellow, Bengio & Courville (2016).


-------> Hopfield net: 
---------> This is a recurrent neural network in which all connections are symmetric
---------> A Hopfield network (or Ising model of a neural network or Ising–Lenz–Little model) is a form of recurrent artificial neural network 
-----------> and a type of spin glass system popularised by John Hopfield in 1982 as described earlier by Little in 1974
-----------> based on Ernst Ising's work with Wilhelm Lenz on the Ising model.
-----------> Hopfield networks serve as content-addressable ("associative") memory systems with binary threshold nodes, or with continuous variables.
-----------> Hopfield networks also provide a model for understanding human memory.

-------> Perceptron: 
---------> This is the simplest kind of feedforward neural network: a linear classifier.
---------> In machine learning, the perceptron (or McCullough-Pitts neuron) is an algorithm for supervised learning of binary classifiers. 
-----------> A binary classifier is a function which can decide whether or not an input, represented by a vector of numbers, belongs to some specific class.
-----------> It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. 

-------> Pulse-coupled neural networks (PCNN): 
---------> These are neural models proposed by modeling a cat's visual cortex and developed for high-performance biomimetic image processing.
---------> Pulse-coupled networks or pulse-coupled neural networks (PCNNs) are neural models proposed 
-----------> by modeling a cat's visual cortex, and developed for high-performance biomimetic image processing.
---------> In 1989, Eckhorn introduced a neural model to emulate the mechanism of cat's visual cortex. 
-----------> The Eckhorn model provided a simple and effective tool for studying small mammal’s visual cortex, 
-----------> and was soon recognized as having significant application potential in image processing.
---------> In 1994, Johnson adapted the Eckhorn model to an image processing algorithm, calling this algorithm a pulse-coupled neural network. 
-----------> Over the past decade, PCNNs have been used in a variety of image processing applications, including: 
-----------> image segmentation, feature generation, face extraction, motion detection, region growing, and noise reduction.
---------> The basic property of the Eckhorn's linking-field model (LFM) is the coupling term. 
-----------> LFM is a modulation of the primary input by a biased offset factor driven by the linking input. 
-----------> These drive a threshold variable that decays from an initial high value. 
-----------> When the threshold drops below zero it is reset to a high value and the process starts over. 
-----------> This is different than the standard integrate-and-fire neural model, 
-----------> which accumulates the input until it passes an upper limit and effectively "shorts out" to cause the pulse.
---------> LFM uses this difference to sustain pulse bursts, something the standard model does not do on a single neuron level. 
-----------> It is valuable to understand, however, that a detailed analysis of the standard model must include a shunting term, 
-----------> due to the floating voltages level in the dendritic compartment(s), 
-----------> and in turn this causes an elegant multiple modulation effect that enables a true higher-order network (HON).
-----------> Multidimensional pulse image processing of chemical structure data using PCNN has been discussed by Kinser, et al.
---------> A PCNN is a two-dimensional neural network. 
-----------> Each neuron in the network corresponds to one pixel in an input image, 
-----------> receiving its corresponding pixel's color information (e.g. intensity) as an external stimulus. 
-----------> Each neuron also connects with its neighboring neurons, receiving local stimuli from them. 
-----------> The external and local stimuli are combined in an internal activation system, 
-----------> which accumulates the stimuli until it exceeds a dynamic threshold, resulting in a pulse output. 
-----------> Through iterative computation, PCNN neurons produce temporal series of pulse outputs. 
-----------> The temporal series of pulse outputs contain information of input images and can be used for various image processing applications, 
-----------> such as image segmentation and feature generation. 
-----------> Compared with conventional image processing means, PCNNs have several significant merits, including robustness against noise, 
-----------> independence of geometric variations in input patterns, capability of bridging minor intensity variations in input patterns, etc.
---------> A simplified PCNN called a spiking cortical model was developed in 2009.
---------> PCNNs are useful for image processing, as discussed in a book by Thomas Lindblad and Jason M. Kinser.[5] 

-------> Radial basis function network: 
---------> This is an artificial neural network that uses radial basis functions as activation functions
---------> In the field of mathematical modeling, a radial basis function network is an artificial neural network 
-----------> that uses radial basis functions as activation functions. 
-----------> The output of the network is a linear combination of radial basis functions of the inputs and neuron parameters. 
-----------> Radial basis function networks have many uses, including function approximation, 
-----------> time series prediction, classification, and system control. 
-----------> They were first formulated in a 1988 paper by Broomhead and Lowe, both researchers at the Royal Signals and Radar Establishment.

-------> Self-organizing map: 
---------> This is an unsupervised network that produces a low-dimensional representation of the input space of the training samples
---------> A self-organizing map (SOM) or self-organizing feature map (SOFM) is an unsupervised machine learning technique used to produce a low-dimensional (typically two-dimensional) 
-----------> representation of a higher dimensional data set while preserving the topological structure of the data. 
-----------> For example, a data set with p variables measured in n observations could be represented as clusters of observations with similar values for the variables. 
-----------> These clusters then could be visualized as a two-dimensional "map" such that observations in proximal clusters have more similar values than observations in distal clusters. 
-----------> This can make high-dimensional data easier to visualize and analyze.
---------> An SOM is a type of artificial neural network but is trained using competitive learning rather than 
-----------> the error-correction learning (e.g., backpropagation with gradient descent) used by other artificial neural networks. 
-----------> The SOM was introduced by the Finnish professor Teuvo Kohonen in the 1980s 
-----------> and therefore is sometimes called a Kohonen map or Kohonen network.
-----------> The Kohonen map or network is a computationally convenient abstraction building on biological models of neural systems from the 1970s
-----------> and morphogenesis models dating back to Alan Turing in the 1950s.

-----> Random forest: 
-------> This is to classify using many decision trees
-------> Random forests or random decision forests is an ensemble learning method for classification, regression 
---------> and other tasks that operates by constructing a multitude of decision trees at training time. 
---------> For classification tasks, the output of the random forest is the class selected by most trees. 
---------> For regression tasks, the mean or average prediction of the individual trees is returned.
---------> Random decision forests correct for decision trees' habit of overfitting to their training set.
---------> Random forests generally outperform decision trees, but their accuracy is lower than gradient boosted trees.
---------> However, data characteristics can affect their performance.
-------> The first algorithm for random decision forests was created in 1995 by Tin Kam Ho using the random subspace method, which, in Ho's formulation, 
---------> is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.
-------> An extension of the algorithm was developed by Leo Breiman and Adele Cutler, who registered[11] "Random Forests" as a trademark in 2006 (as of 2019, owned by Minitab, Inc.). 
---------> The extension combines Breiman's "bagging" idea and random selection of features, introduced first by Ho
---------> and later independently by Amit and Geman in order to construct a collection of decision trees with controlled variance.
-------> Random forests are frequently used as "blackbox" models in businesses, as they generate reasonable predictions across a wide range of data while requiring little configuration. 

-----> Reinforcement learning:
-------> Reinforcement learning (RL) is an area of machine learning concerned with how intelligent agents ought 
---------> to take actions in an environment in order to maximize the notion of cumulative reward. 
---------> Reinforcement learning is one of three basic machine learning paradigms, 
---------> alongside supervised learning and unsupervised learning.
-------> Reinforcement learning differs from supervised learning in not needing labelled input/output pairs be presented, 
---------> and in not needing sub-optimal actions to be explicitly corrected. 
---------> Instead the focus is on finding a balance between exploration (of uncharted territory) and exploitation (of current knowledge).
---------> Partially supervised RL algorithms can combine the advantages of supervised and RL algorithms.
-------> The environment is typically stated in the form of a Markov decision process (MDP), 
---------> because many reinforcement learning algorithms for this context use dynamic programming techniques.
---------> The main difference between the classical dynamic programming methods and reinforcement learning algorithms 
---------> is that the latter do not assume knowledge of an exact mathematical model of the MDP and they target large MDPs where exact methods become infeasible.

-------> Q-learning: 
---------> This learns an action-value function that gives the expected utility of taking a given action in a given state and following a fixed policy thereafter
---------> Q-learning is a model-free reinforcement learning algorithm to learn the value of an action in a particular state. 
-----------> It does not require a model of the environment (hence "model-free"), 
-----------> and it can handle problems with stochastic transitions and rewards without requiring adaptations.
---------> For any finite Markov decision process (FMDP), Q-learning finds an optimal policy in the sense of maximizing the expected value 
-----------> of the total reward over any and all successive steps, starting from the current state. 
-----------> Q-learning can identify an optimal action-selection policy for any given FMDP, given infinite exploration time and a partly-random policy.
-----------> "Q" refers to the function that the algorithm computes – the expected rewards for an action taken in a given state.

-------> State–Action–Reward–State–Action (SARSA): 
---------> This learns a Markov decision process policy
---------> State–action–reward–state–action (SARSA) is an algorithm for learning a Markov decision process policy, 
-----------> used in the reinforcement learning area of machine learning. It was proposed by Rummery and Niranjan in a technical note[1] 
-----------> with the name "Modified Connectionist Q-Learning" (MCQ-L). 
-----------> The alternative name SARSA, proposed by Rich Sutton, was only mentioned as a footnote.
---------> This name reflects the fact that the main function for updating the Q-value depends on the current state of the agent "S1", 
-----------> the action the agent chooses "A1", the reward "R" the agent gets for choosing this action, the state "S2"
-----------> that the agent enters after taking that action, and finally the next action "A2" the agent chooses in its new state. 
-----------> The acronym for the quintuple (st, at, rt, st+1, at+1) is SARSA.
-----------> Some authors use a slightly different convention and write the quintuple (st, at, rt+1, st+1, at+1),
-----------> depending to which time step the reward is formally assigned. 
-----------> The rest of the article uses the former convention. 

-------> Temporal difference learning
---------> Temporal difference (TD) learning refers to a class of model-free reinforcement learning methods 
-----------> which learn by bootstrapping from the current estimate of the value function. 
-----------> These methods sample from the environment, like Monte Carlo methods, 
-----------> and perform updates based on current estimates, like dynamic programming methods.
---------> While Monte Carlo methods only adjust their estimates once the final outcome is known,
-----------> TD methods adjust predictions to match later, more accurate, predictions about the future before the final outcome is known.
-----------> This is a form of bootstrapping, as illustrated with the following example:
-------------> "Suppose you wish to predict the weather for Saturday, and you have some model that predicts Saturday's weather, 
-------------> given the weather of each day in the week. In the standard case, you would wait until Saturday and then adjust all your models. 
-------------> However, when it is, for example, Friday, you should have a pretty good idea of what the weather would be on Saturday – 
-------------> and thus be able to change, say, Saturday's model before Saturday arrives."
---------> Temporal difference methods are related to the temporal difference model of animal learning.

-----> Relevance-Vector Machine (RVM): 
---------> This is similar to SVM, but provides probabilistic classification
---------> In mathematics, a Relevance Vector Machine (RVM) is a machine learning technique 
-----------> that uses Bayesian inference to obtain parsimonious solutions for regression and probabilistic classification.
-----------> The RVM has an identical functional form to the support vector machine, but provides probabilistic classification.
---------> It is actually equivalent to a Gaussian process model with covariance function:
----------->     k(x, x′) = summation of j from 1 to N using 1/αj * φ(x, xj) * φ(x′, xj) 
-----------> where φ  is the kernel function (usually Gaussian), αj are the variances of the prior on the weight vector w ∼ N (0, α^(−1)*I), 
-----------> and x1, …, xN are the input vectors of the training set.
---------> Compared to that of support vector machines (SVM), the Bayesian formulation of the RVM 
-----------> avoids the set of free parameters of the SVM (that usually require cross-validation-based post-optimizations). 
-----------> However RVMs use an expectation maximization (EM)-like learning method and are therefore at risk of local minima. 
-----------> This is unlike the standard sequential minimal optimization (SMO)-based algorithms employed by SVMs, 
-----------> which are guaranteed to find a global optimum (of the convex problem).
---------> The relevance vector machine was patented in the United States by Microsoft (patent expired September 4, 2019).

-----> Supervised learning: 
-------> This is learning by examples (labelled data-set split into training-set and test-set)
-------> Supervised learning (SL) is the machine learning task of learning a function that maps an input to an output based on example input-output pairs.
---------> It infers a function from labeled training data consisting of a set of training examples.
---------> In supervised learning, each example is a pair consisting of an input object (typically a vector) and a desired output value (also called the supervisory signal). 
---------> A supervised learning algorithm analyzes the training data and produces an inferred function, which can be used for mapping new examples. 
---------> An optimal scenario will allow for the algorithm to correctly determine the class labels for unseen instances. 
---------> This requires the learning algorithm to generalize from the training data to unseen situations in a "reasonable" way (see inductive bias). 
---------> This statistical quality of an algorithm is measured through the so-called generalization error.
-------> The parallel task in human and animal psychology is often referred to as concept learning. 

-----> Support-Vector Machine (SVM): 
-------> This is a set of methods which divide multidimensional data by finding a dividing hyperplane with the maximum margin between the two sets
-------> In machine learning, support-vector machines (SVMs, also support-vector networks[1]) 
---------> are supervised learning models with associated learning algorithms that analyze data for classification and regression analysis. 
---------> Developed at AT&T Bell Laboratories by Vladimir Vapnik with colleagues (Boser et al., 1992, Guyon et al., 1993, Cortes and Vapnik, 1995,[1] Vapnik et al., 1997) 
---------> SVMs are one of the most robust prediction methods, being based on statistical learning frameworks or VC theory proposed by Vapnik (1982, 1995) and Chervonenkis (1974). 
---------> Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that assigns new examples to one category or the other, 
---------> making it a non-probabilistic binary linear classifier (although methods such as Platt scaling exist to use SVM in a probabilistic classification setting). 
---------> SVM maps training examples to points in space so as to maximise the width of the gap between the two categories. 
---------> New examples are then mapped into that same space and predicted to belong to a category based on which side of the gap they fall.
-------> In addition to performing linear classification, 
---------> SVMs can efficiently perform a non-linear classification using what is called the kernel trick, 
---------> implicitly mapping their inputs into high-dimensional feature spaces.
-------> When data are unlabelled, supervised learning is not possible, and an unsupervised learning approach is required, 
---------> which attempts to find natural clustering of the data to groups, and then map new data to these formed groups. 
---------> The support-vector clustering[2] algorithm, created by Hava Siegelmann and Vladimir Vapnik, applies the statistics of support vectors, 
---------> developed in the support vector machines algorithm, to categorize unlabeled data.[citation needed] 

-------> Structured SVM: 
---------> This allows training of a classifier for general structured output labels.
---------> The structured support-vector machine is a machine learning algorithm that generalizes the Support-Vector Machine (SVM) classifier. 
-----------> Whereas the SVM classifier supports binary classification, multiclass classification and regression, 
-----------> the structured SVM allows training of a classifier for general structured output labels.
---------> As an example, a sample instance might be a natural language sentence, and the output label is an annotated parse tree. 
-----------> Training a classifier consists of showing pairs of correct sample and output label pairs. 
-----------> After training, the structured SVM model allows one to predict for new sample instances the corresponding output label; 
-----------> that is, given a natural language sentence, the classifier can produce the most likely parse tree. 

-----> Winnow algorithm: 
-------> This is related to the perceptron, but uses a multiplicative weight-update scheme
-------> The winnow algorithm is a technique from machine learning for learning a linear classifier from labeled examples. 
---------> It is very similar to the perceptron algorithm. However, the perceptron algorithm uses an additive weight-update scheme, 
---------> while Winnow uses a multiplicative scheme that allows it to perform much better when many dimensions are irrelevant (hence its name winnow). 
---------> It is a simple algorithm that scales well to high-dimensional data. 
---------> During training, Winnow is shown a sequence of positive and negative examples. 
---------> From these it learns a decision hyperplane that can then be used to label novel examples as positive or negative. 
---------> The algorithm can also be used in the online learning setting, where the learning and the classification phase are not clearly separated. 



---> Programming language theory

-----> C3 linearization: 
-------> This is an algorithm used primarily to obtain a consistent linearization of a multiple inheritance hierarchy in object-oriented programming
-------> "In object-oriented systems with multiple inheritance, some mechanism must be used 
---------> for resolving conflicts when inheriting different definitions of the same property from multiple superclasses."
---------> C3 superclass linearization is an algorithm used primarily to obtain the order in which methods should be inherited in the presence of multiple inheritance. 
---------> In other words, the output of C3 superclass linearization is a deterministic Method Resolution Order (MRO).
-------> C3 superclass linearization is called C3 because it "is consistent with three properties":[1]
---------> a consistent extended precedence graph,
---------> preservation of local precedence order, and
---------> fitting a monotonicity criterion.
-------> It was first published at the 1996 OOPSLA conference, in a paper entitled "A Monotonic Superclass Linearization for Dylan".
---------> It was adapted to the Open Dylan implementation in January 2012[2] following an enhancement proposal.
---------> It has been chosen as the default algorithm for method resolution in Python 2.3 (and newer), Raku, Parrot, Solidity, and PGF/TikZ's Object-Oriented Programming module.
---------> It is also available as an alternative, non-default MRO in the core of Perl 5 starting with version 5.10.0.
---------> An extension implementation for earlier versions of Perl 5 named Class::C3 exists on CPAN.
-------> Python's Guido van Rossum summarizes C3 superclass linearization thus:
---------> Basically, the idea behind C3 is that if you write down all of the ordering rules imposed by inheritance relationships in a complex class hierarchy, 
---------> the algorithm will determine a monotonic ordering of the classes that satisfies all of them. 
---------> If such an ordering can not be determined, the algorithm will fail.
 
-----> Chaitin's algorithm: 
-------> This is a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric
-------> Chaitin's algorithm is a bottom-up, graph coloring register allocation algorithm that uses cost/degree as its spill metric. 
---------> It is named after its designer, Gregory Chaitin.
---------> Chaitin's algorithm was the first register allocation algorithm that made use of coloring of the interference graph for both register allocations and spilling.
-------> Chaitin's algorithm was presented on the 1982 SIGPLAN Symposium on Compiler Construction, and published in the symposium proceedings.
---------> It was extension of an earlier 1981 paper on the use of graph coloring for register allocation. 
---------> Chaitin's algorithm formed the basis of a large section of research into register allocators. 

-----> Hindley–Milner type inference algorithm
-------> A Hindley–Milner (HM) type system is a classical type system for the lambda calculus with parametric polymorphism. 
---------> It is also known as Damas–Milner or Damas–Hindley–Milner. 
---------> It was first described by J. Roger Hindley[1] and later rediscovered by Robin Milner.
---------> Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.
-------> Among HM's more notable properties are its completeness and its ability to infer the most general type 
---------> of a given program without programmer-supplied type annotations or other hints. 
---------> Algorithm W is an efficient type inference method in practice, and has been successfully applied on large code bases,
---------> although it has a high theoretical complexity.[note 1] HM is preferably used for functional languages. 
---------> It was first implemented as part of the type system of the programming language ML. 
---------> Since then, HM has been extended in various ways, most notably with type class constraints like those in Haskell. 

-----> Rete algorithm: 
-------> This is an efficient pattern matching algorithm for implementing production rule systems
-------> The Rete algorithm is a pattern matching algorithm for implementing rule-based systems. 
---------> The algorithm was developed to efficiently apply many rules or patterns to many objects, or facts, in a knowledge base. 
---------> It is used to determine which of the system's rules should fire based on its data store, its facts.
---------> The Rete algorithm was designed by Charles L. Forgy of Carnegie Mellon University, first published in a working paper in 1974, and later elaborated in his 1979 Ph.D. thesis and a 1982 paper

-----> Sethi-Ullman algorithm: 
-------> This generates optimal code for arithmetic expressions
-------> The Sethi–Ullman algorithm is for translating abstract syntax trees into machine code that uses as few registers as possible. 
-------> Note: This is an algorithm named after its inventors: Ravi Sethi and Jeffrey D. Ullman.

-----> Parsing
-------> Parsing, syntax analysis, or syntactic analysis is the process of analyzing a string of symbols, 
---------> either in natural language, computer languages or data structures, conforming to the rules of a formal grammar.
---------> The term parsing comes from Latin pars (orationis), meaning part (of speech).[
-------> The term has slightly different meanings in different branches of linguistics and computer science. 
---------> Traditional sentence parsing is often performed as a method of understanding the exact meaning of a sentence or word, 
---------> sometimes with the aid of devices such as sentence diagrams. 
---------> It usually emphasizes the importance of grammatical divisions such as subject and predicate.
-------> Within computational linguistics the term is used to refer to the formal analysis 
---------> by a computer of a sentence or other string of words into its constituents, 
---------> resulting in a parse tree showing their syntactic relation to each other, 
---------> which may also contain semantic and other information (p-values).
---------> Some parsing algorithms may generate a parse forest or list of parse trees for a syntactically ambiguous input.
-------> The term is also used in psycholinguistics when describing language comprehension. 
---------> In this context, parsing refers to the way that human beings analyze a sentence or phrase (in spoken language or text) "in terms of grammatical constituents, 
---------> identifying the parts of speech, syntactic relations, etc."
---------> This term is especially common when discussing which linguistic cues help speakers interpret garden-path sentences.
-------> Within computer science, the term is used in the analysis of computer languages, 
---------> referring to the syntactic analysis of the input code into its component parts in order to facilitate the writing of compilers and interpreters. 
---------> The term may also be used to describe a split or separation. 

-------> CYK algorithm: 
---------> This is an O(n3) algorithm for parsing context-free grammars in Chomsky normal form
---------> In computer science, the Cocke–Younger–Kasami algorithm (alternatively called CYK, or CKY) 
-----------> is a parsing algorithm for context-free grammars published by Itiroo Sakai in 1961.
-----------> The algorithm is named after some of its rediscoverers: John Cocke, Daniel Younger, Tadao Kasami, and Jacob T. Schwartz. 
-----------> It employs bottom-up parsing and dynamic programming.
---------> The standard version of CYK operates only on context-free grammars given in Chomsky normal form (CNF). 
-----------> However any context-free grammar may be transformed (after convention) to a CNF grammar expressing the same language (Sipser 1997).
---------> The importance of the CYK algorithm stems from its high efficiency in certain situations. 
-----------> Using big O notation, the worst case running time of CYK is O(n^3 * |G|) where n is the length of the parsed string and |G| is the size of the CNF grammar G. 
-----------> This makes it one of the most efficient parsing algorithms in terms of worst-case asymptotic complexity, 
-----------> although other algorithms exist with better average running time in many practical scenarios. 

-------> Earley parser: 
---------> This is another O(n3) algorithm for parsing any context-free grammar
---------> The Earley parser is an algorithm for parsing strings that belong to a given context-free language, 
-----------> though (depending on the variant) it may suffer problems with certain nullable grammars.
-----------> The algorithm, named after its inventor, Jay Earley, is a chart parser that uses dynamic programming; 
-----------> it is mainly used for parsing in computational linguistics. 
-----------> It was first introduced in his dissertation[2] in 1968 (and later appeared in an abbreviated, more legible, form in a journal[3]).
---------> Earley parsers are appealing because they can parse all context-free languages, unlike LR parsers and LL parsers, 
-----------> which are more typically used in compilers but which can only handle restricted classes of languages. 
-----------> The Earley parser executes in cubic time in the general case O(n^3), where n is the length of the parsed string, 
-----------> quadratic time for unambiguous grammars O(n^2), and linear time for all deterministic context-free grammars. 
-----------> It performs particularly well when the rules are written left-recursively. 

-------> GLR parser:
---------> This is an algorithm for parsing any context-free grammar by Masaru Tomita. 
---------> It is tuned for deterministic grammars, on which it performs almost linear time and O(n3) in worst case.
---------> A GLR parser (GLR standing for "Generalized LR", where L stands for "left-to-right" and R stands for "rightmost (derivation)") 
-----------> is an extension of an LR parser algorithm to handle non-deterministic and ambiguous grammars.
-----------> The theoretical foundation was provided in a 1974 paper[2] by Bernard Lang (along with other general Context-Free parsers such as GLL). 
-----------> It describes a systematic way to produce such algorithms, and provides uniform results regarding correctness proofs, 
-----------> complexity with respect to grammar classes, and optimization techniques. 
-----------> The first actual implementation of GLR was described in a 1984 paper by Masaru Tomita, it has also been referred to as a "parallel parser". 
-----------> Tomita presented five stages in his original work,[3] though in practice it is the second stage that is recognized as the GLR parser.
---------> Though the algorithm has evolved since its original forms, the principles have remained intact. 
-----------> As shown by an earlier publication,[4] Lang was primarily interested in more easily used and more flexible parsers for extensible programming languages. 
-----------> Tomita's goal was to parse natural language text thoroughly and efficiently. 
-----------> Standard LR parsers cannot accommodate the nondeterministic and ambiguous nature of natural language, and the GLR algorithm can. 

-------> Inside-outside algorithm: 
---------> This is an O(n3) algorithm for re-estimating production probabilities in probabilistic context-free grammars
---------> For parsing algorithms in computer science, the inside–outside algorithm
-----------> is a way of re-estimating production probabilities in a probabilistic context-free grammar. 
-----------> It was introduced by James K. Baker in 1979 as a generalization of the forward–backward algorithm 
-----------> for parameter estimation on hidden Markov models to stochastic context-free grammars. 
-----------> It is used to compute expectations, for example as part of the expectation–maximization algorithm (an unsupervised learning algorithm). 

-------> LL parser: 
---------> This is a relatively simple linear time parsing algorithm for a limited class of context-free grammars
---------> In computer science, an LL parser (Left-to-right, leftmost derivation) is a top-down parser for a restricted context-free language. 
-----------> It parses the input from Left to right, performing Leftmost derivation of the sentence.
---------> An LL parser is called an LL(k) parser if it uses k tokens of lookahead when parsing a sentence. 
-----------> A grammar is called an LL(k) grammar if an LL(k) parser can be constructed from it. A formal language is called an LL(k) language if it has an LL(k) grammar. 
-----------> The set of LL(k) languages is properly contained in that of LL(k+1) languages, for each k ≥ 0.
-----------> A corollary of this is that not all context-free languages can be recognized by an LL(k) parser.
---------> An LL parser is called LL-regular (LLR) if it parses an LL-regular language.
-----------> The class of LLR grammars contains every LL(k) grammar for every k. 
-----------> For every LLR grammar there exists an LLR parser that parses the grammar in linear time.
---------> Two nomenclative outlier parser types are LL(*) and LL(finite). 
-----------> A parser is called LL(*)/LL(finite) if it uses the LL(*)/LL(finite) parsing strategy. 
-----------> LL(*) and LL(finite) parsers are functionally more closely resemblant to PEG parsers. 
-----------> An LL(finite) parser can parse an arbitrary LL(k) grammar optimally in the amount of lookahead and lookahead comparisons.
-----------> The class of grammars parsable by the LL(*) strategy encompasses some context-sensitive languages due to the use of syntactic and semantic predicates and has not been identified. 
-----------> It has been suggested that LL(*) parsers are better thought of as TDPL parsers.[7] Against the popular misconception, LL(*) parsers are not LLR in general,
-----------> and are guaranteed by construction to perform worse on average (super-linear against linear time) and far worse in the worst-case (exponential against linear time).
---------> LL grammars, particularly LL(1) grammars, are of great practical interest, as parsers for these grammars are easy to construct, 
-----------> and many computer languages are designed to be LL(1) for this reason. 
-----------> LL parsers are table-based parsers, similar to LR parsers. 
-----------> LL grammars can also be parsed by recursive descent parsers. A
-----------> ccording to Waite and Goos (1984), LL(k) grammars were introduced by Stearns and Lewis (1969).

-------> LR parser: 
---------> This is a more complex linear time parsing algorithm for a larger class of context-free grammars.
---------> In computer science, LR parsers are a type of bottom-up parser that analyse deterministic context-free languages in linear time.
-----------> There are several variants of LR parsers: SLR parsers, LALR parsers, Canonical LR(1) parsers, Minimal LR(1) parsers, and GLR parsers. 
-----------> LR parsers can be generated by a parser generator from a formal grammar defining the syntax of the language to be parsed. 
-----------> They are widely used for the processing of computer languages.
---------> An LR parser (Left-to-right, Rightmost derivation in reverse) reads input text from left to right without backing up (this is true for most parsers), 
-----------> and produces a rightmost derivation in reverse: it does a bottom-up parse – not a top-down LL parse or ad-hoc parse. 
-----------> The name LR is often followed by a numeric qualifier, as in LR(1) or sometimes LR(k). 
-----------> To avoid backtracking or guessing, the LR parser is allowed to peek ahead at k lookahead input symbols before deciding how to parse earlier symbols. 
-----------> Typically k is 1 and is not mentioned. The name LR is often preceded by other qualifiers, as in SLR and LALR. 
-----------> The LR(k) notation for a grammar was suggested by Knuth to stand for "translatable from left to right with bound k."
---------> LR parsers are deterministic; they produce a single correct parse without guesswork or backtracking, in linear time. 
-----------> This is ideal for computer languages, but LR parsers are not suited for human languages which need more flexible but inevitably slower methods. 
-----------> Some methods which can parse arbitrary context-free languages (e.g., Cocke–Younger–Kasami, Earley, GLR) have worst-case performance of O(n3) time. 
-----------> Other methods which backtrack or yield multiple parses may even take exponential time when they guess badly.
---------> The above properties of L, R, and k are actually shared by all shift-reduce parsers, including precedence parsers. 
-----------> But by convention, the LR name stands for the form of parsing invented by Donald Knuth, and excludes the earlier, 
-----------> less powerful precedence methods (for example Operator-precedence parser).
-----------> LR parsers can handle a larger range of languages and grammars than precedence parsers or top-down LL parsing. 
-----------> This is because the LR parser waits until it has seen an entire instance of some grammar pattern before committing to what it has found. 
-----------> An LL parser has to decide or guess what it is seeing much sooner, when it has only seen the leftmost input symbol of that pattern. 

---------> Canonical LR parser
-----------> In computer science, a canonical LR parser or LR(1) parser is an LR(k) parser for k=1, i.e. with a single lookahead terminal. 
-------------> The special attribute of this parser is that any LR(k) grammar with k>1 can be transformed into an LR(1) grammar.
-------------> However, back-substitutions are required to reduce k and as back-substitutions increase, 
-------------> the grammar can quickly become large, repetitive and hard to understand. 
-------------> LR(k) can handle all deterministic context-free languages.
-------------> In the past this LR(k) parser has been avoided because of its huge memory requirements in favor of less powerful alternatives such as the LALR and the LL(1) parser. 
-------------> Recently, however, a "minimal LR(1) parser" whose space requirements are close to LALR parsers, is being offered by several parser generators.
-----------> Like most parsers, the LR(1) parser is automatically generated by compiler-compilers like GNU Bison, MSTA, Menhir, HYACC, LRSTAR.

---------> LALR (look-ahead LR) parser
-----------> In computer science, an LALR parser[a] or Look-Ahead LR parser is a simplified version of a canonical LR parser, 
-------------> to parse a text according to a set of production rules specified by a formal grammar for a computer language. ("LR" means left-to-right, rightmost derivation.)
-----------> The LALR parser was invented by Frank DeRemer in his 1969 PhD dissertation, Practical Translators for LR(k) languages,
-------------> in his treatment of the practical difficulties at that time of implementing LR(1) parsers. 
-------------> He showed that the LALR parser has more language recognition power than the LR(0) parser, 
-------------> while requiring the same number of states as the LR(0) parser for a language that can be recognized by both parsers. 
-------------> This makes the LALR parser a memory-efficient alternative to the LR(1) parser for languages that are LALR. 
-------------> It was also proven that there exist LR(1) languages that are not LALR. 
-------------> Despite this weakness, the power of the LALR parser is sufficient for many mainstream computer languages, including Java, 
-------------> though the reference grammars for many languages fail to be LALR due to being ambiguous.
-----------> The original dissertation gave no algorithm for constructing such a parser given a formal grammar. 
-------------> The first algorithms for LALR parser generation were published in 1973. 
-------------> In 1982, DeRemer and Tom Pennello published an algorithm that generated highly memory-efficient LALR parsers.
-------------> LALR parsers can be automatically generated from a grammar by an LALR parser generator such as Yacc or GNU Bison. 
-------------> The automatically generated code may be augmented by hand-written code to augment the power of the resulting parser. 

---------> Operator-precedence parser
-----------> In computer science, an operator precedence parser is a bottom-up parser that interprets an operator-precedence grammar. 
-------------> For example, most calculators use operator precedence parsers to convert from the human-readable infix notation relying on order of operations 
-------------> to a format that is optimized for evaluation such as Reverse Polish notation (RPN).
-----------> Edsger Dijkstra's shunting yard algorithm is commonly used to implement operator precedence parsers. 

---------> SLR (Simple LR) parser
-----------> In computer science, a Simple LR or SLR parser is a type of LR parser with small parse tables and a relatively simple parser generator algorithm. 
-------------> As with other types of LR(1) parser, an SLR parser is quite efficient at finding the single correct bottom-up parse
------------->  in a single left-to-right scan over the input stream, without guesswork or backtracking. 
-------------> The parser is mechanically generated from a formal grammar for the language.
-----------> SLR and the more-general methods LALR parser and Canonical LR parser have identical methods and similar tables at parse time; 
-------------> they differ only in the mathematical grammar analysis algorithms used by the parser generator tool. 
-------------> SLR and LALR generators create tables of identical size and identical parser states. 
-------------> SLR generators accept fewer grammars than do LALR generators like yacc and Bison. 
-------------> Many computer languages don't readily fit the restrictions of SLR, as is. 
-------------> Bending the language's natural grammar into SLR grammar form requires more compromises and grammar hackery. 
-------------> So LALR generators have become much more widely used than SLR generators, despite being somewhat more complicated tools. 
-------------> SLR methods remain a useful learning step in college classes on compiler theory.
-----------> SLR and LALR were both developed by Frank DeRemer as the first practical uses of Donald Knuth's LR parser theory.
-------------> The tables created for real grammars by full LR methods were impractically large, larger than most computer memories of that decade, 
-------------> with 100 times or more parser states than the SLR and LALR methods. 

---------> Simple precedence parser
-----------> In computer science, a simple precedence parser is a type of bottom-up parser for context-free grammars that can be used only by simple precedence grammars.
-----------> The implementation of the parser is quite similar to the generic bottom-up parser. 
-------------> A stack is used to store a viable prefix of a sentential form from a rightmost derivation. 
-------------> Symbols ⋖, ≐ and ⋗ are used to identify the pivot, and to know when to Shift or when to Reduce. 
-----------> Implementation
-------------> Compute the Wirth–Weber precedence relationship table for a grammar with initial symbol S.
-------------> Initialize a stack with the starting marker $.
-------------> Append an ending marker $ to the string being parsed (Input).
-------------> Until Stack equals "$ S" and Input equals "$"
---------------> Search the table for the relationship between Top(stack) and NextToken(Input)
---------------> if the relationship is ⋖ or ≐
-----------------> Shift:
-----------------> Push(Stack, relationship)
-----------------> Push(Stack, NextToken(Input))
-----------------> RemoveNextToken(Input)
---------------> if the relationship is ⋗
-----------------> Reduce:
-----------------> SearchProductionToReduce(Stack)
-----------------> Remove the Pivot from the Stack
-----------------> Search the table for the relationship between the nonterminal from the production and first symbol in the stack (Starting from top)
-----------------> Push(Stack, relationship)
-----------------> Push(Stack, Non terminal)
-------------> SearchProductionToReduce (Stack)
---------------> Find the topmost ⋖ in the stack; this and all the symbols above it are the Pivot.
---------------> Find the production of the grammar which has the Pivot as its right side.

-------> Packrat parser: 
---------> This is a linear time parsing algorithm supporting some context-free grammars and parsing expression grammars
---------> Any parsing expression grammar can be converted directly into a recursive descent parser.
-----------> Due to the unlimited lookahead capability that the grammar formalism provides, 
-----------> however, the resulting parser could exhibit exponential time performance in the worst case.
---------> It is possible to obtain better performance for any parsing expression grammar by converting its recursive descent parser into a packrat parser, 
-----------> which always runs in linear time, at the cost of substantially greater storage space requirements. 
-----------> A packrat parser is a form of parser similar to a recursive descent parser in construction, 
-----------> except that during the parsing process it memoizes the intermediate results of all invocations of the mutually recursive parsing functions, 
-----------> ensuring that each parsing function is only invoked at most once at a given input position. 
-----------> Because of this memoization, a packrat parser has the ability to parse many context-free grammars and any parsing expression grammar 
-----------> (including some that do not represent context-free languages) in linear time. 
-----------> Examples of memoized recursive descent parsers are known from at least as early as 1993.
-----------> This analysis of the performance of a packrat parser assumes that enough memory is available to hold all of the memoized results; 
-----------> in practice, if there is not enough memory, some parsing functions might have to be invoked more than once at the same input position, 
-----------> and consequently the parser could take more than linear time.
---------> It is also possible to build LL parsers and LR parsers from parsing expression grammars, with better worst-case performance than a recursive descent parser, 
-----------> but the unlimited lookahead capability of the grammar formalism is then lost. 
-----------> Therefore, not all languages that can be expressed using parsing expression grammars can be parsed by LL or LR parsers. 

-------> Recursive descent parser: 
---------> This is a top-down parser suitable for LL(k) grammars
---------> In computer science, a recursive descent parser is a kind of top-down parser built from a set of mutually recursive procedures (or a non-recursive equivalent) 
-----------> where each such procedure implements one of the nonterminals of the grammar. 
-----------> Thus the structure of the resulting program closely mirrors that of the grammar it recognizes.[1]
---------> A predictive parser is a recursive descent parser that does not require backtracking. 
-----------> Predictive parsing is possible only for the class of LL(k) grammars, which are the context-free grammars for which there exists some positive integer k 
-----------> that allows a recursive descent parser to decide which production to use by examining only the next k tokens of input. 
-----------> The LL(k) grammars therefore exclude all ambiguous grammars, as well as all grammars that contain left recursion. 
-----------> Any context-free grammar can be transformed into an equivalent grammar that has no left recursion, but removal of left recursion does not always yield an LL(k) grammar. 
-----------> A predictive parser runs in linear time.
---------> Recursive descent with backtracking is a technique that determines which production to use by trying each production in turn. 
-----------> Recursive descent with backtracking is not limited to LL(k) grammars, but is not guaranteed to terminate unless the grammar is LL(k). 
-----------> Even when they terminate, parsers that use recursive descent with backtracking may require exponential time.
---------> Although predictive parsers are widely used, and are frequently chosen if writing a parser by hand, 
-----------> programmers often prefer to use a table-based parser produced by a parser generator,[citation needed] either for an LL(k) language or using an alternative parser, such as LALR or LR. 
-----------> This is particularly the case if a grammar is not in LL(k) form, as transforming the grammar to LL to make it suitable for predictive parsing is involved. 
-----------> Predictive parsers can also be automatically generated, using tools like ANTLR.
---------> Predictive parsers can be depicted using transition diagrams for each non-terminal symbol where the edges between the initial 
-----------> and the final states are labelled by the symbols (terminals and non-terminals) of the right side of the production rule.

-------> Shunting-yard algorithm: 
---------> This converts an infix-notation math expression to postfix
---------> In computer science, the shunting yard algorithm is a method for parsing arithmetical or logical expressions, 
-----------> or a combination of both, specified in infix notation. 
-----------> It can produce either a postfix notation string, also known as Reverse Polish notation (RPN), or an abstract syntax tree (AST). 
-----------> The algorithm was invented by Edsger Dijkstra and named the "shunting yard" algorithm because its operation resembles that of a railroad shunting yard. 
-----------> Dijkstra first described the shunting yard algorithm in the Mathematisch Centrum report MR 34/61.
---------> Like the evaluation of RPN, the shunting yard algorithm is stack-based. 
-----------> Infix expressions are the form of mathematical notation most people are used to, for instance "3 + 4" or "3 + 4 × (2 − 1)". 
-----------> For the conversion there are two text variables (strings), the input and the output. 
-----------> There is also a stack that holds operators not yet added to the output queue. 
-----------> To convert, the program reads each symbol in order and does something based on that symbol. 
-----------> The result for the above examples would be (in Reverse Polish notation) "3 4 +" and "3 4 2 1 − × +", respectively.
---------> The shunting yard algorithm will correctly parse all valid infix expressions, but does not reject all invalid expressions. 
-----------> For example, "1 2 +" is not a valid infix expression, but would be parsed as "1 + 2". 
-----------> The algorithm can however reject expressions with mismatched parentheses.
---------> The shunting yard algorithm was later generalized into operator-precedence parsing. 

-------> Pratt parser
---------> Another precedence parser known as Pratt parsing was first described by Vaughan Pratt in the 1973 paper "Top down operator precedence", based on recursive descent. 
-----------> Though it predates precedence climbing, it can be viewed as a generalization of precedence climbing.[4]
---------> Pratt designed the parser originally to implement the CGOL programming language, and it was treated in much more depth in a Masters Thesis under his supervision.[5]

-------> Lexical analysis
---------> In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) 
-----------> into a sequence of lexical tokens (strings with an assigned and thus identified meaning). 
-----------> A program that performs lexical analysis may be termed a lexer, tokenizer, or scanner, although scanner is also a term for the first stage of a lexer. 
---------> A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth. 


---> Quantum algorithms

-----> Deutsch–Jozsa algorithm: 
-------> This is criterion of balance for Boolean function
-------> The Deutsch–Jozsa algorithm is a deterministic quantum algorithm proposed by David Deutsch and Richard Jozsa in 1992 
---------> with improvements by Richard Cleve, Artur Ekert, Chiara Macchiavello, and Michele Mosca in 1998.
---------> Although of little current practical use, it is one of the first examples of a quantum algorithm 
---------> that is exponentially faster than any possible deterministic classical algorithm.[3]


-----> Grover's algorithm: 
-------> This provides quadratic speedup for many search problems
-------> In quantum computing, Grover's algorithm, also known as the quantum search algorithm, 
---------> refers to a quantum algorithm for unstructured search that finds with high probability the unique input to a black box function 
---------> that produces a particular output value, using just O (sqrt(N)) evaluations of the function, 
---------> where n is the size of the function's domain. It was devised by Lov Grover in 1996.[1]
-------> The analogous problem in classical computation cannot be solved in fewer than O(N) evaluations 
---------> (because, on average, one has to check half of the domain to get a 50% chance of finding the right input). 
---------> Charles H. Bennett, Ethan Bernstein, Gilles Brassard, and Umesh Vazirani proved that any quantum solution to the problem needs to evaluate the function Ω (sqrt(N)) times, 
---------> so Grover's algorithm is asymptotically optimal.[2] Since classical algorithms for NP-complete problems require exponentially many steps, 
---------> and Grover's algorithm provides at most a quadratic speedup over the classical solution for unstructured search,
--------->  this suggests that Grover's algorithm by itself will not provide polynomial-time solutions for NP-complete problems 
--------->  (as the square root of an exponential function is an exponential, not polynomial, function).
-------> Unlike other quantum algorithms, which may provide exponential speedup over their classical counterparts, 
---------> Grover's algorithm provides only a quadratic speedup. 
---------> However, even quadratic speedup is considerable when n is large, and Grover's algorithm can be applied to speed up broad classes of algorithms.
---------> Grover's algorithm could brute-force a 128-bit symmetric cryptographic key in roughly 264 iterations, or a 256-bit key in roughly 2128 iterations. 
---------> As a result, it is sometimes suggested that symmetric key lengths be doubled to protect against future quantum attacks. 

-----> Shor's algorithm: 
-------> This provides exponential speedup (relative to currently known non-quantum algorithms) for factoring a number
-------> Shor's algorithm is a quantum computer algorithm for finding the prime factors of an integer. 
---------> It was developed in 1994 by the American mathematician Peter Shor.[1]
-------> On a quantum computer, to factor an integer n, Shor's algorithm runs in polynomial time, meaning the time taken is polynomial in log ⁡ N, the size of the integer given as input.
---------> Specifically, it takes quantum gates of order O((log⁡ N)^2 * (log⁡ log⁡N) * (log⁡ log⁡ logN⁡ )) using fast multiplication,
---------> Thus demonstrating that the integer factorization problem can be efficiently solved on a quantum computer and is consequently in the complexity class BQP. 
---------> This is almost exponentially faster than the most efficient known classical factoring algorithm, the general number field sieve, 
---------> which works in sub-exponential time: O(e^(1.9(logN)^(1/3)*(log⁡ log⁡ N)^(2/3))).
---------> The efficiency of Shor's algorithm is due to the efficiency of the quantum Fourier transform, and modular exponentiation by repeated squarings.
-------> If a quantum computer with a sufficient number of qubits could operate without succumbing to quantum noise and other quantum-decoherence phenomena, 
---------> then Shor's algorithm could be used to break public-key cryptography schemes, such as:
-----------> The RSA scheme
-----------> The Finite Field Diffie-Hellman key exchange
-----------> The Elliptic Curve Diffie-Hellman key exchange[6]
-------> RSA is based on the assumption that factoring large integers is computationally intractable. 
---------> As far as is known, this assumption is valid for classical (non-quantum) computers; no classical algorithm is known that can factor integers in polynomial time. 
---------> However, Shor's algorithm shows that factoring integers is efficient on an ideal quantum computer, so it may be feasible to defeat RSA by constructing a large quantum computer. 
---------> It was also a powerful motivator for the design and construction of quantum computers, and for the study of new quantum-computer algorithms. 
---------> It has also facilitated research on new cryptosystems that are secure from quantum computers, collectively called post-quantum cryptography.
-------> In 2001, Shor's algorithm was demonstrated by a group at IBM, who factored 15 into 3 × 5, using an NMR implementation of a quantum computer with 7 qubits.
---------> After IBM's implementation, two independent groups implemented Shor's algorithm using photonic qubits, 
---------> emphasizing that multi-qubit entanglement was observed when running the Shor's algorithm circuits.
---------> In 2012, the factorization of 15 was performed with solid-state qubits.
---------> Also, in 2012, the factorization of 21 was achieved, setting the record for the largest integer factored with Shor's algorithm.
---------> In 2019 an attempt was made to factor the number 35 using Shor's algorithm on an IBM Q System One, but the algorithm failed because of accumulating errors.
---------> Though larger numbers have been factored by quantum computers using other algorithms, these algorithms are similar to classical brute-force checking of factors, 
---------> so unlike Shor's algorithm, they are not expected to ever perform better than classical factoring algorithms.

-----> Simon's algorithm: 
-------> This provides a provably exponential speedup (relative to any non-quantum algorithm) for a black-box problem
-------> In computational complexity theory and quantum computing, 
---------> Simon's problem is a computational problem that is proven to be solved exponentially faster on a quantum computer than on a classical (that is, traditional) computer. 
---------> The quantum algorithm solving Simon's problem, usually called Simon's algorithm, served as the inspiration for Shor's algorithm.
---------> Both problems are special cases of the abelian hidden subgroup problem, which is now known to have efficient quantum algorithms.
-------> The problem is set in the model of decision tree complexity or query complexity and was conceived by Daniel Simon in 1994.
---------> Simon exhibited a quantum algorithm that solves Simon's problem exponentially faster 
---------> and with exponentially fewer queries than the best probabilistic (or deterministic) classical algorithm. 
---------> In particular, Simon's algorithm uses a linear number of queries and any classical probabilistic algorithm must use an exponential number of queries.
-------> This problem yields an oracle separation between the complexity classes BPP (bounded-error classical query complexity) 
---------> and BQP (bounded-error quantum query complexity).
---------> This is the same separation that the Bernstein–Vazirani algorithm achieves, 
---------> and different from the separation provided by the Deutsch–Jozsa algorithm, which separates P and EQP. 
---------> Unlike the Bernstein–Vazirani algorithm, Simon's algorithm's separation is exponential.
-------> Because this problem assumes the existence of a highly-structured "black box" oracle to achieve its speedup, this problem has little practical value.
---------> However, without such an oracle, exponential speedups cannot easily be proven, since this would prove that P is different from PSPACE. 


---> Theory of computation and automata

-----> Hopcroft's algorithm, Moore's algorithm, and Brzozowski's algorithm: 
-------> These algorithms are for minimizing the number of states in a deterministic finite automaton
-------> One algorithm for merging the nondistinguishable states of a DFA, due to Hopcroft (1971), 
---------> is based on partition refinement, partitioning the DFA states into groups by their behavior. 
---------> These groups represent equivalence classes of the Myhill–Nerode equivalence relation, 
---------> whereby every two states of the same partition are equivalent if they have the same behavior for all the input sequences. 
---------> That is, for every two states p1 and p2 that belong to the same equivalence class within the partition P, and every input word w, 
---------> the transitions determined by w should always take states p1 and p2 to equal states, states that both accept, or states that both reject. It should not be possible for w to take p1 to an accepting state and p2 to a rejecting state or vice versa.
-------> The following pseudocode describes the form of the algorithm as given by Xu.
---------> Alternative forms have also been presented.
---------> P := {F, Q \ F}
---------> W := {F, Q \ F}
---------> while (W is not empty) do
--------->      choose and remove a set A from W
--------->      for each c in Σ do
--------->           let X be the set of states for which a transition on c leads to a state in A
--------->           for each set Y in P for which X ∩ Y is nonempty and Y \ X is nonempty do
--------->                replace Y in P by the two sets X ∩ Y and Y \ X
--------->                if Y is in W
--------->                     replace Y in W by the same two sets
--------->                else
--------->                     if |X ∩ Y| <= |Y \ X|
--------->                          add X ∩ Y to W
--------->                     else
--------->                          add Y \ X to W
-------> The algorithm starts with a partition that is too coarse: 
---------> every pair of states that are equivalent according to the Myhill–Nerode relation belong to the same set in the partition, 
---------> but pairs that are inequivalent might also belong to the same set. 
---------> It gradually refines the partition into a larger number of smaller sets, at each step splitting sets of states into pairs of subsets that are necessarily inequivalent. 
---------> The initial partition is a separation of the states into two subsets of states that clearly do not have the same behavior as each other: 
---------> the accepting states and the rejecting states. The algorithm then repeatedly chooses a set A from the current partition and an input symbol c, 
---------> and splits each of the sets of the partition into two (possibly empty) subsets: 
---------> the subset of states that lead to A on input symbol c, and the subset of states that do not lead to A. 
---------> Since A is already known to have different behavior than the other sets of the partition, 
---------> the subsets that lead to A also have different behavior than the subsets that do not lead to A. 
---------> When no more splits of this type can be found, the algorithm terminates. 

-----> Powerset construction: 
-------> This algorithm is to convert nondeterministic automaton to deterministic automaton.
-------> In the theory of computation and automata theory, the powerset construction or subset construction 
---------> is a standard method for converting a nondeterministic finite automaton (NFA) into a deterministic finite automaton (DFA) which recognizes the same formal language. 
---------> It is important in theory because it establishes that NFAs, despite their additional flexibility, 
---------> are unable to recognize any language that cannot be recognized by some DFA. 
---------> It is also important in practice for converting easier-to-construct NFAs into more efficiently executable DFAs. 
---------> However, if the NFA has n states, the resulting DFA may have up to 2n states, an exponentially larger number, 
---------> which sometimes makes the construction impractical for large NFAs.
-------> The construction, sometimes called the Rabin–Scott powerset construction (or subset construction) 
---------> to distinguish it from similar constructions for other types of automata, was first published by Michael O. Rabin and Dana Scott in 1959.[1] 

-----> Tarski–Kuratowski algorithm: 
-------> This a non-deterministic algorithm which provides an upper bound for the complexity of formulas in the arithmetical hierarchy and analytical hierarchy
-------> In computability theory and mathematical logic the Tarski–Kuratowski algorithm is a non-deterministic algorithm 
---------> which produces an upper bound for the complexity of a given formula in the arithmetical hierarchy and analytical hierarchy.
-------> The algorithm is named after Alfred Tarski and Kazimierz Kuratowski.
-------> Algorithm
---------> The Tarski–Kuratowski algorithm for the arithmetical hierarchy consists of the following steps:
-----------> (1) Convert the formula to prenex normal form. 
-------------> (This is the non-deterministic part of the algorithm, as there may be more than one valid prenex normal form for the given formula.)
-----------> (2) If the formula is quantifier-free, it is in Σ from 0 to 0  and Π from 0 to 0.
-----------> (3) Otherwise, count the number of alternations of quantifiers; call this k.
-----------> (4) If the first quantifier is ∃, the formula is in Σ from k+1 to 0.
-----------> (5) If the first quantifier is ∀, the formula is in Π from k+1 to 0.



-> Information theory and signal processing

---> Coding theory
-----> Coding theory is the study of the properties of codes and their respective fitness for specific applications. 
-------> Codes are used for data compression, cryptography, error detection and correction, data transmission and data storage. 
-------> Codes are studied by various scientific disciplines—such as information theory, electrical engineering, mathematics, linguistics, 
-------> and computer science—for the purpose of designing efficient and reliable data transmission methods. 
-------> This typically involves the removal of redundancy and the correction or detection of errors in the transmitted data.
-----> There are four types of coding:
-------> (1) Data compression (or source coding)
-------> (2) Error control (or channel coding)
-------> (3) Cryptographic coding
-------> (4) Line coding
-----> Data compression attempts to remove unwanted redundancy from the data from a source in order to transmit it more efficiently. 
-------> For example, ZIP data compression makes data files smaller, for purposes such as to reduce Internet traffic. 
-------> Data compression and error correction may be studied in combination.
-----> Error correction adds useful redundancy to the data from a source to make the transmission more robust to disturbances present on the transmission channel. 
-------> The ordinary user may not be aware of many applications using error correction. 
-------> A typical music compact disc (CD) uses the Reed–Solomon code to correct for scratches and dust. 
-------> In this application the transmission channel is the CD itself. 
-------> Cell phones also use coding techniques to correct for the fading and noise of high frequency radio transmission. 
-------> Data modems, telephone transmissions, and the NASA Deep Space Network 
---------> all employ channel coding techniques to get the bits through, for example the turbo code and LDPC codes. 

-----> Error detection and correction
-------> In information theory and coding theory with applications in computer science and telecommunication, 
---------> error detection and correction (EDAC) or error control are techniques that enable reliable delivery of digital data over unreliable communication channels. 
---------> Many communication channels are subject to channel noise, and thus errors may be introduced during transmission from the source to a receiver. 
---------> Error detection techniques allow detecting such errors, while error correction enables reconstruction of the original data in many cases. 

-------> BCH Codes
---------> In coding theory, the Bose–Chaudhuri–Hocquenghem codes (BCH codes) form a class of cyclic error-correcting codes 
-----------> that are constructed using polynomials over a finite field (also called Galois field). 
---------> One of the key features of BCH codes is that during code design, there is a precise control over the number of symbol errors correctable by the code. 
-----------> In particular, it is possible to design binary BCH codes that can correct multiple bit errors. 
-----------> Another advantage of BCH codes is the ease with which they can be decoded, namely, via an algebraic method known as syndrome decoding. 
-----------> This simplifies the design of the decoder for these codes, using small low-power electronic hardware.
---------> BCH codes are used in applications such as satellite communications, compact disc players, DVDs, disk drives, 
-----------> USB flash drives, solid-state drives, quantum-resistant cryptography[6] and two-dimensional bar codes. 
---------> Note: BCH codes were invented in 1959 by French mathematician Alexis Hocquenghem, and independently in 1960 by Raj Chandra Bose and D.K. Ray-Chaudhuri. 
-----------> The name Bose–Chaudhuri–Hocquenghem (and the acronym BCH) arises from the initials of the inventors' surnames (mistakenly, in the case of Ray-Chaudhuri).

---------> Berlekamp–Massey algorithm
-----------> The Berlekamp–Massey algorithm is an algorithm that will find the shortest linear-feedback shift register (LFSR) for a given binary output sequence. 
-------------> The algorithm will also find the minimal polynomial of a linearly recurrent sequence in an arbitrary field. 
-------------> The field requirement means that the Berlekamp–Massey algorithm requires all non-zero elements to have a multiplicative inverse.
-------------> Reeds and Sloane offer an extension to handle a ring.
-----------> Elwyn Berlekamp invented an algorithm for decoding Bose–Chaudhuri–Hocquenghem (BCH) codes.
-------------> James Massey recognized its application to linear feedback shift registers and simplified the algorithm.
-------------> Massey termed the algorithm the LFSR Synthesis Algorithm (Berlekamp Iterative Algorithm) but it is now known as the Berlekamp–Massey algorithm. 

---------> Peterson–Gorenstein–Zierler algorithm
-----------> Peterson's algorithm is the step 2 of the generalized BCH decoding procedure. 
-------------> Peterson's algorithm is used to calculate the error locator polynomial coefficients λ1, λ2, …, λv  of a polynomial
---------------> Λ(x) = 1 + λ1*x + λ2*(x^2) + ... + λv*(x^v).
-----------> Now the procedure of the Peterson–Gorenstein–Zierler algorithm.
-------------> Expect we have at least 2t syndromes sc, …, sc+2t−1. Let v = t. 

---------> Reed–Solomon error correction
-----------> Reed–Solomon codes are a group of error-correcting codes that were introduced by Irving S. Reed and Gustave Solomon in 1960.
-------------> They have many applications, the most prominent of which include consumer technologies such as MiniDiscs, CDs, DVDs, Blu-ray discs, QR codes,
-------------> data transmission technologies such as DSL and WiMAX, broadcast systems such as satellite communications, DVB and ATSC, and storage systems such as RAID 6.
-----------> Reed–Solomon codes operate on a block of data treated as a set of finite-field elements called symbols. 
-------------> Reed–Solomon codes are able to detect and correct multiple symbol errors. By adding t = n − k check symbols to the data, 
-------------> a Reed–Solomon code can detect (but not correct) any combination of up to t erroneous symbols, 
-------------> or locate and correct up to floor(t/2) erroneous symbols at unknown locations. 
-------------> As an erasure code, it can correct up to t erasures at locations that are known and provided to the algorithm, 
-------------> or it can detect and correct combinations of errors and erasures. 
-------------> Reed–Solomon codes are also suitable as multiple-burst bit-error correcting codes, 
-------------> since a sequence of b+1 consecutive bit errors can affect at most two symbols of size b. 
-------------> The choice of t is up to the designer of the code and may be selected within wide limits.
-----------> There are two basic types of Reed–Solomon codes – original view and BCH view – with BCH view being the most common,
-------------> as BCH view decoders are faster and require less working storage than original view decoders. 

-------> BCJR algorithm: 
---------> This is decoding of error correcting codes defined on trellises (principally convolutional codes)
---------> The BCJR algorithm is an algorithm for maximum a posteriori decoding of error correcting codes defined on trellises (principally convolutional codes). 
-----------> The algorithm is named after its inventors: Bahl, Cocke, Jelinek and Raviv.
-----------> This algorithm is critical to modern iteratively-decoded error-correcting codes including turbo codes and low-density parity-check codes. 

-------> Forward error correction
---------> In telecommunication, information theory, and coding theory, forward error correction (FEC) 
-----------> or channel coding is a technique used for controlling errors in data transmission over unreliable or noisy communication channels. 
-----------> The central idea is that the sender encodes the message in a redundant way, most often by using an ECC.
---------> The redundancy allows the receiver to detect a limited number of errors that may occur anywhere in the message, 
-----------> and often to correct these errors without re-transmission. 
-----------> FEC gives the receiver the ability to correct errors without needing a reverse channel to request re-transmission of data, 
-----------> but at the cost of a fixed, higher forward channel bandwidth. 
-----------> FEC is therefore applied in situations where re-transmissions are costly or impossible, 
-----------> such as one-way communication links and when transmitting to multiple receivers in multicast. 
-----------> FEC information is usually added to mass storage (magnetic, optical and solid state/flash based) devices to enable recovery of corrupted data, 
-----------> is widely used in modems, is used on systems where the primary memory is ECC memory and in broadcast situations, 
-----------> where the receiver does not have capabilities to request re-transmission or doing so would induce significant latency. 
-----------> For example, in the case of a satellite orbiting Uranus, a re-transmission because of decoding errors would create a delay of at least 5 hours.
---------> FEC processing in a receiver may be applied to a digital bit stream or in the demodulation of a digitally modulated carrier. 
-----------> For the latter, FEC is an integral part of the initial analog-to-digital conversion in the receiver. 
-----------> The Viterbi decoder implements a soft-decision algorithm to demodulate digital data from an analog signal corrupted by noise. 
-----------> Many FEC coders can also generate a bit-error rate (BER) signal which can be used as feedback to fine-tune the analog receiving electronics.
---------> The maximum proportion of errors or missing bits that can be corrected is determined by the design of the ECC, 
-----------> so different forward error correcting codes are suitable for different conditions. 
-----------> In general, a stronger code induces more redundancy that needs to be transmitted using the available bandwidth, 
-----------> which reduces the effective bit-rate while improving the received effective signal-to-noise ratio. 
-----------> The noisy-channel coding theorem of Claude Shannon answers the question of how much bandwidth 
-----------> is left for data communication while using the most efficient code that turns the decoding error probability to zero. 
-----------> This establishes bounds on the theoretical maximum information transfer rate of a channel with some given base noise level.
-----------> His proof is not constructive, and hence gives no insight of how to build a capacity achieving code. 
-----------> However, after years of research, some advanced FEC systems like polar code achieve the Shannon channel capacity under the hypothesis of an infinite length frame.

-------> Gray code
---------> The reflected binary code (RBC), also known as reflected binary (RB) or Gray code after Frank Gray, 
-----------> is an ordering of the binary numeral system such that two successive values differ in only one bit (binary digit).
---------> For example, the representation of the decimal value "1" in binary would normally be "001" and "2" would be "010". 
-----------> In Gray code, these values are represented as "001" and "011". 
-----------> That way, incrementing a value from 1 to 2 requires only one bit to change, instead of two.
---------> Gray codes are widely used to prevent spurious output from electromechanical switches 
-----------> and to facilitate error correction in digital communications such as digital terrestrial television and some cable TV systems. 

-------> Hamming codes
---------> In computer science and telecommunication, Hamming codes are a family of linear error-correcting codes. 
-----------> Hamming codes can detect one-bit and two-bit errors, or correct one-bit errors without detection of uncorrected errors. 
-----------> By contrast, the simple parity code cannot correct errors, and can detect only an odd number of bits in error. 
-----------> Hamming codes are perfect codes, that is, they achieve the highest possible rate for codes with their block length and minimum distance of three.
---------> In mathematical terms, Hamming codes are a class of binary linear code. 
-----------> For each integer r ≥ 2 there is a code-word with block length n = (2^r)−1 and message length k = (2^r)−r−1. 
-----------> Hence the rate of Hamming codes is R = k/n = 1−r/((2^r)−1), which is the highest possible for codes with minimum distance of three
-----------> (i.e., the minimal number of bit changes needed to go from any code word to any other code word is three) and block length (2^r)−1. 
-----------> The parity-check matrix of a Hamming code is constructed by listing all columns of length r that are non-zero, 
-----------> which means that the dual code of the Hamming code is the shortened Hadamard code. 
-----------> The parity-check matrix has the property that any two columns are pairwise linearly independent.
---------> Due to the limited redundancy that Hamming codes add to the data, they can only detect and correct errors when the error rate is low. 
-----------> This is the case in computer memory (usually RAM), where bit errors are extremely rare and Hamming codes are widely used, 
-----------> and a RAM with this correction system is a ECC RAM (ECC memory). 
-----------> In this context, an extended Hamming code having one extra parity bit is often used. 
-----------> Extended Hamming codes achieve a Hamming distance of four, which allows the decoder to distinguish between when at most one one-bit error occurs and when any two-bit errors occur. 
-----------> In this sense, extended Hamming codes are single-error correcting and double-error detecting, abbreviated as SECDED. 
---------> Note: Richard W. Hamming invented Hamming codes in 1950 as a way of automatically correcting errors introduced by punched card readers.
--------->  In his original paper, Hamming elaborated his general idea, but specifically focused on the Hamming(7,4) code which adds three parity bits to four bits of data.

---------> Hamming(7,4): 
-----------> This is a Hamming code that encodes 4 bits of data into 7 bits by adding 3 parity bits
-----------> In coding theory, Hamming(7,4) is a linear error-correcting code 
-------------> that encodes four bits of data into seven bits by adding three parity bits. 
-------------> It is a member of a larger family of Hamming codes, 
-------------> but the term Hamming code often refers to this specific code that Richard W. Hamming introduced in 1950. 
-------------> At the time, Hamming worked at Bell Telephone Laboratories and was frustrated with the error-prone punched card reader,
------------->  which is why he started working on error-correcting codes.
-----------> The Hamming code adds three additional check bits to every four data bits of the message. 
-------------> Hamming's (7,4) algorithm can correct any single-bit error, or detect all single-bit and two-bit errors. 
-------------> In other words, the minimal Hamming distance between any two correct codewords is 3, 
-------------> and received words can be correctly decoded if they are at a distance of at most one from the codeword that was transmitted by the sender. 
-------------> This means that for transmission medium situations where burst errors do not occur, 
-------------> Hamming's (7,4) code is effective (as the medium would have to be extremely noisy for two out of seven bits to be flipped).
-----------> In quantum information, the Hamming (7,4) is used as the base for the Steane code, a type of CSS code used for quantum error correction. 

---------> Hamming distance: 
-----------> This is the sum number of positions which are different
-----------> In information theory, the Hamming distance between two strings of equal length is the number of positions at which the corresponding symbols are different. 
-------------> In other words, it measures the minimum number of substitutions required to change one string into the other,
-------------> or the minimum number of errors that could have transformed one string into the other. 
-------------> In a more general context, the Hamming distance is one of several string metrics for measuring the edit distance between two sequences. 
-------------> It is named after the American mathematician Richard Hamming.
-----------> A major application is in coding theory, more specifically to block codes, in which the equal-length strings are vectors over a finite field. 

---------> Hamming weight (population count): 
-----------> This is to find the number of 1 bits in a binary word
-----------> The Hamming weight of a string is the number of symbols that are different from the zero-symbol of the alphabet used. 
-------------> It is thus equivalent to the Hamming distance from the all-zero string of the same length. 
-------------> For the most typical case, a string of bits, this is the number of 1's in the string, 
-------------> or the digit sum of the binary representation of a given number and the ℓ₁ norm of a bit vector. 
-------------> In this binary case, it is also called the population count, popcount, sideways sum, or bit summation.


-------> Redundancy checks
---------> Error detection schemes
-----------> Error detection is most commonly realized using a suitable hash function (or specifically, a checksum, cyclic redundancy check or other algorithm). 
-------------> A hash function adds a fixed-length tag to a message, which enables receivers to verify the delivered message by recomputing the tag and comparing it with the one provided.
-----------> There exists a vast variety of different hash function designs. 
-------------> However, some are of particularly widespread use because of either their simplicity 
-------------> or their suitability for detecting certain kinds of errors (e.g., the cyclic redundancy check's performance in detecting burst errors). 

---------> Adler-32
-----------> Adler-32 is a checksum algorithm written by Mark Adler in 1995, modifying Fletcher's checksum. 
-------------> Compared to a cyclic redundancy check of the same length, it trades reliability for speed (preferring the latter). 
-------------> Adler-32 is more reliable than Fletcher-16, and slightly less reliable than Fletcher-32.

---------> Cyclic redundancy check
-----------> A cyclic redundancy check (CRC) is an error-detecting code commonly used in digital networks 
-------------> and storage devices to detect accidental changes to digital data. 
-------------> Blocks of data entering these systems get a short check value attached, 
-------------> based on the remainder of a polynomial division of their contents. 
-------------> On retrieval, the calculation is repeated and, in the event the check values do not match, 
-------------> corrective action can be taken against data corruption. 
-------------> CRCs can be used for error correction (see bitfilters).[1]
-----------> CRCs are so called because the check (data verification) value is a redundancy (it expands the message without adding information) 
-------------> and the algorithm is based on cyclic codes. 
-------------> CRCs are popular because they are simple to implement in binary hardware, easy to analyze mathematically, 
-------------> and particularly good at detecting common errors caused by noise in transmission channels. 
-------------> Because the check value has a fixed length, the function that generates it is occasionally used as a hash function. 

---------> Damm algorithm
-----------> In error detection, the Damm algorithm is a check digit algorithm that detects all single-digit errors and all adjacent transposition errors. 
-------------> It was presented by H. Michael Damm in 2004.[1]

---------> Fletcher's checksum
-----------> The Fletcher checksum is an algorithm for computing a position-dependent checksum devised 
-------------> by John G. Fletcher (1934–2012) at Lawrence Livermore Labs in the late 1970s.[1] 
-------------> The objective of the Fletcher checksum was to provide error-detection properties approaching those 
-------------> of a cyclic redundancy check but with the lower computational effort associated with summation techniques. 

---------> Longitudinal redundancy check (LRC)
-----------> In telecommunication, a longitudinal redundancy check (LRC), or horizontal redundancy check, 
-------------> is a form of redundancy check that is applied independently to each of a parallel group of bit streams.
-------------> The data must be divided into transmission blocks, to which the additional check data is added.
-----------> The term usually applies to a single parity bit per bit stream, calculated independently of all the other bit streams (BIP-8),
-------------> although it could also be used to refer to a larger Hamming code.
-----------> This "extra" LRC word at the end of a block of data is very similar to checksum and cyclic redundancy check (CRC). 

---------> Luhn algorithm: 
-----------> This is a method of validating identification numbers
----------> The Luhn algorithm or Luhn formula, also known as the "modulus 10" or "mod 10" algorithm, named after its creator, 
-------------> IBM scientist Hans Peter Luhn, is a simple checksum formula used to validate a variety of identification numbers, 
-------------> such as credit card numbers, IMEI numbers, National Provider Identifier numbers in the United States, 
-------------> Canadian Social Insurance Numbers, Israeli ID Numbers, South African ID Numbers, Swedish National identification numbers, 
-------------> Swedish Corporate Identity Numbers (OrgNr), Greek Social Security Numbers (ΑΜΚΑ), 
-------------> SIM card numbers and survey codes appearing on McDonald's, Taco Bell, and Tractor Supply Co. receipts. 
-------------> It is described in U.S. Patent No. 2,950,048, granted on August 23, 1960.
-----------> The algorithm is in the public domain and is in wide use today. It is specified in ISO/IEC 7812-1. 
-------------> It is not intended to be a cryptographically secure hash function; 
-------------> it was designed to protect against accidental errors, not malicious attacks. 
-------------> Most credit cards and many government identification numbers use the algorithm 
-------------> as a simple method of distinguishing valid numbers from mistyped or otherwise incorrect numbers. 

---------> Luhn mod N algorithm: 
-----------> This extension of Luhn to non-numeric characters
-----------> The Luhn mod N algorithm is an extension to the Luhn algorithm (also known as mod 10 algorithm) 
-------------> that allows it to work with sequences of values in any even-numbered base. 
-------------> This can be useful when a check digit is required to validate an identification string composed of letters, 
-------------> a combination of letters and digits or any arbitrary set of N characters where N is divisible by 2. 

---------> Parity: 
-----------> This simple/fast error detection technique
-----------> A parity bit, or check bit, is a bit added to a string of binary code. 
-------------> Parity bits are a simple form of error detecting code. 
-------------> Parity bits are generally applied to the smallest units of a communication protocol, typically 8-bit octets (bytes), 
-------------> although they can also be applied separately to an entire message string of bits.
-----------> The parity bit ensures that the total number of 1-bits in the string is even or odd.
-------------> Accordingly, there are two variants of parity bits: even parity bit and odd parity bit. 
-------------> In the case of even parity, for a given set of bits, the occurrences of bits whose value is 1 are counted. 
-------------> If that count is odd, the parity bit value is set to 1, making the total count of occurrences of 1s in the whole set (including the parity bit) an even number. 
-------------> If the count of 1s in a given set of bits is already even, the parity bit's value is 0. 
-------------> In the case of odd parity, the coding is reversed. 
-------------> For a given set of bits, if the count of bits with a value of 1 is even, 
-------------> the parity bit value is set to 1 making the total count of 1s in the whole set (including the parity bit) an odd number. 
-------------> If the count of bits with a value of 1 is odd, the count is already odd so the parity bit's value is 0. 
-------------> Even parity is a special case of a cyclic redundancy check (CRC), where the 1-bit CRC is generated by the polynomial x+1. 

---------> Verhoeff algorithm
-----------> The Verhoeff algorithm[1] is a checksum formula for error detection developed 
-------------> by the Dutch mathematician Jacobus Verhoeff and was first published in 1969.
-------------> It was the first decimal check digit algorithm which detects all single-digit errors, 
-------------> and all transposition errors involving two adjacent digits, which was at the time thought impossible with such a code. 



-----> Lossless compression algorithms

-------> Burrows–Wheeler transform: 
---------> This preprocessing useful for improving lossless compression
---------> The Burrows–Wheeler transform (BWT, also called block-sorting compression) rearranges a character string into runs of similar characters. 
-----------> This is useful for compression, since it tends to be easy to compress a string 
-----------> that has runs of repeated characters by techniques such as move-to-front transform and run-length encoding. 
-----------> More importantly, the transformation is reversible, without needing to store any additional data except the position of the first original character. 
-----------> The BWT is thus a "free" method of improving the efficiency of text compression algorithms, costing only some extra computation. 
-----------> The Burrows–Wheeler transform is an algorithm used to prepare data for use with data compression techniques such as bzip2. 
-----------> It was invented by Michael Burrows and David Wheeler in 1994 while Burrows was working at DEC Systems Research Center in Palo Alto, California. 
-----------> It is based on a previously unpublished transformation discovered by Wheeler in 1983. 
-----------> The algorithm can be implemented efficiently using a suffix array thus reaching linear time complexity.

-------> Context tree weighting
---------> The context tree weighting method (CTW) is a lossless compression and prediction algorithm by Willems, Shtarkov & Tjalkens 1995. 
-----------> The CTW algorithm is among the very few such algorithms that offer both theoretical guarantees and good practical performance (see, e.g. Begleiter, El-Yaniv & Yona 2004). 
-----------> The CTW algorithm is an "ensemble method," mixing the predictions of many underlying variable order Markov models, 
-----------> where each such model is constructed using zero-order conditional probability estimators. 

-------> Delta encoding: 
---------> This aid to compression of data in which sequential data occurs frequently
---------> Delta encoding is a way of storing or transmitting data in the form of differences (deltas) between sequential data rather than complete files; 
-----------> more generally this is known as data differencing. Delta encoding is sometimes called delta compression, 
-----------> particularly where archival histories of changes are required (e.g., in revision control software).
---------> The differences are recorded in discrete files called "deltas" or "diffs". In situations where differences are small
-----------> for example, the change of a few words in a large document or the change of a few records in a large table
-----------> delta encoding greatly reduces data redundancy. Collections of unique deltas are substantially more space-efficient than their non-encoded equivalents.
---------> From a logical point of view the difference between two data values is the information required to obtain one value from the other – see relative entropy. 
-----------> The difference between identical values (under some equivalence) is often called 0 or the neutral element. 

-------> Dynamic Markov compression: 
---------> This compression using predictive arithmetic coding
---------> Dynamic Markov compression (DMC) is a lossless data compression algorithm developed by Gordon Cormack and Nigel Horspool.
-----------> It uses predictive arithmetic coding similar to prediction by partial matching (PPM), except that the input is predicted one bit at a time (rather than one byte at a time). 
-----------> DMC has a good compression ratio and moderate speed, similar to PPM, but requires somewhat more memory and is not widely implemented. 
-----------> Some recent implementations include the experimental compression programs hook by Nania Francesco Antonio, ocamyd by Frank Schwellinger, and as a submodel in paq8l by Matt Mahoney. 
-----------> These are based on the 1993 implementation in C by Gordon Cormack. 

-------> Dictionary coders
---------> A dictionary coder, also sometimes known as a substitution coder, is a class of lossless data compression algorithms 
-----------> which operate by searching for matches between the text to be compressed 
-----------> and a set of strings contained in a data structure (called the 'dictionary') maintained by the encoder. 
-----------> When the encoder finds such a match, it substitutes a reference to the string's position in the data structure. 

---------> Byte pair encoding (BPE)
-----------> Byte pair encoding[1][2] or digram coding[3] is a simple form of data compression in
-------------> which the most common pair of consecutive bytes of data is replaced with a byte that does not occur within that data. 
-------------> A table of the replacements is required to rebuild the original data. 
-------------> The algorithm was first described publicly by Philip Gage in a February 1994 article "A New Algorithm for Data Compression" in the C Users Journal.
-----------> A variant of the technique has shown to be useful in several natural language processing (NLP) applications, 
-------------> such as Google's SentencePiece,[5] and OpenAI's GPT-3. 
-------------> Here, the goal is not data compression, but encoding text in a given language as a sequence of 'tokens', using a fixed vocabulary of different tokens. 
-------------> Typically, most words will be encoded as a single token, while rare words will be encoded as a sequence of a few tokens, 
-------------> where these tokens represent meaningful word parts. 
-------------> This translation of text into tokens can be found by a variant of byte pair encoding.

---------> Deflate
-----------> In computing, Deflate (stylized as DEFLATE) is a lossless data compression file format that uses a combination of LZ77 and Huffman coding. 
-------------> It was designed by Phil Katz, for version 2 of his PKZIP archiving tool. 
-------------> Deflate was later specified in RFC 1951 (1996).
-----------> Katz also designed the original algorithm used to construct Deflate streams. 
-------------> This algorithm was patented as U.S. Patent 5,051,745, and assigned to PKWARE, Inc.
-------------> As stated in the RFC document, an algorithm producing Deflate files was widely thought to be implementable in a manner not covered by patents.
-------------> This led to its widespread use – for example, in gzip compressed files and PNG image files, in addition to the ZIP file format for which Katz originally designed it. 
-------------> The patent has since expired. 

---------> Lempel–Ziv

-----------> LZ77 and LZ78
-------------> LZ77 and LZ78 are the two lossless data compression algorithms published in papers by Abraham Lempel and Jacob Ziv in 1977 and 1978.
---------------> They are also known as LZ1 and LZ2 respectively.
---------------> These two algorithms form the basis for many variations including LZW, LZSS, LZMA and others. 
---------------> Besides their academic influence, these algorithms formed the basis of several ubiquitous compression schemes, 
---------------> including GIF and the DEFLATE algorithm used in PNG and ZIP.
-------------> They are both theoretically dictionary coders. LZ77 maintains a sliding window during compression. 
---------------> This was later shown to be equivalent to the explicit dictionary constructed by LZ78—however, 
---------------> they are only equivalent when the entire data is intended to be decompressed.
-------------> Since LZ77 encodes and decodes from a sliding window over previously seen characters, 
---------------> decompression must always start at the beginning of the input.
---------------> Conceptually, LZ78 decompression could allow random access to the input if the entire dictionary were known in advance. 
---------------> However, in practice the dictionary is created during encoding and decoding by creating a new phrase whenever a token is output.
-------------> The algorithms were named an IEEE Milestone in 2004. 
---------------> In 2021 Jacob Ziv was awarded the IEEE Medal of Honor for his involvement in their development.

-----------> Lempel–Ziv Jeff Bonwick (LZJB)
-------------> LZJB is a lossless data compression algorithm invented by Jeff Bonwick to compress crash dumps and data in ZFS.
---------------> The software is CDDL license licensed. 
---------------> It includes a number of improvements to the LZRW1 algorithm, a member of the Lempel–Ziv family of compression algorithms.
---------------> The name LZJB is derived from its parent algorithm and its creator—Lempel Ziv Jeff Bonwick. 
---------------> Bonwick is also one of two architects of ZFS, and the creator of the Slab Allocator. 

-----------> Lempel–Ziv–Markov chain algorithm (LZMA)
-------------> The Lempel–Ziv–Markov chain algorithm (LZMA) is an algorithm used to perform lossless data compression. 
---------------> It has been under development since either 1996 or 1998 by Igor Pavlov[1] and was first used in the 7z format of the 7-Zip archiver. 
---------------> This algorithm uses a dictionary compression scheme somewhat similar to the LZ77 algorithm published by Abraham Lempel and Jacob Ziv in 1977 
---------------> and features a high compression ratio (generally higher than bzip2) and a variable compression-dictionary size (up to 4 GB),
---------------> while still maintaining decompression speed similar to other commonly used compression algorithms.
-------------> LZMA2 is a simple container format that can include both uncompressed data and LZMA data, possibly with multiple different LZMA encoding parameters. 
---------------> LZMA2 supports arbitrarily scalable multithreaded compression and decompression and efficient compression of data which is partially incompressible.

-----------> Lempel–Ziv–Oberhumer (LZO): 
-------------> This is speed oriented.
-------------> Lempel–Ziv–Oberhumer (LZO) is a lossless data compression algorithm that is focused on decompression speed.[1] 

-----------> Lempel–Ziv–Stac (LZS)
-------------> Lempel–Ziv–Stac (LZS, or Stac compression or Stacker compression[1]) is a lossless data compression algorithm 
---------------> that uses a combination of the LZ77 sliding-window compression algorithm and fixed Huffman coding. 
---------------> It was originally developed by Stac Electronics for tape compression, 
---------------> and subsequently adapted for hard disk compression and sold as the Stacker disk compression software. 
---------------> It was later specified as a compression algorithm for various network protocols. 
---------------> LZS is specified in the Cisco IOS stack. 

-----------> Lempel–Ziv–Storer–Szymanski (LZSS)
-------------> Lempel–Ziv–Storer–Szymanski (LZSS) is a lossless data compression algorithm, a derivative of LZ77, that was created in 1982 by James A. Storer and Thomas Szymanski. 
---------------> LZSS was described in article "Data compression via textual substitution" published in Journal of the ACM (1982, pp. 928–951).
-------------> LZSS is a dictionary coding technique. It attempts to replace a string of symbols with a reference to a dictionary location of the same string.
-------------> The main difference between LZ77 and LZSS is that in LZ77 the dictionary reference could actually be longer than the string it was replacing. 
---------------> In LZSS, such references are omitted if the length is less than the "break even" point. 
---------------> Furthermore, LZSS uses one-bit flags to indicate whether the next chunk of data is a literal (byte) or a reference to an offset/length pair. 

-----------> Lempel–Ziv–Welch (LZW)
-------------> Lempel–Ziv–Welch (LZW) is a universal lossless data compression algorithm created by Abraham Lempel, Jacob Ziv, and Terry Welch. 
---------------> It was published by Welch in 1984 as an improved implementation of the LZ78 algorithm published by Lempel and Ziv in 1978. 
---------------> The algorithm is simple to implement and has the potential for very high throughput in hardware implementations.
---------------> It is the algorithm of the widely used Unix file compression utility compress and is used in the GIF image format. 

-----------> LZWL: syllable-based variant
-------------> LZWL is a syllable-based variant of the character-based LZW compression algorithm 
---------------> that can work with syllables obtained by all algorithms of decomposition into syllables. 
-------------> The algorithm can be used for words too. 

-----------> LZX
-------------> LZX is an LZ77 family compression algorithm, a slightly improved version of DEFLATE.
---------------> It is also the name of a file archiver with the same name. Both were invented by Jonathan Forbes and Tomi Poutanen in 1990s. 

-----------> Lempel–Ziv Ross Williams (LZRW)
-------------> Lempel–Ziv Ross Williams (LZRW) refers to variants of the LZ77 lossless data compression algorithms 
---------------> with an emphasis on improving compression speed through the use of hash tables and other techniques. 
-------------> This family was explored by Ross Williams, who published a series of algorithms beginning with LZRW1 in 1991. 

-------> Entropy encoding: 
---------> This is coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols
---------> In information theory, an entropy coding (or entropy encoding) is any lossless data compression method 
-----------> that attempts to approach the lower bound declared by Shannon's source coding theorem, 
-----------> which states that any lossless data compression method must have expected code length greater or equal to the entropy of the source.
---------> More precisely, the source coding theorem states that for any source distribution, the expected code length satisfies Ex P[l(d(x))] ≥ E x P[−log b(P(x))], 
-----------> where l is the number of symbols in a code word, d is the coding function, 
-----------> b is the number of symbols used to make output codes and P is the probability of the source symbol. 
-----------> An entropy coding attempts to approach this lower bound.
---------> One of the main types of entropy coding creates and assigns a unique prefix-free code to each unique symbol that occurs in the input.
-----------> These entropy encoders then compress data by replacing each fixed-length input symbol with the corresponding variable-length prefix-free output codeword. 
-----------> The length of each codeword is approximately proportional to the negative logarithm of the probability of occurrence of that codeword. 
-----------> Therefore, the most common symbols use the shortest codes.[3]
---------> Two of the most common entropy coding techniques are Huffman coding and arithmetic coding.
-----------> If the approximate entropy characteristics of a data stream are known in advance (especially for signal compression), a simpler static code may be useful. 
-----------> These static codes include universal codes (such as Elias gamma coding or Fibonacci coding) and Golomb codes (such as unary coding or Rice coding).
---------> Since 2014, data compressors have started using the asymmetric numeral systems family of entropy coding techniques, 
-----------> which allows combination of the compression ratio of arithmetic coding with a processing cost similar to Huffman coding. 

---------> Arithmetic coding: 
-----------> This is advanced entropy coding
-----------> Arithmetic coding (AC) is a form of entropy encoding used in lossless data compression. 
-------------> Normally, a string of characters is represented using a fixed number of bits per character, as in the ASCII code. 
-------------> When a string is converted to arithmetic encoding, frequently used characters will be stored with fewer bits
-------------> and not-so-frequently occurring characters will be stored with more bits, resulting in fewer bits used in total. 
-------------> Arithmetic coding differs from other forms of entropy encoding, such as Huffman coding, 
-------------> in that rather than separating the input into component symbols and replacing each with a code, 
-------------> arithmetic coding encodes the entire message into a single number, an arbitrary-precision fraction q, where 0.0 ≤ q < 1.0. 
-------------> It represents the current information as a range, defined by two numbers.
-------------> A recent family of entropy coders called asymmetric numeral systems allows for faster implementations thanks 
-------------> to directly operating on a single natural number representing the current information.

-----------> Range encoding: 
-------------> This is the same as arithmetic coding, but looked at in a slightly different way
-------------> Range coding (or range encoding) is an entropy coding method defined by G. Nigel N. Martin in a 1979 paper,
---------------> which effectively rediscovered the FIFO arithmetic code first introduced by Richard Clark Pasco in 1976.
---------------> Given a stream of symbols and their probabilities, a range coder produces a space-efficient stream of bits to represent these symbols and, 
---------------> given the stream and the probabilities, a range decoder reverses the process.
-------------> Range coding is very similar to arithmetic coding, except that coding is done with digits in any base, instead of with bits, 
---------------> and so it is faster when using larger bases (e.g. a byte) at small cost in compression efficiency.
---------------> After the expiration of the first (1978) arithmetic coding patent,[4] range coding appeared to clearly be free of patent encumbrances. 
---------------> This particularly drove interest in the technique in the open source community. 
---------------> Since that time, patents on various well-known arithmetic coding techniques have also expired. 

---------> Huffman coding: 
-------------> This simple lossless compression taking advantage of relative character frequencies
-------------> In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. 
---------------> The process of finding or using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, 
---------------> and published in the 1952 paper "A Method for the Construction of Minimum-Redundancy Codes".
-------------> The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol (such as a character in a file). 
---------------> The algorithm derives this table from the estimated probability or frequency of occurrence (weight) for each possible value of the source symbol. 
---------------> As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. 
---------------> Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted. 
---------------> However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods 
---------------> it is replaced with arithmetic coding or asymmetric numeral systems[4] if better compression ratio is required. 


-----------> Adaptive Huffman coding: 
-------------> This adaptive coding technique based on Huffman coding
-------------> Adaptive Huffman coding (also called Dynamic Huffman coding) is an adaptive coding technique based on Huffman coding. 
---------------> It permits building the code as the symbols are being transmitted, having no initial knowledge of source distribution, that allows one-pass encoding and adaptation to changing conditions in data.
-------------> The benefit of one-pass procedure is that the source can be encoded in real time, though it becomes more sensitive to transmission errors, since just a single loss ruins the whole code. 

-----------> Package-merge algorithm: 
-------------> This Optimizes Huffman coding subject to a length restriction on code strings
-------------> The package-merge algorithm is an O(nL)-time algorithm for finding an optimal length-limited Huffman code 
---------------> for a given distribution on a given alphabet of size n, 
---------------> where no code word is longer than L. It is a greedy algorithm, and a generalization of Huffman's original algorithm. 
---------------> Package-merge works by reducing the code construction problem to the binary coin collector's problem.

---------> Shannon–Fano coding
-----------> In the field of data compression, Shannon–Fano coding, named after Claude Shannon and Robert Fano, 
-----------> is a name given to two different but related techniques for constructing a prefix code based on a set of symbols and their probabilities (estimated or measured).
-------------> Shannon's method chooses a prefix code where a source symbol i is given the codeword length l i = ⌈−log2(⁡ pi)⌉. 
---------------> One common way of choosing the codewords uses the binary expansion of the cumulative probabilities. 
---------------> This method was proposed in Shannon's "A Mathematical Theory of Communication" (1948), his article introducing the field of information theory.
-------------> Fano's method divides the source symbols into two sets ("0" and "1") with probabilities as close to 1/2 as possible. 
---------------> Then those sets are themselves divided in two, and so on, until each set contains only one symbol. 
---------------> The codeword for that symbol is the string of "0"s and "1"s that records which half of the divides it fell on. 
---------------> This method was proposed in a later technical report by Fano (1949).
-----------> Shannon–Fano codes are suboptimal in the sense that they do not always achieve the lowest possible expected codeword length, as Huffman coding does.
-------------> However, Shannon–Fano codes have an expected codeword length within 1 bit of optimal. 
-------------> Fano's method usually produces encoding with shorter expected lengths than Shannon's method. 
-------------> However, Shannon's method is easier to analyse theoretically.
-----------> Shannon–Fano coding should not be confused with Shannon–Fano–Elias coding (also known as Elias coding), the precursor to arithmetic coding. 

---------> Shannon–Fano–Elias coding: 
-----------> This precursor to arithmetic encoding
-----------> In information theory, Shannon–Fano–Elias coding is a precursor to arithmetic coding, in which probabilities are used to determine codewords.

-------> Entropy coding with known entropy characteristics
---------> In information theory, an entropy coding (or entropy encoding) is any lossless data compression method that attempts to approach the lower bound declared by Shannon's source coding theorem, 
-----------> which states that any lossless data compression method must have expected code length greater or equal to the entropy of the source.
---------> More precisely, the source coding theorem states that for any source distribution, the expected code length satisfies Ex P[l(d(x))] ≥ Ex P[−log b(P(x))], 
-----------> where l is the number of symbols in a code word, d is the coding function, b is the number of symbols used to make output codes and P is the probability of the source symbol. A
-----------> n entropy coding attempts to approach this lower bound.
---------> One of the main types of entropy coding creates and assigns a unique prefix-free code to each unique symbol that occurs in the input.
-----------> These entropy encoders then compress data by replacing each fixed-length input symbol with the corresponding variable-length prefix-free output codeword. 
-----------> The length of each codeword is approximately proportional to the negative logarithm of the probability of occurrence of that codeword. 
-----------> Therefore, the most common symbols use the shortest codes.
---------> Two of the most common entropy coding techniques are Huffman coding and arithmetic coding.
-----------> If the approximate entropy characteristics of a data stream are known in advance (especially for signal compression), a simpler static code may be useful. 
-----------> These static codes include universal codes (such as Elias gamma coding or Fibonacci coding) and Golomb codes (such as unary coding or Rice coding).
---------> Since 2014, data compressors have started using the asymmetric numeral systems family of entropy coding techniques, 
-----------> which allows combination of the compression ratio of arithmetic coding with a processing cost similar to Huffman coding. 

---------> Golomb coding: 
-----------> This form of entropy coding that is optimal for alphabets following geometric distributions
-----------> Golomb coding is a lossless data compression method using a family of data compression codes invented by Solomon W. Golomb in the 1960s. 
-------------> Alphabets following a geometric distribution will have a Golomb code as an optimal prefix code, 
-------------> making Golomb coding highly suitable for situations in which the occurrence of small values in the input stream is significantly more likely than large values. 

---------> Rice coding: 
-----------> This form of entropy coding that is optimal for alphabets following geometric distributions
-----------> Rice coding (invented by Robert F. Rice) denotes using a subset of the family of Golomb codes to produce a simpler (but possibly suboptimal) prefix code. 
-------------> Rice used this set of codes in an adaptive coding scheme; "Rice coding" can refer either to that adaptive scheme or to using that subset of Golomb codes. 
-------------> Whereas a Golomb code has a tunable parameter that can be any positive integer value, Rice codes are those in which the tunable parameter is a power of two. 
-------------> This makes Rice codes convenient for use on a computer since multiplication and division by 2 can be implemented more efficiently in binary arithmetic.
-----------> Rice was motivated to propose this simpler subset due to the fact that geometric distributions are often varying with time, 
-------------> not precisely known, or both, so selecting the seemingly optimal code might not be very advantageous.
-----------> Rice coding is used as the entropy encoding stage in a number of lossless image compression and audio data compression methods. 

---------> Truncated binary encoding
-----------> Truncated binary encoding is an entropy encoding typically used for uniform probability distributions with a finite alphabet. 
-------------> It is parameterized by an alphabet with total size of number n. It is a slightly more general form of binary encoding when n is not a power of two.
-----------> If n is a power of two then the coded value for 0 ≤ x < n is the simple binary code for x of length log2(n). 
-------------> Otherwise let k = floor(log2(n)) such that 2k < n < 2k+1 and let u = 2k+1 - n.
-----------> Truncated binary encoding assigns the first u symbols codewords of length k and then assigns the remaining n - u symbols the last n - u codewords of length k+1. 
-------------> Because all the codewords of length k + 1 consist of an unassigned codeword of length k with a "0" or "1" appended, the resulting code is a prefix code. 

---------> Unary coding: 
-----------> This code that represents a number n with n ones followed by a zero
-----------> Unary coding, or the unary numeral system and also sometimes called thermometer code, 
-------------> is an entropy encoding that represents a natural number, 
-------------> n, with a code of length n+1 (or n), usually n ones followed by a zero (if natural number is understood as non-negative integer) 
-------------> or with n−1 ones followed by a zero (if natural number is understood as strictly positive integer). 
-------------> For example 5 is represented as 111110 or 11110. 
-------------> Some representations use n or n − 1 zeros followed by a one. 
-------------> The ones and zeros are interchangeable without loss of generality. 
-------------> Unary coding is both a prefix-free code and a self-synchronizing code. 

---------> Universal codes: 
-----------> This encodes positive integers into binary code words
-----------> In data compression, a universal code for integers is a prefix code that maps the positive integers onto binary codewords, 
-------------> with the additional property that whatever the true probability distribution on integers, 
-------------> as long as the distribution is monotonic (i.e., p(i) ≥ p(i + 1) for all positive i), 
-------------> the expected lengths of the codewords are within a constant factor of the expected lengths that the optimal code for that probability distribution would have assigned. 
-------------> A universal code is asymptotically optimal if the ratio between actual and optimal expected lengths is bounded by a function of the information entropy of the code that, 
-------------> in addition to being bounded, approaches 1 as entropy approaches infinity.
-----------> In general, most prefix codes for integers assign longer codewords to larger integers. 
-------------> Such a code can be used to efficiently communicate a message drawn from a set of possible messages, 
-------------> by simply ordering the set of messages by decreasing probability and then sending the index of the intended message. 
-------------> Universal codes are generally not used for precisely known probability distributions, 
-------------> and no universal code is known to be optimal for any distribution used in practice.
-----------> A universal code should not be confused with universal source coding, in which the data compression method 
-------------> need not be a fixed prefix code and the ratio between actual and optimal expected lengths must approach one. 
-------------> However, note that an asymptotically optimal universal code can be used on independent identically-distributed sources, 
-------------> by using increasingly large blocks, as a method of universal source coding. 

-----------> Elias delta, gamma, and omega coding
-------------> Elias δ code or Elias delta code is a universal code encoding the positive integers developed by Peter Elias.
-------------> Elias γ code or Elias gamma code is a universal code encoding positive integers developed by Peter Elias.
---------------> It is used most commonly when coding integers whose upper-bound cannot be determined beforehand. 
-------------> Elias ω coding or Elias omega coding is a universal code encoding the positive integers developed by Peter Elias. 
---------------> Like Elias gamma coding and Elias delta coding, it works by prefixing the positive integer with a representation of its order of magnitude in a universal code. 
-----------------> Unlike those other two codes, however, Elias omega recursively encodes that prefix; thus, they are sometimes known as recursive Elias codes.
---------------> Omega coding is used in applications where the largest encoded value is not known ahead of time, or to compress data in which small values are much more frequent than large values.
-----------------> To encode a positive integer N:
-------------------> Place a "0" at the end of the code.
-------------------> If N = 1, stop; encoding is complete.
-------------------> Prepend the binary representation of N to the beginning of the code. This will be at least two bits, the first bit of which is a 1.
-------------------> Let N equal the number of bits just prepended, minus one.
-------------------> Return to Step 2 to prepend the encoding of the new N.
-----------------> To decode an Elias omega-encoded positive integer:
-------------------> Start with a variable N, set to a value of 1.
-------------------> If the next bit is a "0", stop. The decoded number is N.
-------------------> If the next bit is a "1", then read it plus N more bits, and use that binary number as the new value of N. Go back to Step 2.


-----------> Exponential-Golomb coding
-------------> An exponential-Golomb code (or just Exp-Golomb code) is a type of universal code. 
---------------> To encode any nonnegative integer x using the exp-Golomb code:
-----------------> (1) Write down x+1 in binary
-----------------> (2) Count the bits written, subtract one, and write that number of starting zero bits preceding the previous bit string.
-------------> The first few values of the code are:
--------------->  0 ⇒ 1 ⇒ 1
--------------->  1 ⇒ 10 ⇒ 010
--------------->  2 ⇒ 11 ⇒ 011
--------------->  3 ⇒ 100 ⇒ 00100
--------------->  4 ⇒ 101 ⇒ 00101
--------------->  5 ⇒ 110 ⇒ 00110
--------------->  6 ⇒ 111 ⇒ 00111
--------------->  7 ⇒ 1000 ⇒ 0001000
--------------->  8 ⇒ 1001 ⇒ 0001001
---------------> ...
-------------> In the above examples, consider the case 3. For 3, x+1 = 3 + 1 = 4. 4 in binary is '100'. '100' has 3 bits, and 3-1 = 2. 
---------------> Hence add 2 zeros before '100', which is '00100'
-------------> Similarly, consider 8. '8 + 1' in binary is '1001'. '1001' has 4 bits, and 4-1 is 3. 
---------------> Hence add 3 zeros before 1001, which is '0001001'.
-------------> This is identical to the Elias gamma code of x+1, allowing it to encode 0.

-----------> Fibonacci coding
-------------> In mathematics and computing, Fibonacci coding is a universal code which encodes positive integers into binary code words. 
---------------> It is one example of representations of integers based on Fibonacci numbers. 
---------------> Each code word ends with "11" and contains no other instances of "11" before the end.
-------------> The Fibonacci code is closely related to the Zeckendorf representation, 
---------------> a positional numeral system that uses Zeckendorf's theorem and has the property that no number has a representation with consecutive 1s. 
---------------> The Fibonacci code word for a particular integer is exactly the integer's Zeckendorf representation 
---------------> with the order of its digits reversed and an additional "1" appended to the end. 

-----------> Levenshtein coding
-------------> Levenstein coding, or Levenshtein coding, is a universal code encoding the non-negative integers developed by Vladimir Levenshtein.[1][2] 
-------------> The code of zero is "0"; to code a positive number:
---------------> (1) Initialize the step count variable C to 1.
---------------> (2) Write the binary representation of the number without the leading "1" to the beginning of the code.
---------------> (3) Let M be the number of bits written in step 2.
---------------> (4) If M is not 0, increment C, repeat from step 2 with M as the new number.
---------------> (5) Write C "1" bits and a "0" to the beginning of the code.
-------------> The code begins:
--------------------------------------------------
| Number | Encoding        | Implied probability |
--------------------------------------------------
| 0      | 0               | 1/2                 |
| 1      | 10              | 1/4                 |
| 2      | 110 0           | 1/16                |
| 3      | 110 1           | 1/16                |
| 4      | 1110 0 00       | 1/128               |
| 5      | 1110 0 01       | 1/128               |
| 6      | 1110 0 10       | 1/128               |
| 7      | 1110 0 11       | 1/128               |
| 8      | 1110 1 000      | 1/256               |
| 9      | 1110 1 001      | 1/256               |
| 10     | 1110 1 010      | 1/256               |
| 11     | 1110 1 011      | 1/256               |
| 12     | 1110 1 100      | 1/256               |
| 13     | 1110 1 101      | 1/256               |
| 14     | 1110 1 110      | 1/256               |
| 15     | 1110 1 111      | 1/256               |
| 16     | 11110 0 00 0000 | 1/4096              |
| 17     | 11110 0 00 0001 | 1/4096              |
--------------------------------------------------
---------------> To decode a Levenstein-coded integer:
-----------------> (1) Count the number of "1" bits until a "0" is encountered.
-----------------> (2) If the count is zero, the value is zero, otherwise
-----------------> (3) Start with a variable N, set it to a value of 1 and repeat count minus 1 times:
-----------------> (4) Read N bits, prepend "1", assign the resulting value to N
---------------> The Levenstein code of a positive integer is always one bit longer than the Elias omega code of that integer.
-----------------> However, there is a Levenstein code for zero, whereas Elias omega coding would require the numbers to be shifted so that a zero is represented by the code for one instead. 

-------> Fast Efficient & Lossless Image Compression System (FELICS): 
---------> This a lossless image compression algorithm
---------> FELICS, which stands for Fast Efficient & Lossless Image Compression System, 
-----------> is a lossless image compression algorithm that performs 5-times faster than the original lossless JPEG codec and achieves a similar compression ratio.

-------> Incremental encoding: 
---------> This delta encoding applied to sequences of strings
---------> Incremental encoding, also known as front compression, back compression, or front coding, 
-----------> is a type of delta encoding compression algorithm whereby common prefixes or suffixes 
-----------> and their lengths are recorded so that they need not be duplicated. 
-----------> This algorithm is particularly well-suited for compressing sorted data, e.g., a list of words from a dictionary. 

-------> Prediction by partial matching (PPM): 
---------> This an adaptive statistical data compression technique based on context modeling and prediction
---------> Prediction by partial matching (PPM) is an adaptive statistical data compression technique based on context modeling and prediction. 
-----------> PPM models use a set of previous symbols in the uncompressed symbol stream to predict the next symbol in the stream. 
-----------> PPM algorithms can also be used to cluster data into predicted groupings in cluster analysis. 

-------> Run-length encoding: 
---------> This lossless data compression taking advantage of strings of repeated characters
---------> Run-length encoding (RLE) is a form of lossless data compression in which runs of data (sequences in which the same data value occurs in many consecutive data elements) 
-----------> are stored as a single data value and count, rather than as the original run. 
-----------> This is most efficient on data that contains many such runs, for example, simple graphic images such as icons, line drawings, Conway's Game of Life, and animations. 
-----------> For files that do not have many runs, RLE could increase the file size.
---------> RLE may also be used to refer to an early graphics file format supported by CompuServe for compressing black and white images, 
-----------> but was widely supplanted by their later Graphics Interchange Format (GIF). 
-----------> RLE also refers to a little-used image format in Windows 3.x, with the extension rle, 
-----------> which is a run-length encoded bitmap, used to compress the Windows 3.x startup screen. 

-------> SEQUITUR algorithm: 
---------> This lossless compression by incremental grammar inference on a string
---------> Sequitur (or Nevill-Manning algorithm) is a recursive algorithm developed by Craig Nevill-Manning and Ian H. Witten in 1997[1] 
-----------> that infers a hierarchical structure (context-free grammar) from a sequence of discrete symbols. 
-----------> The algorithm operates in linear space and time. It can be used in data compression software applications.[2] 



-----> Lossy compression algorithms

-------> 3Dc: 
---------> This a lossy data compression algorithm for normal maps
---------> 3Dc (FourCC : ATI2), also known as DXN, BC5, or Block Compression 5 
-----------> is a lossy data compression algorithm for normal maps invented and first implemented by ATI. 
---------> It builds upon the earlier DXT5 algorithm and is an open standard. 3Dc is now implemented by both ATI and Nvidia. 

-------> Audio and Speech compression
---------> Audio data compression, not to be confused with dynamic range compression, 
-----------> has the potential to reduce the transmission bandwidth and storage requirements of audio data. 
-----------> Audio compression algorithms are implemented in software as audio codecs. 
-----------> In both lossy and lossless compression, information redundancy is reduced, using methods such as coding, quantization, discrete cosine transform 
-----------> and linear prediction to reduce the amount of information used to represent the uncompressed data.
---------> Lossy audio compression algorithms provide higher compression and are used in numerous audio applications including Vorbis and MP3. 
-----------> These algorithms almost all rely on psychoacoustics to eliminate or reduce fidelity of less audible sounds, thereby reducing the space required to store or transmit them.[2][40] 
---------> Speech coding is an application of data compression of digital audio signals containing speech. 
-----------> Speech coding uses speech-specific parameter estimation using audio signal processing techniques to model the speech signal, 
-----------> combined with generic data compression algorithms to represent the resulting modeled parameters in a compact bitstream.
---------> Some applications of speech coding are mobile telephony and voice over IP (VoIP).
-----------> The most widely used speech coding technique in mobile telephony is linear predictive coding (LPC), 
-----------> while the most widely used in VoIP applications are the LPC and modified discrete cosine transform (MDCT) techniques.

---------> A-law algorithm: 
-----------> This standard companding algorithm
-----------> An A-law algorithm is a standard companding algorithm, used in European 8-bit PCM digital communications systems to optimize, 
-------------> i.e. modify, the dynamic range of an analog signal for digitizing. 
-------------> It is one of two versions of the G.711 standard from ITU-T, the other version being the similar μ-law, used in North America and Japan. 

---------> Code-excited linear prediction (CELP): 
-----------> This low bit-rate speech compression
-----------> Code-excited linear prediction (CELP) is a linear predictive speech coding algorithm originally proposed by Manfred R. Schroeder and Bishnu S. Atal in 1985. 
-------------> At the time, it provided significantly better quality than existing low bit-rate algorithms, such as residual-excited linear prediction (RELP) 
-------------> and linear predictive coding (LPC) vocoders (e.g., FS-1015). Along with its variants, such as algebraic CELP, relaxed CELP, low-delay CELP and vector sum excited linear prediction, 
-------------> it is currently the most widely used speech coding algorithm[citation needed]. It is also used in MPEG-4 Audio speech coding. 
-------------> CELP is commonly used as a generic term for a class of algorithms and not for a particular codec. 

---------> Linear predictive coding (LPC): 
-----------> This lossy compression by representing the spectral envelope of a digital signal of speech in compressed form
-----------> Linear predictive coding (LPC) is a method used mostly in audio signal processing and speech processing for representing the spectral envelope 
-------------> of a digital signal of speech in compressed form, using the information of a linear predictive model.
-----------> LPC is the most widely used method in speech coding and speech synthesis. 
-------------> It is a powerful speech analysis technique, and a useful method for encoding good quality speech at a low bit rate. 

---------> Mu-law algorithm: 
-----------> This standard analog signal compression or companding algorithm
-----------> The μ-law algorithm (sometimes written mu-law, often approximated as u-law) is a companding algorithm, 
-------------> primarily used in 8-bit PCM digital telecommunication systems in North America and Japan. 
-------------> It is one of two versions of the G.711 standard from ITU-T, the other version being the similar A-law. 
-------------> A-law is used in regions where digital telecommunication signals are carried on E-1 circuits, e.g. Europe.
-----------> Companding algorithms reduce the dynamic range of an audio signal. 
-------------> In analog systems, this can increase the signal-to-noise ratio (SNR) achieved during transmission; in the digital domain, 
-------------> it can reduce the quantization error (hence increasing the signal-to-quantization-noise ratio). 
-------------> These SNR increases can be traded instead for reduced bandwidth for equivalent SNR. 

---------> Warped Linear Predictive Coding (WLPC)
-----------> Warped linear predictive coding (warped LPC or WLPC) is a variant of linear predictive coding in which the spectral representation of the system is modified, 
-------------> for example by replacing the unit delays used in an LPC implementation with first-order all-pass filters. 
-----------> This can have advantages in reducing the bitrate required for a given level of perceived audio quality/intelligibility, especially in wideband audio coding. 


-------> Image compression
---------> Image compression is a type of data compression applied to digital images, to reduce their cost for storage or transmission. 
-----------> Algorithms may take advantage of visual perception and the statistical properties of image data 
---------> to provide superior results compared with generic data compression methods which are used for other digital data.

---------> Block Truncation Coding (BTC): 
-----------> This a type of lossy image compression technique for greyscale images
-----------> Block Truncation Coding (BTC) is a type of lossy image compression technique for greyscale images. 
-------------> It divides the original images into blocks and then uses a quantizer to reduce the number of grey levels in each block whilst maintaining the same mean and standard deviation. 
-------------> It is an early predecessor of the popular hardware DXTC technique, 
-------------> although BTC compression method was first adapted to color long before DXTC using a very similar approach called Color Cell Compression.
-------------> BTC has also been adapted to video compression.
-----------> BTC was first proposed by Professors Mitchell and Delp at Purdue University.
-------------> Another variation of BTC is Absolute Moment Block Truncation Coding or AMBTC, 
-------------> in which instead of using the standard deviation the first absolute moment is preserved along with the mean. 
-------------> AMBTC is computationally simpler than BTC and also typically results in a lower Mean Squared Error (MSE). 
-------------> AMBTC was proposed by Maximo Lema and Robert Mitchell.
-----------> Using sub-blocks of 4×4 pixels gives a compression ratio of 4:1 assuming 8-bit integer values are used during transmission or storage. 
-------------> Larger blocks allow greater compression ("a" and "b" values spread over more pixels) 
-------------> however quality also reduces with the increase in block size due to the nature of the algorithm.
-----------> The BTC algorithm was used for compressing Mars Pathfinder's rover images.

---------> Embedded Zerotree Wavelet (EZW)
-----------> Embedded Zerotrees of Wavelet transforms (EZW) is a lossy image compression algorithm. 
-------------> At low bit rates, i.e. high compression ratios, most of the coefficients 
-------------> produced by a subband transform (such as the wavelet transform) will be zero, or very close to zero. 
-------------> This occurs because "real world" images tend to contain mostly low frequency information (highly correlated). 
-------------> However where high frequency information does occur (such as edges in the image) this is particularly important in terms of human perception of the image quality, 
-------------> and thus must be represented accurately in any high quality coding scheme. 

---------> Fast Cosine Transform algorithms (FCT algorithms): 
-----------> This computes Discrete Cosine Transform (DCT) efficiently
-----------> A discrete cosine transform (DCT) expresses a finite sequence of data points in terms of a sum of cosine functions oscillating at different frequencies. 
-------------> The DCT, first proposed by Nasir Ahmed in 1972, is a widely used transformation technique in signal processing and data compression. 
-------------> It is used in most digital media, including digital images (such as JPEG and HEIF, where small high-frequency components can be discarded), 
-------------> digital video (such as MPEG and H.26x), digital audio (such as Dolby Digital, MP3 and AAC), digital television (such as SDTV, HDTV and VOD), 
-------------> digital radio (such as AAC+ and DAB+), and speech coding (such as AAC-LD, Siren and Opus). 
-------------> DCTs are also important to numerous other applications in science and engineering, such as digital signal processing, telecommunication devices, 
-------------> reducing network bandwidth usage, and spectral methods for the numerical solution of partial differential equations.
-----------> The use of cosine rather than sine functions is critical for compression, 
-------------> since it turns out (as described below) that fewer cosine functions are needed to approximate a typical signal, 
-------------> whereas for differential equations the cosines express a particular choice of boundary conditions. 
-------------> In particular, a DCT is a Fourier-related transform similar to the discrete Fourier transform (DFT), but using only real numbers. 
-------------> The DCTs are generally related to Fourier Series coefficients of a periodically 
-------------> and symmetrically extended sequence whereas DFTs are related to Fourier Series coefficients of only periodically extended sequences.
-------------> DCTs are equivalent to DFTs of roughly twice the length, operating on real data with even symmetry (since the Fourier transform of a real and even function is real and even), 
-------------> whereas in some variants the input and/or output data are shifted by half a sample. There are eight standard DCT variants, of which four are common. 

---------> Fractal compression: 
-----------> This method used to compress images using fractals
-----------> Fractal compression is a lossy compression method for digital images, based on fractals. 
-------------> The method is best suited for textures and natural images, relying on the fact that parts of an image often resemble other parts of the same image.
-------------> Fractal algorithms convert these parts into mathematical data called "fractal codes" which are used to recreate the encoded image. 

---------> Set Partitioning in Hierarchical Trees (SPIHT)
-----------> Set partitioning in hierarchical trees (SPIHT) is an image compression algorithm that exploits the inherent similarities across the subbands in a wavelet decomposition of an image. 
-------------> The algorithm was developed by Brazilian engineer Amir Said with William A. Pearlman in 1996.[1] 

---------> Wavelet compression: 
-----------> This form of data compression well suited for image compression (sometimes also video compression and audio compression)
-----------> Wavelet compression is a form of data compression well suited for image compression (sometimes also video compression and audio compression). 
-------------> Notable implementations are JPEG 2000, DjVu and ECW for still images, JPEG XS, CineForm, and the BBC's Dirac. 
-------------> The goal is to store image data in as little space as possible in a file. Wavelet compression can be either lossless or lossy.
-----------> Using a wavelet transform, the wavelet compression methods are adequate for representing transients, 
-------------> such as percussion sounds in audio, or high-frequency components in two-dimensional images, for example an image of stars on a night sky. 
-------------> This means that the transient elements of a data signal can be represented by a smaller amount of information than would be the case if some other transform, 
-------------> such as the more widespread discrete cosine transform, had been used.
-----------> Discrete wavelet transform has been successfully applied for the compression of electrocardiograph (ECG) signals.
-------------> In this work, the high correlation between the corresponding wavelet coefficients of signals of successive cardiac cycles is utilized employing linear prediction.
-----------> Wavelet compression is not effective for all kinds of data. Wavelet compression handles transient signals well. 
-------------> But smooth, periodic signals are better compressed using other methods, 
-------------> particularly traditional harmonic analysis in the frequency domain with Fourier-related transforms. 
-------------> Compressing data that has both transient and periodic characteristics may be done with hybrid techniques that use wavelets along with traditional harmonic analysis. 
-------------> For example, the Vorbis audio codec primarily uses the modified discrete cosine transform to compress audio (which is generally smooth and periodic), 
-------------> however allows the addition of a hybrid wavelet filter bank for improved reproduction of transients.

-------> Transform coding: 
---------> This type of data compression for "natural" data like audio signals or photographic images
---------> Transform coding is a type of data compression for "natural" data like audio signals or photographic images. 
-----------> The transformation is typically lossless (perfectly reversible) on its own but is used to enable better (more targeted) quantization, 
-----------> which then results in a lower quality copy of the original input (lossy compression).
---------> In transform coding, knowledge of the application is used to choose information to discard, thereby lowering its bandwidth. 
-----------> The remaining information can then be compressed via a variety of methods. 
-----------> When the output is decoded, the result may not be identical to the original input, 
-----------> but is expected to be close enough for the purpose of the application. 

-------> Video compression
---------> Uncompressed video requires a very high data rate. 
-----------> Although lossless video compression codecs perform at a compression factor of 5 to 12, 
-----------> a typical H.264 lossy compression video has a compression factor between 20 and 200.[60]
---------> The two key video compression techniques used in video coding standards are the discrete cosine transform (DCT) and motion compensation (MC). 
-----------> Most video coding standards, such as the H.26x and MPEG formats, typically use motion-compensated DCT video coding (block motion compensation).
---------> Most video codecs are used alongside audio compression techniques to store the separate 
-----------> but complementary data streams as one combined package using so-called container formats.

-------> Vector quantization: 
---------> This technique often used in lossy data compression
---------> Vector quantization (VQ) is a classical quantization technique from signal processing 
-----------> that allows the modeling of probability density functions by the distribution of prototype vectors. 
-----------> It was originally used for data compression. 
-----------> It works by dividing a large set of points (vectors) into groups having approximately the same number of points closest to them. 
-----------> Each group is represented by its centroid point, as in k-means and some other clustering algorithms.
---------> The density matching property of vector quantization is powerful, especially for identifying the density of large and high-dimensional data. 
-----------> Since data points are represented by the index of their closest centroid, commonly occurring data have low error, and rare data high error. 
-----------> This is why VQ is suitable for lossy data compression. It can also be used for lossy data correction and density estimation.
---------> Vector quantization is based on the competitive learning paradigm, 
-----------> so it is closely related to the self-organizing map model and to sparse coding models used in deep learning algorithms such as autoencoder. 



---> Digital signal processing
-----> Digital signal processing (DSP) is the use of digital processing, 
-------> such as by computers or more specialized digital signal processors, 
-------> to perform a wide variety of signal processing operations. 
-------> The digital signals processed in this manner are a sequence of numbers 
-------> that represent samples of a continuous variable in a domain such as time, space, or frequency. 
-------> In digital electronics, a digital signal is represented as a pulse train, 
-------> which is typically generated by the switching of a transistor.
-----> Digital signal processing and analog signal processing are subfields of signal processing. 
-------> DSP applications include audio and speech processing, sonar, radar and other sensor array processing, spectral density estimation,
-------> statistical signal processing, digital image processing, data compression, video coding, audio coding, image compression, 
-------> signal processing for telecommunications, control systems, biomedical engineering, and seismology, among others.
-----> DSP can involve linear or nonlinear operations. 
-------> Nonlinear signal processing is closely related to nonlinear system identificatio and can be implemented in the time, frequency, and spatio-temporal domains.
-----> The application of digital computation to signal processing allows for many advantages over analog processing in many applications, 
-------> such as error detection and correction in transmission as well as data compression.
-------> Digital signal processing is also fundamental to digital technology, 
-------> such as digital telecommunication and wireless communications.[6] DSP is applicable to both streaming data and static (stored) data. 

-----> Adaptive-additive algorithm (AA algorithm): 
-------> This find the spatial frequency phase of an observed wave source
-------> In the studies of Fourier optics, sound synthesis, stellar interferometry, optical tweezers, 
---------> and diffractive optical elements (DOEs) it is often important to know the spatial frequency phase of an observed wave source. 
---------> In order to reconstruct this phase the Adaptive-Additive Algorithm (or AA algorithm), 
---------> which derives from a group of adaptive (input-output) algorithms, can be used. 
---------> The AA algorithm is an iterative algorithm that utilizes the Fourier Transform to calculate an unknown part of a propagating wave, 
---------> normally the spatial frequency phase (k space). 
---------> This can be done when given the phase’s known counterparts, usually an observed amplitude (position space) and an assumed starting amplitude (k space). 
---------> To find the correct phase the algorithm uses error conversion, or the error between the desired and the theoretical intensities. 

-----> Discrete Fourier transform: 
-------> This determines the frequencies contained in a (segment of a) signal
-------> In mathematics, the discrete Fourier transform (DFT) converts a finite sequence of equally-spaced samples of a function into a same-length sequence 
---------> of equally-spaced samples of the discrete-time Fourier transform (DTFT), which is a complex-valued function of frequency. 
---------> The interval at which the DTFT is sampled is the reciprocal of the duration of the input sequence. 
---------> An inverse DFT is a Fourier series, using the DTFT samples as coefficients of complex sinusoids at the corresponding DTFT frequencies. 
---------> It has the same sample-values as the original input sequence. 
---------> The DFT is therefore said to be a frequency domain representation of the original input sequence. 
---------> If the original sequence spans all the non-zero values of a function, its DTFT is continuous (and periodic), and the DFT provides discrete samples of one cycle. 
---------> If the original sequence is one cycle of a periodic function, the DFT provides all the non-zero values of one DTFT cycle.
-------> The DFT is the most important discrete transform, used to perform Fourier analysis in many practical applications.
---------> In digital signal processing, the function is any quantity or signal that varies over time, such as the pressure of a sound wave, 
---------> a radio signal, or daily temperature readings, sampled over a finite time interval (often defined by a window function). 
---------> In image processing, the samples can be the values of pixels along a row or column of a raster image. 
---------> The DFT is also used to efficiently solve partial differential equations, and to perform other operations such as convolutions or multiplying large integers.
-------> Since it deals with a finite amount of data, it can be implemented in computers by numerical algorithms or even dedicated hardware. 
---------> These implementations usually employ efficient fast Fourier transform (FFT) algorithms;
---------> so much so that the terms "FFT" and "DFT" are often used interchangeably. 
---------> Prior to its current usage, the "FFT" initialism may have also been used for the ambiguous term "finite Fourier transform". 

-------> Bluestein's FFT algorithm
---------> Bluestein's algorithm[7][8] expresses the CZT as a convolution and implements it efficiently using FFT/IFFT.
---------> As the DFT is a special case of the CZT, this allows the efficient calculation of discrete Fourier transform (DFT) of arbitrary sizes, including prime sizes. 
-----------> (The other algorithm for FFTs of prime sizes, Rader's algorithm, also works by rewriting the DFT as a convolution.) 
-----------> It was conceived in 1968 by Leo Bluestein.[7] Bluestein's algorithm can be used to compute more general transforms than the DFT, 
-----------> based on the (unilateral) z-transform (Rabiner et al., 1969). 

-------> Bruun's FFT algorithm
---------> Bruun's algorithm is a fast Fourier transform (FFT) algorithm based on an unusual recursive polynomial-factorization approach, 
-----------> proposed for powers of two by G. Bruun in 1978 and generalized to arbitrary even composite sizes by H. Murakami in 1996. 
-----------> Because its operations involve only real coefficients until the last computation stage, 
-----------> it was initially proposed as a way to efficiently compute the discrete Fourier transform (DFT) of real data. 
-----------> Bruun's algorithm has not seen widespread use, however, as approaches based 
-----------> on the ordinary Cooley–Tukey FFT algorithm have been successfully adapted to real data with at least as much efficiency. 
-----------> Furthermore, there is evidence that Bruun's algorithm may be intrinsically less accurate than Cooley–Tukey in the face of finite numerical precision (Storn, 1993).
---------> Nevertheless, Bruun's algorithm illustrates an alternative algorithmic framework that can express both itself and the Cooley–Tukey algorithm, 
-----------> and thus provides an interesting perspective on FFTs that permits mixtures of the two algorithms and other generalizations. 

-------> Cooley–Tukey FFT algorithm
---------> The Cooley–Tukey algorithm, named after J. W. Cooley and John Tukey, is the most common fast Fourier transform (FFT) algorithm. 
-----------> It re-expresses the discrete Fourier transform (DFT) of an arbitrary composite size N = N1*N2 in terms of N1 smaller DFTs of sizes N2, 
-----------> recursively, to reduce the computation time to O(N log N) for highly composite N (smooth numbers). 
-----------> Because of the algorithm's importance, specific variants and implementation styles have become known by their own names, as described below.
---------> Because the Cooley–Tukey algorithm breaks the DFT into smaller DFTs, it can be combined arbitrarily with any other algorithm for the DFT. 
-----------> For example, Rader's or Bluestein's algorithm can be used to handle large prime factors that cannot be decomposed by Cooley–Tukey, 
-----------> or the prime-factor algorithm can be exploited for greater efficiency in separating out relatively prime factors.
---------> The algorithm, along with its recursive application, was invented by Carl Friedrich Gauss. Cooley and Tukey independently rediscovered and popularized it 160 years later. 

-------> Fast Fourier transform
---------> A fast Fourier transform (FFT) is an algorithm that computes the discrete Fourier transform (DFT) of a sequence, or its inverse (IDFT). 
-----------> Fourier analysis converts a signal from its original domain (often time or space) to a representation in the frequency domain and vice versa. 
-----------> The DFT is obtained by decomposing a sequence of values into components of different frequencies. 
-----------> This operation is useful in many fields, but computing it directly from the definition is often too slow to be practical. 
-----------> An FFT rapidly computes such transformations by factorizing the DFT matrix into a product of sparse (mostly zero) factors.
-----------> As a result, it manages to reduce the complexity of computing the DFT from O(N^2), 
-----------> which arises if one simply applies the definition of DFT, to O(N*logN), where n is the data size. 
-----------> The difference in speed can be enormous, especially for long data sets where N may be in the thousands or millions. 
-----------> In the presence of round-off error, many FFT algorithms are much more accurate than evaluating the DFT definition directly or indirectly. 
-----------> There are many different FFT algorithms based on a wide range of published theories, from simple complex-number arithmetic to group theory and number theory.
---------> Fast Fourier transforms are widely used for applications in engineering, music, science, and mathematics. 
-----------> The basic ideas were popularized in 1965, but some algorithms had been derived as early as 1805.
-----------> In 1994, Gilbert Strang described the FFT as "the most important numerical algorithm of our lifetime",
-----------> and it was included in Top 10 Algorithms of 20th Century by the IEEE magazine Computing in Science & Engineering.
---------> The best-known FFT algorithms depend upon the factorization of N, 
-----------> but there are FFTs with O(N log N) complexity for all N, even for prime N. 
-----------> Many FFT algorithms depend only on the fact that e^(−2*π*i/N)  is an N-th primitive root of unity, 
-----------> and thus can be applied to analogous transforms over any finite field, such as number-theoretic transforms. 
-----------> Since the inverse DFT is the same as the DFT, but with the opposite sign in the exponent and a 1/N factor, any FFT algorithm can easily be adapted for it. 

-------> Prime-factor FFT algorithm
---------> The prime-factor algorithm (PFA), also called the Good–Thomas algorithm (1958/1963), 
-----------> is a fast Fourier transform (FFT) algorithm that re-expresses the discrete Fourier transform (DFT) of a size N = N1*N2 as a two-dimensional N1×N2 DFT, 
-----------> but only for the case where N1 and N2 are relatively prime. 
-----------> These smaller transforms of size N1 and N2 can then be evaluated by applying PFA recursively or by using some other FFT algorithm.
---------> PFA should not be confused with the mixed-radix generalization of the popular Cooley–Tukey algorithm, 
-----------> which also subdivides a DFT of size N = N1N2 into smaller transforms of size N1 and N2. 
-----------> The latter algorithm can use any factors (not necessarily relatively prime), 
-----------> but it has the disadvantage that it also requires extra multiplications by roots of unity called twiddle factors, 
-----------> in addition to the smaller transforms. 
-----------> On the other hand, PFA has the disadvantages that it only works for relatively prime factors (e.g. it is useless for power-of-two sizes) 
-----------> and that it requires a more complicated re-indexing of the data based on the Chinese remainder theorem (CRT). 
-----------> Note, however, that PFA can be combined with mixed-radix Cooley–Tukey, 
-----------> with the former factorizing N into relatively prime components and the latter handling repeated factors.
---------> PFA is also closely related to the nested Winograd FFT algorithm, 
-----------> where the latter performs the decomposed N1 by N2 transform via more sophisticated two-dimensional convolution techniques. 
-----------> Some older papers therefore also call Winograd's algorithm a PFA FFT.
---------> (Although the PFA is distinct from the Cooley–Tukey algorithm, 
-----------> Good's 1958 work on the PFA was cited as inspiration by Cooley and Tukey in their 1965 paper, 
-----------> and there was initially some confusion about whether the two algorithms were different. 
-----------> In fact, it was the only prior FFT work cited by them, as they were not then aware of the earlier research by Gauss and others.) 

-------> Rader's FFT algorithm
---------> Rader's algorithm (1968),[1] named for Charles M. Rader of MIT Lincoln Laboratory, 
-----------> is a fast Fourier transform (FFT) algorithm that computes the discrete Fourier transform (DFT) of prime sizes by re-expressing the DFT as a cyclic convolution 
-----------> (the other algorithm for FFTs of prime sizes, Bluestein's algorithm, also works by rewriting the DFT as a convolution).
---------> Since Rader's algorithm only depends upon the periodicity of the DFT kernel, 
-----------> it is directly applicable to any other transform (of prime order) with a similar property, 
-----------> such as a number-theoretic transform or the discrete Hartley transform.
---------> The algorithm can be modified to gain a factor of two savings for the case of DFTs of real data, 
-----------> using a slightly modified re-indexing/permutation to obtain two half-size cyclic convolutions of real data;
-----------> an alternative adaptation for DFTs of real data uses the discrete Hartley transform.
---------> Winograd extended Rader's algorithm to include prime-power DFT sizes p^m, 
-----------> and today Rader's algorithm is sometimes described as a special case of Winograd's FFT algorithm, 
-----------> also called the multiplicative Fourier transform algorithm (Tolimieri et al., 1997), which applies to an even larger class of sizes. 
-----------> However, for composite sizes such as prime powers, the Cooley–Tukey FFT algorithm is much simpler and more practical to implement, 
-----------> so Rader's algorithm is typically only used for large-prime base cases of Cooley–Tukey's recursive decomposition of the DFT. 

-----> Fast folding algorithm: 
-------> This is an efficient algorithm for the detection of approximately periodic events within time series data
-------> In signal processing, the fast folding algorithm (Staelin, 1969) is an efficient algorithm for the detection of approximately-periodic events within time series data. 
---------> It computes superpositions of the signal modulo various window sizes simultaneously.
-------> The FFA is best known for its use in the detection of pulsars, as popularised by SETI@home and Astropulse. 

-----> Gerchberg–Saxton algorithm: 
-------> This is phase retrieval algorithm for optical planes
-------> The Gerchberg–Saxton (GS) algorithm is an iterative phase retrieval algorithm 
---------> for retrieving the phase of a complex-valued wavefront from two intensity measurements acquired in two different planes.
---------> Typically, the two planes are the image plane and the far field (diffraction) plane, 
---------> and the wavefront propagation between these two planes is given by the Fourier transform. 
---------> The original paper by Gerchberg and Saxton considered image and diffraction pattern of a sample acquired in an electron microscope.
-------> It is often necessary to know only the phase distribution from one of the planes, 
---------> since the phase distribution on the other plane can be obtained 
---------> by performing a Fourier transform on the plane whose phase is known. 
---------> Although often used for two-dimensional signals, the GS algorithm is also valid for one-dimensional signals.
-------> The pseudocode below performs the GS algorithm to obtain a phase distribution for the plane "Source", 
---------> such that its Fourier transform would have the amplitude distribution of the plane "Target". 

-----> Goertzel algorithm: 
-------> This is identify a particular frequency component in a signal. Can be used for DTMF digit decoding.
-------> The Goertzel algorithm is a technique in digital signal processing (DSP) 
---------> for efficient evaluation of the individual terms of the discrete Fourier transform (DFT). 
---------> It is useful in certain practical applications, such as recognition of dual-tone multi-frequency signaling (DTMF) tones 
---------> produced by the push buttons of the keypad of a traditional analog telephone. 
---------> The algorithm was first described by Gerald Goertzel in 1958.
-------> Like the DFT, the Goertzel algorithm analyses one selectable frequency component from a discrete signal.
---------> Unlike direct DFT calculations, the Goertzel algorithm applies a single real-valued coefficient at each iteration, 
---------> using real-valued arithmetic for real-valued input sequences. 
---------> For covering a full spectrum, the Goertzel algorithm has a higher order of complexity than fast Fourier transform (FFT) algorithms, 
---------> but for computing a small number of selected frequency components, it is more numerically efficient. 
---------> The simple structure of the Goertzel algorithm makes it well suited to small processors and embedded applications.
-------> The Goertzel algorithm can also be used "in reverse" as a sinusoid synthesis function, 
---------> which requires only 1 multiplication and 1 subtraction per generated sample.

-----> Karplus-Strong string synthesis: 
-------> This is physical modelling synthesis to simulate the sound of a hammered or plucked string or some types of percussion
-------> Karplus–Strong string synthesis is a method of physical modelling synthesis that loops a short waveform 
---------> through a filtered delay line to simulate the sound of a hammered or plucked string or some types of percussion.
-------> At first glance, this technique can be viewed as subtractive synthesis based on a 
---------> feedback loop similar to that of a comb filter for z-transform analysis. 
---------> However, it can also be viewed as the simplest class of wavetable-modification algorithms now known as digital waveguide synthesis, 
---------> because the delay line acts to store one period of the signal.
-------> Alexander Strong invented the algorithm, and Kevin Karplus did the first analysis of how it worked. 
---------> Together they developed software and hardware implementations of the algorithm, including a custom VLSI chip. 
---------> They named the algorithm "Digitar" synthesis, as a portmanteau for "digital guitar". 



-----> Image processing
-------> Digital image processing is the use of a digital computer to process digital images through an algorithm.
---------> As a subcategory or field of digital signal processing, digital image processing has many advantages over analog image processing. 
---------> It allows a much wider range of algorithms to be applied to the input data and can avoid problems such as the build-up of noise and distortion during processing. 
---------> Since images are defined over two dimensions (perhaps more) digital image processing may be modeled in the form of multidimensional systems. 
---------> The generation and development of digital image processing are mainly affected by three factors: first, the development of computers; 
---------> second, the development of mathematics (especially the creation and improvement of discrete mathematics theory); 
---------> third, the demand for a wide range of applications in environment, agriculture, military, industry and medical science has increased. 

-------> Contrast Enhancement

---------> Histogram equalization: 
-----------> This uses histogram to improve image contrast
-----------> Histogram equalization is a method in image processing of contrast adjustment using the image's histogram. 

---------> Adaptive histogram equalization: 
-----------> This histogram equalization which adapts to local changes in contrast
-----------> Adaptive histogram equalization (AHE) is a computer image processing technique used to improve contrast in images. 
-------------> It differs from ordinary histogram equalization in the respect that the adaptive method computes several histograms, 
-------------> each corresponding to a distinct section of the image, and uses them to redistribute the lightness values of the image. 
-------------> It is therefore suitable for improving the local contrast and enhancing the definitions of edges in each region of an image.
-----------> However, AHE has a tendency to overamplify noise in relatively homogeneous regions of an image. 
-------------> A variant of adaptive histogram equalization called contrast limited adaptive histogram equalization (CLAHE) prevents this by limiting the amplification. 

-------> Connected-component labeling: 
---------> This find and label disjoint regions
---------> Connected-component labeling (CCL), connected-component analysis (CCA), blob extraction, 
-----------> region labeling, blob discovery, or region extraction is an algorithmic application of graph theory,
-----------> where subsets of connected components are uniquely labeled based on a given heuristic. 
-----------> Connected-component labeling is not to be confused with segmentation.
---------> Connected-component labeling is used in computer vision to detect connected regions in binary digital images, 
-----------> although color images and data with higher dimensionality can also be processed.
-----------> When integrated into an image recognition system or human-computer interaction interface, connected component labeling can operate on a variety of information.
-----------> Blob extraction is generally performed on the resulting binary image from a thresholding step, 
-----------> but it can be applicable to gray-scale and color images as well. Blobs may be counted, filtered, and tracked.
---------> Blob extraction is related to but distinct from blob detection. 

-------> Dithering and half-toning
---------> Dither is an intentionally applied form of noise used to randomize quantization error, 
-----------> preventing large-scale patterns such as color banding in images. 
-----------> Dither is routinely used in processing of both digital audio and video data, and is often one of the last stages of mastering audio to a CD.
---------> A common use of dither is converting a grayscale image to black and white, 
-----------> such that the density of black dots in the new image approximates the average gray level in the original. 
---------> Halftone is the reprographic technique that simulates continuous-tone imagery through the use of dots, 
-----------> varying either in size or in spacing, thus generating a gradient-like effect.
-----------> "Halftone" can also be used to refer specifically to the image that is produced by this process.
---------> Where continuous-tone imagery contains an infinite range of colors or greys, the halftone process reduces visual reproductions to an image that is printed with only one color of ink, 
-----------> in dots of differing size (pulse-width modulation) or spacing (frequency modulation) or both. This reproduction relies on a basic optical illusion: 
-----------> when the halftone dots are small, the human eye interprets the patterned areas as if they were smooth tones. 
-----------> At a microscopic level, developed black-and-white photographic film also consists of only two colors, and not an infinite range of continuous tones. 
-----------> For details, see film grain.
---------> Just as color photography evolved with the addition of filters and film layers,
-----------> color printing is made possible by repeating the halftone process for each subtractive color
-----------> most commonly using what is called the "CMYK color model".
-----------> The semi-opaque property of ink allows halftone dots of different colors to create another optical effect: full-color imagery.

---------> Error diffusion
-----------> Error diffusion is a type of halftoning in which the quantization residual is distributed to neighboring pixels that have not yet been processed. 
-------------> Its main use is to convert a multi-level image into a binary image, though it has other applications.
-----------> Unlike many other halftoning methods, error diffusion is classified as an area operation, 
-------------> because what the algorithm does at one location influences what happens at other locations. 
-------------> This means buffering is required, and complicates parallel processing. 
-------------> Point operations, such as ordered dither, do not have these complications.
-----------> Error diffusion has the tendency to enhance edges in an image. 
-------------> This can make text in images more readable than in other halftoning techniques. 

---------> Floyd–Steinberg dithering
-----------> Floyd–Steinberg dithering is an image dithering algorithm first published in 1976 by Robert W. Floyd and Louis Steinberg. 
-------------> It is commonly used by image manipulation software, for example when an image is converted into GIF format that is restricted to a maximum of 256 colors. 

---------> Ordered dithering
-----------> Ordered dithering is an image dithering algorithm. 
-------------> It is commonly used to display a continuous image on a display of smaller color depth. 
-------------> For example, Microsoft Windows uses it in 16-color graphics modes. 
-------------> The algorithm is characterized by noticeable crosshatch patterns in the result. 

---------> Riemersma dithering

-------> Elser difference-map algorithm: 
---------> This a search algorithm for general constraint satisfaction problems. 
-----------> Originally used for X-Ray diffraction microscopy
---------> The difference-map algorithm is a search algorithm for general constraint satisfaction problems. 
-----------> It is a meta-algorithm in the sense that it is built from more basic algorithms that perform projections onto constraint sets. 
-----------> From a mathematical perspective, the difference-map algorithm is a dynamical system based on a mapping of Euclidean space. 
-----------> Solutions are encoded as fixed points of the mapping.
---------> Although originally conceived as a general method for solving the phase problem, 
-----------> the difference-map algorithm has been used for the boolean satisfiability problem, 
-----------> protein structure prediction, 
-----------> Ramsey numbers, diophantine equations, and Sudoku,[1] as well as sphere
-----------> and disk-packing problems.[2] Since these applications include NP-complete problems, 
-----------> the scope of the difference map is that of an incomplete algorithm. 
-----------> Whereas incomplete algorithms can efficiently verify solutions (once a candidate is found), 
-----------> they cannot prove that a solution does not exist.
---------> The difference-map algorithm is a generalization of two iterative methods: 
-----------> Fienup's Hybrid input output (HIO) algorithm for phase retrieval[3] and the Douglas-Rachford algorithm[4] for convex optimization. 
-----------> Iterative methods, in general, have a long history in phase retrieval and convex optimization. 
-----------> The use of this style of algorithm for hard, non-convex problems is a more recent development. 

-------> Feature detection
---------> Feature detection includes methods for computing abstractions of image information 
-----------> and making local decisions at every image point whether there is an image feature of a given type at that point or not. 
-----------> The resulting features will be subsets of the image domain, 
-----------> often in the form of isolated points, continuous curves or connected regions.
---------> The extraction of features are sometimes made over several scalings. 
-----------> One of these methods is the scale-invariant feature transform (SIFT). 

---------> Canny edge detector: 
-----------> This detects a wide range of edges in images
-----------> The Canny edge detector is an edge detection operator that uses a multi-stage algorithm to detect a wide range of edges in images. 
-------------> It was developed by John F. Canny in 1986. 
-------------> Canny also produced a computational theory of edge detection explaining why the technique works. 

---------> Generalised Hough transform
-----------> The generalized Hough transform (GHT), introduced by Dana H. Ballard in 1981, 
-------------> is the modification of the Hough transform using the principle of template matching.
-------------> The Hough transform was initially developed to detect analytically defined shapes (e.g., line, circle, ellipse etc.). 
-------------> In these cases, we have knowledge of the shape and aim to find out its location and orientation in the image. 
-------------> This modification enables the Hough transform to be used to detect an arbitrary object described with its model.
-----------> The problem of finding the object (described with a model) in an image can be solved by finding the model's position in the image. 
-------------> With the generalized Hough transform, the problem of finding the model's position is transformed to a problem 
-------------> of finding the transformation's parameter that maps the model into the image. 
-------------> Given the value of the transformation's parameter, the position of the model in the image can be determined.
-----------> The original implementation of the GHT used edge information to define a mapping from orientation of an edge point to a reference point of the shape. 
-------------> In the case of a binary image where pixels can be either black or white, 
-------------> every black pixel of the image can be a black pixel of the desired pattern thus creating a locus of reference points in the Hough space. 
-------------> Every pixel of the image votes for its corresponding reference points. 
-------------> The maximum points of the Hough space indicate possible reference points of the pattern in the image. 
-------------> This maximum can be found by scanning the Hough space or by solving a relaxed set of equations, each of them corresponding to a black pixel.

---------> Hough transform
-----------> The Hough transform is a feature extraction technique used in image analysis, computer vision, and digital image processing.
-------------> The purpose of the technique is to find imperfect instances of objects within a certain class of shapes by a voting procedure. 
-------------> This voting procedure is carried out in a parameter space, from which object candidates 
-------------> are obtained as local maxima in a so-called accumulator space that is explicitly constructed by the algorithm for computing the Hough transform.
-----------> The classical Hough transform was concerned with the identification of lines in the image, 
-------------> but later the Hough transform has been extended to identifying positions of arbitrary shapes, most commonly circles or ellipses. 
-------------> The Hough transform as it is universally used today was invented by Richard Duda and Peter Hart in 1972, 
-------------> who called it a "generalized Hough transform"[2] after the related 1962 patent of Paul Hough.
-------------> The transform was popularized in the computer vision community by Dana H. 
-------------> Ballard through a 1981 journal article titled "Generalizing the Hough transform to detect arbitrary shapes". 

---------> Marr–Hildreth algorithm: 
-----------> This is an early edge detection algorithm
-----------> In computer vision, the Marr–Hildreth algorithm is a method of detecting edges in digital images, 
-------------> that is, continuous curves where there are strong and rapid variations in image brightness.
-------------> The Marr–Hildreth edge detection method is simple and operates by convolving the image with the Laplacian of the Gaussian function,
-------------> or, as a fast approximation by difference of Gaussians. 
-------------> Then, zero crossings are detected in the filtered result to obtain the edges. 
-------------> The Laplacian-of-Gaussian image operator is sometimes also referred to as the Mexican hat wavelet due to its visual shape when turned upside-down. 
-------------> David Marr and Ellen C. Hildreth are two of the inventors.[2]

---------> SIFT (Scale-invariant feature transform): 
-----------> This is an algorithm to detect and describe local features in images.
-----------> The scale-invariant feature transform (SIFT) is a computer vision algorithm to detect, describe, 
-------------> and match local features in images, invented by David Lowe in 1999. 
-------------> Applications include object recognition, robotic mapping and navigation, image stitching, 
-------------> 3D modeling, gesture recognition, video tracking, individual identification of wildlife and match moving.
-----------> SIFT keypoints of objects are first extracted from a set of reference images[1] and stored in a database. 
-------------> An object is recognized in a new image by individually comparing each feature from the new image to this database 
-------------> and finding candidate matching features based on Euclidean distance of their feature vectors. 
-------------> From the full set of matches, subsets of keypoints that agree on the object and its location, scale, 
-------------> and orientation in the new image are identified to filter out good matches. 
-------------> The determination of consistent clusters is performed rapidly by using an efficient hash table implementation of the generalised Hough transform. 
-------------> Each cluster of 3 or more features that agree on an object and its pose is then subject 
-------------> to further detailed model verification and subsequently outliers are discarded. 
-------------> Finally the probability that a particular set of features indicates the presence of an object is computed, 
-------------> given the accuracy of fit and number of probable false matches. 
-------------> Object matches that pass all these tests can be identified as correct with high confidence.[2] 

---------> SURF (Speeded Up Robust Features): 
-----------> This is a robust local feature detector, first presented by Herbert Bay et al. in 2006, 
-------------> that can be used in computer vision tasks like object recognition or 3D reconstruction. 
-----------> It is partly inspired by the SIFT descriptor. 
-----------> The standard version of SURF is several times faster than SIFT 
-------------> and claimed by its authors to be more robust against different image transformations than SIFT.
-----------> In computer vision, speeded up robust features (SURF) is a patented local feature detector and descriptor. 
-------------> It can be used for tasks such as object recognition, image registration, classification, or 3D reconstruction. 
-------------> It is partly inspired by the scale-invariant feature transform (SIFT) descriptor. 
-------------> The standard version of SURF is several times faster than SIFT and claimed by its authors to be more robust against different image transformations than SIFT.
-----------> To detect interest points, SURF uses an integer approximation of the determinant of Hessian blob detector, 
-------------> which can be computed with 3 integer operations using a precomputed integral image. 
-------------> Its feature descriptor is based on the sum of the Haar wavelet response around the point of interest. 
-------------> These can also be computed with the aid of the integral image.
-----------> SURF descriptors have been used to locate and recognize objects, people or faces, 
-------------> to reconstruct 3D scenes, to track objects and to extract points of interest.
-----------> SURF was first published by Herbert Bay, Tinne Tuytelaars, and Luc Van Gool, 
-------------> and presented at the 2006 European Conference on Computer Vision. 
-------------> An application of the algorithm is patented in the United States.
-------------> An "upright" version of SURF (called U-SURF) is not invariant to image rotation 
-------------> and therefore faster to compute and better suited for application where the camera remains more or less horizontal.
-----------> The image is transformed into coordinates, using the multi-resolution pyramid technique, 
-------------> to copy the original image with Pyramidal Gaussian or Laplacian Pyramid shape to obtain an image with the same size but with reduced bandwidth. 
-------------> This achieves a special blurring effect on the original image, called Scale-Space and ensures that the points of interest are scale invariant. 

-------> Richardson–Lucy deconvolution: 
---------> This image de-blurring algorithm
---------> The Richardson–Lucy algorithm, also known as Lucy–Richardson deconvolution, 
-----------> is an iterative procedure for recovering an underlying image that has been blurred by a known point spread function. 
-----------> It was named after William Richardson and Leon Lucy, who described it independently.

-------> Blind deconvolution: 
---------> This image de-blurring algorithm when point spread function is unknown.
---------> In electrical engineering and applied mathematics, blind deconvolution is deconvolution without explicit knowledge of the impulse response function used in the convolution. 
-----------> This is usually achieved by making appropriate assumptions of the input to estimate the impulse response by analyzing the output. 
-----------> Blind deconvolution is not solvable without making assumptions on input and impulse response. 
-----------> Most of the algorithms to solve this problem are based on assumption that both input and impulse response live in respective known subspaces. 
-----------> However, blind deconvolution remains a very challenging non-convex optimization problem even with this assumption. 

-------> Median filtering
---------> The median filter is a non-linear digital filtering technique, 
-----------> often used to remove noise from an image or signal. 
-----------> Such noise reduction is a typical pre-processing step to improve the results of later processing (for example, edge detection on an image). 
-----------> Median filtering is very widely used in digital image processing because, under certain conditions, 
-----------> it preserves edges while removing noise (but see the discussion below), also having applications in signal processing. 

-------> Seam carving: 
---------> This content-aware image resizing algorithm
---------> Seam carving (or liquid rescaling) is an algorithm for content-aware image resizing, developed by Shai Avidan,
-----------> of Mitsubishi Electric Research Laboratories (MERL), and Ariel Shamir, of the Interdisciplinary Center and MERL. 
-----------> It functions by establishing a number of seams (paths of least importance) in an image 
-----------> and automatically removes seams to reduce image size or inserts seams to extend it. 
-----------> Seam carving also allows manually defining areas in which pixels may not be modified, 
-----------> and features the ability to remove whole objects from photographs.
---------> The purpose of the algorithm is image retargeting, 
-----------> which is the problem of displaying images without distortion on media of various sizes (cell phones, projection screens) 
-----------> using document standards, like HTML, that already support dynamic changes in page layout and text but not images.
---------> Image Retargeting was invented by Vidya Setlur, Saeko Takage, Ramesh Raskar, Michael Gleicher and Bruce Gooch in 2005.
-----------> The work by Setlur et al. won the 10-year impact award in 2015[where?]. 

-------> Segmentation: 
---------> This partition a digital image into two or more regions
---------> In digital image processing and computer vision, image segmentation is the process of partitioning a digital image into multiple image segments,
-----------> also known as image regions or image objects (sets of pixels). 
-----------> The goal of segmentation is to simplify and/or change the representation of an image into something that is more meaningful and easier to analyze.
-----------> Image segmentation is typically used to locate objects and boundaries (lines, curves, etc.) in images. 
-----------> More precisely, image segmentation is the process of assigning a label to every pixel in an image such that pixels with the same label share certain characteristics.
---------> The result of image segmentation is a set of segments that collectively cover the entire image, or a set of contours extracted from the image (see edge detection). 
-----------> Each of the pixels in a region are similar with respect to some characteristic or computed property, such as color, intensity, or texture. 
-----------> Adjacent regions are significantly different color respect to the same characteristic(s).
-----------> When applied to a stack of images, typical in medical imaging, the resulting contours after image segmentation 
-----------> can be used to create 3D reconstructions with the help of interpolation algorithms like marching cubes.

---------> GrowCut algorithm: 
-----------> This is an interactive segmentation algorithm
-----------> GrowCut is an interactive segmentation algorithm. 
-------------> It uses Cellular Automaton as an image model. 
-------------> Automata evolution models segmentation process. 
-------------> Each cell of the automata has some label (in case of binary segmentation - 'object', 'background' and 'empty'). 
-------------> During automata evolution some cells capture their neighbours, replacing their labels.
-----------> In GrowCut, a user vaguely draws some strokes inside the object of interest with an object brush, and outside the object with a background brush.
-------------> In simple cases only a few strokes suffice for segmentation. 

---------> Random walker algorithm
-----------> The random walker algorithm is an algorithm for image segmentation. 
-------------> In the first description of the algorithm,[1] a user interactively labels a small number of pixels with known labels (called seeds), e.g., "object" and "background". 
-------------> The unlabeled pixels are each imagined to release a random walker, 
-------------> and the probability is computed that each pixel's random walker first arrives at a seed bearing each label, i.e., if a user places K seeds, each with a different label, 
-------------> then it is necessary to compute, for each pixel, the probability that a random walker leaving the pixel will first arrive at each seed. 
-------------> These probabilities may be determined analytically by solving a system of linear equations. 
-------------> After computing these probabilities for each pixel, the pixel is assigned to the label for which it is most likely to send a random walker. 
-------------> The image is modeled as a graph, in which each pixel corresponds to a node which is connected to neighboring pixels by edges,
-------------> and the edges are weighted to reflect the similarity between the pixels. 
-------------> Therefore, the random walk occurs on the weighted graph (see Doyle and Snell for an introduction to random walks on graphs).
-----------> Although the initial algorithm was formulated as an interactive method for image segmentation, 
-------------> it has been extended to be a fully automatic algorithm, given a data fidelity term (e.g., an intensity prior). 
-------------> It has also been extended to other applications.
-----------> The algorithm was initially published by Leo Grady as a conference paper and later as a journal paper.

---------> Region growing
-----------> Region growing is a simple region-based image segmentation method. 
-------------> It is also classified as a pixel-based image segmentation method since it involves the selection of initial seed points.
-----------> This approach to segmentation examines neighboring pixels of initial seed points 
-------------> and determines whether the pixel neighbors should be added to the region. 
-------------> The process is iterated on, in the same manner as general data clustering algorithms.
-------------> A general discussion of the region growing algorithm is described below. 

---------> Watershed transformation: 
-----------> This is a class of algorithms based on the watershed analogy
-----------> In the study of image processing, a watershed is a transformation defined on a grayscale image. 
-------------> The name refers metaphorically to a geological watershed, or drainage divide, which separates adjacent drainage basins.
-------------> The watershed transformation treats the image it operates upon like a topographic map, 
-------------> with the brightness of each point representing its height, and finds the lines that run along the tops of ridges.
-----------> There are different technical definitions of a watershed. 
-------------> In graphs, watershed lines may be defined on the nodes, on the edges, or hybrid lines on both nodes and edges. 
-------------> Watersheds may also be defined in the continuous domain.
-------------> There are also many different algorithms to compute watersheds. 
-------------> Watershed algorithms are used in image processing primarily for object segmentation purposes, that is, for separating different objects in an image. 
-------------> This allows for counting the objects or for further analysis of the separated objects. 



-> Software engineering

---> Cache algorithms
-----> In computing, cache algorithms (also frequently called cache replacement algorithms or cache replacement policies) are optimizing instructions, or algorithms, 
-------> that a computer program or a hardware-maintained structure can utilize in order to manage a cache of information stored on the computer. 
-------> Caching improves performance by keeping recent or often-used data items in memory locations that are faster or computationally cheaper to access than normal memory stores. 
-------> When the cache is full, the algorithm must choose which items to discard to make room for the new ones. 

---> CHS conversion: 
-----> This converts between disk addressing systems
-----> In the LBA addressing scheme, sectors are numbered as integer indexes; 
-------> when mapped to CHS (cylinder-head-sector) tuples, 
-------> LBA numbering starts with the first cylinder, first head, and track's first sector. 
-------> Once the track is exhausted, numbering continues to the second head, while staying inside the first cylinder. 
-------> Once all heads inside the first cylinder are exhausted, numbering continues from the second cylinder, etc. 
-------> Thus, the lower the LBA value is, the closer the physical sector is to the hard drive's first (that is, outermost) cylinder. 

---> Double dabble: 
-----> This converts binary numbers to BCD
-----> In computer science, the double dabble algorithm is used to convert binary numbers into binary-coded decimal (BCD) notation.
-------> It is also known as the shift-and-add-3 algorithm, and can be implemented using a small number of gates in computer hardware, but at the expense of high latency.

---> Hash Function: 
-----> This convert a large, possibly variable-sized amount of data into a small datum, usually a single integer that may serve as an index into an array
-----> A hash function is any function that can be used to map data of arbitrary size to fixed-size values. 
-------> The values returned by a hash function are called hash values, hash codes, digests, or simply hashes. 
-------> The values are usually used to index a fixed-size table called a hash table. 
-------> Use of a hash function to index a hash table is called hashing or scatter storage addressing.
-----> Hash functions and their associated hash tables are used in data storage and retrieval applications to access data in a small and nearly constant time per retrieval. 
-------> They require an amount of storage space only fractionally greater than the total space required for the data or records themselves. 
-------> Hashing is a computationally and storage space-efficient form of data access that avoids the non-constant access time of ordered 
-------> and unordered lists and structured trees, and the often exponential storage requirements of direct access of state spaces of large or variable-length keys.
-----> Use of hash functions relies on statistical properties of key and function interaction: 
-------> worst-case behaviour is intolerably bad with a vanishingly small probability, and average-case behaviour can be nearly optimal (minimal collision).
-----> Hash functions are related to (and often confused with) checksums, check digits, fingerprints, 
-------> lossy compression, randomization functions, error-correcting codes, and ciphers. 
-------> Although the concepts overlap to some extent, each one has its own uses and requirements and is designed and optimized differently. 
-------> The hash function differs from these concepts mainly in terms of data integrity. 

-----> Fowler–Noll–Vo hash function: 
-------> This is fast with low collision rate
-------> Fowler–Noll–Vo (or FNV) is a non-cryptographic hash function created by Glenn Fowler, Landon Curt Noll, and Kiem-Phong Vo.
-------> The basis of the FNV hash algorithm was taken from an idea sent as reviewer comments to the IEEE POSIX P1003.2 committee by Glenn Fowler and Phong Vo in 1991. 
---------> In a subsequent ballot round, Landon Curt Noll improved on their algorithm. In an email message to Landon, they named it the Fowler/Noll/Vo or FNV hash.

-----> Pearson hashing: 
-------> This computes 8 bit value only, optimized for 8 bit computers
-------> Pearson hashing is a hash function designed for fast execution on processors with 8-bit registers. 
---------> Given an input consisting of any number of bytes, it produces as output a single byte that is strongly dependent on every byte of the input. 
---------> Its implementation requires only a few instructions, plus a 256-byte lookup table containing a permutation of the values 0 through 255.
-------> This hash function is a CBC-MAC that uses an 8-bit substitution cipher implemented via the substitution table. 
---------> An 8-bit cipher has negligible cryptographic security, so the Pearson hash function is not cryptographically strong,
---------> but it is useful for implementing hash tables or as a data integrity check code, for which purposes it offers these benefits:
-----------> (1) It is extremely simple.
-----------> (2) It executes quickly on resource-limited processors.
-----------> (3) There is no simple class of inputs for which collisions (identical outputs) are especially likely.
-----------> (4) Given a small, privileged set of inputs (e.g., reserved words for a compiler), 
-------------> the permutation table can be adjusted so that those inputs yield distinct hash values, producing what is called a perfect hash function.
-----------> (5) Two input strings differing by exactly one character never collide.
-------------> E.g., applying the algorithm on the strings ABC and AEC will never produce the same value.
-------> One of its drawbacks when compared with other hashing algorithms designed for 8-bit processors is the suggested 256 byte lookup table, 
---------> which can be prohibitively large for a small microcontroller with a program memory size on the order of hundreds of bytes. 
---------> A workaround to this is to use a simple permutation function instead of a table stored in program memory. 
---------> However, using a too simple function, such as T[i] = 255-i, partly defeats the usability as a hash function as anagrams will result in the same hash value; 
---------> using a too complex function, on the other hand, will affect speed negatively. 
---------> Using a function rather than a table also allows extending the block size. Such functions naturally have to be bijective, like their table variants. 

-----> Zobrist hashing: 
-------> This is used in the implementation of transposition tables
-------> Zobrist hashing (also referred to as Zobrist keys or Zobrist signatures) is a hash function construction used in computer programs that play abstract board games, 
---------> such as chess and Go, to implement transposition tables, a special kind of hash table that is indexed by a board position and used to avoid analyzing the same position more than once. 
---------> Zobrist hashing is named for its inventor, Albert Lindsey Zobrist.
---------> It has also been applied as a method for recognizing substitutional alloy configurations in simulations of crystalline materials.
---------> Zobrist hashing is the first known instance of the generally useful underlying technique called tabulation hashing. 

---> Unicode Collation Algorithm
-----> The Unicode collation algorithm (UCA) is an algorithm defined in Unicode Technical Report #10, 
-------> which is a customizable method to produce binary keys from strings representing text 
-------> in any writing system and language that can be represented with Unicode. 
-------> These keys can then be efficiently byte-by-byte compared in order to collate 
-------> or sort them according to the rules of the language, with options for ignoring case, accents, etc.
-----> Unicode Technical Report #10 also specifies the Default Unicode Collation Element Table (DUCET). 
-------> This data file specifies a default collation ordering. 
-------> The DUCET is customizable for different languages. 
-------> Some such customisations can be found in the Unicode Common Locale Data Repository (CLDR).
-----> An open source implementation of UCA is included with the International Components for Unicode, ICU. 
-------> ICU supports tailoring, and the collation tailorings from CLDR are included in ICU. 
-------> The effects of tailoring and many language-specific tailorings are displayed in the on-line ICU Locale Explorer. 

---> Xor swap algorithm: swaps the values of two variables without using a buffer
-----> In computer programming, the exclusive or swap (sometimes shortened to XOR swap) is an algorithm 
-------> that uses the exclusive or bitwise operation to swap the values of two variables without using the temporary variable which is normally required.
-----> The algorithm is primarily a novelty and a way of demonstrating properties of the exclusive or operation. 
-------> It is sometimes discussed as a program optimization, 
-------> but there are almost no cases where swapping via exclusive or provides benefit over the standard, obvious technique. 



-> Database algorithms

---> Algorithms for Recovery and Isolation Exploiting Semantics (ARIES): 
-----> This transaction recovery
-----> Algorithms for Recovery and Isolation Exploiting Semantics, 
-------> or ARIES is a recovery algorithm designed to work with a no-force, steal database approach; 
-------> it is used by IBM Db2, Microsoft SQL Server and many other database systems.
-------> IBM Fellow Dr. C. Mohan is the primary inventor of the ARIES family of algorithms.
-----> Three main principles lie behind ARIES
-------> Write-ahead logging: 
---------> Any change to an object is first recorded in the log, 
---------> and the log must be written to stable storage before changes to the object are written to disk.
-------> Repeating history during Redo: 
---------> On restart after a crash, ARIES retraces the actions of a database before the crash 
---------> and brings the system back to the exact state that it was in before the crash. Then it undoes the transactions still active at crash time.
-------> Logging changes during Undo: 
---------> Changes made to the database while undoing transactions are logged to ensure such an action isn't repeated in the event of repeated restarts.
 
---> Join algorithms
-----> A join clause in SQL – corresponding to a join operation in relational algebra – combines columns from one or more tables into a new table. 
-------> ANSI-standard SQL specifies five types of JOIN: INNER, LEFT OUTER, RIGHT OUTER, FULL OUTER and CROSS. 

-----> Block nested loop
-------> A block-nested loop (BNL) is an algorithm used to join two relations in a relational database.[1]
-------> This algorithm[2] is a variation on the simple nested loop join used to join two relations R and S (the "outer" and "inner" join operands, respectively). 
---------> Suppose | R | < | S | {\displaystyle |R|<|S|} |R|<|S|. 
---------> In a traditional nested loop join, S will be scanned once for every tuple of R. If there are many qualifying R tuples, 
---------> and particularly if there is no applicable index for the join key on S, this operation will be very expensive.
-------> The block nested loop join algorithm improves on the simple nested loop join by only scanning S once for every group of R tuples. 
---------> For example, one variant of the block nested loop join reads an entire page of R tuples into memory and loads them into a hash table. 
---------> It then scans S, and probes the hash table to find S tuples that match any of the tuples in the current page of R. 
---------> This reduces the number of scans of S that are necessary.
-------> A more aggressive variant of this algorithm loads as many pages of R as can be fit in the available memory, 
---------> loading all such tuples into a hash table, and then repeatedly scans S. 
---------> This further reduces the number of scans of S that are necessary.
--------->  In fact, this algorithm is essentially a special-case of the classic hash join algorithm.
-------> The block nested loop runs in O(Pr * Ps / M) I/Os where M is the number of available pages of internal memory and Pr and Ps is size of R and S respectively in pages. 
---------> Note that block nested loop runs in O(Pr + Ps I/Os if R fits in the available internal memory. 

-----> Hash join
-------> The hash join is an example of a join algorithm and is used in the implementation of a relational database management system. 
---------> All variants of hash join algorithms involve building hash tables from the tuples of one or both of the joined relations, 
---------> and subsequently probing those tables so that only tuples with the same hash code need to be compared for equality in equijoins.
-------> Hash joins are typically more efficient than nested loops joins, except when the probe side of the join is very small. 
---------> They require an equijoin predicate (a predicate comparing records from one table with those from the other table using a conjunction of equality operators '=' on one or more columns). 

-----> Nested loop join
-------> A nested loop join is a naive algorithm that joins two sets by using two nested loops.
---------> Join operations are important for database management. 
-------> Algorithm
---------> Two relations R and S are joined as follows:
-----------> algorithm nested_loop_join is
----------->     for each tuple r in R do
----------->         for each tuple s in S do
----------->             if r and s satisfy the join condition then
----------->                 yield tuple <r,s>
---------> This algorithm will involve nr*bs+ br block transfers and nr+br seeks, 
-----------> where br and bs are number of blocks in relations R and S respectively, 
-----------> and nr is the number of tuples in relation R.
---------> The algorithm runs in  O(|R||S|) I/Os, where |R| and |S| is the number of tuples contained in R 
-----------> and S respectively and can easily be generalized to join any number of relations ...
---------> The block nested loop join algorithm is a generalization of the simple nested loops algorithm 
-----------> that takes advantage of additional memory to reduce the number of times that the S relation is scanned. 
-----------> It loads large chunks of relation R into main memory. 
-----------> For each chunk, it scans S and evaluates the join condition on all tuple pairs, currently in memory. 
-----------> This reduces the number of times S is scanned to once per chunk. 

-----> Sort-Merge Join
-------> The sort-merge join (also known as merge join) is a join algorithm and is used in the implementation of a relational database management system.
-------> The basic problem of a join algorithm is to find, for each distinct value of the join attribute, 
---------> the set of tuples in each relation which display that value. 
---------> The key idea of the sort-merge algorithm is to first sort the relations by the join attribute, 
---------> so that interleaved linear scans will encounter these sets at the same time.
-------> In practice, the most expensive part of performing a sort-merge join is arranging for both inputs to the algorithm to be presented in sorted order. 
---------> This can be achieved via an explicit sort operation (often an external sort), 
---------> or by taking advantage of a pre-existing ordering in one or both of the join relations. 
---------> The latter condition, called interesting order, can occur because an input to the join might be produced by an index scan of a tree-based index,
---------> another merge join, or some other plan operator that happens to produce output sorted on an appropriate key.
---------> Interesting orders need not be serendipitous: 
---------> the optimizer may seek out this possibility and choose a plan that is suboptimal 
---------> for a specific preceding operation if it yields an interesting order that one or more downstream nodes can exploit.
-------> Let's say that we have two relations R and S and |R|<|S|. R fits in Pr pages memory and S fits in Ps pages memory. 
---------> So, in the worst case sort-merge join will run in O(Pr + Ps) I/Os. 
---------> In the case that R and S are not ordered the worst case time cost will contain additional terms of sorting time: 
---------> O(Pr + Ps + Pr log⁡ (Pr) + Ps log⁡ (Ps)) , which equals O(Pr log⁡ (Pr) + Ps log⁡ (Ps))  (as linearithmic terms outweigh the linear terms). 



-> Distributed systems algorithms
---> Distributed computing is a field of computer science that studies distributed systems. 
-----> A distributed system is a system whose components are located on different networked computers,
----->  which communicate and coordinate their actions by passing messages to one another from any system.
-----> The components interact with one another in order to achieve a common goal. 
-----> Three significant challenges of distributed systems are: maintaining concurrency of components, 
-----> overcoming the lack of a global clock, and managing the independent failure of components.
-----> When a component of one system fails, the entire system does not fail.
-----> Examples of distributed systems vary from SOA-based systems to massively multiplayer online games to peer-to-peer applications.
---> A computer program that runs within a distributed system is called a distributed program, and distributed programming is the process of writing such programs.
-----> There are many different types of implementations for the message passing mechanism, including pure HTTP, RPC-like connectors and message queues.[6]
---> Distributed computing also refers to the use of distributed systems to solve computational problems. 
-----> In distributed computing, a problem is divided into many tasks, each of which is solved by one or more computers, 
-----> which communicate with each other via message passing.

---> Clock synchronization
-----> Clock synchronization is a topic in computer science and engineering that aims to coordinate otherwise independent clocks. 
-------> Even when initially set accurately, real clocks will differ after some amount of time due to clock drift, caused by clocks counting time at slightly different rates. 
-------> There are several problems that occur as a result of clock rate differences and several solutions, some being more acceptable than others in certain contexts.

-----> Berkeley algorithm
-------> The Berkeley algorithm is a method of clock synchronisation in distributed computing which assumes no machine has an accurate time source. 
---------> It was developed by Gusella and Zatti at the University of California, Berkeley in 1989.
---------> Like Cristian's algorithm, it is intended for use within intranets. 
-------> Algorithm
---------> Unlike Cristian's algorithm, the server process in the Berkeley algorithm, called the leader, periodically polls other follower processes. 
-----------> Generally speaking, the algorithm is:
-------------> (1) A leader is chosen via an election process such as Chang and Roberts algorithm.
-------------> (2) The leader polls the followers who reply with their time in a similar way to Cristian's algorithm.
-------------> (3) The leader observes the round-trip time (RTT) of the messages and estimates the time of each follower and its own.
-------------> (4) The leader then averages the clock times, ignoring any values it receives far outside the values of the others.
-------------> (5) Instead of sending the updated current time back to the other process, 
---------------> the leader then sends out the amount (positive or negative) that each follower must adjust its clock. 
---------------> This avoids further uncertainty due to RTT at the follower processes.
---------> With this method the average cancels out individual clock's tendencies to drift.
-----------> Gusella and Zatti released results involving 15 computers whose clocks were synchronised to within about 20-25 milliseconds using their protocol.
---------> Computer systems normally avoid rewinding their clock when they receive a negative clock alteration from the leader. 
-----------> Doing so would break the property of monotonic time, which is a fundamental assumption in certain algorithms in the system itself or in programs such as make. 
-----------> A simple solution to this problem is to halt the clock for the duration specified by the leader, 
-----------> but this simplistic solution can also cause problems, although they are less severe. 
-----------> For minor corrections, most systems slow the clock, applying the correction over a longer period of time.
---------> Often, any client whose clock differs by a value outside of a given tolerance is disregarded when averaging the results. 
-----------> This prevents the overall system time from being drastically skewed due to one erroneous clock. 

-----> Cristian's algorithm
-------> Cristian's algorithm (introduced by Flaviu Cristian in 1989)[1] is a method for clock synchronization 
---------> which can be used in many fields of distributive computer science but is primarily used in low-latency intranets. 
---------> Cristian observed that this simple algorithm is probabilistic, in that it only achieves synchronization 
---------> if the round-trip time (RTT) of the request is short compared to required accuracy. 
---------> It also suffers in implementations using a single server, 
---------> making it unsuitable for many distributive applications where redundancy may be crucial.
-------> Algorithm
---------> Cristian's algorithm works between a process P, and a time server S connected to a time reference source. Put simply:
-----------> (1) P requests the time from S at time t0.
-----------> (2) After receiving the request from P, S prepares a response and appends the time T from its own clock.
-----------> (3) P receives the response at time t1 then sets its time to be T + RTT/2, where RTT=t1-t0.
---------> If the RTT is actually split equally between request and response, the synchronisation is error-free. 
-----------> But due to unpredictable influences, this assumption is regularly not true. 
-----------> Longer RTTs indicate interference that is generally asymmetrical. 
-----------> Offset and jitter of the synchronisation are thus minimised by selecting suitable RTT from a set of many request/response pairs. 
-----------> Whether an RTT can be accepted at a given time depends on the drift of the clock and on the statistics of the RTT.
-----------> These quantities can be measured in the course of synchronisation, which optimises the method by itself. 

-----> Intersection algorithm
-------> The intersection algorithm is an agreement algorithm used to select sources for estimating accurate time from a number of noisy time sources. 
---------> It forms part of the modern Network Time Protocol. 
---------> It is a modified form of Marzullo's algorithm.
-------> While Marzullo's algorithm will return the smallest interval consistent with the largest number of sources, 
---------> the returned interval does not necessarily include the center point (calculated offset) of all the sources in the intersection. 
---------> The intersection algorithm returns an interval that includes that returned by Marzullo's algorithm but may be larger since it will include the center points. 
---------> This larger interval allows using additional statistical data to select a point within the interval, reducing the jitter in repeated execution.
-------> Method
---------> Given M intervals of the form c ± r (which means [c−r,c+r]), the algorithm seeks to find an interval with M−f sources.
-----------> The value f is referred to as the number of falsetickers, those sources which are in error (the actual value is outside the confidence band). 
-----------> The best estimate is that which assumes the fewest falsetickers, f. 
-----------> The results will be considered valid if f < M/2, otherwise the algorithm will return failure instead of an interval.
---------> The intersection algorithm begins by creating a table of tuples <offset, type>. For each interval there are three entries: 
-----------> the lower endpoint, the midpoint and the upper endpoint, labelled with types −1, 0 and +1 respectively. Thus the interval c ± r results in the entries <c−r,−1>, <c,0> and <c+r,+1>. 
-----------> These entries are then sorted by offset.
---------> Variables: This algorithm uses f as number of false tickers, endcount and midcount are integers. 
-----------> Lower and upper are values of offsets.
-------------> (1) [initialize best f] Start with f=0, assuming all input intervals are valid. 
---------------> Each time no interval is found f will be incremented until either an interval is found or f ≥ M/2.
-------------> (2) [initialize] endcount=0 and midcount=0.
-------------> (3) [find lower endpoint] Start at beginning of the list (lowest offset) consider each tuple in order. endcount = endcount−type. 
---------------> If endcount ≥ M−f then lower = offset and goto step 3 because the (possible) lower endpoint has been found. 
---------------> If the type = 0 then midcount = midcount+1. Repeat with next tuple. 
---------------> If reach end of list then goto step 6.
-------------> (4) [tentative lower endpoint found, initialize to find upper endpoint] set endcount=0.
-------------> (5) [determine number of midpoints] Start from end of list and work towards lower offsets. endcount = endcount+type. 
---------------> If endcount ≥ M−f then upper = offset, goto step 5. 
---------------> If type = 0 then midcount = midcount+1. 
---------------> Repeat for next tuple. 
---------------> If reach end of list then goto step 6.
-------------> (6) if lower ≤ upper and midcount ≤ f then return interval [lowerendpoint, upperendpoint] as resulting confidence interval.
-------------> (7) [increment number of falsetickers] f = f+1. If f ≥ M/2 then terminate and return FAILED, otherwise goto step 1.

-----> Marzullo's algorithm
-------> Marzullo's algorithm, invented by Keith Marzullo for his Ph.D. dissertation in 1984, is an agreement algorithm 
---------> used to select sources for estimating accurate time from a number of noisy time sources. 
---------> A refined version of it, renamed the "intersection algorithm", forms part of the modern Network Time Protocol. 
---------> Marzullo's algorithm is also used to compute the relaxed intersection of n boxes (or more generally n subsets of Rn), as required by several robust set estimation methods. 

---> Consensus (computer science): agreeing on a single value or history among unreliable processors
-----> A fundamental problem in distributed computing and multi-agent systems is to achieve overall system reliability in the presence of a number of faulty processes. 
-------> This often requires coordinating processes to reach consensus, or agree on some data value that is needed during computation. 
-------> Example applications of consensus include agreeing on what transactions to commit to a database in which order, state machine replication, and atomic broadcasts. 
-------> Real-world applications often requiring consensus include cloud computing, clock synchronization, PageRank, opinion formation, 
-------> smart power grids, state estimation, control of UAVs (and multiple robots/agents in general), load balancing, blockchain, and others. 

-----> Chandra–Toueg consensus algorithm
-------> The Chandra–Toueg consensus algorithm, published by Tushar Deepak Chandra and Sam Toueg in 1996, 
---------> is an algorithm for solving consensus in a network of unreliable processes equipped with an eventually strong failure detector. 
---------> The failure detector is an abstract version of timeouts; it signals to each process when other processes may have crashed. 
---------> An eventually strong failure detector is one that never identifies some specific non-faulty process as having failed after some initial period of confusion, 
---------> and, at the same time, eventually identifies all faulty processes as failed (where a faulty process is a process which eventually fails or crashes and a non-faulty process never fails). 
---------> The Chandra–Toueg consensus algorithm assumes that the number of faulty processes,
---------> denoted by f, is less than n/2 (i.e. the minority), i.e. it assumes f < n/2, where n is the total number of processes. 

-----> Paxos algorithm
-------> Paxos is a family of protocols for solving consensus in a network of unreliable or fallible processors. 
---------> Consensus is the process of agreeing on one result among a group of participants. 
---------> This problem becomes difficult when the participants or their communications may experience failures.
-------> Consensus protocols are the basis for the state machine replication approach to distributed computing, 
---------> as suggested by Leslie Lamport and surveyed by Fred Schneider.
---------> State machine replication is a technique for converting an algorithm into a fault-tolerant, distributed implementation. 
---------> Ad-hoc techniques may leave important cases of failures unresolved. 
---------> The principled approach proposed by Lamport et al. ensures all cases are handled safely.
-------> The Paxos protocol was first submitted in 1989 and named after a fictional legislative consensus system used on the Paxos island in Greece, 
---------> where Lamport wrote that the parliament had to function "even though legislators continually wandered in and out of the parliamentary Chamber".
---------> It was later published as a journal article in 1998.[5]
-------> The Paxos family of protocols includes a spectrum of trade-offs between the number of processors, 
---------> number of message delays before learning the agreed value, the activity level of individual participants, 
---------> number of messages sent, and types of failures. 
---------> Although no deterministic fault-tolerant consensus protocol can guarantee progress in an asynchronous network 
---------> (a result proved in a paper by Fischer, Lynch and Paterson[6]), 
---------> Paxos guarantees safety (consistency), and the conditions that could prevent it from making progress are difficult to provoke.
-------> Paxos is usually used where durability is required (for example, to replicate a file or a database), in which the amount of durable state could be large. 
---------> The protocol attempts to make progress even during periods when some bounded number of replicas are unresponsive. 
---------> There is also a mechanism to drop a permanently failed replica or to add a new replica. 

-----> Raft (computer science)
-------> Raft is a consensus algorithm designed as an alternative to the Paxos family of algorithms. 
---------> It was meant to be more understandable than Paxos by means of separation of logic, 
---------> but it is also formally proven safe and offers some additional features.
---------> Raft offers a generic way to distribute a state machine across a cluster of computing systems, 
---------> ensuring that each node in the cluster agrees upon the same series of state transitions. 
---------> It has a number of open-source reference implementations, with full-specification implementations in Go, C++, Java, and Scala.
---------> It is named after Reliable, Replicated, Redundant, And Fault-Tolerant.
-------> Raft is not a Byzantine fault tolerant algorithm: the nodes trust the elected leader.[1] 

---> Detection of Process Termination

-----> Dijkstra-Scholten algorithm
-------> The Dijkstra–Scholten algorithm (named after Edsger W. Dijkstra and Carel S. Scholten) is an algorithm for detecting termination in a distributed system.
---------> The algorithm was proposed by Dijkstra and Scholten in 1980.
-------> First, consider the case of a simple process graph which is a tree. 
---------> A distributed computation which is tree-structured is not uncommon. 
---------> Such a process graph may arise when the computation is strictly a divide-and-conquer type. 
---------> A node starts the computation and divides the problem in two (or more, usually a multiple of 2) roughly equal parts 
---------> and distribute those parts to other processors. 
---------> This process continues recursively until the problems are of sufficiently small size to solve in a single processor. 
-------> Algorithm
---------> The Dijkstra–Scholten algorithm is a tree-based algorithm which can be described by the following:
-----------> The initiator of a computation is the root of the tree.
-----------> Upon receiving a computational message:
-------------> If the receiving process is currently not in the computation: 
---------------> the process joins the tree by becoming a child of the sender of the message. (No acknowledgment message is sent at this point.)
-------------> If the receiving process is already in the computation: 
---------------> the process immediately sends an acknowledgment message to the sender of the message.
-----------> When a process has no more children and has become idle, the process detaches itself from the tree by sending an acknowledgment message to its tree parent.
-----------> Termination occurs when the initiator has no children and has become idle.

-----> Huang's algorithm
-------> Huang's algorithm is an algorithm for detecting termination in a distributed system. 
---------> The algorithm was proposed by Shing-Tsaan Huang in 1989 in the Journal of Computers.[1] 
-------> Termination detection
---------> The basis of termination detection is in the concept of a distributed system process' state.
-----------> At any time, a process in a distributed system is either in an active state or in an idle state. 
-----------> An active process may become idle at any time but an idle process may only become active again upon receiving a computational message.
---------> Termination occurs when all processes in the distributed system become idle and there are no computational messages in transit.
---------> Algorithm
-----------> Huang's algorithm can be described by the following:
-------------> Initially all processes are idle.
-------------> A distributed task is started by a process sending a computational message to another process. 
---------------> This initial process to send the message is the "controlling agent".
---------------> The initial weight of the controlling agent is w (usually 1).
-------------> The following rules are applied throughout the computation:
---------------> A process sending a message splits its current weight between itself and the message.
---------------> A process receiving a message adds the weight of the message to itself.
---------------> Upon becoming idle, a process sends a message containing its entire weight back to the controlling agent and it goes idle.
---------------> Termination occurs when the controlling agent has a weight of w and is in the idle state.
-----------> Some weaknesses to Huang's algorithm are that it is unable to detect termination if a message is lost in transit or if a process fails while in an active state. 

---> Lamport ordering: 
-----> This a partial ordering of events based on the happened-before relation
-----> In computer science, the happened-before relation (denoted: →) is a relation between the result of two events, 
-------> such that if one event should happen before another event, the result must reflect that,
-------> even if those events are in reality executed out of order (usually to optimize program flow). 
-------> This involves ordering events based on the potential causal relationship of pairs of events in a concurrent system, 
-------> especially asynchronous distributed systems. 
-------> It was formulated by Leslie Lamport.
-----> The happened-before relation is formally defined as the least strict partial order on events such that:
---------> If events a and b occur on the same process, a->b if the occurrence of event a preceded the occurrence of event b.
---------> If event a is the sending of a message and event b is the reception of the message sent in event a, a->b
-----> If two events happen in different isolated processes (that do not exchange messages directly or indirectly via third-party processes), 
-------> then the two processes are said to be concurrent, that is neither a->b nor b->a is true.
-----> If there are other causal relationships between events in a given system, 
-------> as between the creation of a process and its first event, these relationships are also added to the definition. 
-------> For example, in some programming languages such as Java, C, C++ or Rust, 
-------> a happens-before edge exists if memory written to by statement A is visible to statement B, 
-------> that is, if statement A completes its write before statement B starts its read.
-----> Like all strict partial orders, the happened-before relation is transitive, irreflexive and antisymmetric, i.e.:
-------> ∀ a,b,c, if a->b and b->c, then a->c (transitivity). 
---------> This means that for any three events a,b,c, if a happened before b, and b happened before c, then a must have happened before c.
-------> ∀ a,a ↛ a (irreflexivity). 
---------> This means that no event can happen before itself.
-------> ∀ a,b, where a ≠ b, if a->b then b ↛ a (antisymmetry). 
---------> This means that for any two distinct events a,b, if a happened before b then b cannot have happened before a.
-------> The processes that make up a distributed system have no knowledge of the happened-before relation 
---------> unless they use a logical clock, like a Lamport clock or a vector clock. 
---------> This allows one to design algorithms for mutual exclusion, and tasks like debugging or optimising distributed systems. 

---> Leader election: 
-----> This a method for dynamically selecting a coordinator
-----> In distributed computing, leader election is the process of designating a single process as the organizer of some task distributed among several computers (nodes). 
-------> Before the task has begun, all network nodes are either unaware which node will serve as the "leader" (or coordinator) of the task, or unable to communicate with the current coordinator. 
-------> After a leader election algorithm has been run, however, each node throughout the network recognizes a particular, unique node as the task leader.
-----> The network nodes communicate among themselves in order to decide which of them will get into the "leader" state. 
-------> For that, they need some method in order to break the symmetry among them. 
-------> For example, if each node has unique and comparable identities, then the nodes can compare their identities, and decide that the node with the highest identity is the leader.
-----> The definition of this problem is often attributed to LeLann, who formalized it as a method to create a new token in a token ring network in which the token has been lost.
-----> Leader election algorithms are designed to be economical in terms of total bytes transmitted, and time. 
-------> The algorithm suggested by Gallager, Humblet, and Spira[1] for general undirected graphs has had a strong impact on the design of distributed algorithms in general, 
-------> and won the Dijkstra Prize for an influential paper in distributed computing.
-----> Many other algorithms have been suggested for different kinds of network graphs,
-------> such as undirected rings, unidirectional rings, complete graphs, grids, directed Euler graphs, and others. 
-------> A general method that decouples the issue of the graph family from the design of the leader election algorithm was suggested by Korach, Kutten, and Moran.

-----> Bully algorithm
-------> In distributed computing, the bully algorithm is a method for dynamically electing a coordinator or leader from a group of distributed computer processes. 
-------> The process with the highest process ID number from amongst the non-failed processes is selected as the coordinator. 

---> Mutual exclusion
-----> In computer science, mutual exclusion is a property of concurrency control, which is instituted for the purpose of preventing race conditions. 
-------> It is the requirement that one thread of execution never enters a critical section while a concurrent thread of execution is already accessing critical section, 
-------> which refers to an interval of time during which a thread of execution accesses a shared resource or shared memory.
-----> The shared resource is a data object, which two or more concurrent threads are trying to modify (where two concurrent read operations are permitted but, 
-------> no two concurrent write operations or one read and one write are permitted, since it leads to data inconsistency). 
-------> Mutual exclusion algorithm ensures that if a process is already performing write operation on a data object no other process/thread is allowed to access/modify the same object 
-------> until the first process has finished writing upon the data object [critical section] and released the object for other processes to read and write upon.
-----> The requirement of mutual exclusion was first identified and solved by Edsger W. Dijkstra in his seminal 1965 paper "Solution of a problem in concurrent programming control",
-------> which is credited as the first topic in the study of concurrent algorithms.

-----> Lamport's Distributed Mutual Exclusion Algorithm
-------> Lamport's Distributed Mutual Exclusion Algorithm is a contention-based algorithm for mutual exclusion on a distributed system. 

-----> Naimi-Trehel's log(n) Algorithm
-------> Naimi-Trehel algorithm is an algorithm for achieving mutual exclusion in a distributed system. 
---------> Unlike Lamport's distributed mutual exclusion algorithm and its related version, this algorithm does not use logical clocks. 
---------> This method requires only O(Log(number of processes in the network)) messages on average. 
---------> When a process invokes a critical section, it sends a request to a queue at a particular processor 
---------> which is specified by a path built by the algorithm as it runs. 

-----> Maekawa's Algorithm
-------> Maekawa's algorithm is an algorithm for mutual exclusion on a distributed system. 
-------> The basis of this algorithm is a quorum like approach where any one site needs only to seek permissions from a subset of other sites. 
-------> Algorithm
---------> A site is any computing device which runs the Maekawa's Algorithm
---------> For any one request of entering the critical section:
-----------> The requesting site is the site which is requesting to enter the critical section.
-----------> The receiving site is every other site which is receiving the request from the requesting site.
---------> ts refers to the local time stamp of the system according to its logical clock.

-----> Raymond's Algorithm
-------> Raymond's Algorithm is a lock based algorithm for mutual exclusion on a distributed system. 
---------> It imposes a logical structure (a K-ary tree) on distributed resources. 
---------> As defined, each node has only a single parent, to which all requests to attain the token are made. 

-----> Ricart–Agrawala Algorithm
-------> The Ricart–Agrawala algorithm is an algorithm for mutual exclusion on a distributed system. 
---------> This algorithm is an extension and optimization of Lamport's Distributed Mutual Exclusion Algorithm, 
---------> by removing the need for a,c,k messages.
---------> It was developed by Glenn Ricart and Ashok Agrawala.

---> Snapshot algorithm: 
-----> This record a consistent global state for an asynchronous system
-----> A snapshot algorithm is used to create a consistent snapshot of the global state of a distributed system.
-------> Due to the lack of globally shared memory and a global clock, this is not trivially possible. 

-----> Chandy–Lamport algorithm
-------> The Chandy–Lamport algorithm is a snapshot algorithm that is used in distributed systems for recording a consistent global state of an asynchronous system. 
-------> It was developed by and named after Leslie Lamport and K. Mani Chandy.

---> Vector clocks:
-----> This generates a partial ordering of events in a distributed system and detect causality violations
-----> A vector clock is a data structure used for determining the partial ordering of events in a distributed system and detecting causality violations. 
-------> Just as in Lamport timestamps, inter-process messages contain the state of the sending process's logical clock. 
-------> A vector clock of a system of N processes is an array/vector of N logical clocks, one clock per process; 
-------> a local "largest possible values" copy of the global clock-array is kept in each process.
-----> Denote VCi as the vector clock maintained by process i, the clock updates proceed as follows:
-------> Example of a system of vector clocks. Events in the blue region are the causes leading to event B4, whereas those in the red region are the effects of event B4.
---------> Initially all clocks are zero.
---------> Each time a process experiences an internal event, it increments its own logical clock in the vector by one. 
-----------> For instance, upon an event at process i, it updates VCi[i] ← VCi[i]+1.
---------> Each time a process sends a message, it increments its own logical clock in the vector by one 
-----------> (as in the bullet above, but not twice for the same event) and then the message piggybacks a copy of its own vector.
---------> Each time a process receives a message, it increments its own logical clock in the vector by one and updates each element in its vector 
-----------> by taking the maximum of the value in its own vector clock and the value in the vector in the received message (for every element). 
-----------> For example, if process Pj receives a message m from Pi, it updates by setting VCj ← max(VCj[k]+1 , VCi[k]), ∀ k.
 


-> Memory allocation and deallocation algorithms

---> Buddy memory allocation: 
-----> This algorithm to allocate memory such that fragmentation is less.
-----> The buddy memory allocation technique is a memory allocation algorithm that divides memory into partitions to try to satisfy a memory request as suitably as possible. 
-------> This system makes use of splitting memory into halves to try to give a best fit. 
-------> According to Donald Knuth, the buddy system was invented in 1963 by Harry Markowitz, and was first described by Kenneth C. Knowlton (published 1965).
-------> The Buddy memory allocation is relatively easy to implement. It supports limited but efficient splitting and coalescing of memory blocks. 

---> Garbage collectors
-----> In computer science, garbage collection (GC) is a form of automatic memory management. 
-------> The garbage collector attempts to reclaim memory which was allocated by the program, but is no longer referenced; such memory is called garbage. 
-------> Garbage collection was invented by American computer scientist John McCarthy around 1959 to simplify manual memory management in Lisp.
-----> Garbage collection relieves the programmer from doing manual memory management, 
-------> where the programmer specifies what objects to de-allocate and return to the memory system and when to do so.
-------> Other, similar, techniques include stack allocation, region inference, and memory ownership, and combinations thereof. 
-------> Garbage collection may take a significant proportion of a program's total processing time, and affect performance as a result.
-----> Resources other than memory, such as network sockets, database handles, windows, file descriptors, and device descriptors, 
-------> are not typically handled by garbage collection, but rather by other methods (e.g. destructors). 
-------> Some such methods de-allocate memory as well. 

-----> Cheney's algorithm: 
-------> This An improvement on the Semi-space collector
-------> Cheney's algorithm, first described in a 1970 ACM paper by C.J. Cheney, 
---------> is a stop and copy method of tracing garbage collection in computer software systems. 
---------> In this scheme, the heap is divided into two equal halves, only one of which is in use at any one time. 
---------> Garbage collection is performed by copying live objects from one semispace (the from-space) 
---------> to the other (the to-space), which then becomes the new heap. 
---------> The entire old heap is then discarded in one piece. 
---------> It is an improvement on the previous stop and copy technique.[citation needed]
-------> Cheney's algorithm reclaims items as follows:
---------> Object references on the stack. 
-----------> Object references on the stack are checked. 
-----------> One of the two following actions is taken for each object reference that points to an object in from-space:
-------------> If the object has not yet been moved to the to-space, this is done by creating an identical copy in the to-space, 
---------------> and then replacing the from-space version with a forwarding pointer to the to-space copy. 
---------------> Then update the object reference to refer to the new version in to-space.
-------------> If the object has already been moved to the to-space, simply update the reference from the forwarding pointer in from-space.
---------> Objects in the to-space. 
-----------> The garbage collector examines all object references in the objects that have been migrated to the to-space, 
-----------> and performs one of the above two actions on the referenced objects.
---------> Once all to-space references have been examined and updated, garbage collection is complete.
---------> The algorithm needs no stack and only two pointers outside of the from-space and to-space: 
-----------> a pointer to the beginning of free space in the to-space, and a pointer to the next word in to-space that needs to be examined. 
-----------> The data between the two pointers represents work remaining for it to do (those objects are gray in the tri-color terminology, see later).
---------> The forwarding pointer (sometimes called a "broken heart") is used only during the garbage collection process; 
-----------> when a reference to an object already in to-space (thus having a forwarding pointer in from-space) is found, 
-----------> the reference can be updated quickly simply by updating its pointer to match the forwarding pointer.
---------> Because the strategy is to exhaust all live references, and then all references in referenced objects, this is known as a breadth-first list copying garbage collection scheme. 

-----> Generational garbage collector: 
-------> This Fast garbage collectors that segregate memory by age
-------> In computer science, garbage collection (GC) is a form of automatic memory management. 
---------> The garbage collector attempts to reclaim memory which was allocated by the program, 
---------> but is no longer referenced; such memory is called garbage. 
---------> Garbage collection was invented by American computer scientist John McCarthy
---------> around 1959 to simplify manual memory management in Lisp.
-------> Garbage collection relieves the programmer from doing manual memory management, 
---------> where the programmer specifies what objects to de-allocate and return to the memory system and when to do so. 
---------> Other, similar, techniques include stack allocation, region inference, and memory ownership, and combinations thereof. 
---------> Garbage collection may take a significant proportion of a program's total processing time, and affect performance as a result.
-------> Resources other than memory, such as network sockets, database handles, windows, file descriptors, and device descriptors, 
---------> are not typically handled by garbage collection, but rather by other methods (e.g. destructors). 
---------> Some such methods de-allocate memory as well. 

-----> Mark-compact algorithm: 
-------> This a combination of the mark-sweep algorithm and Cheney's copying algorithm
-------> In computer science, a mark–compact algorithm is a type of garbage collection algorithm used to reclaim unreachable memory. 
---------> Mark–compact algorithms can be regarded as a combination of the mark–sweep algorithm and Cheney's copying algorithm. 
---------> First, reachable objects are marked, then a compacting step relocates the reachable (marked) objects towards the beginning of the heap area. 
---------> Compacting garbage collection is used by modern JVMs, Microsoft's Common Language Runtime and by the Glasgow Haskell Compiler. 

-----> Mark and sweep
-------> In the naive mark-and-sweep method, each object in memory has a flag (typically a single bit) reserved for garbage collection use only. 
---------> This flag is always cleared, except during the collection cycle.
-------> The first stage is the mark stage which does a tree traversal of the entire 'root set' and marks each object that is pointed to by a root as being 'in-use'. 
---------> All objects that those objects point to, and so on, are marked as well, so that every object that is reachable via the root set is marked.
-------> In the second stage, the sweep stage, all memory is scanned from start to finish, examining all free or used blocks; 
---------> those not marked as being 'in-use' are not reachable by any roots, and their memory is freed. 
---------> For objects which were marked in-use, the in-use flag is cleared, preparing for the next cycle.
-------> This method has several disadvantages, the most notable being that the entire system must be suspended during collection; 
---------> no mutation of the working set can be allowed. 
---------> This can cause programs to 'freeze' periodically (and generally unpredictably), making some real-time and time-critical applications impossible. 
---------> In addition, the entire working memory must be examined, much of it twice, potentially causing problems in paged memory systems. 

-----> Semi-space collector: 
-------> This An early copying collector

---> Reference counting
-----> In computer science, reference counting is a programming technique of storing the number of references, 
-------> pointers, or handles to a resource, such as an object, a block of memory, disk space, and others.
-----> In garbage collection algorithms, reference counts may be used to deallocate objects that are no longer needed. 



-> Networking
---> A network scheduler, also called packet scheduler, queueing discipline (qdisc) or queueing algorithm, 
-----> is an arbiter on a node in a packet switching communication network. 
-----> It manages the sequence of network packets in the transmit and receive queues of the protocol stack and network interface controller. 
-----> There are several network schedulers available for the different operating systems, 
-----> that implement many of the existing network scheduling algorithms.
---> The network scheduler logic decides which network packet to forward next. 
-----> The network scheduler is associated with a queuing system, storing the network packets temporarily until they are transmitted. 
-----> Systems may have a single or multiple queues in which case each may hold the packets of one flow, classification, or priority.
---> In some cases it may not be possible to schedule all transmissions within the constraints of the system. 
-----> In these cases the network scheduler is responsible for deciding which traffic to forward and what gets dropped. 

---> Karn's algorithm: 
-----> This addresses the problem of getting accurate estimates of the round-trip time for messages when using TCP
-----> Karn's algorithm addresses the problem of getting accurate estimates of the round-trip time for messages 
-------> when using the Transmission Control Protocol (TCP) in computer networking. 
-------> The algorithm, also sometimes termed as the Karn-Partridge algorithm was proposed in a paper by Phil Karn and Craig Partridge in 1987.[2]
-----> Accurate round trip estimates in TCP can be difficult to calculate because of an ambiguity created by retransmitted segments. 
-------> The round trip time is estimated as the difference between the time that a segment was sent and the time that its acknowledgment was returned to the sender, 
-------> but when packets are re-transmitted there is an ambiguity: 
-------> the acknowledgment may be a response to the first transmission of the segment or to a subsequent re-transmission.
-----> Karn's Algorithm ignores retransmitted segments when updating the round-trip time estimate. 
-------> Round trip time estimation is based only on unambiguous acknowledgments, which are acknowledgments for segments that were sent only once.
-----> This simplistic implementation of Karn's algorithm can lead to problems as well. 
-------> Consider what happens when TCP sends a segment after a sharp increase in delay. 
-------> Using the prior round-trip time estimate, TCP computes a timeout and retransmits a segment. 
-------> If TCP ignores the round-trip time of all retransmitted packets, the round trip estimate will never be updated, 
-------> and TCP will continue retransmitting every segment, never adjusting to the increased delay.
-----> A solution to this problem is to incorporate transmission timeouts with a timer backoff strategy. 
-------> The timer backoff strategy computes an initial timeout. 
-------> If the timer expires and causes a retransmission, TCP increases the timeout generally by a factor of two. 
-------> This algorithm has proven to be extremely effective in balancing performance and efficiency in networks with high packet loss.
-------> Ideally, Karn's algorithm would not be needed. 
-------> Networks that have high round-trip time and retransmission timeouts should be investigated using root cause analysis techniques. [4] 

---> Luleå algorithm: 
-----> This is a technique for storing and searching internet routing tables efficiently
-----> The Luleå algorithm of computer science, designed by Degermark et al. (1997), 
-------> is a technique for storing and searching internet routing tables efficiently. 
-------> It is named after the Luleå University of Technology, the home institute/university of the technique's authors. 
-------> The name of the algorithm does not appear in the original paper describing it, 
-------> but was used in a message from Craig Partridge to the Internet Engineering Task Force describing that paper prior to its publication.
-----> The key task to be performed in internet routing is to match a given IPv4 address (viewed as a sequence of 32 bits) 
-------> to the longest prefix of the address for which routing information is available. 
-------> This prefix matching problem may be solved by a trie, but trie structures use a significant amount of space (a node for each bit of each address)
------->  and searching them requires traversing a sequence of nodes with length proportional to the number of bits in the address. 
------->  The Luleå algorithm shortcuts this process by storing only the nodes at three levels of the trie structure, rather than storing the entire trie.
-----> Before building the Luleå trie, the routing table entries need to be preprocessed. 
-------> Any bigger prefix that overlaps a smaller prefix must be repeatedly split into smaller prefixes, 
-------> and only the split prefixes which does not overlap the smaller prefix is kept. 
-------> It is also required that the prefix tree is complete. If there is no routing table entries for the entire address space, 
-------> it must be completed by adding dummy entries, which only carries the information that no route is present for that range. 
-------> This enables the simplified lookup in the Luleå trie (Sundström 2007).
-----> The main advantage of the Luleå algorithm for the routing task is that it uses very little memory, 
-------> averaging 4–5 bytes per entry for large routing tables. 
-------> This small memory footprint often allows the entire data structure to fit into the routing processor's cache, speeding operations. 
-------> However, it has the disadvantage that it cannot be modified easily: 
-------> small changes to the routing table may require most or all of the data structure to be reconstructed. 
-------> A modern home-computer (PC) has enough hardware/memory to perform the algorithm. 

---> Network congestion
-----> Network congestion in data networking and queueing theory is the reduced quality of service that occurs 
-------> when a network node or link is carrying more data than it can handle. 
-------> Typical effects include queueing delay, packet loss or the blocking of new connections. 
-------> A consequence of congestion is that an incremental increase in offered load leads 
-------> either only to a small increase or even a decrease in network throughput.
-----> Network protocols that use aggressive retransmissions to compensate for packet loss due to congestion can increase congestion, 
-------> even after the initial load has been reduced to a level that would not normally have induced network congestion. 
-------> Such networks exhibit two stable states under the same level of load. The stable state with low throughput is known as congestive collapse.
-----> Networks use congestion control and congestion avoidance techniques to try to avoid collapse. 
-------> These include: exponential backoff in protocols such as CSMA/CA in 802.11 and the similar CSMA/CD in the original Ethernet, 
-------> window reduction in TCP, and fair queueing in devices such as routers and network switches. 
-------> Other techniques that address congestion include priority schemes which transmit some packets with higher priority ahead of others 
-------> and the explicit allocation of network resources to specific flows through the use of admission control. 

-----> Exponential backoff
-------> Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, 
---------> in order to gradually find an acceptable rate. These algorithms find usage in a wide range of systems and processes, 
---------> with radio networks and computer networks being particularly notable. 

-----> Nagle's algorithm: 
-------> This improve the efficiency of TCP/IP networks by coalescing packets
-------> Nagle's algorithm is a means of improving the efficiency of TCP/IP networks 
---------> by reducing the number of packets that need to be sent over the network. 
---------> It was defined by John Nagle while working for Ford Aerospace. 
---------> It was published in 1984 as a Request for Comments (RFC) with title Congestion Control in IP/TCP Internetworks in RFC 896.
-------> The RFC describes what he called the "small-packet problem", where an application repeatedly emits data in small chunks, frequently only 1 byte in size. 
---------> Since TCP packets have a 40-byte header (20 bytes for TCP, 20 bytes for IPv4), this results in a 41-byte packet for 1 byte of useful information, a huge overhead. 
---------> This situation often occurs in Telnet sessions, where most keypresses generate a single byte of data that is transmitted immediately. 
---------> Worse, over slow links, many such packets can be in transit at the same time, potentially leading to congestion collapse.
-------> Nagle's algorithm works by combining a number of small outgoing messages and sending them all at once. 
---------> Specifically, as long as there is a sent packet for which the sender has received no acknowledgment, 
---------> the sender should keep buffering its output until it has a full packet's worth of output, thus allowing output to be sent all at once. 

-----> Truncated binary exponential backoff
-------> Exponential backoff is an algorithm that uses feedback to multiplicatively decrease the rate of some process, in order to gradually find an acceptable rate. 
-------> These algorithms find usage in a wide range of systems and processes, with radio networks and computer networks being particularly notable. 



-> Operating systems algorithms

---> Banker's algorithm: 
-----> This Algorithm used for deadlock avoidance.
-----> The Banker algorithm, sometimes referred to as the detection algorithm, 
-------> is a resource allocation and deadlock avoidance algorithm developed by Edsger Dijkstra that tests 
-------> for safety by simulating the allocation of predetermined maximum possible amounts of all resources, 
-------> and then makes an "s-state" check to test for possible deadlock conditions for all other pending activities, 
-------> before deciding whether allocation should be allowed to continue.
-----> The algorithm was developed in the design process for the THE operating system 
-------> and originally described (in Dutch) in EWD108.[1] When a new process enters a system, 
-------> it must declare the maximum number of instances of each resource type that it may ever claim; clearly, 
-------> that number may not exceed the total number of resources in the system. 
-------> Also, when a process gets all its requested resources it must return them in a finite amount of time. 

---> Page replacement algorithms: 
-----> This Selecting the victim page under low memory conditions.
-----> In a computer operating system that uses paging for virtual memory management, 
-------> page replacement algorithms decide which memory pages to page out, sometimes called swap out, 
-------> or write to disk, when a page of memory needs to be allocated. 
-------> Page replacement happens when a requested page is not in memory (page fault) 
-------> and a free page cannot be used to satisfy the allocation, 
-------> either because there are none, or because the number of free pages is lower than some threshold.
-----> When the page that was selected for replacement and paged out is referenced again it has to be paged in (read in from disk), 
-------> and this involves waiting for I/O completion. 
-------> This determines the quality of the page replacement algorithm: the less time waiting for page-ins, the better the algorithm. 
-------> A page replacement algorithm looks at the limited information about accesses to the pages provided by hardware, 
-------> and tries to guess which pages should be replaced to minimize the total number of page misses, 
-------> while balancing this with the costs (primary storage and processor time) of the algorithm itself.
-----> The page replacing problem is a typical online problem from the competitive analysis perspective in the sense that the optimal deterministic algorithm is known. 

-----> Adaptive replacement cache: 
-------> This better performance than LRU.
-------> Adaptive Replacement Cache (ARC) is a page replacement algorithm with better performance[1] than LRU (least recently used). 
-------> This is accomplished by keeping track of both frequently used and recently used pages plus a recent eviction history for both. 
-------> The algorithm was developed[2] at the IBM Almaden Research Center. 
-------> In 2006, IBM was granted a patent for the adaptive replacement cache policy. 

-----> Clock with Adaptive Replacement (CAR): 
-------> This is a page replacement algorithm that has performance comparable to Adaptive replacement cache
-------> Clock is a more efficient version of FIFO than Second-chance because pages don't have to be constantly pushed to the back of the list, 
---------> but it performs the same general function as Second-Chance. 
---------> The clock algorithm keeps a circular list of pages in memory, with the "hand" (iterator) pointing to the last examined page frame in the list. 
---------> When a page fault occurs and no empty frames exist, then the R (referenced) bit is inspected at the hand's location.
---------> If R is 0, the new page is put in place of the page the "hand" points to, and the hand is advanced one position. 
---------> Otherwise, the R bit is cleared, then the clock hand is incremented and the process is repeated until a page is replaced.
---------> This algorithm was first described in 1969 by F. J. Corbató.
-------> Variants of clock
---------> (1) GCLOCK: Generalized clock page replacement algorithm.[9]
---------> (2) Clock-Pro keeps a circular list of information about recently referenced pages, 
-----------> including all M pages in memory as well as the most recent M pages that have been paged out. 
-----------> This extra information on paged-out pages, like the similar information maintained by ARC, helps it work better than LRU on large loops and one-time scans.[10]
---------> (3) WSclock.[11] By combining the Clock algorithm with the concept of a working set (i.e., the set of pages expected to be used by that process during some time interval), 
-----------> the performance of the algorithm can be improved. 
-----------> In practice, the "aging" algorithm and the "WSClock" algorithm are probably the most important page replacement algorithms.
---------> (4) Clock with Adaptive Replacement (CAR) is a page replacement algorithm that has performance comparable to ARC, 
-----------> and substantially outperforms both LRU and CLOCK.[14] 
-----------> The algorithm CAR is self-tuning and requires no user-specified magic parameters.
-------> CLOCK is a conservative algorithm, so it is k/(k−h+1) competitive. 



-> Process synchronization

---> Dekker's algorithm
-----> Dekker's algorithm is the first known correct solution to the mutual exclusion problem 
-------> in concurrent programming where processes only communicate via shared memory. 
-------> The solution is attributed to Dutch mathematician Th. J. Dekker by Edsger W. Dijkstra 
-------> in an unpublished paper on sequential process descriptions and his manuscript on cooperating sequential processes.
-------> It allows two threads to share a single-use resource without conflict, using only shared memory for communication.
-----> It avoids the strict alternation of a naïve turn-taking algorithm, and was one of the first mutual exclusion algorithms to be invented. 
-----> Algorithm
-------> If two processes attempt to enter a critical section at the same time, the algorithm will allow only one process in, based on whose turn it is. 
-------> If one process is already in the critical section, the other process will busy wait for the first process to exit.
-------> This is done by the use of two flags, wants_to_enter[0] and wants_to_enter, 
-------> which indicate an intention to enter the critical section on the part of processes 0 and 1, respectively, 
-------> and a variable turn that indicates who has priority between the two processes.
-------> Dekker's algorithm can be expressed in pseudocode, as follows:
---------> variables
--------->     wants_to_enter : array of 2 booleans
--------->     turn : integer
---------> wants_to_enter[0] ← false
---------> wants_to_enter[1] ← false
---------> turn ← 0   // or 1
---------> p0:
--------->    wants_to_enter[0] ← true
--------->    while wants_to_enter[1] {
--------->       if turn ≠ 0 {
--------->          wants_to_enter[0] ← false
--------->          while turn ≠ 0 {
--------->            // busy wait
--------->          }
--------->          wants_to_enter[0] ← true
--------->       }
--------->    }
--------->    // critical section
--------->    ...
--------->    turn ← 1
--------->    wants_to_enter[0] ← false
--------->    // remainder section
---------> p1:
--------->    wants_to_enter[1] ← true
--------->    while wants_to_enter[0] {
--------->       if turn ≠ 1 {
--------->          wants_to_enter[1] ← false
--------->          while turn ≠ 1 {
--------->            // busy wait
--------->          }
--------->          wants_to_enter[1] ← true
--------->       }
--------->    }
--------->    // critical section
--------->    ...
--------->    turn ← 0
--------->    wants_to_enter[1] ← false
--------->    // remainder section


---> Lamport's Bakery algorithm
-----> Lamport's bakery algorithm is a computer algorithm devised by computer scientist Leslie Lamport, 
-------> as part of his long study of the formal correctness of concurrent systems, 
-------> which is intended to improve the safety in the usage of shared resources among multiple threads by means of mutual exclusion.
-----> In computer science, it is common for multiple threads to simultaneously access the same resources. 
-------> Data corruption can occur if two or more threads try to write into the same memory location, 
-------> or if one thread reads a memory location before another has finished writing into it. 
-------> Lamport's bakery algorithm is one of many mutual exclusion algorithms designed to prevent concurrent threads 
-------> entering critical sections of code concurrently to eliminate the risk of data corruption. 
-----> Algorithm
-------> Analogy
---------> Lamport envisioned a bakery with a numbering machine at its entrance so each customer is given a unique number. 
-----------> Numbers increase by one as customers enter the store. 
-----------> A global counter displays the number of the customer that is currently being served. 
-----------> All other customers must wait in a queue until the baker finishes serving the current customer and the next number is displayed. 
-----------> When the customer is done shopping and has disposed of his or her number, the clerk increments the number, allowing the next customer to be served. 
-----------> That customer must draw another number from the numbering machine in order to shop again.
---------> According to the analogy, the "customers" are threads, identified by the letter i, obtained from a global variable.
---------> Due to the limitations of computer architecture, some parts of Lamport's analogy need slight modification. 
-----------> It is possible that more than one thread will get the same number n when they request it; 
-----------> this cannot be avoided (without first solving the mutual exclusion problem, which is the goal of the algorithm). 
-----------> Therefore, it is assumed that the thread identifier i is also a priority. 
-----------> A lower value of i means a higher priority and threads with higher priority will enter the critical section first.
-------> Critical section
---------> The critical section is that part of code that requires exclusive access to resources and may only be executed by one thread at a time. 
-----------> In the bakery analogy, it is when the customer trades with the baker that others must wait.
---------> When a thread wants to enter the critical section, it has to check whether now is its turn to do so. 
-----------> It should check the number n of every other thread to make sure that it has the smallest one. In case another thread has the same number, 
-----------> the thread with the smallest i will enter the critical section first.
---------> In pseudocode this comparison between threads a and b can be written in the form:
-----------> // Let na - the customer number for thread a, and
-----------> // ia - the thread number for thread a, then
-----------> (na, ia) < (nb, ib) 
-----------> which is equivalent to:
-----------> (na < nb) or ((na == nb) and (ia < ib))
---------> Once the thread ends its critical job, it gets rid of its number and enters the non-critical section.
-------> Non-critical section
---------> The non-critical section is the part of code that doesn't need exclusive access. 
---------> It represents some thread-specific computation that doesn't interfere with other threads' resources and execution.
---------> This part is analogous to actions that occur after shopping, such as putting change back into the wallet. 

---> Peterson's algorithm
-----> Peterson's algorithm (or Peterson's solution) is a concurrent programming algorithm for mutual exclusion that allows two or more processes 
-------> to share a single-use resource without conflict, using only shared memory for communication. 
-------> It was formulated by Gary L. Peterson in 1981.
-------> While Peterson's original formulation worked with only two processes, the algorithm can be generalized for more than two.
-----> Algorithm
-------> The algorithm uses two variables: flag and turn.
---------> A flag[n] value of true indicates that the process n wants to enter the critical section. 
-------> Entrance to the critical section is granted for process P0 if P1 does not want to enter its critical section and if P1 has given priority to P0 by setting turn to 0.
---------> bool flag[2] = {false, false};
---------> int turn;
---------> P0:      flag[0] = true;
---------> P0_gate: turn = 1;
--------->          while (flag[1] == true && turn == 1)
--------->          {
--------->              // busy wait
--------->          }
--------->          // critical section
--------->          ...
--------->          // end of critical section
--------->          flag[0] = false;
---------> P1:      flag[1] = true;
---------> P1_gate: turn = 0;
--------->          while (flag[0] == true && turn == 0)
--------->          {
--------->              // busy wait
--------->          }
--------->          // critical section
--------->          ...
--------->          // end of critical section
--------->          flag[1] = false;
-------> The algorithm satisfies the three essential criteria to solve the critical-section problem. 
---------> The while condition works even with preemption.
---------> The three criteria are mutual exclusion, progress, and bounded waiting. 
---------> Since turn can take on one of two values, it can be replaced by a single bit, 
-----------> meaning that the algorithm requires only three bits of memory.



-> Scheduling

---> Earliest deadline first scheduling
-----> Earliest deadline first (EDF) or least time to go is a dynamic priority scheduling algorithm 
-------> used in real-time operating systems to place processes in a priority queue. 
-------> Whenever a scheduling event occurs (task finishes, new task released, etc.) 
-------> the queue will be searched for the process closest to its deadline. 
-------> This process is the next to be scheduled for execution.
-----> EDF is an optimal scheduling algorithm on preemptive uniprocessors, in the following sense: 
-------> if a collection of independent jobs, each characterized by an arrival time, an execution requirement and a deadline, 
-------> can be scheduled (by any algorithm) in a way that ensures all the jobs complete by their deadline, 
-------> the EDF will schedule this collection of jobs so they all complete by their deadline. 

---> Fair-share scheduling
-----> Fair-share scheduling is a scheduling algorithm for computer operating systems in 
-------> which the CPU usage is equally distributed among system users or groups, 
-------> as opposed to equal distribution of resources among processes.
-----> One common method of logically implementing the fair-share scheduling strategy 
-------> is to recursively apply the round-robin scheduling strategy at each level of abstraction (processes, users, groups, etc.) 
-------> The time quantum required by round-robin is arbitrary, as any equal division of time will produce the same results.
-----> This was first developed by Judy Kay and Piers Lauder through their research at Sydney University in the 1980s.
-----> For example, if four users (A,B,C,D) are concurrently executing one process each, 
-------> the scheduler will logically divide the available CPU cycles such that each user gets 25% of the whole (100% / 4 = 25%). 
-------> If user B starts a second process, each user will still receive 25% of the total cycles, 
-------> but each of user B's processes will now be attributed 12.5% of the total CPU cycles each, 
-------> totalling user B's fair share of 25%. On the other hand, if a new user starts a process on the system, 
-------> the scheduler will reapportion the available CPU cycles such that each user gets 20% of the whole (100% / 5 = 20%).
-----> Another layer of abstraction allows us to partition users into groups, and apply the fair share algorithm to the groups as well. 
-------> In this case, the available CPU cycles are divided first among the groups, 
-------> then among the users within the groups, and then among the processes for that user. 
-------> For example, if there are three groups (1,2,3) containing three, two, and four users respectively, 
-------> the available CPU cycles will be distributed as follows:
---------> 100% / 3 groups = 33.3% per group Group 1: (33.3% / 3 users) = 11.1% per user Group 2: (33.3% / 2 users) = 16.7% per user Group 3: (33.3% / 4 users) = 8.3% per user 

---> Least slack time scheduling
-----> Least slack time (LST) scheduling is an algorithm for dynamic priority scheduling. 
-------> It assigns priorities to processes based on their slack time. 
-------> Slack time is the amount of time left after a job if the job was started now. 
-------> This algorithm is also known as least laxity first. 
-------> Its most common use is in embedded systems, especially those with multiple processors. 
-------> It imposes the simple constraint that each process on each available processor possesses the same run time, 
-------> and that individual processes do not have an affinity to a certain processor. 
-------> This is what lends it a suitability to embedded systems. 

---> List scheduling
-----> List scheduling is a greedy algorithm for Identical-machines scheduling. 
-------> The input to this algorithm is a list of jobs that should be executed on a set of m machines. 
-------> The list is ordered in a fixed order, which can be determined e.g. by the priority of executing the jobs, or by their order of arrival. 
-------> The algorithm repeatedly executes the following steps until a valid schedule is obtained:
---------> Take the first job in the list (the one with the highest priority).
---------> Find a machine that is available for executing this job.
-----------> If a machine is found, schedule this job on that machine.
-----------> Otherwise (no suitable machine is available), select the next job in the list.

---> Multi level feedback queue
-----> In computer science, a multilevel feedback queue is a scheduling algorithm. 
-------> Scheduling algorithms are designed to have some process running at all times to keep the central processing unit (CPU) busy.
-------> The multilevel feedback queue extends standard algorithms with the following design requirements:
---------> (1) Separate processes into multiple ready queues based on their need for the processor.
---------> (2) Give preference to processes with short CPU bursts.
---------> (3) Give preference to processes with high I/O bursts. (I/O bound processes will sleep in the wait queue to give other processes CPU time.)
-----> The multilevel feedback queue was first developed by Fernando J. Corbató (1962). 
-------> For this accomplishment, the Association for Computing Machinery awarded Corbató the Turing Award.[2] 

---> Rate-monotonic scheduling
-----> In computer science, rate-monotonic scheduling (RMS) is a priority assignment algorithm.
-------> used in real-time operating systems (RTOS) with a static-priority scheduling class.
-------> The static priorities are assigned according to the cycle duration of the job, 
-------> so a shorter cycle duration results in a higher job priority.
-----> These operating systems are generally preemptive 
-------> and have deterministic guarantees with regard to response times. 
-------> Rate monotonic analysis is used in conjunction with those systems 
-------> to provide scheduling guarantees for a particular application. 

---> Round-robin scheduling
-----> Round-robin (RR) is one of the algorithms employed by process and network schedulers in computing.
-------> As the term is generally used, time slices (also known as time quanta) are assigned to each process in equal portions and in circular order, 
-------> handling all processes without priority (also known as cyclic executive). 
-------> Round-robin scheduling is simple, easy to implement, and starvation-free. 
-------> Round-robin scheduling can be applied to other scheduling problems, 
-------> such as data packet scheduling in computer networks. 
-------> It is an operating system concept.
-----> The name of the algorithm comes from the round-robin principle known from other fields, where each person takes an equal share of something in turn. 

---> Shortest job next
-----> Shortest job next (SJN), also known as shortest job first (SJF) or shortest process next (SPN), 
-------> is a scheduling policy that selects for execution the waiting process with the smallest execution time.
-------> SJN is a non-preemptive algorithm. 
-------> Shortest remaining time is a preemptive variant of SJN.
-----> Shortest job next is advantageous because of its simplicity 
-------> and because it minimizes the average amount of time each process has to wait until its execution is complete. 
-------> However, it has the potential for process starvation for processes which will require a long time to complete if short processes are continually added. 
-------> Highest response ratio next is similar but provides a solution to this problem using a technique called aging.
-----> Another disadvantage of using shortest job next is that the total execution time of a job must be known before execution. 
-------> While it is impossible to predict execution time perfectly, several methods can be used to estimate it, 
-------> such as a weighted average of previous execution times.
-------> Multilevel feedback queue can also be used to approximate SJN without the need for the total execution time oracle.
-----> Shortest job next can be effectively used with interactive processes 
-------> which generally follow a pattern of alternating between waiting for a command and executing it. 
-------> If the execution burst of a process is regarded as a separate "job", 
-------> the past behaviour can indicate which process to run next, based on an estimate of its running time.
-----> Shortest job next is used in specialized environments where accurate estimates of running time are available. 

---> Shortest remaining time
-----> Shortest remaining time, also known as shortest remaining time first (SRTF), 
-------> is a scheduling method that is a preemptive version of shortest job next scheduling. 
-----> In this scheduling algorithm, the process with the smallest amount of time remaining until completion is selected to execute. 
-------> Since the currently executing process is the one with the shortest amount of time remaining by definition, 
-------> and since that time should only reduce as execution progresses, 
-------> the process will either run until it completes or get preempted if a new process is added that requires a smaller amount of time.
-----> Shortest remaining time is advantageous because short processes are handled very quickly. 
-------> The system also requires very little overhead since it only makes a decision when a process completes or a new process is added, 
-------> and when a new process is added the algorithm only needs to compare the currently executing process with the new process, 
-------> ignoring all other processes currently waiting to execute.
-----> Like shortest job next, it has the potential for process starvation: 
-------> long processes may be held off indefinitely if short processes are continually added.
-------> This threat can be minimal when process times follow a heavy-tailed distribution.
-------> A similar algorithm which avoids starvation at the cost of higher tracking overhead is highest response ratio next (HRRN). 

---> Top-nodes algorithm: 
-----> This is resource calendar management
-----> The top-nodes algorithm is an algorithm for managing a resource reservation calendar. 
-------> The algorithm has been first published in 2003, and has been improved in 2009.
-------> It is used when a resource is shared among many users (for example bandwidth in a telecommunication link, or disk capacity in a large data center).
-----> The algorithm allows users to:
-------> check if an amount of resource is available during a specific period of time,
-------> reserve an amount of resource for a specific period of time,
-------> delete a previous reservation,
-------> move the calendar forward (the calendar covers a defined duration, and it must be moved forward as time goes by).

-> I/O scheduling
---> Input/output (I/O) scheduling is the method that computer operating systems 
-----> use to decide in which order I/O operations will be submitted to storage volumes. 
-----> I/O scheduling is sometimes called disk scheduling. 

---> Elevator algorithm: 
-----> This Disk scheduling algorithm that works like an elevator.
-----> The elevator algorithm (also SCAN) is a disk-scheduling algorithm 
-------> to determine the motion of the disk's arm and head in servicing read and write requests.
-----> This algorithm is named after the behavior of a building elevator, 
-------> where the elevator continues to travel in its current direction (up or down) until empty, 
-------> stopping only to let individuals off or to pick up new individuals heading in the same direction.
-----> From an implementation perspective, the drive maintains a buffer of pending read/write requests, 
-------> along with the associated cylinder number of the request, 
-------> in which lower cylinder numbers generally indicate that the cylinder is closer to the spindle, 
-------> and higher numbers indicate the cylinder is farther away. 
 
---> Shortest seek first: 
-----> This disk scheduling algorithm aims to reduce seek time.
-----> Shortest seek first (or shortest seek time first) is a secondary storage scheduling algorithm 
-------> to determine the motion of the disk read-and-write head in servicing read and write requests. 
-----> This is a direct improvement upon a first-come first-served (FCFS) algorithm. 
-------> The drive maintains an incoming buffer of requests, and tied with each request is a cylinder number of the request. 
-------> Lower cylinder numbers indicate that the cylinder is closer to the spindle, while higher numbers indicate the cylinder is farther away. 
-------> The shortest seek first algorithm determines which request is closest to the current position of the head, and then services that request next. 

