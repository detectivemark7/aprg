Taken from Titus Winters and Hyrum Wright's talk on "All your tests are terrible."

-> Five properties of Good Tests
---> Correctness
-----> Testing that your API is actually behaving correctly
---> Readability
-----> A good test should be correct by inspection
---> Completeness
-----> Thorough/complete tests on your API and only your API
---> Demonstrability
-----> Tests show how your API can be used and it should be without "shortcuts"
-----> Alternative to documentation
---> Resilience
-----> Tests should only fail if the thing your trying to demonstrate becomes false, and should not fail for other reasons

-> Step 0: Write Tests!
---> First, have tests
---> This is non negotiable

-> Correctness
---> Tests must verify the requirements of the system are met.
---> Tests must verify if known bugs are corrected
---> Tests must execute actual real scenarios
-----> Avoid testing the mock implementation
-----> Avoid testing the library implementation (as is)

-> Readability
---> Tests should be obvious to the future reader (including yourself)
-----> Avoid too much boilerplate and other distraction
-------> Too much boilerplate setup
-------> Too much noise and very hard for the reader of the test to understand
-------> Extract code that are not relevant steps of the tests
-----> Avoid not having enough context in the test
-------> If you extract too much, you might actually lose the important steps of the test.
-------> Keep enough context for the reader
-------> A good test should be correct by inspection
-----> Gratuitous use of advanced test framework features
-------> Don't use advanced test framework features when it isn't necessary
-------> A framework feature should not obfuscate your code.
---> A test should be like a novel:
----->  There's a "setup", "action", and "conclusion". And it should all make sense.

-> Completeness
---> Avoid writing test only for the easy cases
---> Test for common inputs, corner cases and outlandish cases
---> Think of the edge cases of your function
-----> Where the discontinuity of your inputs may lie
-----> Where the inputs would yield interesting results
-----> Edge cases of your users which likely to encounter
---> As programmers, we really want our code to work and its really antithetical to our hopes/dreams to write test to prove that it doesn't work
-----> Write tests first, so you dont have to be antagonistic about your tests
---> Write tests for APIs that you are responsible for
-----> Avoid testing the library implementation (as is)
-----> If you're testing other stuffs (because you dont trust it), where do you draw the line?
-------> Are testing the STL? allocators? compilers? OS? even your processor???
-------> Where do you draw the line, if you not drawing the line on what you implemented?
-------> Its perfectly reasonable to want to test other stuffs, but putting it with your tests is the wrong place to do that.
-----> Only test that your API behaves properly while it uses that API

-> Demonstrability
---> Tests should serve as a demonstration of how the API works
-----> Avoid reliance on private methods with friends/test-only methods
-----> Avoid bad usage in unit tests, suggesting a bad API
-----> Avoid special initialization for tests, that cannot be used in real scenarios
-------> You are not testing the API on how your users/consumers will use that API 

-> Resilience
---> Avoid writing tests that fail in all sorts of surprising ways
-----> (1) Flaky tests
-------> These are tests that when you re-run them get you different results.
-------> These are tests that can be re-run with the same build in the same state and flip from passing to failing (or timing out)
-------> Example: Half a second should be plenty to cover this async operation
---------> As soon as your operating system starts trashing and the scheduler goes out -> problems
---------> Fix? Create a synchronization mechanism or identify a reasonable timeout
-------> Example: Tests that highly depend on your testing environment
---------> Flaky tests can be actually a symptom of problems on your testing approach
-----> (2) Brittle tests
-------> Brittle tests are like balanced stacked spheres on top of each other, if one of them has a minor change, the whole tower goes down.
-------> Brittle tests depend upon implementation details that you have any control over (or any business depending on).
-------> These are tests that can fail for changes unrelated to the code under tests.
-------> Example: In hash functions, a minor change in the algorithm (like optimizations) can change the expected output hash values.
-------> Example: Don't rely on the ordering of an unordered container. 
---------> If the implementation changes, your tests will fail.
-------> Example: A bad code smell is when your capturing the logs in your tests.
-------> Example: A image compression library that have tests that depend on the output of the image compression.
---------> If the algorithm is changed, a lot of tests will fail.
---------> This is done by running the code twice and capturing the output of the code and putting it as expectation.
-----------> I am not a fan. Dont run the code/test twice (copy the output and set it as expectation)
---------> A better test is to convert the image into a bitmap and check the bits of the bitmap if its a reasonable output.
-----> (3) Tests that depend on execution ordering
-------> These are tests that fail if they aren't run all together or in a particular order.
---------> Example: Static values on tests. If they aren't run in a specific order, will all the test still pass?
---------> Example: Changing the system clock.
---------> Example: Changing the files on the filesystem.
-------> There are tons of ways to modify global state in your tests.
-----> (4) Non-hermetic tests
-------> The Hermetic test pattern is the polar opposite of the Spaghetti pattern; it states that each test should be completely independent and self-sufficient.
---------> Any dependency on other tests or third-party services that cannot be controlled should be avoided at all costs.
-------> These are tests that fail if anyone else in the company runs the same test at the same time.
---------> Dont call APIs to production services.
---------> Try to avoid actually doing file I/O to disk.
---------> Try to avoid touching the network in anyway.
---------> The more you have one and zeros escaping your process, the less likely it is the your tests will survive the future.
-----> (5) Mocks with deep dependence upon underlying APIs.
-------> These are tests that fail if anyone refactors those classes.
---------> Mocks typically heavily depend on the signatures of the mock functions
-------> The law of implicit interfaces ("Hyrum's law", https://www.hyrumslaw.com/)
---------> ME: This looks like something similar in probabilities. In an experiment with lots of trials (large numbers), rare events becomes common.
---------> If you have enough users, you're gonna start seeing this.
---------> The law states that: "Given an implementation and explicit interface (published interface), sooner of later they will start depending upon the implicit guarantees of the implementation."
-----------> They start to depend be speed, memory consumption, filename and line number, or API call ordering.
-----------> Pretty soon you will have enough people using your system that the implementation becomes the implicit interface.
-------------> There is nothing you can change without breaking somebody.
-------------> Somebody will scream if you change any part of the implementation, this is bad world to be in, unless you've gone deaf to the screams.

-> Recap
---> (0) Write tests
---> (1) Write correct tests: tests what you wanted to test
---> (2) Write readable tests: correct by inspection
---> (3) Write complete tests: test all the edge cases
---> (4) Write demonstrative tests: show how to use the API
---> (5) Write resilient tests: hermetic, only breaks when there is an unacceptable behavior change.



-> Hyrum's law (https://www.hyrumslaw.com/)
---> An interface should theoretically provide a clear separation between consumers of a system and its implementers
---> In practice, this theory breaks down as the use of a system grows and its users starts to rely upon implementation details intentionally exposed through the interface.
---> Spolsky’s “Law of Leaky Abstractions” embodies consumers’ reliance upon internal implementation details.
-----> As coined by Joel Spolsky, the Law of Leaky Abstractions states: "All non-trivial abstractions, to some degree, are leaky."
-------> This statement highlights a particularly problematic cause of software defects: the reliance of the software developer on an abstraction's infallibility. 
---> Taken to its logical extreme, this leads to the following observation, colloquially referred to as "The Law of Implicit Interfaces".
---> The law states that: "Given enough use, there is no such thing as a private implementation." 
-----> That is, if an interface has enough consumers, they will collectively depend on every aspect of the implementation, intentionally or not. 
-----> This effect serves to constrain changes to the implementation, which must now conform to both the explicitly documented interface, as well as the implicit interface captured by usage. 
-----> We often refer to this phenomenon as "bug-for-bug compatibility."
---> The creation of the implicit interface usually happens gradually, and interface consumers generally aren’t aware as it’s happening. 
-----> For example, an interface may make no guarantees about performance, yet consumers often come to expect a certain level of performance from its implementation. 
-----> Those expectations become part of the implicit interface to a system, and changes to the system must maintain these performance characteristics to continue functioning for its consumers.
---> Not all consumers depend upon the same implicit interface, but given enough consumers, the implicit interface will eventually exactly match the implementation. 
-----> At this point, the interface has evaporated: the implementation has become the interface, and any changes to it will violate consumer expectations. 
-----> With a bit of luck, widespread, comprehensive, and automated testing can detect these new expectations but not ameliorate them.
---> Implicit interfaces result from the organic growth of large systems, designers and engineers would be wise to consider it when building and maintaining complex systems. 
-----> So be aware of how the implicit interface constrains your system design and evolution, and know that for any reasonably popular system, the interface reaches much deeper than you think.









From CleanCode:

---> F.I.R.S.T.
-----> Clean tests follow five other rules that form the above acronym:
-------> Fast
---------> Fast Tests should be fast. 
---------> They should run quickly. 
---------> When tests run slow, you won’t want to run them frequently. 
---------> If you don’t run them frequently, you won’t find problems early enough to fix them easily. 
---------> You won’t feel as free to clean up the code. 
---------> Eventually the code will begin to rot.
-------> Independent
---------> Independent Tests should not depend on each other. 
---------> One test should not set up the conditions for the next test. 
---------> You should be able to run each test independently and run the tests in any order you like. 
---------> When tests depend on each other, then the first one to fail causes a cascade of downstream failures, making diagnosis difficult and hiding downstream defects.
-------> Repeatable
---------> Repeatable Tests should be repeatable in any environment. 
---------> You should be able to run the tests in the production environment, in the QA environment, and on your laptop while riding home on the train without a network. 
---------> If your tests aren’t repeatable in any environment, then you’ll always have an excuse for why they fail. 
---------> You’ll also find yourself unable to run the tests when the environment isn’t available.
-------> Self-Validating
---------> The tests should have a boolean output. 
---------> Either they pass or fail. 
---------> You should not have to read through a log file to tell whether the tests pass. 
---------> You should not have to manually compare two different text files to see whether the tests pass. 
---------> If the tests aren’t self-validating, then failure can become subjective and running the tests can require a long manual evaluation.
-------> Timely 
---------> The tests need to be written in a timely fashion. 
---------> Unit tests should be written just before the production code that makes them pass. 
---------> If you write tests after the production code, then you may find the production code to be hard to test. 
---------> You may decide that some production code is too hard to test. 
---------> You may not design the production code to be testable.

From CleanCode(SmellsAnd Heuristics):

---> Tests

-----> T1: Insufficient Tests
-------> How many tests should be in a test suite? 
---------> Unfortunately, the metric many programmers use is “That seems like enough.” 
---------> A test suite should test everything that could possibly break.
---------> The tests are insufficient so long as there are conditions 
---------> that have not been explored by the tests or calculations that have not been validated.

-----> T2: Use a Coverage Tool!
-------> Coverage tools reports gaps in your testing strategy. 
---------> They make it easy to find modules, classes, and functions that are insufficiently tested. 
---------> Most IDEs give you a visual indication, marking lines that are covered in green and those that are uncovered in red. 
---------> This makes it quick and easy to find if or catch statements whose bodies haven’t been checked.

-----> T3: Don’t Skip Trivial Tests
-------> They are easy to write and their documentary value is higher than the cost to produce them.

-----> T4: An Ignored Test Is a Question about an Ambiguity
-------> Sometimes we are uncertain about a behavioral detail because the requirements are unclear. 
---------> We can express our question about the requirements as a test that is commented out, or as a test that annotated with @Ignore. 
---------> Which you choose depends upon whether the ambiguity is about something that would compile or not.

-----> T5: Test Boundary Conditions
-------> Take special care to test boundary conditions. 
---------> We often get the middle of an algorithm right but misjudge the boundaries.

-----> T6: Exhaustively Test Near Bugs
-------> Bugs tend to congregate. 
---------> When you find a bug in a function, it is wise to do an exhaustive test of that function. 
---------> You’ll probably find that the bug was not alone.

-----> T7: Patterns of Failure Are Revealing
-------> Sometimes you can diagnose a problem by finding patterns in the way the test cases fail.
---------> This is another argument for making the test cases as complete as possible. 
---------> Complete test cases, ordered in a reasonable way, expose patterns.
-------> As a simple example, suppose you noticed that all tests with an input larger than five characters failed? 
---------> Or what if any test that passed a negative number into the second argument of a function failed? 
---------> Sometimes just seeing the pattern of red and green on the test report is enough to spark the “Aha!” that leads to the solution. 
---------> Look back at page 267 to see an interesting example of this in the SerialDate example.

-----> T8: Test Coverage Patterns Can Be Revealing
-------> Looking at the code that is or is not executed by the passing tests gives clues to why the failing tests fail.

-----> T9: Tests Should Be Fast
-------> A slow test is a test that won’t get run. 
---------> When things get tight, it’s the slow tests that will be dropped from the suite. 
---------> So do what you must to keep your tests fast.


