Source: https://missing.csail.mit.edu/2019/




-> Motivation
---> This class is about hacker tools, not hacker tools.
---> MIT classes do not cover any of this content in detail. 
-----> It’s hugely beneficial to be proficient with your tools: it’ll save you a lot of time (and the payoff time is very short).
---> We want to teach you about new tools, how to make the most of your tools, how to customize your tools, and how to extend your tools.

-> Class structure
---> We have 6 lectures covering a variety of topics. 
-----> We have lecture notes online, but there will be a lot of content covered in class (e.g. in the form of demos) that may not be in the notes. 
-----> We will be recording lectures.
---> Each class is split into two 50-minute lectures with a 10-minute break in between. 
-----> Lectures are mostly live demonstrations followed by hands-on exercises. 
-----> We might have a short amount of time at the end of each class to get started on the exercises in an office-hours-style setting.
---> To make the most of the class, you should go through all the exercises on your own. 
-----> We’ll inspire you to learn more about your tools, and we’ll show you what’s possible and cover some of the basics in detail, but we can’t teach you everything in the time we have.

-> Virtual Machines
---> Virtual machines are simulated computers. 
-----> You can configure a guest virtual machine with some operating system and configuration and use it without affecting your host environment.
-----> For this class, you can use VMs to experiment with operating systems, software, and configurations without risk: you won’t affect your primary development environment.
---> In general, VMs have lots of uses. 
-----> They are commonly used for running software that only runs on a certain operating system (e.g. using a Windows VM on Linux to run Windows-specific software). 
-----> They are often used for experimenting with potentially malicious software.

---> Useful features
-----> Isolation: hypervisors do a pretty good job of isolating the guest from the host, so you can use VMs to run buggy or untrusted software reasonably safely.
-----> Snapshots: you can take “snapshots” of your virtual machine, capturing the entire machine state (disk, memory, etc.), make changes to your machine, and then restore to an earlier state. 
-------> This is useful for testing out potentially destructive actions, among other things.

---> Disadvantages
-----> Virtual machines are generally slower than running on bare metal, so they may be unsuitable for certain applications.
-----> Setup
-------> Resources: shared with host machine; be aware of this when allocating physical resources.
-------> Networking: many options, default NAT should work fine for most use cases.
-------> Guest addons: many hypervisors can install software in the guest to enable nicer integration with host system. You should use this if you can.
-----> Resources
-------> Hypervisors
---------> VirtualBox (open-source)
---------> Virt-manager (open-source, manages KVM virtual machines and LXC containers)
---------> VMWare (commercial, available from IS&T for MIT students)

---> If you are already familiar with popular hypervisors/VMs you may want to learn more about how to do this from a command line friendly way. 
-----> One option is the libvirt toolkit which allows you to manage multiple different virtualization providers/hypervisors.

---> Exercises
-----> Download and install a hypervisor.
-----> Create a new virtual machine and install a Linux distribution (e.g. Debian).
-----> Experiment with snapshots. Try things that you’ve always wanted to try, like running sudo rm -rf --no-preserve-root /, and see if you can recover easily.
-----> Read what a fork-bomb (:(){ :|:& };:) is and run it on the VM to see that the resource isolation (CPU, Memory, &c) works.
-----> Install guest addons and experiment with different windowing modes, file sharing, and other features.





-> Containers

---> Virtual Machines are relatively heavy-weight; what if you want to spin up machines in an automated fashion? Enter containers!
-----> Amazon Firecracker
-----> Docker
-----> rkt
-----> lxc

---> Containers are mostly just an assembly of various Linux security features, like virtual file system, virtual network interfaces, chroots, virtual memory tricks, 
-----> and the like, that together give the appearance of virtualization.
---> Not quite as secure or isolated as a VM, but pretty close and getting better. 
-----> Usually higher performance, and much faster to start, but not always.
---> The performance boost comes from the fact that unlike VMs which run an entire copy of the operating system, containers share the linux kernel with the host. 
-----> However note that if you are running linux containers on Windows/macOS a Linux VM will need to be active as a middle layer between the two.

---> Containers are handy for when you want to run an automated task in a standardized setup:
-------> Build systems
-------> Development environments
-------> Pre-packaged servers
-------> Running untrusted programs
---------> Grading student submissions
---------> (Some) cloud computing
-------> Continuous integration
---------> Travis CI
---------> GitHub Actions

---> Moreover, container software like Docker has also been extensively used as a solution for dependency hell. 
-----> If a machine needs to be running many services with conflicting dependencies they can be isolated using containers.
---> Usually, you write a file that defines how to construct your container. You start with some minimal base image (like Alpine Linux), 
-----> and then a list of commands to run to set up the environment you want (install packages, copy files, build stuff, write config files, etc.). 
-----> Normally, there’s also a way to specify any external ports that should be available, and an entrypoint that dictates what command should be run when the container is started (like a grading script).
---> In a similar fashion to code repository websites (like GitHub) there are some container repository websites (like DockerHub)where many software services have prebuilt images that one can easily deploy.
---> Exercises
-----> Choose a container software (Docker, LXC, …) and install a simple Linux image. Try SSHing into it.
-----> Search and download a prebuilt container image for a popular web server (nginx, apache, …)



Shell and Scripting

The shell is an efficient, textual interface to your computer.

The shell prompt: what greets you when you open a terminal. 
Lets you run programs and commands; common ones are:
    cd to change directory
    ls to list files and directories
    mv and cp to move and copy files

But the shell lets you do so much more; you can invoke any program on your computer, and command-line tools exist for doing pretty much anything you may want to do. 
And they’re often more efficient than their graphical counterparts. We’ll go through a bunch of those in this class.

The shell provides an interactive programming language (“scripting”). There are many shells:

    You’ve probably used sh or bash.
    Also shells that match languages: csh.
    Or “better” shells: fish, zsh, ksh.

In this class we’ll focus on the ubiquitous sh and bash, but feel free to play around with others. I like fish.

Shell programming is a very useful tool in your toolbox. Can either write programs directly at the prompt, or into a file. #!/bin/sh + chmod +x to make shell executable.
Working with the shell

Run a command a bunch of times:

for i in $(seq 1 5); do echo hello; done

There’s a lot to unpack:

    for x in list; do BODY; done
        ; terminates a command – equivalent to newline
        split list, assign each to x, and run body
        splitting is “whitespace splitting”, which we’ll get back to
        no curly braces in shell, so do + done
    $(seq 1 5)
        run the program seq with arguments 1 and 5
        substitute entire $() with the output of that program
        equivalent to

        for i in 1 2 3 4 5

    echo hello
        everything in a shell script is a command
        in this case, run the echo command, which prints its arguments with the argument hello.
        all commands are searched for in $PATH (colon-separated)

We have variables:

for f in $(ls); do echo $f; done

Will print each file name in the current directory. Can also set variables using = (no space!):

foo=bar
echo $foo

There are a bunch of “special” variables too:

    $1 to $9: arguments to the script
    $0 name of the script itself
    $# number of arguments
    $$ process ID of current shell

To only print directories

for f in $(ls); do if test -d $f; then echo dir $f; fi; done

More to unpack here:

    if CONDITION; then BODY; fi
        CONDITION is a command; if it returns with exit status 0 (success), then BODY is run.
        can also hook in an else or elif
        again, no curly braces, so then + fi
    test is another program that provides various checks and comparisons, and exits with 0 if they’re true ($?)
        man COMMAND is your friend: man test
        can also be invoked with [ + ]: [ -d $f ]
            take a look at man test and which "["

But wait! This is wrong! What if a file is called “My Documents”?

    for f in $(ls) expands to for f in My Documents
    first do the test on My, then on Documents
    not what we wanted!
    biggest source of bugs in shell scripts

Argument splitting

Bash splits arguments by whitespace; not always what you want!

    need to use quoting to handle spaces in arguments for f in "My Documents" would work correctly
    same problem somewhere else – do you see where? test -d $f: if $f contains whitespace, test will error!
    echo happens to be okay, because split + join by space but what if a filename contains a newline?! turns into space!
    quote all use of variables that you don’t want split
    but how do we fix our script above? what does for f in "$(ls)" do do you think?

Globbing is the answer!

    bash knows how to look for files using patterns:
        * any string of characters
        ? any single character
        {a,b,c} any of these characters
    for f in *: all files in this directory
    when globbing, each matching file becomes its own argument
        still need to make sure to quote when using: test -d "$f"
    can make advanced patterns:
        for f in a*: all files starting with a in the current directory
        for f in foo/*.txt: all .txt files in foo
        for f in foo/*/p??.txt all three-letter text files starting with p in subdirs of foo

Whitespace issues don’t stop there:

    if [ $foo = "bar" ]; then – see the issue?
    what if $foo is empty? arguments to [ are = and bar…
    can work around this with [ x$foo = "xbar" ], but bleh
    instead, use [[: bash built-in comparator that has special parsing
        also allows && instead of -a, || over -o, etc.

Composability

Shell is powerful in part because of composability. Can chain multiple programs together rather than have one program that does everything.

The key character is | (pipe).

    a | b means run both a and b send all output of a as input to b print the output of b

All programs you launch (“processes”) have three “streams”:

    STDIN: when the program reads input, it comes from here
    STDOUT: when the program prints something, it goes here
    STDERR: a 2nd output the program can choose to use
    by default, STDIN is your keyboard, STDOUT and STDERR are both your terminal. but you can change that!
        a | b makes STDOUT of a STDIN of b.
        also have:
            a > foo (STDOUT of a goes to the file foo)
            a 2> foo (STDERR of a goes to the file foo)
            a < foo (STDIN of a is read from the file foo)
            hint: tail -f will print a file as it’s being written
    why is this useful? lets you manipulate output of a program!
        ls | grep foo: all files that contain the word foo
        ps | grep foo: all processes that contain the word foo
        journalctl | grep -i intel | tail -n5: last 5 system log messages with the word intel (case insensitive)
        who | sendmail -t me@example.com send the list of logged-in users to me@example.com
        forms the basis for much data-wrangling, as we’ll cover later

Bash also provides a number of other ways to compose programs.

You can group commands with (a; b) | tac: run a, then b, and send all their output to tac, which prints its input in reverse order.

A lesser-known, but super useful one is process substitution. b <(a) will run a, generate a temporary file-name for its output stream, and pass that file-name to b. For example:

diff <(journalctl -b -1 | head -n20) <(journalctl -b -2 | head -n20)

will show you the difference between the first 20 lines of the last boot log and the one before that.
Job and process control

What if you want to run longer-term things in the background?

    the & suffix runs a program “in the background”
        it will give you back your prompt immediately
        handy if you want to run two programs at the same time like a server and client: server & client
        note that the running program still has your terminal as STDOUT! try: server > server.log & client
    see all such processes with jobs
        notice that it shows “Running”
    bring it to the foreground with fg %JOB (no argument is latest)
    if you want to background the current program: ^Z + bg (Here ^Z means pressing Ctrl+Z)
        ^Z stops the current process and makes it a “job”
        bg runs the last job in the background (as if you did &)
    background jobs are still tied to your current session, and exit if you log out. disown lets you sever that connection. or use nohup.
    $! is pid of last background process

What about other stuff running on your computer?

    ps is your friend: lists running processes
        ps -A: print processes from all users (also ps ax)
        ps has many arguments: see man ps
    pgrep: find processes by searching (like ps -A | grep)
        pgrep -af: search and display with arguments
    kill: send a signal to a process by ID (pkill by search + -f)
        signals tell a process to “do something”
        most common: SIGKILL (-9 or -KILL): tell it to exit now equivalent to ^\
        also SIGTERM (-15 or -TERM): tell it to exit gracefully equivalent to ^C

Flags

Most command line utilities take parameters using flags. Flags usually come in short form (-h) and long form (--help). Usually running CMD -h or man CMD will give you a list of the flags the program takes. Short flags can usually be combined, running rm -r -f is equivalent to running rm -rf or rm -fr. Some common flags are a de facto standard and you will seem them in many applications:

    -a commonly refers to all files (i.e. also including those that start with a period)
    -f usually refers to forcing something, like rm -f
    -h displays the help for most commands
    -v usually enables a verbose output
    -V usually prints the version of the command

Also, a double dash -- is used in built-in commands and many other commands to signify the end of command options, after which only positional parameters are accepted. So if you have a file called -v (which you can) and want to grep it grep pattern -- -v will work whereas grep pattern -v won’t. In fact, one way to create such file is to do touch -- -v.
Exercises

    If you are completely new to the shell you may want to read a more comprehensive guide about it such as BashGuide. If you want a more in-depth introduction The Linux Command Line is a good resource.

    PATH, which, type

    We briefly discussed that the PATH environment variable is used to locate the programs that you run through the command line. Let’s explore that a little further
        Run echo $PATH (or echo $PATH | tr -s ':' '\n' for pretty printing) and examine its contents, what locations are listed?
        The command which locates a program in the user PATH. Try running which for common commands like echo, ls or mv. Note that which is a bit limited since it does not understand shell aliases. Try running type and command -v for those same commands. How is the output different?
        Run PATH= and try running the previous commands again, some work and some don’t, can you figure out why?
    Special Variables
        What does the variable ~ expands as? What about .? And ..?
        What does the variable $? do?
        What does the variable $_ do?
        What does the variable !! expand to? What about !!*? And !l?
        Look for documentation for these options and familiarize yourself with them

    xargs

    Sometimes piping doesn’t quite work because the command being piped into does not expect the newline separated format. For example file command tells you properties of the file.

    Try running ls | file and ls | xargs file. What is xargs doing?

    Shebang

    When you write a script you can specify to your shell what interpreter should be used to interpret the script by using a shebang line. Write a script called hello with the following contentsmake it executable with chmod +x hello. Then execute it with ./hello. Then remove the first line and execute it again? How is the shell using that first line?

       #! /usr/bin/python

       print("Hello World!")

    You will often see programs that have a shebang that looks like #! usr/bin/env bash. This is a more portable solution with it own set of advantages and disadvantages. How is env different from which? What environment variable does env use to decide what program to run?

    Pipes, process substitution, subshell

    Create a script called slow_seq.sh with the following contents and do chmod +x slow_seq.sh to make it executable.

       #! /usr/bin/env bash

       for i in $(seq 1 10); do
               echo $i;
               sleep 1;
       done

    There is a way in which pipes (and process substitution) differ from using subshell execution, i.e. $(). Run the following commands and observe the differences:
        ./slow_seq.sh | grep -P "[3-6]"
        grep -P "[3-6]" <(./slow_seq.sh)
        echo $(./slow_seq.sh) | grep -P "[3-6]"
    Misc
        Try running touch {a,b}{a,b} then ls what did appear?
        Sometimes you want to keep STDIN and still pipe it to a file. Try running echo HELLO | tee hello.txt
        Try running cat hello.txt > hello.txt what do you expect to happen? What does happen?
        Run echo HELLO > hello.txt and then run echo WORLD >> hello.txt. What are the contents of hello.txt? How is > different from >>?
        Run printf "\e[38;5;81mfoo\e[0m\n". How was the output different? If you want to know more, search for ANSI color escape sequences.
        Run touch a.txt then run ^txt^log what did bash do for you? In the same vein, run fc. What does it do?

    Keyboard shortcuts

    As with any application you use frequently is worth familiarising yourself with its keyboard shortcuts. Type the following ones and try figuring out what they do and in what scenarios it might be convenient knowing about them. For some of them it might be easier searching online about what they do. (remember that ^X means pressing Ctrl+X)
        ^A, ^E
        ^R
        ^L
        ^C, ^\ and ^D
        ^U and ^Y

Command-line environment
Aliases & Functions

As you can imagine it can become tiresome typing long commands that involve many flags or verbose options. Nevertheless, most shells support aliasing. For instance, an alias in bash has the following structure (note there is no space around the = sign):

alias alias_name="command_to_alias"

Alias have many convenient features

# Alias can summarize good default flags
alias ll="ls -lh"

# Save a lot of typing for common commands
alias gc="git commit"

# Alias can overwrite existing commands
alias mv="mv -i"
alias mkdir="mkdir -p"

# Alias can be composed
alias la="ls -A"
alias lla="la -l"

# To ignore an alias run it prepended with \
\ls
# Or can be disabled using unalias
unalias la

However in many scenarios aliases can be limiting, specially when you are trying to write chain commands together that take the same arguments. An alternative exists which is functions which are a midpoint between aliases and custom shell scripts.

Here is an example function that makes a directory and move into it.

mcd () {
    mkdir -p $1
    cd $1
}

Alias and functions will not persist shell sessions by default. To make an alias persistent you need to include it a one the shell startup script files like .bashrc or .zshrc. My suggestion is to write them separately in a .alias and source that file from your different shell config files.
Shells & Frameworks

During shell and scripting we covered the bash shell since it is by far the most ubiquitous shell and most systems have it as the default option. Nevertheless, it is not the only option.

For example the zsh shell is a superset of bash and provides many convenient features out of the box such as:

    Smarter globbing, **
    Inline globbing/wildcard expansion
    Spelling correction
    Better tab completion/selection
    Path expansion (cd /u/lo/b will expand as /usr/local/bin)

Moreover many shells can be improved with frameworks, some popular general frameworks like prezto or oh-my-zsh, and smaller ones that focus on specific features like for example zsh-syntax-highlighting or zsh-history-substring-search. Other shells like fish include a lot of these user-friendly features by default. Some of these features include:

    Right prompt
    Command syntax highlighting
    History substring search
    manpage based flag completions
    Smarter autocompletion
    Prompt themes

One thing to note when using these frameworks is that if the code they run is not properly optimized or it is too much code, your shell can start slowing down. You can always profile it and disable the features that you do not use often or value over speed.
Terminal Emulators & Multiplexers

Along with customizing your shell it is worth spending some time figuring out your choice of terminal emulator and its settings. There are many many terminal emulators out there (here is a comparison).

Since you might be spending hundreds to thousands of hours in your terminal it pays off to look into its settings. Some of the aspects that you may want to modify in your terminal include:

    Font choice
    Color Scheme
    Keyboard shortcuts
    Tab/Pane support
    Scrollback configuration
    Performance (some newer terminals like Alacritty offer GPU acceleration)

It is also worth mentioning terminal multiplexers like tmux. tmux allows you to pane and tab multiple shell sessions. It also supports attaching and detaching which is a very common use-case when you are working on a remote server and want to keep you shell running without having to worry about disowning you current processes (by default when you log out your processes are terminated). This way, with tmux you can jump into and out of complex terminal layouts. Similar to terminal emulators tmux supports heavy customization by editing the ~/.tmux.conf file.
Command-line utilities

The command line utilities that most UNIX based operating systems have by default are more than enough to do 99% of the stuff you usually need to do.

In the next few subsections I will cover alternative tools for extremely common shell operations which are more convenient to use. Some of these tools add new improved functionality to the command whereas others just focus on providing a simpler, more intuitive interface with better defaults.
fasd vs cd

Even with improved path expansion and tab autocomplete, changing directories can become quite repetitive. Fasd (or autojump) solves this issue by keeping track of recent and frequent folders you have been to and performing fuzzy matching.

Thus if I have visited the path /home/user/awesome_project/code running z code will cd to it. If I have multiple folders called code I can disambiguate by running z awe code which will be closer match. Unlike autojump, fasd also provides commands that instead of performing cd just expand frequent and /or recent files,folders or both.
bat vs cat

Even though cat does it job perfectly, bat improves it by providing syntax highlighting, paging, line numbers and git integration.
exa/ranger vs ls

ls is a great command but some of the defaults can be annoying such as displaying the size in raw bytes. exa provides better defaults

If you are in need of navigating many folders and/or previewing many files, ranger can be much more efficient than cd and cat due to its wonderful interface. It is quite customizable and with a correct setup you can even preview images in your terminal
fd vs find

fd is a simple, fast and user-friendly alternative to find. find defaults like having to use the --name flag (which is what you want to do 99% of the time) make it easier to use in an every day basis. It is also git aware and will skip files in your .gitignore and .git folder by default. It also has nice color coding by default.
rg/fzf vs grep

grep is a great tool but if you want to grep through many files at once, there are better tools for that purpose. ack, ag & rg recursively search your current directory for a regex pattern while respecting your gitignore rules. They all work pretty similar but I favor rg due to how fast it can search my entire home directory.

Similarly, it can be easy to find yourself doing CMD | grep PATTERN over an over again. fzf is a command line fuzzy finder that enables you to interactively filter the output of pretty much any command.
rsync vs cp/scp

Whereas mv and scp are perfect for most scenarios, when copying/moving around large amounts of files, large files or when some of the data is already on the destination rsync is a huge improvement. rsync will skip files that have already been transferred and with the --partial flag it can resume from a previously interrupted copy.
trash vs rm

rm is a dangerous command in the sense that once you delete a file there is no turning back. However, modern OS do not behave like that when you delete something in the file explorer, they just move it to the Trash folder which is cleared periodically.

Since how the trash is managed varies from OS to OS there is not a single CLI utility. In macOS there is trash and in linux there is trash-cli among others.
mosh vs ssh

ssh is a very handy tool but if you have a slow connection, the lag can become annoying and if the connection interrupts you have to reconnect. mosh is a handy tool that works allows roaming, supports intermittent connectivity, and provides intelligent local echo.
tldr vs man

You can figure out what a commands does and what options it has using man and the -h/’–help’ flag most of the time. However, in some cases it can be a bit daunting navigating these if they are detailed

The tldr command is a community driven documentation system that’s available from the command line and gives a few simple illustrative examples of what the command does and the most common argument options.
aunpack vs tar/unzip/unrar

As this xkcd references, it can be quite tricky to remember the options for tar and sometimes you need a different tool altogether such as unrar for .rar files. The atool package provides the aunpack command which will figure out the correct options and always put the extracted archives in a new folder.
Exercises

    Run cat .bash_history | sort | uniq -c | sort -rn | head -n 10 (or cat .zhistory | sort | uniq -c | sort -rn | head -n 10 for zsh) to get top 10 most used commands and consider writing shorter aliases for them
    Choose a terminal emulator and figure out how to change the following properties:
        Font choice
        Color scheme. How many colors does a standard scheme have? why?
        Scrollback history size
    Install fasd or some similar software and write a bash/zsh function called v that performs fuzzy matching on the passed arguments and opens up the top result in your editor of choice. Then, modify it so that if there are multiple matches you can select them with fzf.
    Since fzf is quite convenient for performing fuzzy searches and the shell history is quite prone to those kind of searches, investigate how to bind fzf to ^R. You can find some info here
    What does the --bar option do in ack?



Data Wrangling

Have you ever had a bunch of text and wanted to do something with it? Good. That’s what data wrangling is all about! Specifically, adapting data from one format to another, until you end up with exactly what you wanted.

We’ve already seen basic data wrangling: journalctl | grep -i intel.

    find all system log entries that mention Intel (case insensitive)
    really, most of data wrangling is about knowing what tools you have, and how to combine them.

Let’s start from the beginning: we need a data source, and something to do with it. Logs often make for a good use-case, because you often want to investigate things about them, and reading the whole thing isn’t feasible. Let’s figure out who’s trying to log into my server by looking at my server’s log:

ssh myserver journalctl

That’s far too much stuff. Let’s limit it to ssh stuff:

ssh myserver journalctl | grep sshd

Notice that we’re using a pipe to stream a remote file through grep on our local computer! ssh is magical. This is still way more stuff than we wanted though. And pretty hard to read. Let’s do better:

ssh myserver journalctl | grep sshd | grep "Disconnected from"

There’s still a lot of noise here. There are a lot of ways to get rid of that, but let’s look at one of the most powerful tools in your toolkit: sed.

sed is a “stream editor” that builds on top of the old ed editor. In it, you basically give short commands for how to modify the file, rather than manipulate its contents directly (although you can do that too). There are tons of commands, but one of the most common ones is s: substitution. For example, we can write:

ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed 's/.*Disconnected from //'

What we just wrote was a simple regular expression; a powerful construct that lets you match text against patterns. The s command is written on the form: s/REGEX/SUBSTITUTION/, where REGEX is the regular expression you want to search for, and SUBSTITUTION is the text you want to substitute matching text with.
Regular expressions

Regular expressions are common and useful enough that it’s worthwhile to take some time to understand how they work. Let’s start by looking at the one we used above: /.*Disconnected from /. Regular expressions are usually (though not always) surrounded by /. Most ASCII characters just carry their normal meaning, but some characters have “special” matching behavior. Exactly which characters do what vary somewhat between different implementations of regular expressions, which is a source of great frustration. Very common patterns are:

    . means “any single character” except newline
    * zero or more of the preceding match
    + one or more of the preceding match
    [abc] any one character of a, b, and c
    (RX1|RX2) either something that matches RX1 or RX2
    ^ the start of the line
    $ the end of the line

sed’s regular expressions are somewhat weird, and will require you to put a \ before most of these to give them their special meaning. Or you can pass -E.

So, looking back at /.*Disconnected from /, we see that it matches any text that starts with any number of characters, followed by the literal string “Disconnected from “. Which is what we wanted. But beware, regular expressions are trixy. What if someone tried to log in with the username “Disconnected from”? We’d have:

Jan 17 03:13:00 thesquareplanet.com sshd[2631]: Disconnected from invalid user Disconnected from 46.97.239.16 port 55920 [preauth]

What would we end up with? Well, * and + are, by default, “greedy”. They will match as much text as they can. So, in the above, we’d end up with just

46.97.239.16 port 55920 [preauth]

Which may not be what we wanted. In some regular expression implementations, you can just suffix * or + with a ? to make them non-greedy, but sadly sed doesn’t support that. We could switch to perl’s command-line mode though, which does support that construct:

perl -pe 's/.*?Disconnected from //'

We’ll stick to sed for the rest of this though, because it’s by far the more common tool for these kinds of jobs. sed can also do other handy things like print lines following a given match, do multiple substitutions per invocation, search for things, etc. But we won’t cover that too much here. sed is basically an entire topic in and of itself, but there are often better tools.

Okay, so we also have a suffix we’d like to get rid of. How might we do that? It’s a little tricky to match just the text that follows the username, especially if the username can have spaces and such! What we need to do is match the whole line:

 | sed -E 's/.*Disconnected from (invalid |authenticating )?user .* [^ ]+ port [0-9]+( \[preauth\])?$//'

Let’s look at what’s going on with a regex debugger. Okay, so the start is still as before. Then, we’re matching any of the “user” variants (there are two prefixes in the logs). Then we’re matching on any string of characters where the username is. Then we’re matching on any single word ([^ ]+; any non-empty sequence of non-space characters). Then the word “port” followed by a sequence of digits. Then possibly the suffix ` [preauth]`, and then the end of the line.

Notice that with this technique, as username of “Disconnected from” won’t confuse us any more. Can you see why?

There is one problem with this though, and that is that the entire log becomes empty. We want to keep the username after all. For this, we can use “capture groups”. Any text matched by a regex surrounded by parentheses is stored in a numbered capture group. These are available in the substitution (and in some engines, even in the pattern itself!) as \1, \2, \3, etc. So:

 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'

As you can probably imagine, you can come up with really complicated regular expressions. For example, here’s an article on how you might match an e-mail address. It’s not easy. And there’s lots of discussion. And people have written tests. And test matrices. You can even write a regex for determining if a given number is a prime number.

Regular expressions are notoriously hard to get right, but they are also very handy to have in your toolbox!
Back to data wrangling

Okay, so we now have

ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'

We could do it just with sed, but why would we? For fun is why.

ssh myserver journalctl
 | sed -E
   -e '/Disconnected from/!d'
   -e 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'

This shows off some of sed’s capabilities. sed can also inject text (with the i command), explicitly print lines (with the p command), select lines by index, and lots of other things. Check man sed!

Anyway. What we have now gives us a list of all the usernames that have attempted to log in. But this is pretty unhelpful. Let’s look for common ones:

ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c

sort will, well, sort its input. uniq -c will collapse consecutive lines that are the same into a single line, prefixed with a count of the number of occurrences. We probably want to sort that too and only keep the most common logins:

ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | sort -nk1,1 | tail -n10

sort -n will sort in numeric (instead of lexicographic) order. -k1,1 means “sort by only the first whitespace-separated column”. The ,n part says “sort until the nth field, where the default is the end of the line. In this particular example, sorting by the whole line wouldn’t matter, but we’re here to learn!

If we wanted the least common ones, we could use head instead of tail. There’s also sort -r, which sorts in reverse order.

Okay, so that’s pretty cool, but we’d sort of like to only give the usernames, and maybe not one per line?

ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | sort -nk1,1 | tail -n10
 | awk '{print $2}' | paste -sd,

Let’s start with paste: it lets you combine lines (-s) by a given single-character delimiter (-d). But what’s this awk business?
awk – another editor

awk is a programming language that just happens to be really good at processing text streams. There is a lot to say about awk if you were to learn it properly, but as with many other things here, we’ll just go through the basics.

First, what does {print $2} do? Well, awk programs take the form of an optional pattern plus a block saying what to do if the pattern matches a given line. The default pattern (which we used above) matches all lines. Inside the block, $0 is set to the entire line’s contents, and $1 through $n are set to the nth field of that line, when separated by the awk field separator (whitespace by default, change with -F). In this case, we’re saying that, for every line, print the contents of the second field, which happens to be the username!

Let’s see if we can do something fancier. Let’s compute the number of single-use usernames that start with c and end with e:

 | awk '$1 == 1 && $2 ~ /^c[^ ]*e$/ { print $2 }' | wc -l

There’s a lot to unpack here. First, notice that we now have a pattern (the stuff that goes before {...}). The pattern says that the first field of the line should be equal to 1 (that’s the count from uniq -c), and that the second field should match the given regular expression. And the block just says to print the username. We then count the number of lines in the output with wc -l.

However, awk is a programming language, remember?

BEGIN { rows = 0 }
$1 == 1 && $2 ~ /^c[^ ]*e$/ { rows += $1 }
END { print rows }

BEGIN is a pattern that matches the start of the input (and END matches the end). Now, the per-line block just adds the count from the first field (although it’ll always be 1 in this case), and then we print it out at the end. In fact, we could get rid of grep and sed entirely, because awk can do it all, but we’ll leave that as an exercise to the reader.
Analyzing data

You can do math!

 | paste -sd+ | bc -l

echo "2*($(data | paste -sd+))" | bc -l

You can get stats in a variety of ways. st is pretty neat, but if you already have R:

ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | awk '{print $1}' | R --slave -e 'x <- scan(file="stdin", quiet=TRUE); summary(x)'

R is another (weird) programming language that’s great at data analysis and plotting. We won’t go into too much detail, but suffice to say that summary prints summary statistics about a matrix, and we computed a matrix from the input stream of numbers, so R gives us the statistics we wanted!

If you just want some simple plotting, gnuplot is your friend:

ssh myserver journalctl
 | grep sshd
 | grep "Disconnected from"
 | sed -E 's/.*Disconnected from (invalid |authenticating )?user (.*) [^ ]+ port [0-9]+( \[preauth\])?$/\2/'
 | sort | uniq -c
 | sort -nk1,1 | tail -n10
 | gnuplot -p -e 'set boxwidth 0.5; plot "-" using 1:xtic(2) with boxes'

Data wrangling to make arguments

Sometimes you want to do data wrangling to find things to install or remove based on some longer list. The data wrangling we’ve talked about so far + xargs can be a powerful combo:

rustup toolchain list | grep nightly | grep -vE "nightly-x86|01-17" | sed 's/-x86.*//' | xargs rustup toolchain uninstall

Exercises

    If you are not familiar with Regular Expressions here is a short interactive tutorial that covers most of the basics
    How is sed s/REGEX/SUBSTITUTION/g different from the regular sed? What about /I or /m?
    To do in-place substitution it is quite tempting to do something like sed s/REGEX/SUBSTITUTION/ input.txt > input.txt. However this is a bad idea, why? Is this particular to sed?
    Implement a simple grep equivalent tool in a language you are familiar with using regex. If you want the output to be color highlighted like grep is, search for ANSI color escape sequences.
    Sometimes some operations like renaming files can be tricky with raw commands like mv . rename is a nifty tool to achieve this and has a sed-like syntax. Try creating a bunch of files with spaces in their names and use rename to replace them with underscores.
    Look for boot messages that are not shared between your past three reboots (see journalctl’s -b flag). You may want to just mash all the boot logs together in a single file, as that may make things easier.
    Produce some statistics of your system boot time over the last ten boots using the log timestamp of the messages

    Logs begin at ...

    and

    systemd[577]: Startup finished in ...

    Find the number of words (in /usr/share/dict/words) that contain at least three as and don’t have a 's ending. What are the three most common last two letters of those words? sed’s y command, or the tr program, may help you with case insensitivity. How many of those two-letter combinations are there? And for a challenge: which combinations do not occur?
    Find an online data set like this one or this one. Maybe another one from here. Fetch it using curl and extract out just two columns of numerical data. If you’re fetching HTML data, pup might be helpful. For JSON data, try jq. Find the min and max of one column in a single command, and the sum of the difference between the two columns in another.





Editors
Importance of Editors

As programmers, we spend most of our time editing plain-text files. It’s worth investing time learning an editor that fits your needs.

How do you learn a new editor? You force yourself to use that editor for a while, even if it temporarily hampers your productivity. It’ll pay off soon enough (two weeks is enough to learn the basics).

We are going to teach you Vim, but we encourage you to experiment with other editors. It’s a very personal choice, and people have strong opinions.

We can’t teach you how to use a powerful editor in 50 minutes, so we’re going to focus on teaching you the basics, showing you some of the more advanced functionality, and giving you the resources to master the tool. We’ll teach you lessons in the context of Vim, but most ideas will translate to any other powerful editor you use (and if they don’t, then you probably shouldn’t use that editor!).

Editor Learning Curves

The editor learning curves graph is a myth. Learning the basics of a powerful editor is quite easy (even though it might take years to master).

Which editors are popular today? See this Stack Overflow survey (there may be some bias because Stack Overflow users may not be representative of programmers as a whole).
Command-line Editors

Even if you eventually settle on using a GUI editor, it’s worth learning a command-line editor for easily editing files on remote machines.
Nano

Nano is a simple command-line editor.

    Move with arrow keys
    All other shortcuts (save, exit) shown at the bottom

Vim

Vi/Vim is a powerful text editor. It’s a command-line program that’s usually installed everywhere, which makes it convenient for editing files on a remote machine.

Vim also has graphical versions, such as GVim and MacVim. These provide additional features such as 24-bit color, menus, and popups.
Philosophy of Vim

    When programming, you spend most of your time reading/editing, not writing
        Vim is a modal editor: different modes for inserting text vs manipulating text
    Vim is programmable (with Vimscript and also other languages like Python)
    Vim’s interface itself is like a programming language
        Keystrokes (with mnemonic names) are commands
        Commands are composable
    Don’t use the mouse: too slow
    Editor should work at the speed you think

Introductory Vim
Modes

Vim shows the current mode in the bottom left.

    Normal mode: for moving around a file and making edits
        Spend most of your time here
    Insert mode: for inserting text
    Visual (visual, line, or block) mode: for selecting blocks of text

You change modes by pressing <ESC> to switch from any mode back to normal mode. From normal mode, enter insert mode with i, visual mode with v, visual line mode with V, and visual block mode with <C-v>.

You use the <ESC> key a lot when using Vim: consider remapping Caps Lock to Escape.
Basics

Vim ex commands are issued through :{command} in normal mode.

    :q quit (close window)
    :w save
    :wq save and quit
    :e {name of file} open file for editing
    :ls show open buffers
    :help {topic} open help
        :help :w opens help for the :w ex command
        :help w opens help for the w movement

Movement

Vim is all about efficient movement. Navigate the file in Normal mode.

    Disable arrow keys to avoid bad habits

    nnoremap <Left> :echoe "Use h"<CR>
    nnoremap <Right> :echoe "Use l"<CR>
    nnoremap <Up> :echoe "Use k"<CR>
    nnoremap <Down> :echoe "Use j"<CR>

    Basic movement: hjkl (left, down, up, right)
    Words: w (next word), b (beginning of word), e (end of word)
    Lines: 0 (beginning of line), ^ (first non-blank character), $ (end of line)
    Screen: H (top of screen), M (middle of screen), L (bottom of screen)
    File: gg (beginning of file), G (end of file)
    Line numbers: :{number}<CR> or {number}G (line {number})
    Misc: % (corresponding item)
    Find: f{character}, t{character}, F{character}, T{character}
        find/to forward/backward {character} on the current line
    Repeating N times: {number}{movement}, e.g. 10j moves down 10 lines
    Search: /{regex}, n / N for navigating matches

Selection

Visual modes:

    Visual
    Visual Line
    Visual Block

Can use movement keys to make selection.
Manipulating text

Everything that you used to do with the mouse, you now do with keyboards (and powerful, composable commands).

    i enter insert mode
        but for manipulating/deleting text, want to use something more than backspace
    o / O insert line below / above
    d{motion} delete {motion}
        e.g. dw is delete word, d$ is delete to end of line, d0 is delete to beginning of line
    c{motion} change {motion}
        e.g. cw is change word
        like d{motion} followed by i
    x delete character (equal do dl)
    s substitute character (equal to xi)
    visual mode + manipulation
        select text, d to delete it or c to change it
    u to undo, <C-r to redo
    Lots more to learn: e.g. ~ flips the case of a character

Resources

    vimtutor command-line program to teach you vim
    Vim Adventures game to learn Vim

Customizing Vim

Vim is customized through a plain-text configuration file in ~/.vimrc (containing Vimscript commands). There are probably lots of basic settings that you want to turn on.

Look at people’s dotfiles on GitHub for inspiration, but try not to copy-and-paste people’s full configuration. Read it, understand it, and take what you need.

Some customizations to consider:

    Syntax highlighting: syntax on
    Color schemes
    Line numbers: set nu / set rnu
    Backspacing through everything: set backspace=indent,eol,start

Advanced Vim

Here are a few examples to show you the power of the editor. We can’t teach you all of these kinds of things, but you’ll learn them as you go. A good heuristic: whenever you’re using your editor and you think “there must be a better way of doing this”, there probably is: look it up online.
Search and replace

:s (substitute) command (documentation).

    %s/foo/bar/g
        replace foo with bar globally in file
    %s/\[.*\](\(.*\))/\1/g
        replace named Markdown links with plain URLs

Multiple windows

    sp / vsp to split windows
    Can have multiple views of the same buffer.

Mouse support

    set mouse+=a
        can click, scroll select

Macros

    q{character} to start recording a macro in register {character}
    q to stop recording
    @{character} replays the macro
    Macro execution stops on error
    {number}@{character} executes a macro {number} times
    Macros can be recursive
        first clear the macro with q{character}q
        record the macro, with @{character} to invoke the macro recursively (will be a no-op until recording is complete)
    Example: convert xml to json (file)
        Array of objects with keys “name” / “email”
        Use a Python program?
        Use sed / regexes
            g/people/d
            %s/<person>/{/g
            %s/<name>\(.*\)<\/name>/"name": "\1",/g
            …
        Vim commands / macros
            Gdd, ggdd delete first and last lines
            Macro to format a single element (register e)
                Go to line with <name>
                qe^r"f>s": "<ESC>f<C"<ESC>q
            Macro to format a person
                Go to line with <person>
                qpS{<ESC>j@eA,<ESC>j@ejS},<ESC>q
            Macro to format a person and go to the next person
                Go to line with <person>
                qq@pjq
            Execute macro until end of file
                999@q
            Manually remove last , and add [ and ] delimiters

Extending Vim

There are tons of plugins for extending vim.

First, get set up with a plugin manager like vim-plug, Vundle, or pathogen.vim.

Some plugins to consider:

    ctrlp.vim: fuzzy file finder
    vim-fugitive: git integration
    vim-surround: manipulating “surroundings”
    gundo.vim: navigate undo tree
    nerdtree: file explorer
    syntastic: syntax checking
    vim-easymotion: magic motions
    vim-over: substitute preview

Lists of plugins:

    Vim Awesome

Vim-mode in Other Programs

For many popular editors (e.g. vim and emacs), many other tools support editor emulation.

    Shell
        bash: set -o vi
        zsh: bindkey -v
        export EDITOR=vim (environment variable used by programs like git)
    ~/.inputrc
        set editing-mode vi

There are even vim keybinding extensions for web browsers, some popular ones are Vimium for Google Chrome and Tridactyl for Firefox.
Resources

    Vim Tips Wiki
    Vim Advent Calendar: various Vim tips
    Neovim is a modern vim reimplementation with more active development.
    Vim Golf: Various Vim challenges

Exercises

    Experiment with some editors. Try at least one command-line editor (e.g. Vim) and at least one GUI editor (e.g. Atom). Learn through tutorials like vimtutor (or the equivalents for other editors). To get a real feel for a new editor, commit to using it exclusively for a couple days while going about your work.

    Customize your editor. Look through tips and tricks online, and look through other people’s configurations (often, they are well-documented).

    Experiment with plugins for your editor.

    Commit to using a powerful editor for at least a couple weeks: you should start seeing the benefits by then. At some point, you should be able to get your editor to work as fast as you think.

    Install a linter (e.g. pyflakes for python) link it to your editor and test it is working.





Version Control

Whenever you are working on something that changes over time, it’s useful to be able to track those changes. This can be for a number of reasons: it gives you a record of what changed, how to undo it, who changed it, and possibly even why. Version control systems (VCS) give you that ability. They let you commit changes to a set of files, along with a message describing the change, as well as look at and undo changes you’ve made in the past.

Most VCS support sharing the commit history between multiple users. This allows for convenient collaboration: you can see the changes I’ve made, and I can see the changes you’ve made. And since the VCS tracks changes, it can often (though not always) figure out how to combine our changes as long as they touch relatively disjoint things.

There a lot of VCSes out there that differ a lot in what they support, how they function, and how you interact with them. Here, we’ll focus on git, one of the more commonly used ones, but I recommend you also take a look at Mercurial.

With that all said – to the cliffnotes!
Is git dark magic?

not quite.. you need to understand the data model. we’re going to skip over some of the details, but roughly speaking, the core “thing” in git is a commit.

    every commit has a unique name, “revision hash” a long hash like 998622294a6c520db718867354bf98348ae3c7e2 often shortened to a short (unique-ish) prefix: 9986222
    commit has author + commit message
    also has the hash of any ancestor commits usually just the hash of the previous commit
    commit also represents a diff, a representation of how you get from the commit’s ancestors to the commit (e.g., remove this line in this file, add these lines to this file, rename that file, etc.)
        in reality, git stores the full before and after state
        probably don’t want to store big files that change!

initially, the repository (roughly: the folder that git manages) has no content, and no commits. let’s set that up:

$ git init hackers
$ cd hackers
$ git status

the output here actually gives us a good starting point. let’s dig in and make sure we understand it all.

first, “On branch master”.

    don’t want to use hashes all the time.
    branches are names that point to hashes.
    master is traditionally the name for the “latest” commit. every time a new commit is made, the master name will be made to point to the new commit’s hash.
    special name HEAD refers to “current” name
    you can also make your own names with git branch (or git tag) we’ll get back to that

let’s skip over “No commits yet” because that’s all there is to it.

then, “nothing to commit”.

    every commit contains a diff with all the changes you made. but how is that diff constructed in the first place?
    could just always commit all changes you’ve made since the last commit
        sometimes you want to only commit some of them (e.g., not TODOs)
        sometimes you want to break up a change into multiple commits to give a separate commit message for each one
    git lets you stage changes to construct a commit
        add changes to a file or files to the staged changes with git add
            add only some changes in a file with git add -p
            without argument git add operates on “all known files”
        remove a file and stage its removal with git rm
        empty the set of staged changes git reset
            note that this does not change any of your files! it only means that no changes will be included in a commit
            to remove only some staged changes: git reset FILE or git reset -p
        check staged changes with git diff --staged
        see remaining changes with git diff
        when you’re happy with the stage, make a commit with git commit
            if you just want to commit all changes: git commit -a
            git help add has a bunch more helpful info

while you’re playing with the above, try to run git status to see what git thinks you’re doing – it’s surprisingly helpful!
A commit you say…

okay, we have a commit, now what?

    we can look at recent changes: git log (or git log --oneline)
    we can look at the full changes: git log -p
    we can show a particular commit: git show master
        or with -p for full diff/patch
    we can go back to the state at a commit using git checkout NAME
        if NAME is a commit hash, git says we’re “detached”. this just means there’s no NAME that refers to this commit, so if we make commits, no-one will know about them.
    we can revert a change with git revert NAME
        applies the diff in the commit at NAME in reverse.
    we can compare an older version to this one using git diff NAME..
        a..b is a commit range. if either is left out, it means HEAD.
    we can show all the commits between using git log NAME..
        -p works here too
    we can change master to point to a particular commit (effectively undoing everything since) with git reset NAME:
        huh, why? wasn’t reset to change staged changes? reset has a “second” form (see git help reset) which sets HEAD to the commit pointed to by the given name.
        notice that this didn’t change any files – git diff now effectively shows git diff NAME...

What’s in a name?

clearly, names are important in git. and they’re the key to understanding a lot of what goes on in git. so far, we’ve talked about commit hashes, master, and HEAD. but there’s more!

    you can make your own branches (like master) with git branch b
        creates a new name, b, which points to the commit at HEAD
        you’re still “on” master though, so if you make a new commit, master will point to that new commit, b will not.
        switch to a branch with git checkout b
            any commits you make will now update the b name
            switch back to master with git checkout master
                all your changes in b are hidden away
            a very handy way to be able to easily test out changes
    tags are other names that never change, and that have their own message. often used to mark releases + changelogs.
    NAME^ means “the commit before NAME
        can apply recursively: NAME^^^
        you most likely mean ~ when you use ~
            ~ is “temporal”, whereas ^ goes by ancestors
            ~~ is the same as ^^
            with ~ you can also write X~3 for “3 commits older than X
            you don’t want ^3
        git diff HEAD^
    - means “the previous name”
    most commands operate on HEAD unless you give another argument

Clean up your mess

your commit history will very often end up as:

    add feature x – maybe even with a commit message about x!
    forgot to add file
    fix bug
    typo
    typo2
    actually fix
    actually actually fix
    tests pass
    fix example code
    typo
    x
    x
    x
    x

that’s fine as far as git is concerned, but is not very helpful to your future self, or to other people who are curious about what has changed. git lets you clean up these things:

    git commit --amend: fold staged changes into previous commit
        note that this changes the previous commit, giving it a new hash!
    git rebase -i HEAD~13 is magical. for each commit from past 13, choose what to do:
        default is pick; do nothing
        r: change commit message
        e: change commit (add or remove files)
        s: combine commit with previous and edit commit message
        f: “fixup” – combine commit with previous; discard commit msg
        at the end, HEAD is made to point to what is now the last commit
        often referred to as squashing commits
        what it really does: rewind HEAD to rebase start point, then re-apply the commits in order as directed.
    git reset --hard NAME: reset the state of all files to that of NAME (or HEAD if no name is given). handy for undoing changes.

Playing with others

a common use-case for version control is to allow multiple people to make changes to a set of files without stepping on each other’s toes. or rather, to make sure that if they step on each other’s toes, they won’t just silently overwrite each other’s changes.

git is a distributed VCS: everyone has a local copy of the entire repository (well, of everything others have chosen to publish). some VCSes are centralized (e.g., subversion): a server has all the commits, clients only have the files they have “checked out”. basically, they only have the current files, and need to ask the server if they want anything else.

every copy of a git repository can be listed as a “remote”. you can copy an existing git repository using git clone ADDRESS (instead of git init). this creates a remote called origin that points to ADDRESS. you can fetch names and the commits they point to from a remote with git fetch REMOTE. all names at a remote are available to you as REMOTE/NAME, and you can use them just like local names.

if you have write access to a remote, you can change names at the remote to point to commits you’ve made using git push. for example, let’s make the master name (branch) at the remote origin point to the commit that our master branch currently points to:

    git push origin master:master
    for convenience, you can set origin/master as the default target for when you git push from the current branch with -u
    consider: what does this do? git push origin master:HEAD^

often you’ll use GitHub, GitLab, BitBucket, or something else as your remote. there’s nothing “special” about that as far as git is concerned. it’s all just names and commits. if someone makes a change to master and updates github/master to point to their commit (we’ll get back to that in a second), then when you git fetch github, you’ll be able to see their changes with git log github/master.
Working with others

so far, branches seem pretty useless: you can create them, do work on them, but then what? eventually, you’ll just make master point to them anyway, right?

    what if you had to fix something while working on a big feature?
    what if someone else made a change to master in the meantime?

inevitably, you will have to merge changes in one branch with changes in another, whether those changes are made by you or someone else. git lets you do this with, unsurprisingly, git merge NAME. merge will:

    look for the latest point where HEAD and NAME shared a commit ancestor (i.e., where they diverged)
    (try to) apply all those changes to the current HEAD
    produce a commit that contains all those changes, and lists both HEAD and NAME as its ancestors
    set HEAD to that commit’s hash

once your big feature has been finished, you can merge its branch into master, and git will ensure that you don’t lose any changes from either branch!

if you’ve used git in the past, you may recognize merge by a different name: pull. when you do git pull REMOTE BRANCH, that is:

    git fetch REMOTE
    git merge REMOTE/BRANCH
    where, like push, REMOTE and BRANCH are often omitted and use the “tracking” remote branch (remember -u?)

this usually works great. as long as the changes to the branches being merged are disjoint. if they are not, you get a merge conflict. sounds scary…

    a merge conflict is just git telling you that it doesn’t know what the final diff should look like
    git pauses and asks you to finish staging the “merge commit”
    open the conflicted file in your editor and look for lots of angle brackets (<<<<<<<). the stuff above ======= is the change made in the HEAD since the shared ancestor commit. the stuff below is the change made in the NAME since the shared commit.
    git mergetool is pretty handy – opens a diff editor
    once you’ve resolved the conflict by figuring out what the file should now look like, stage those changes with git add.
    when all the conflicts are resolved, finish with git commit
        you can give up with git merge --abort

you’ve just resolved your first git merge conflict! \o/ now you can publish your finished changes with git push
When worlds collide

when you push, git checks that no-one else’s work is lost if you update the remote name you’re pushing too. it does this by checking that the current commit of the remote name is an ancestor of the commit you are pushing. if it is, git can safely just update the name; this is called fast-forwarding. if it is not, git will refuse to update the remote name, and tell you there have been changes.

if your push is rejected, what do you do?

    merge remote changes with git pull (i.e., fetch + merge)
    force the push with --force: this will lose other people’s changes!
        there’s also --force-with-lease, which will only force the change if the remote name hasn’t changed since the last time you fetched from that remote. much safer!
        if you’ve rebased local commits that you’ve previously pushed (“history rewriting”; probably don’t do this), you’ll have to force push. think about why!
    try to re-apply your changes “on top of” the changes made remotely
        this is a rebase!
            rewind all local commits since shared ancestor
            fast-forward HEAD to commit at remote name
            apply local commits in-order
                may have conflicts you have to manually resolve
                git rebase --continue or --abort
            lots more here
        git pull --rebase will start this process for you
        whether you should merge or rebase is a hot topic! some good reads:
            this
            this
            this

Further reading

XKCD on git

    Learn git branching
    How to explain git in simple words
    Git from the bottom up
    Git for computer scientists
    Oh shit, git!
    The Pro Git book

Exercises

    On a repo try modifying an existing file. What happens when you do git stash? What do you see when running git log --all --oneline? Run git stash pop to undo what you did with git stash. In what scenario might this be useful?

    One common mistake when learning git is to commit large files that should not be managed by git or adding sensitive information. Try adding a file to a repository, making some commits and then deleting that file from history (you may want to look at this). Also if you do want git to manage large files for you, look into Git-LFS
    Git is really convenient for undoing changes but one has to be familiar even with the most unlikely changes
        If a file is mistakenly modified in some commit it can be reverted with git revert. However if a commit involves several changes revert might not be the best option. How can we use git checkout to recover a file version from a specific commit?
        Create a branch, make a commit in said branch and then delete it. Can you still recover said commit? Try looking into git reflog. (Note: Recover dangling things quickly, git will periodically automatically clean up commits that nothing points to.)
        If one is too trigger happy with git reset --hard instead of git reset changes can be easily lost. However since the changes were staged, we can recover them. (look into git fsck --lost-found and .git/lost-found)

    In any git repo look under the folder .git/hooks you will find a bunch of scripts that end with .sample. If you rename them without the .sample they will run based on their name. For instance pre-commit will execute before doing a commit. Experiment with them

    Like many command line tools git provides a configuration file (or dotfile) called ~/.gitconfig . Create and alias using ~/.gitconfig so that when you run git graph you get the output of git log --oneline --decorate --all --graph (this is a good command to quickly visualize the commit graph)

    Git also lets you define global ignore patterns under ~/.gitignore_global, this is useful to prevent common errors like adding RSA keys. Create a ~/.gitignore_global file and add the pattern *rsa, then test that it works in a repo.

    Once you start to get more familiar with git, you will find yourself running into common tasks, such as editing your .gitignore. git extras provides a bunch of little utilities that integrate with git. For example git ignore PATTERN will add the specified pattern to the .gitignore file in your repo and git ignore-io LANGUAGE will fetch the common ignore patterns for that language from gitignore.io. Install git extras and try using some tools like git alias or git ignore.

    Git GUI programs can be a great resource sometimes. Try running gitk in a git repo an explore the different parts of the interface. Then run gitk --all what are the differences?
    Once you get used to command line applications GUI tools can feel cumbersome/bloated. A nice compromise between the two are ncurses based tools which can be navigated from the command line and still provide an interactive interface. Git has tig, try installing it and running it in a repo. You can find some usage examples here.




Dotfiles

Many programs are configured using plain-text files known as “dotfiles” (because the file names begin with a ., e.g. ~/.gitconfig, so that they are hidden in the directory listing ls by default).

A lot of the tools you use probably have a lot of settings that can be tuned pretty finely. Often times, tools are customized with specialized languages, e.g. Vimscript for Vim or the shell’s own language for a shell.

Customizing and adapting your tools to your preferred workflow will make you more productive. We advise you to invest time in customizing your tool yourself rather than cloning someone else’s dotfiles from GitHub.

You probably have some dotfiles set up already. Some places to look:

    ~/.bashrc
    ~/.emacs
    ~/.vim
    ~/.gitconfig

Some programs don’t put the files under your home folder directly and instead they put them in a folder under ~/.config.

Dotfiles are not exclusive to command line applications, for instance the MPV video player can be configured editing files under ~/.config/mpv
Learning to customize tools

You can learn about your tool’s settings by reading online documentation or man pages. Another great way is to search the internet for blog posts about specific programs, where authors will tell you about their preferred customizations. Yet another way to learn about customizations is to look through other people’s dotfiles: you can find tons of dotfiles repositories on GitHub — see the most popular one here (we advise you not to blindly copy configurations though).
Organization

How should you organize your dotfiles? They should be in their own folder, under version control, and symlinked into place using a script. This has the benefits of:

    Easy installation: if you log in to a new machine, applying your customizations will only take a minute
    Portability: your tools will work the same way everywhere
    Synchronization: you can update your dotfiles anywhere and keep them all in sync
    Change tracking: you’re probably going to be maintaining your dotfiles for your entire programming career, and version history is nice to have for long-lived projects

cd ~/src
mkdir dotfiles
cd dotfiles
git init
touch bashrc
# create a bashrc with some settings, e.g.:
#     PS1='\w > '
touch install
chmod +x install
# insert the following into the install script:
#     #!/usr/bin/env bash
#     BASEDIR=$(dirname $0)
#     cd $BASEDIR
#
#     ln -s ${PWD}/bashrc ~/.bashrc
git add bashrc install
git commit -m 'Initial commit'

Advanced topics
Machine-specific customizations

Most of the time, you’ll want the same configuration across machines, but sometimes, you’ll want a small delta on a particular machine. Here are a couple ways you can handle this situation:
Branch per machine

Use version control to maintain a branch per machine. This approach is logically straightforward but can be pretty heavyweight.
If statements

If the configuration file supports it, use the equivalent of if-statements to apply machine specific customizations. For example, your shell could have something like:

if [[ "$(uname)" == "Linux" ]]; then {do_something else}; fi

# Darwin is the architecture name for macOS systems
if [[ "$(uname)" == "Darwin" ]]; then {do_something}; fi

# You can also make it machine specific
if [[ "$(hostname)" == "myServer" ]]; then {do_something}; fi

Includes

If the configuration file supports it, make use of includes. For example, a ~/.gitconfig can have a setting:

[include]
    path = ~/.gitconfig_local

And then on each machine, ~/.gitconfig_local can contain machine-specific settings. You could even track these in a separate repository for machine-specific settings.

This idea is also useful if you want different programs to share some configurations. For instance if you want both bash and zsh to share the same set of aliases you can write them under .aliases and have the following block in both.

# Test if ~/.aliases exists and source it
if [ -f ~/.aliases ]; then
    source ~/.aliases
fi

Resources

    Your instructors’ dotfiles: Anish, Jon, Jose
    GitHub does dotfiles: dotfile frameworks, utilities, examples, and tutorials
    Shell startup scripts: an explanation of the different configuration files used for your shell

Exercises

    Create a folder for your dotfiles and set up version control.

    Add a configuration for at least one program, e.g. your shell, with some customization (to start off, it can be something as simple as customizing your shell prompt by setting $PS1).

    Set up a method to install your dotfiles quickly (and without manual effort) on a new machine. This can be as simple as a shell script that calls ln -s for each file, or you could use a specialized utility.

    Test your installation script on a fresh virtual machine.

    Migrate all of your current tool configurations to your dotfiles repository.

    Publish your dotfiles on GitHub.




Backups

There are two types of people:

    Those who do backups
    Those who will do backups

Any data you own that you haven’t backed up is data that could be gone at any moment, forever. Here we will cover some good backup basics and the pitfalls of some approaches.
3-2-1 Rule

The 3-2-1 rule is a general recommended strategy for backing up your data. It state that you should have:

    at least 3 copies of your data
    2 copies in different mediums
    1 of the copies being offsite

The main idea behind this recommendation is not to put all your eggs in one basket. Having 2 different devices/disks ensures that a single hardware failure doesn’t take away all your data. Similarly, if you store your only backup at home and the house burns down or gets robbed you lose everything, that’s what the offsite copy is there for. Onsite backups give you availability and speed, offsite give you the resiliency should a disaster happen.
Testing your backups

A common pitfall when performing backups is blindly trusting whatever the system says it’s doing and not verifying that the data can be properly recovered. Toy Story 2 was almost lost and their backups were not working, luck ended up saving them.
Versioning

You should understand that RAID is not a backup, and in general mirroring is not a backup solution. Simply syncing your files somewhere will not help in several scenarios, such as:

    Data corruption
    Malicious software
    Deleting files by mistake

If the changes on your data propagate to the backup then you won’t be able to recover in these scenarios. Note that this is the case for a lot of cloud storage solutions like Dropbox, Google Drive, One Drive, &c. Some of them do keep deleted data around for short amounts of time but usually the interface to recover is not something you want to be using to recover large amounts of files.

A proper backup system should be versioned in order to prevent this failure mode. By providing different snapshots in time one can easily navigate them to restore whatever was lost. The most widely known software of this kind is macOS Time Machine.
Deduplication

However, making several copies of your data might be extremely costly in terms of disk space. Nevertheless, from one version to the next, most data will be identical and needs not be transferred again. This is where data deduplication comes into play, by keeping track of what has already been stored one can do incremental backups where only the changes from one version to the next need to be stored. This significantly reduces the amount of space needed for backups beyond the first copy.
Encryption

Since we might be backing up to untrusted third parties like cloud providers it is worth considering that if you backup your data is copied as is then it could potentially be looked by unwanted agents. Documents like your taxes are sensitive information that should not be backed up in plain format. To prevent this, many backup solutions offer client side encryption where data is encrypted before being sent to the server. That way the server cannot read the data it is storing but you can decrypt it with your secret key.

As a side note, if your disk (or home partition) is not encrypted, then anyone that get hold of your computer can manage to override the user access controls and read your data. Modern hardware supports fast and efficient read and writes of encrypted data so you might want to consider enabling full disk encryption.
Append only

The properties reviewed so far focus on hardware failure or user mistakes but fail to address what happens if a malicious agent wanted to delete your data. Namely, say someone hacks into your system, are they able to wipe all your copies of the data you care about? If you worry about that scenario then you need some sort of append only backup solution. In general, this means having a server that will allow you to send new data but will refuse to delete existing data. Usually users have two keys, an append only key that supports creating new backups and a full access key that also allows for deleting old backups that are no longer needed. The latter one is stored offline.

Note that this is a quite challenging scenario since you need the ability to make changes whilst still preventing a malicious user from deleting your data. Existing commercial solutions include Tarsnap and Borgbase.
Additional considerations

Some other things you may want to look into are:

    Periodic backups: outdated backups can become pretty useless. Making backups regularly should be a consideration for your system
    Bootable backups: some programs allow you to clone your entire disk. That way you have an image that contains an entire copy of your system you can boot directly from.
    Differential backup strategies, you may not necessarily care the same about all your data. You can define different backup policies for different types of data.
    Append only backups an additional consideration is to enforce append only operations to your backup repositories in order to prevent malicious agents to delete them if they get hold of your machine.

Webservices

Not all the data that you use lives on your hard disk. If you use webservices, then it might be the case that some data you care about, such as Google Docs presentations or Spotify playlists, is stored online. Another easy example that is easy to forget is email accounts with web access, such as Gmail. Figuring out a backup solution in these cases is somewhat trickier. However, there are many services that allow you to download your data, either directly or via an API. Tools such as gmvault for Gmail are available to download the email files to your computer.
Webpages

Similarly, some high quality content can be found online in the form of webpages. If said content is static one can easily back it up by just saving the website and all of its attachments. Another alternative is the Wayback Machine, a massive digital archive of the World Wide Web managed by the Internet Archive, a non profit organization focused on the preservation of all sorts of media. The Wayback Machine allows you to capture and archive webpages being able to later retrieve all the snapshots that have been archived for that website. If you find it useful, consider donating to the project.
Resources

Some good backup programs and services we have used and can honestly recommend:

    Tarsnap - deduplicated, encrypted online backup service for the truly paranoid.
    Borg Backup - deduplicated backup program that supports compression and authenticated encryption. If you need a cloud provider rsync.net has special offerings for Borg users.
    rsync is a utility that provides fast incremental file transfer. It is not a full backup solution.
    rclone like rsync but for cloud storage providers such as Amazon S3, Dropbox, Google Drive, rsync.net, &c. Supports client side encryption of remote folders.

Exercises

    Consider how you are (not) backing up your data and look into fixing/improving that.

    Figure out how to backup your email accounts

    Choose a webservice you use often (Spotify, Google Music, etc.) and figure out what options for backing up your data are. Often people have already made tools (such as youtube-dl) solutions based on available APIs.

    Think of a website you have visited repeatedly over the years and look it up in archive.org, how many versions does it have?

    One way to efficiently implement deduplication is to use hardlinks. Whereas symbolic link (also called a soft link or a symlink) is a file that points to another file or folder, a hardlink is a exact copy of the pointer (it uses the same inode and points to the same place in the disk). Thus if the original file is removed a symlink stops working whereas a hard link doesn’t. However, hardlinks only work for files. Try using the command ln to create hard links and compare them to symlinks created with ln -s. (In macOS you will need to install the gnu coreutils or the hln package).



Automation

Sometimes you write a script that does something but you want for it to run periodically, say a backup task. You can always write an ad hoc solution that runs in the background and comes online periodically. However, most UNIX systems come with the cron daemon which can run task with a frequency up to a minute based on simple rules.

On most UNIX systems the cron daemon, crond will be running by default but you can always check using ps aux | grep crond.
The crontab

The configuration file for cron can be displayed running crontab -l edited running crontab -e The time format that cron uses are five space separated fields along with the user and command

    minute - What minute of the hour the command will run on, and is between ‘0’ and ‘59’
    hour - This controls what hour the command will run on, and is specified in the 24 hour clock, values must be between 0 and 23 (0 is midnight)
    dom - This is the Day of Month, that you want the command run on, e.g. to run a command on the 19th of each month, the dom would be 19.
    month - This is the month a specified command will run on, it may be specified numerically (0-12), or as the name of the month (e.g. May)
    dow - This is the Day of Week that you want a command to be run on, it can also be numeric (0-7) or as the name of the day (e.g. sun).
    user - This is the user who runs the command.
    command - This is the command that you want run. This field may contain multiple words or spaces.

Note that using an asterisk * means all and using an asterisk followed by a slash and number means every nth value. So */5 means every five. Some examples are

*/5   *    *   *   *       # Every five minutes
  0   *    *   *   *       # Every hour at o'clock
  0   9    *   *   *       # Every day at 9:00 am
  0   9-17 *   *   *       # Every hour between 9:00am and 5:00pm
  0   0    *   *   5       # Every Friday at 12:00 am
  0   0    1   */2 *       # Every other month, the first day, 12:00am

You can find many more examples of common crontab schedules in crontab.guru
Shell environment and logging

A common pitfall when using cron is that it does not load the same environment scripts that common shells do such as .bashrc, .zshrc, &c and it does not log the output anywhere by default. Combined with the maximum frequency being one minute, it can become quite painful to debug cronscripts initially.

To deal with the environment, make sure that you use absolute paths in all your scripts and modify your environment variables such as PATH so the script can run successfully. To simplify logging, a good recommendation is to write your crontab in a format like this

* * * * *   user  /path/to/cronscripts/every_minute.sh >> /tmp/cron_every_minute.log 2>&1

And write the script in a separate file. Remember that >> appends to the file and that 2>&1 redirects stderr to stdout (you might to want keep them separate though).
Anacron

One caveat of using cron is that if the computer is powered off or asleep when the cron script should run then it is not executed. For frequent tasks this might be fine, but if a task runs less often, you may want to ensure that it is executed. anacron works similar to cron except that the frequency is specified in days. Unlike cron, it does not assume that the machine is running continuously. Hence, it can be used on machines that aren’t running 24 hours a day, to control regular jobs as daily, weekly, and monthly jobs.
Exercises

    Make a script that looks every minute in your downloads folder for any file that is a picture (you can look into MIME types or use a regular expression to match common extensions) and moves them into your Pictures folder.

    Write a cron script to weekly check for outdated packages in your system and prompts you to update them or updates them automatically.




Machine Introspection

Sometimes, computers misbehave. And very often, you want to know why. Let’s look at some tools that help you do that!

But first, let’s make sure you’re able to do introspection. Often, system introspection requires that you have certain privileges, like being the member of a group (like power for shutdown). The root user is the ultimate privilege; they can do pretty much anything. You can run a command as root (but be careful!) using sudo.
What happened?

If something goes wrong, the first place to start is to look at what happened around the time when things went wrong. For this, we need to look at logs.

Traditionally, logs were all stored in /var/log, and many still are. Usually there’s a file or folder per program. Use grep or less to find your way through them.

There’s also a kernel log that you can see using the dmesg command. This used to be available as a plain-text file, but nowadays you often have to go through dmesg to get at it.

Finally, there is the “system log”, which is increasingly where all of your log messages go. On most, though not all, Linux systems, that log is managed by systemd, the “system daemon”, which controls all the services that run in the background (and much much more at this point). That log is accessible through the somewhat inconvenient journalctl tool if you are root, or part of the admin or wheel groups.

For journalctl, you should be aware of these flags in particular:

    -u UNIT: show only messages related to the given systemd service
    --full: don’t truncate long lines (the stupidest feature)
    -b: only show messages from the latest boot (see also -b -2)
    -n100: only show last 100 entries

What is happening?

If something is wrong, or you just want to get a feel for what’s going on in your system, you have a number of tools at your disposal for inspecting the currently running system:

First, there’s top, and the improved version htop, which show you various statistics for the currently running processes on the system. CPU use, memory use, process trees, etc. There are lots of shortcuts, but t is particularly useful for enabling the tree view. You can also see the process tree with pstree (+ -p to include PIDs). If you want to know what those programs are doing, you’ll often want to tail their log files. journalctl -f, dmesg -w, and tail -f are you friends here.

Sometimes, you want to know more about the resources being used overall on your system. dstat is excellent for that. It gives you real-time resource metrics for lots of different subsystems like I/O, networking, CPU utilization, context switches, and the like. man dstat is the place to start.

If you’re running out of disk space, there are two primary utilities you’ll want to know about: df and du. The former shows you the status of all the partitions on your system (try it with -h), whereas the latter measures the size of all the folders you give it, including their contents (see also -h and -s).

To figure out what network connections you have open, ss is the way to go. ss -t will show all open TCP connections. ss -tl will show all listening (i.e., server) ports on your system. -p will also include which process is using that connection, and -n will give you the raw port numbers.
System configuration

There are many ways to configure your system, but we’ll got through two very common ones: networking and services. Most applications on your system tell you how to configure them in their manpage, and usually it will involve editing files in /etc; the system configuration directory.

If you want to configure your network, the ip command lets you do that. Its arguments take on a slightly weird form, but ip help command will get you pretty far. ip addr shows you information about your network interfaces and how they’re configured (IP addresses and such), and ip route shows you how network traffic is routed to different network hosts. Network problems can often be resolved purely through the ip tool. There’s also iw for managing wireless network interfaces. ping is a handy tool for checking how deeply things are broken. Try pinging a hostname (google.com), an external IP address (1.1.1.1), and an internal IP address (192.168.1.1 or default gw). You may also want to fiddle with /etc/resolv.conf to check your DNS settings (how hostnames are resolved to IP addresses).

To configure services, you pretty much have to interact with systemd these days, for better or for worse. Most services on your system will have a systemd service file that defines a systemd unit. These files define what command to run when that services is started, how to stop it, where to log things, etc. They’re usually not too bad to read, and you can find most of them in /usr/lib/systemd/system/. You can also define your own in /etc/systemd/system .

Once you have a systemd service in mind, you use the systemctl command to interact with it. systemctl enable UNIT will set the service to start on boot (disable removes it again), and start, stop, and restart will do what you expect. If something goes wrong, systemd will let you know, and you can use journalctl -u UNIT to see the application’s log. You can also use systemctl status to see how all your system services are doing. If your boot feels slow, it’s probably due to a couple of slow services, and you can use systemd-analyze (try it with blame) to figure out which ones.
Exercises

locate? dmidecode? tcpdump? /boot? iptables? /proc?



Program Introspection
Debugging

When printf-debugging isn’t good enough: use a debugger.

Debuggers let you interact with the execution of a program, letting you do things like:

    halt execution of the program when it reaches a certain line
    single-step through the program
    inspect values of variables
    many more advanced features

GDB/LLDB

GDB and LLDB. Supports many C-like languages.

Let’s look at example.c. Compile with debug flags: gcc -g -o example example.c.

Open GDB:

gdb example

Some commands:

    run
    b {name of function} - set a breakpoint
    b {file}:{line} - set a breakpoint
    c - continue
    step / next / finish - step in / step over / step out
    p {variable} - print value of variable
    watch {expression} - set a watchpoint that triggers when the value of the expression changes
    rwatch {expression} - set a watchpoint that triggers when the value is read
    layout

PDB

PDB is the Python debugger.

Insert import pdb; pdb.set_trace() where you want to drop into PDB, basically a hybrid of a debugger (like GDB) and a Python shell.
Web browser Developer Tools

Another example of a debugger, this time with a graphical interface.
strace

Observe system calls a program makes: strace {program}.
Profiling

Types of profiling: CPU, memory, etc.

Simplest profiler: time.
Go

Run test code with CPU profiler: go test -cpuprofile=cpu.out

Analyze profile: go tool pprof -web cpu.out

Run test code with CPU profiler: go test -memprofile=cpu.out

Analyze profile: go tool pprof -web mem.out
Perf

Basic performance stats: perf stat {command}

Run a program with the profiler: perf record {command}

Analyze profile: perf report



Package Management and Dependency Management

Software usually builds on (a collection of) other software, which necessitates dependency management.

Package/dependency management programs are language-specific, but many share common ideas.
Package repositories

Packages are hosted in package repositories. There are different repositories for different languages (and sometimes multiple for a particular language), such as PyPI for Python, RubyGems for Ruby, and crates.io for Rust. They generally store software (source code and sometimes pre-compiled binaries for specific platforms) for all versions of a package.
Semantic versioning

Software evolves over time, and we need a way to refer to software versions. Some simple ways could be to refer to software by a sequence number or a commit hash, but we can do better in terms of communicating more information: using version numbers.

There are many approaches; one popular one is Semantic Versioning:

x.y.z
^ ^ ^
| | +- patch
| +--- minor
+----- major

Increment major version when you make incompatible API changes.

Increment minor version when you add functionality in a backward-compatible manner.

Increment patch when you make backward-compatible bug fixes.

For example, if you depend on a feature introduced in v1.2.0 of some software, then you can install v1.x.y for any minor version x >= 2 and any patch version y. You need to install major version 1 (because 2 can introduce backward-incompatible changes), and you need to install a minor version >= 2 (because you depend on a feature introduced in that minor version). You can use any newer minor version or patch version because they should not introduce any backward-incompatible changes.
Lock files

In addition to specifying versions, it can be nice to enforce that the contents of the dependency have not changed to prevent tampering. Some tools use lock files to specify cryptographic hashes of dependencies (along with versions) that are checked on package install.
Specifying versions

Tools often let you specify versions in multiple ways, such as:

    exact version, e.g. 2.3.12
    minimum major version, e.g. >= 2
    specific major version and minimum patch version, e.g. >= 2.3, <3.0

Specifying an exact version can be advantageous to avoid different behaviors based on installed dependencies (this shouldn’t happen if all dependencies faithfully follow semver, but sometimes people make mistakes). Specifying a minimum requirement has the advantage of allowing bug fixes to be installed (e.g. patch upgrades).
Dependency resolution

Package managers use various dependency resolution algorithms to satisfy dependency requirements. This often gets challenging with complex dependencies (e.g. a package can be indirectly depended on by multiple top-level dependencies, and different versions could be required). Different package managers have different levels of sophistication in their dependency resolution, but it’s something to be aware of: you may need to understand this if you are debugging dependencies.
Virtual environments

If you’re developing multiple software projects, they may depend on different versions of a particular piece of software. Sometimes, your build tool will handle this naturally (e.g. by building a static binary).

For other build tools and programming languages, one approach is handling this with virtual environments (e.g. with the virtualenv tool for Python). Instead of installing dependencies system-wide, you can install dependencies per-project in a virtual environment, and activate the virtual environment that you want to use when you’re working on a specific project.
Vendoring

Another very different approach to dependency management is vendoring. Instead of using a dependency manager or build tool to fetch software, you copy the entire source code for a dependency into your software’s repository. This has the advantage that you’re always building against the same version of the dependency and you don’t need to rely on a package repository, but it is more effort to upgrade dependencies.



OS Customization

There is a lot you can do to customize your operating system beyond what is available in the settings menus.
Keyboard remapping

Your keyboard probably has keys that you aren’t using very much. Instead of having useless keys, you can remap them to do useful things.
Remapping to other keys

The simplest thing is to remap keys to other keys. For example, if you don’t use the caps lock key very much, then you can remap it to something more useful. If you are a Vim user, for example, you might want to remap caps lock to escape.

On macOS, you can do some remappings through Keyboard settings in System Preferences; for more complicated mappings, you need special software.
Remapping to arbitrary commands

You don’t just have to remap keys to other keys: there are tools that will let you remap keys (or combinations of keys) to arbitrary commands. For example, you could make command-shift-t open a new terminal window.
Customizing hidden OS settings
macOS

macOS exposes a lot of useful settings through the defaults command. For example, you can make Dock icons of hidden applications translucent:

defaults write com.apple.dock showhidden -bool true

There is no single list of all possible settings, but you can find lists of specific customizations online, such as Mathias Bynens’ .macos.
Window management
Tiling window management

Tiling window management is one approach to window management, where you organize windows into non-overlapping frames. If you’re using a Unix-based operating system, you can install a tiling window manager; if you’re using something like Windows or macOS, you can install applications that let you approximate this behavior.
Screen management

You can set up keyboard shortcuts to help you manipulate windows across screens.
Layouts

If there are specific ways you lay out windows on a screen, rather than “executing” that layout manually, you can script it, making instantiating a layout trivial.
Resources

    Hammerspoon - macOS desktop automation
    Spectacle - macOS window manager
    Karabiner - sophisticated macOS keyboard remapping
    r/unixporn - screenshots and documentation of people’s fancy configurations

Exercises

    Figure out how to remap your Caps Lock key to something you use more often (such as Escape or Ctrl or Backspace).

    Make a custom global keyboard shortcut to open a new terminal window or a new browser window.




Remote Machines

It has become more and more common for programmers to use remote servers in their everyday work. If you need to use remote servers in order to deploy backend software or you need a server with higher computational capabilities, you will end up using a Secure Shell (SSH). As with most tools covered, SSH is highly configurable so it is worth learning about it.
Executing commands

An often overlooked feature of ssh is the ability to run commands directly.

    ssh foobar@server ls will execute ls in the home folder of foobar
    It works with pipes, so ssh foobar@server ls | grep PATTERN will grep locally the remote output of ls and ls | ssh foobar@server grep PATTERN will grep remotely the local output of ls.

SSH Keys

Key-based authentication exploits public-key cryptography to prove to the server that the client owns the secret private key without revealing the key. This way you do not need to reenter your password every time. Nevertheless the private key (e.g. ~/.ssh/id_rsa) is effectively your password so treat it like so.

    Key generation. To generate a pair you can simply run ssh-keygen -t rsa -b 4096. If you do not choose a passphrase anyone that gets hold of your private key will be able to access authorized servers so it is recommended to choose one and use ssh-agent to manage shell sessions.

If you have configured pushing to Github using SSH keys you have probably done the steps outlined here and have a valid pair already. To check if you have a passphrase and validate it you can run ssh-keygen -y -f /path/to/key.

    Key based authentication. ssh will look into .ssh/authorized_keys to determine which clients it should let in. To copy a public key over we can use the

cat .ssh/id_dsa.pub | ssh foobar@remote 'cat >> ~/.ssh/authorized_keys'

A simpler solution can be achieved with ssh-copy-id where available.

ssh-copy-id -i .ssh/id_dsa.pub foobar@remote

Copying files over ssh

There are many ways to copy files over ssh

    ssh+tee, the simplest is to use ssh command execution and stdin input by doing cat localfile | ssh remote_server tee serverfile
    scp when copying large amounts of files/directories, the secure copy scp command is more convenient since it can easily recurse over paths. The syntax is scp path/to/local_file remote_host:path/to/remote_file
    rsync improves upon scp by detecting identical files in local and remote and preventing copying them again. It also provides more fine grained control over symlinks, permissions and has extra features like the --partial flag that can resume from a previously interrupted copy. rsync has a similar syntax to scp.

Backgrounding processes

By default when interrupting a ssh connection, child processes of the parent shell are killed along with it. There are a couple of alternatives

    nohup - the nohup tool effectively allows for a process to live when the terminal gets killed. Although this can sometimes be achieved with & and disown, nohup is a better default. More details can be found here.

    tmux, screen - whereas nohup effectively backgrounds the process it is not convenient for interactive shell sessions. In that case using a terminal multiplexer like screen or tmux is a convenient choice since one can easily detach and reattach the associated shells.

Lastly, if you disown a program and want to reattach it to the current terminal, you can look into reptyr. reptyr PID will grab the process with id PID and attach it to your current terminal.
Port Forwarding

In many scenarios you will run into software that works by listening to ports in the machine. When this happens in your local machine you can simply do localhost:PORT or 127.0.0.1:PORT, but what do you do with a remote server that does not have its ports directly available through the network/internet?. This is called port forwarding and it comes in two flavors: Local Port Forwarding and Remote Port Forwarding (see the pictures for more details, credit of the pictures from this SO post).

Local Port Forwarding Local Port Forwarding

Remote Port Forwarding Remote Port Forwarding

The most common scenario is local port forwarding where a service in the remote machine listens in a port and you want to link a port in your local machine to forward to the remote port. For example if we execute jupyter notebook in the remote server that listens to the port 8888. Thus to forward that to the local port 9999 we would do ssh -L 9999:localhost:8888 foobar@remote_server and then navigate to locahost:9999 in our local machine.
Graphics Forwarding

Sometimes forwarding ports is not enough since we want to run a GUI based program in the server. You can always resort to Remote Desktop Software that sends the entire Desktop Environment (ie. options like RealVNC, Teamviewer, &c). However for a single GUI tool, SSH provides a good alternative: Graphics Forwarding.

Using the -X flag tells SSH to forward

For trusted X11 forwarding the -Y flag can be used.

Final note is that for this to work the sshd_config on the server must have the following options

X11Forwarding yes
X11DisplayOffset 10

Roaming

A common pain when connecting to a remote server are disconnections due to shutting down/sleeping your computer or changing a network. Moreover if one has a connection with significant lag using ssh can become quite frustrating. Mosh, the mobile shell, improves upon ssh, allowing roaming connections, intermittent connectivity and providing intelligent local echo.

Mosh is present in all common distributions and package managers. Mosh requires an ssh server to be working in the server. You do not need to be superuser to install mosh but it does require that ports 60000 through 60010 to be open in the server (they usually are since they are not in the privileged range).

A downside of mosh is that is does not support roaming port/graphics forwarding so if you use those often mosh won’t be of much help.
SSH Configuration
Client

We have covered many many arguments that we can pass. A tempting alternative is to create shell aliases that look like alias my_serer="ssh -X -i ~/.id_rsa -L 9999:localhost:8888 foobar@remote_server, however there is a better alternative, using ~/.ssh/config.

Host vm
    User foobar
    HostName 172.16.174.141
    Port 22
    IdentityFile ~/.ssh/id_rsa
    RemoteForward 9999 localhost:8888

# Configs can also take wildcards
Host *.mit.edu
    User foobaz

An additional advantage of using the ~/.ssh/config file over aliases is that other programs like scp, rsync, mosh, &c are able to read it as well and convert the settings into the corresponding flags.

Note that the ~/.ssh/config file can be considered a dotfile, and in general it is fine for it to be included with the rest of your dotfiles. However if you make it public, think about the information that you are potentially providing strangers on the internet: the addresses of your servers, the users you are using, the open ports, &c. This may facilitate some types of attacks so be thoughtful about sharing your SSH configuration.

Warning: Never include your RSA keys ( ~/.ssh/id_rsa* ) in a public repository!
Server side

Server side configuration is usually specified in /etc/ssh/sshd_config. Here you can make changes like disabling password authentication, changing ssh ports, enabling X11 forwarding, &c. You can specify config settings in a per user basis.
Remote Filesystem

Sometimes it is convenient to mount a remote folder. sshfs can mount a folder on a remote server locally, and then you can use a local editor.
Exercises

    For SSH to work the host needs to be running an SSH server. Install an SSH server (such as OpenSSH) in a virtual machine so you the rest of the exercises. To figure out what is the ip of the machine run the command ip addr and look for the inet field (ignore the 127.0.0.1 entry, that corresponds to the loopback interface).

    Go to ~/.ssh/ and check if you have a pair of SSH keys there. If not, generate them with ssh-keygen -t rsa -b 4096. It is recommended that you use a password and use ssh-agent , more info here.

    Use ssh-copy-id to copy the key to your virtual machine. Test that you can ssh without a password. Then, edit your sshd_config in the server to disable password authentication by editing the value of PasswordAuthentication. Disable root login by editing the value of PermitRootLogin.

    Edit the sshd_config in the server to change the ssh port and check that you can still ssh. If you ever have a public facing server, a non default port and key only login will throttle a significant amount of malicious attacks.

    Install mosh in your server/VM, establish a connection and then disconnect the network adapter of the server/VM. Can mosh properly recover from it?

    Another use of local port forwarding is to tunnel certain host to the server. If your network filters some website like for example reddit.com you can tunnel it through the server as follows:
        Run ssh remote_server -L 80:reddit.com:80
        Set reddit.com and www.reddit.com to 127.0.0.1 in /etc/hosts
        Check that you are accessing that website through the server
        If it is not obvious use a website such as ipinfo.io which will change depending on your host public ip.

    Background port forwarding can easily be achieved with a couple of extra flags. Look into what the -N and -f flags do in ssh and figure out what a command such as this ssh -N -f -L 9999:localhost:8888 foobar@remote_server does.

References

    SSH Hacks
    Secure Secure Shell




Web and Browsers

Apart from the terminal, the web browser is a tool you will find yourself spending significant amounts of time into. Thus it is worth learning how to use it efficiently and
Shortcuts

Clicking around in your browser is often not the fastest option, getting familiar with common shortcuts can really pay off in the long run.

    Middle Button Click in a link opens it in a new tab
    Ctrl+T Opens a new tab
    Ctrl+Shift+T Reopens a recently closed tab
    Ctrl+L selects the contents of the search bar
    Ctrl+F to search within a webpage. If you do this often, you may benefit from an extension that supports regular expressions in searches.

Search operators

Web search engines like Google or DuckDuckGo provide search operators to enable more elaborate web searches:

    "bar foo" enforces an exact match of bar foo
    foo site:bar.com searches for foo within bar.com
    foo -bar excludes the terms containing bar from the search
    foobar filetype:pdf Searches for files of that extension
    (foo|bar) searches for matches that have foo OR bar

More through lists are available for popular engines like Google and DuckDuckGo
Searchbar

The searchbar is a powerful tool too. Most browsers can infer search engines from websites and will store them. By editing the keyword argument

    In Google Chrome they are in chrome://settings/searchEngines
    In Firefox they are in about:preferences#search

For example you can make so that y SOME SEARCH TERMS to directly search in youtube.

Moreover, if you own a domain you can setup subdomain forwards using your registrar. For instance I have mapped https://ht.josejg.com to this course website. That way I can just type ht. and the searchbar will autocomplete. Another good feature of this setup is that unlike bookmarks they will work in every browser.
Privacy extensions

Nowadays surfing the web can get quite annoying due to ads and invasive due to trackers. Moreover a good adblocker not only blocks most ad content but it will also block sketchy and malicious websites since they will be included in the common blacklists. They will also reduce page load times sometimes by reducing the amount of requests performed. A couple of recommendations are:

    uBlock origin (Chrome, Firefox): block ads and trackers based on predefined rules. You should also consider taking a look at the enabled blacklists in settings since you can enable more based on your region or browsing habits. You can even install filters from around the web

    Privacy Badger: detects and blocks trackers automatically. For example when you go from website to website ad companies track which sites you visit and build a profile of you

    HTTPS everywhere is a wonderful extension that redirects to HTTPS version of a website automatically, if available.

You can find about more addons of this kind here
Style customization

Web browsers are just another piece of software running in your machine and thus you usually have the last say about what they should display or how they should behave. An example of this are custom styles. Browsers determine how to render the style of a webpage using Cascading Style Sheets often abbreviated as CSS.

You can access the source code of a website by inspecting it and changing its contents and styles temporarily (this is also a reason why you should never trust webpage screenshots).

If you want to permanently tell your browser to override the style settings for a webpage you will need to use an extension. Our recommendation is Stylus (Firefox, Chrome).

For example, we can write the following style for the class website


body {
    background-color: #2d2d2d;
    color: #eee;
    font-family: Fira Code;
    font-size: 16pt;
}

a:link {
    text-decoration: none;
    color: #0a0;
}

Moreover, Stylus can find styles written by other users and published in userstyles.org. Most common websites have one or several dark theme stylesheets for instance. FYI, you should not use Stylish since it was shown to leak user data, more here
Functionality Customization

In the same way that you can modify the style, you can also modify the behaviour of a website by writing custom javascript and them sourcing it using a web browser extension such as Tampermonkey

For example the following script enables vim-like navigation using the J and K keys.

// ==UserScript==
// @name         VIM HT
// @namespace    http://tampermonkey.net/
// @version      0.1
// @description  Vim JK for our website
// @author       You
// @match        https://hacker-tools.github.io/*
// @grant        none
// ==/UserScript==


(function() {
    'use strict';

    window.onkeyup = function(e) {
        var key = e.keyCode ? e.keyCode : e.which;

        if (key == 74) { // J is key 74
            window.scrollBy(0,500);;
        }else if (key == 75) { // K is key 75
            window.scrollBy(0,-500);;
        }
    }
})();

There are also script repositories such as OpenUserJS and Greasy Fork. However, be warned, installing user scripts from others can be very dangerous since they can pretty much do anything such as steal your credit card numbers. Never install a script unless you read the whole thing yourself, understand what it does, and are absolutely sure that you know it isn’t doing anything suspicious. Never install a script that contains minified or obfuscated code that you can’t read!
Web APIs

It has become more and more common for webservices to offer an application interface aka web API so you can interact with the services making web requests. A more in depth introduction to the topic can be found here. There are many public APIs. Web APIs can be useful for very many reasons:

    Retrieval. Web APIs can quite easily provide you information such as maps, weather or what your public ip address. For instance curl ipinfo.io will return a JSON object with some details about your public ip, region, location, &c. With proper parsing these tools can be integrated even with command line tools. The following bash functions talks to Googles autocompletion API and returns the first ten matches.

function c() {
    url='https://www.google.com/complete/search?client=hp&hl=en&xhr=t'
    # NB: user-agent must be specified to get back UTF-8 data!
    curl -H 'user-agent: Mozilla/5.0' -sSG --data-urlencode "q=$*" "$url" |
        jq -r ".[1][][0]" |
        sed 's,</\?b>,,g'
}

    Interaction. Web API endpoints can also be used to trigger actions. These usually require some sort of authentication token that you can obtain through the service. For example performing the following curl -X POST -H 'Content-type: application/json' --data '{"text":"Hello, World!"}' "https://hooks.slack.com/services/$SLACK_TOKEN" will send a Hello, World! message in a channel.

    Piping. Since some services with web APIs are rather popular, common web API “gluing” has already been implemented and is provided with server included. This is the case for services like If This Then That and Zapier

Web Automation

Sometimes web APIs are not enough. If only reading is needed you can use a html parser like pup or use a library, for example python has BeautifulSoup. However if interactivity or javascript execution is required those solutions fall short. WebDriver

For example, the following script will save the specified url using the wayback machine simulating the interaction of typing the website.

from selenium.webdriver import Firefox
from selenium.webdriver.common.keys import Keys


def snapshot_wayback(driver, url):

    driver.get("https://web.archive.org/")
    elem = driver.find_element_by_class_name('web-save-url-input')
    elem.clear()
    elem.send_keys(url)
    elem.send_keys(Keys.RETURN)
    driver.close()


driver = Firefox()
url = 'https://hacker-tools.github.io'
snapshot_wayback(driver, url)

Exercises

    Edit a keyword search engine that you use often in your web browser
    Install the mentioned extensions. Look into how uBlock Origin/Privacy Badger can be disabled for a website. What differences do you see? Try doing it in a website with plenty of ads like YouTube.
    Install Stylus and write a custom style for the class website using the CSS provided. Here are some common programming characters = == === >= => ++ /= ~=. What happens to them when changing the font to Fira Code? If you want to know more search for programming font ligatures.
    Find a web api to get the weather in your city/area.
    Use a WebDriver software like Selenium to automate some repetitive manual task that you perform often with your browser.




Security and Privacy

The world is a scary place, and everyone’s out to get you.

Okay, maybe not, but that doesn’t mean you want to flaunt all your secrets. Security (and privacy) is generally all about raising the bar for attackers. Find out what your threat model is, and then design your security mechanisms around that! If the threat model is the NSA or Mossad, you’re probably going to have a bad time.

There are many ways to make your technical persona more secure. We’ll touch on a lot of high-level things here, but this is a process, and educating yourself is one of the best things you can do. So:
Follow the Right People

One of the best ways to improve your security know-how is to follow other people who are vocal about security. Some suggestions:

    @TroyHunt
    @SwiftOnSecurity
    @taviso
    @thegrugq
    @tqbf
    @mattblaze
    @moxie

See also this list for more suggestions.
General Security Advice

Tech Solidarity has a pretty great list of do’s and don’ts for journalists that has a lot of sane advice, and is decently up-to-date. @thegrugq also has a good blog post on travel security advice that’s worth reading. We’ll repeat much of the advice from those sources here, plus some more. Also, get a USB data blocker, because USB is scary.
Authentication

The very first thing you should do, if you haven’t already, is download a password manager. Some good ones are:

    1password
    KeePass
    BitWarden
    pass

If you’re particularly paranoid, use one that encrypts the passwords locally on your computer, as opposed to storing them in plain-text at the server. Use it to generate passwords for all the web sites you care about right now. Then, switch on two-factor authentication, ideally with a FIDO/U2F dongle (a YubiKey for example, which has 20% off for students). TOTP (like Google Authenticator or Duo) will also work in a pinch, but doesn’t protect against phishing. SMS is pretty much useless unless your threat model only includes random strangers picking up your password in transit.

Also, a note about paper keys. Often, services will give you a “backup key” that you can use as a second factor if you lose your real second factor (btw, always keep a backup dongle somewhere safe!). While you can stick those in your password managers, that means that should someone get access to your password manager, you’re totally hosed (but maybe you’re okay with that thread model). If you are truly paranoid, print out these paper keys, never store them digitally, and place them in a safe in the real world.
Private Communication

Use Signal (setup instructions. Wire is fine too; WhatsApp is okay; don’t use Telegram). Desktop messengers are pretty broken (partially due to usually relying on Electron, which is a huge trust stack).

E-mail is particularly problematic, even if PGP signed. It’s not generally forward-secure, and the key-distribution problem is pretty severe. keybase.io helps, and is useful for a number of other reasons. Also, PGP keys are generally handled on desktop computers, which is one of the least secure computing environments. Relatedly, consider getting a Chromebook, or just work on a tablet with a keyboard.
File Security

File security is hard, and operates on many level. What is it you’re trying to secure against?

$5 wrench

    Offline attacks (someone steals your laptop while it’s off): turn on full disk encryption. (cryptsetup + LUKS on Linux, BitLocker on Windows, FileVault on macOS. Note that this won’t help if the attacker also has you and really wants your secrets.
    Online attacks (someone has your laptop and it’s on): use file encryption. There are two primary mechanisms for doing so
        Encrypted filesystems: stacked filesystem encryption software encrypts files individually rather than having encrypted block devices. You can “mount” these filesystems by providing the decryption key, and then browse the files inside it freely. When you unmount it, those files are all unavailable. Modern solutions include gocryptfs and eCryptFS. More detailed comparisons can be found here and here
        Encrypted files: encrypt individual files with symmetric encryption (see gpg -c) and a secret key. Or, like pass, also encrypt the key with your public key so only you can read it back later with your private key. Exact encryption settings matter a lot!
    Plausible deniability (what seems to be the problem officer?): usually lower performance, and easier to lose data. Hard to actually prove that it provides deniable encryption! See the discussion here, and then consider whether you may want to try VeraCrypt (the maintained fork of good ol’ TrueCrypt).
    Encrypted backups: use Tarsnap or Borgbase
        Think about whether an attacker can delete your backups if they get a hold of your laptop!

Internet Security & Privacy

The internet is a very scary place. Open WiFi networks are scary. Make sure you delete them afterwards, otherwise your phone will happily announce and re-connect to something with the same name later!

If you’re ever on a network you don’t trust, a VPN may be worthwhile, but keep in mind that you’re trusting the VPN provider a lot. Do you really trust them more than your ISP? If you truly want a VPN, use a provider you’re sure you trust, and you should probably pay for it. Or set up WireGuard for yourself – it’s excellent!

There are also secure configuration settings for a lot of internet-enabled applications at cipherlist.eu. If you’re particularly privacy-oriented, privacytools.io is also a good resource.

Some of you may wonder about Tor. Keep in mind that Tor is not particularly resistant to powerful global attackers, and is weak against traffic analysis attacks. It may be useful for hiding traffic on a small scale, but won’t really buy you all that much in terms of privacy. You’re better off using more secure services in the first place (Signal, TLS + certificate pinning, etc.).
Web Security

So, you want to go on the Web too? Jeez, you’re really pushing your luck here.

Install HTTPS Everywhere. SSL/TLS is critical, and it’s not just about encryption, but also about being able to verify that you’re talking to the right service in the first place! If you run your own web server, test it. TLS configuration can get hairy. HTTPS Everywhere will do its very best to never navigate you to HTTP sites when there’s an alternative. That doesn’t save you, but it helps. If you’re truly paranoid, blacklist any SSL/TLS CAs that you don’t absolutely need.

Install uBlock Origin. It is a wide-spectrum blocker that doesn’t just stop ads, but all sorts of third-party communication a page may try to do. And inline scripts and such. If you’re willing to spend some time on configuration to make things work, go to medium mode or even hard mode. Those will make some sites not work until you’ve fiddled with the settings enough, but will also significantly improve your online security.

If you’re using Firefox, enable Multi-Account Containers. Create separate containers for social networks, banking, shopping, etc. Firefox will keep the cookies and other state for each of the containers totally separate, so sites you visit in one container can’t snoop on sensitive data from the others. In Google Chrome, you can use Chrome Profiles to achieve similar results.

Exercises

TODO

    Encrypt a file using PGP
    Use veracrypt to create a simple encrypted volume
    Enable 2FA for your most data sensitive accounts i.e. GMail, Dropbox, Github, &c

