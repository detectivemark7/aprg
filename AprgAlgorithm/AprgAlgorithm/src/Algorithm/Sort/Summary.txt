Sorting Summary:

Note: lg N means log2(N)

                   | Inplace | Stable | Worst          | Average     | Best    | Remarks
Selection          | Y       | N      | (N^2)/2        | (N^2)/2     | (N^2)/2 | N exchanges
Insertion          | Y       | Y      | (N^2)/2        | (N^2)/4     | N       | use for small N or partially ordered
Shell              | Y       | N      | ?              | ?           | N       | light code, subquadratic
Merge              | N       | Y      | N lg N         | N lg N      | N lg N  | N lg N guarantee, stable, uses N extra space
Quick              | Y       | N      | (N^2)/2        | 1.39 N lg N | N lg N  | N lg N probabilistic guarantee, fastest in practice
3-way quick        | Y       | N      | (N^2)/2        | 1.39 N lg N | N       | improves quicksort in presence of duplicate keys
Heapsort           | Y       | N      | 2*N lg N       | 2*N lg N    | N lg N  | N lg N guarantee, in place
LSD                | N       | Y      | 2*W*N          | 2*W*N       | 2*W*N   | 2*W*N guarantee, stable, uses N+R extra space, uses string.at
MSD                | N       | Y      | 2*W*N          | N logR(N)   | 2*W*N   | 2*W*N guarantee, stable, uses N+DR extra space (D is functional call stack depth, length of longest prefix match), uses string.at
3-way quick string | Y       | N      | 1.39 W*N lg N  | 1.39 N lg N | N       | uses log(N)+W extra space, uses string.at
???                | Y       | Y      | N lg N         | N lg N      | N lg N  | holy sorting grail

Note: W is the length of strings, R is the radix (letters in the alphabet)
Note: There are versions of merge sort that come close to the holy sorting grail but too complex for practitioners to use.



Lower bound for comparison based sorting algorithms

The problem of sorting can be viewed as following. 
Input: A sequence of n numbers <a1, a2, . . . , an>. 
Output: A permutation (reordering) <a‘1, a‘2, . . . , a‘n> of the input sequence such that a‘1 <= a‘2 ….. <= a’n. 

A sorting algorithm is comparison based if it uses comparison operators to find the order between two numbers.  
Comparison sorts can be viewed abstractly in terms of decision trees. 
A decision tree is a full binary tree that represents the comparisons between elements that are performed by a particular sorting algorithm operating on an input of a given size.
The execution of the sorting algorithm corresponds to tracing a path from the root of the decision tree to a leaf. 
At each internal node, a comparison ai <= aj is made. 
The left subtree then dictates subsequent comparisons for ai <= aj, and the right subtree dictates subsequent comparisons for ai > aj. 
When we come to a leaf, the sorting algorithm has established the ordering. 

So we can say following about the decision tree:
1) Each of the n! permutations on n elements must appear as one of the leaves of the decision tree for the sorting algorithm to sort properly. 
2) Let x be the maximum number of comparisons in a sorting algorithm. The maximum height of the decision tree would be x. A tree with maximum height x has at most 2^x leaves. 

After combining the above two facts, we get following relation. 
-> n!  <= 2^x

 Taking Log on both sides.
-> log2(n!)  <= x

 Since log2(n!)  = Θ(nLogn), we can say
-> x = Ω(nLog2n)

Therefore, any comparison based sorting algorithm must make at least nLog2n comparisons to sort the input array, and Heapsort and merge sort are asymptotically optimal comparison sorts. 



Why is Quick Sort preferred for arrays?
Below are recursive and iterative implementations of Quick Sort and Merge Sort for arrays:
-> Quick Sort in its general form is an in-place sort (i.e. it doesn’t require any extra storage) whereas merge sort requires O(N) extra storage, N denoting the array size which may be quite expensive. Allocating and de-allocating the extra space used for merge sort increases the running time of the algorithm.
-> Comparing average complexity we find that both type of sorts have O(NlogN) average complexity but the constants differ. For arrays, merge sort loses due to the use of extra O(N) storage space.
-> Most practical implementations of Quick Sort use randomized version. The randomized version has expected time complexity of O(nLogn). The worst case is possible in randomized version also, but worst case doesn’t occur for a particular pattern (like sorted array) and randomized Quick Sort works well in practice.
-> Quick Sort is also a cache friendly sorting algorithm as it has good locality of reference when used for arrays.
-> Quick Sort is also tail recursive, therefore tail call optimizations is done.


Why is Merge Sort preferred for Linked Lists?
Below are implementations of Quicksort and Mergesort for singly and doubly linked lists:
-> In case of linked lists the case is different mainly due to difference in memory allocation of arrays and linked lists. Unlike arrays, linked list nodes may not be adjacent in memory.
-> Unlike array, in linked list, we can insert items in the middle in O(1) extra space and O(1) time if we are given reference/pointer to the previous node. Therefore merge operation of merge sort can be implemented without extra space for linked lists.
-> In arrays, we can do random access as elements are continuous in memory. Let us say we have an integer (4-byte) array A and let the address of A[0] be x then to access A[i], we can directly access the memory at (x + i*4). Unlike arrays, we can not do random access in linked list.
-> Quick Sort requires a lot of this kind of access. In linked list to access i’th index, we have to travel each and every node from the head to i’th node as we don’t have continuous block of memory. Therefore, the overhead increases for quick sort. Merge sort accesses data sequentially and the need of random access is low.

